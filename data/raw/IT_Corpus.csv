	Uber Technologies, Inc. is an American multinational ridesharing company offering services that include peer-to-peer ridesharing, ride service hailing, food delivery, and a bicycle-sharing system. The company is based in San Francisco and has operations in over 785 metropolitan areas worldwide. Its platforms can be accessed via its websites and mobile apps.As of 2019, Uber is estimated to have 110 million worldwide users a 69% market share in the United States for ride-sharing, and a 25% market share for food delivery. Uber has been so prominent in the sharing economy that the changes in industries as a result of it have been referred to as uberisation, and many startups have described their products as "Uber for X".As with other transportation network companies, Uber has been criticized for unfair treatment of drivers, for disrupting the taxicab business, and for increasing traffic congestion. The company has also been criticized for its aggressive strategy in dealing with regulators and for other unlawful practices.== Business model ===== Stakeholders ======= Passengers ====Passengers use an app to order a  ride, where they are quoted the fare. Uber uses a dynamic pricing model; prices for the same route vary based on the supply and demand for rides at the time the ride is requested. At the end of the ride, payment is made based on the rider's pre-selected preferences, which could be a credit card on file, Google Pay, Apple Pay, PayPal, cash, or, in India, Airtel mobile wallet or Unified Payments Interface. After the ride is over, the rider is given the option to provide a gratuity to the driver, which is also billed to the rider's payment method. In some locations, if the driver has to wait more than a few minutes after arriving to the pickup location, riders are charged a wait time fee.==== Drivers ====The status of drivers as independent contractors is an unresolved issue (see Criticism). Uber drivers use their own cars although drivers can rent or lease a car to drive with Uber. Uber offers car rental or leasing via Getaround, Hertz, and Fair and Uber and BYD Auto have a partnership to provide leasing of electric cars to Uber drivers in Chicago and New York City.Drivers must meet requirements for age, health, car age and type, have a driver's license and a smartphone or tablet, and must pass a background check. In many cities, vehicles used by Uber drivers must pass annual safety inspections and/or must have an Uber emblem posted in the passenger window. Some cities also require Uber drivers to have a business license. A mechanism called "Real-Time ID Check" requires some drivers to occasionally take selfies when logging on to Uber.Drivers use an app to receive and accept offers for transportations and further information. The Uber driver app includes accommodations for hearing-impaired drivers.=== Service options ======= Offered ====UberX, the basic level of service, provides a private ride in a standard car with driver for up to four passengers. UberX and UberXL cars with child safety seats are available for an additional charge. Persons with a service animal may use any type of Uber service, as required by law. Rider service levels, many of which are only available in certain cities, include:ASSIST provides additional assistance to senior citizens and passengers with a physical disability, but cannot transport a non-folding wheelchair (see UberWAV for wheelchair-accessible vehicles).Bike is a dockless bicycle-sharing system that allows users to rent electric bicycles via Uber subsidiary Jump Bikes in nine metropolitan areas in the United States including San Francisco and Washington, D.C. Uber users are also able to rent Lime scooters in 46 cities via the Uber mobile app.BLACK provides a black luxury vehicle.Chapchap, available in Nairobi, Kenya is a low-cost service offering transport via a Suzuki Alto, a kei car. "Chapchap" means "faster" in the Swahili language.ESPAÑOL, available in California, is a version of UberX (see below) that provides a Spanish-speaking driver.Comfort includes newer cars with additional legroom.FLASH, available in Hong Kong and Singapore, is a service that combines both private cars and taxis, operated by ComfortDelGro in Singapore.Green, available in Europe, provides an electric car or hybrid vehicle at the same price as UberX.Health, available for health professionals in the United States, is a HIPAA-compliant method to arrange rides for patients to-and-from their appointments. Patients without smartphones can receive pickup information via Text messaging or via the health professional's office.Hire, available in India, provides a vehicle for hire for local travel.Lux, provides upper-class vehicles.MOTO, available in India, Indonesia, Pakistan, and the Dominican Republic, provides transportation by motorcycle.POOL, available for up to two people per party, provides a ride that is possibly shared with other riders going in the same general direction. Unless the rider pays an additional fee for door-to-door service, the rider(s) are required to walk a short distance at both ends of the ride to save time for the driver and other riders. The pickup/drop-off locations are indicated via a map in the mobile app.Upper-class vehicles are marketed under the names Premier (India), Premium (New York City suburbs), Exec (London, Berlin?) and Select.SUV provides a sport utility vehicle.Taxi allows users to summon a taxi using the Uber software application. Users pay an additional booking fee and can leave a gratuity through the app. The service was implemented to appease taxi drivers who protested the increased competition from Uber.Van, available in Europe, provides a van for groups of up to 6 people.WAV aka ACCESS provides a wheelchair accessible vehicle.UberAuto, available in India, Sri Lanka and Pakistan, provides transportation by auto rickshaw.UberGO, available in India and Sri Lanka, provides for a ride in a hatchback.UberXL provides a ride in a large vehicle that can seat up to 6 passengers.==== Services under development ====UberAIR / UberElevate will provide short flights using VTOL aircraft. Demonstration flights are projected to start in 2020 in Dallas and Los Angeles. Commercial operations are projected to begin in 2023. Although technically feasible, the program is expected to encounter safety and regulatory obstacles.In early October 2019, Uber announced a service called Uber Works that matches Temporary workers with potential jobs and employers. The new service is available only in Chicago.==== Promotional limited services ====Uber has also operated promotional limited services, such as rides of up to 15 minutes each on September 6–8, 2013 in San Francisco in the DeLorean that was featured in the Back to the Future film franchise.Uber allowed users to hire speedboats in the summer to/from certain points on the coast of Croatia. Uber has also offered transport across Biscayne Bay during Miami Art Week and across the Bosporus strait in Istanbul in the summer.=== Rating scores ===After each journey, drivers are required to rate passengers on a scale of 1 to 5 stars. Passengers are not required to rate the driver, although are encouraged to do so using the same 1 to 5 scale. Riders and drivers who have low ratings can be deactivated. In May 2019, Uber began actively banning riders with low ratings. The company has not defined in detail what will be considered a “below average rating”, but the update is intended to remove users who are unable to improve their behavior.=== Other products and services ===Uber Eats provides meal delivery from nearby participating restaurants for a fee of approximately $4.Uber and issuing bank Barclays offer a Visa Inc. credit card that offers customers a cashback reward program and other incentives.Uber Freight matches freight shippers with truckers in a similar fashion to the matching of taxi passengers with drivers.== History ==Uber was founded in 2009 as UberCab by Garrett Camp, a computer programmer and the co-founder of StumbleUpon, and Travis Kalanick, who had sold his Red Swoosh startup for $19 million in 2007.On New Year's Eve, after Camp and his friends spent $800 hiring a private driver, Camp wanted to find a way to reduce the cost of direct transportation. He realized that sharing the cost with people could make it affordable, and his idea morphed into Uber. Kalanick joined Camp and gives him "full credit for the idea" of Uber. The first prototype was built by Camp and his friends, Oscar Salazar and Conrad Whelan, with Kalanick being brought on as a "mega advisor" to the company.Following a beta launch in May 2010, Uber's services and mobile app officially launched in San Francisco in 2011. Originally, the application only allowed users to hail a black luxury car and the price was 1.5 times that of a taxi.In February 2010, Ryan Graves became the first Uber employee, getting the job by responding to a tweet from Kalanick announcing the job opening, and receiving 5–10% of the company. Graves started out as general manager and shortly after the launch was named as CEO. After ten months, in December 2010, Kalanick succeeded Graves as CEO. Graves became the company's chief operating officer (COO).In 2011, the company changed its name from UberCab to Uber after complaints from San Francisco taxi operators.The company's early hires included a nuclear physicist, a computational neuroscientist, and a machinery expert who worked on predicting demand for private hire car drivers and where demand is highest. In April 2012, in Chicago, Uber launched a service where users were able to request a regular taxi or an Uber driver via its mobile app.In July 2012, the company introduced UberX, a cheaper option that lets people drive for Uber using non-luxury vehicles, subject to a background check, registration requirement, and car standards. At first, rates were similar to those of taxis and were 35% cheaper than UberBLACK. By early 2013, the service was operating in 35 cities. Uber allowed drivers to use their personal vehicles as part of UberX starting in April 2013. Rates were quickly lowered, which caused some dissatisfaction among UberBLACK and taxi drivers, whose earnings decreased as a result of the increased competition at lower rates.In August 2014, Uber launched UberPOOL, a carpooling service, in the San Francisco Bay Area. The service was then launched in other cities worldwide: Paris in November 2014, New York City in December 2014, China in August 2015, Washington, D.C. in October 2015, London in December 2015, the suburbs of Boston in January 2016, Hyderabad, Kolkata Mumbai, and Singapore in June 2016, Delaware in September 2016, Toronto (Brampton and Scarborough) in April 2017, Nashville in December 2017, Sydney in April 2018, and Melbourne in June 2018.In August 2014, Uber launched Uber Eats, a food delivery service.In August 2016, after facing tough competition in China, Uber sold its operations in China to DiDi, in exchange for an 18% stake in Didi. Didi also agreed to invest $1 billion into Uber Global. Uber had started operations in China in 2014, under the name 优步 (yōubù).In August 2017, Dara Khosrowshahi, the CEO of Expedia which had led an $11M investment in Wingz, became the CEO of Uber.In fall 2017, Uber became a gold member of the Linux Foundation and received a five star privacy rating from the Electronic Frontier Foundation.In February 2018, Uber combined its operations in Russia, Armenia, Azerbaijan, Belarus, Georgia and Kazakhstan with those of Yandex.Taxi and invested $225 million in the venture.In March 2018, Uber merged its services in Southeast Asia with those of Grab in exchange for a 27.5% ownership stake in Grab.As of March 2018, men accounted for 62.0% of overall company employment, 51.4% of support staff, and 82.1% of technology-related employment. White people made up 48.6% of the overall employment base and Asian people account for 32.3%. However, for technology-related jobs, White people were 46.3% of employees, while Asian people accounted for 44.7% of employment.Rent, powered by Getaround, was a peer-to-peer carsharing service available to some users in San Francisco between May 2018 and November 2018.On May 10, 2019, the company became a public company via an initial public offering underwritten by 30 banks including Morgan Stanley. Following the initial public offering, Uber's shares dropped 11%, resulting in the biggest first-day dollar loss in IPO history for the US. It then posted losses of $1 billion on its first quarter of 2019 in its first earnings report as a public company. A month after going public, both COO Barney Harford and CMO Rebecca Messina stepped down.  Uber posted a drastic US$5.2 billion loss of for the second quarter of 2019. The loss included US$3.9 billion of "stock-based compensation expenses" related to employee equity delivered as a result of the IPO, and an operating loss of US$1.3 billion.Facing continued losses, the marketing department headcount was reduced by a third on July 29, 2019 with the lay-off of 400 people.  Engineer hires were frozen as well.In early September 2019, Uber laid off an additional 435 employees with 265 coming from the engineering team and another 170 coming from the product team.In October 2019, Uber launched Uber Works to connect workers who want temporary jobs with businesses. The app is available in Chicago only as a start.In October 2019, Uber announced airport helicopter taxi service available to all users from JFK airport.=== Self-driving car research ===Advanced Technologies Group (Uber ATG) is a subsidiary of the company that is developing self-driving cars. Uber ATG is minority-owned by Softbank Vision Fund, Toyota, and Denso.In early 2015, the company hired approximately 50 people from the robotics department of Carnegie Mellon University.On September 14, 2016, Uber launched its first self-driving car services to select customers in Pittsburgh, including Pittsburgh Mayor Bill Peduto, using a fleet of Ford Fusion cars each equipped with 20 cameras, seven lasers, Global Positioning System, lidar, and radar equipment that enabled the car to create a three-dimensional map utilizing landmarks and other contextual information to keep track of its position.On December 14, 2016, Uber began using self-driving Volvo XC90 SUVs in its hometown of San Francisco. On December 21, 2016, the California Department of Motor Vehicles revoked the registration of the 16 vehicles Uber was using for the test and forced the program to cease operations in California. Uber then moved the program to Arizona, where the cars were able to pick up passengers, albeit with two Uber engineers in the front seats as a safety precaution. In March 2017, an Uber self-driving car was flipped on its side by a vehicle that failed to yield. In October 2017, Uber started using only 1 test driver despite some employees' safety concerns.In November 2017, Uber announced a non-binding plan to buy up to 24,000 Volvo XC90 SUV vehicles designed to accept autonomous technology (including a different type of steering and braking mechanism and sensors) between 2019 and 2021.In March 2018, the death of Elaine Herzberg by an Uber self-driving vehicle in Tempe, Arizona resulted in temporary pause to Uber's self driving vehicle testing. According to police, the woman was struck by the Uber vehicle while attempting to cross the street, while the person in the vehicle was watching videos on her phone. Uber pulled its self-driving cars off all public roads and quickly reached a settlement with the victim's family. There was disagreement among local authorities as to whether or not the car or the victim was at fault. In December 2018, after receiving local approval, Uber restarted testing of its self driving cars, only during daylight hours and at slower speeds, in Pittsburgh and Toronto. In March 2019, Uber was found not criminally liable by Yavapai County Attorney's Office for the death of Ms. Herzberg. The company changed its approach to self-driving vehicles after Herzberg's death, inviting both Waymo and General Motors’ Cruise self-driving vehicle unit to operate vehicles on Uber’s ride-hailing network.Prior to its IPO, Uber projected the potential operation of 75,000 autonomous vehicles, in 13 cities, by 2022. These projections, developed through an internal effort codenamed Project Rubicon, targeted the possibility of profitable autonomous vehicles by 2018 in an initial January 2016 report, with a May 2016 report claiming that 13,000 autonomous Uber vehicles could be operating by 2019. The 75,000-vehicle figure was proposed in September 2016. To reach these goals, Uber spent a reported $20 million a month on research and development, according to TechCrunch. Other sources have estimated Uber’s spending on self-driving vehicle research to have reached as high as $200 million per quarter.In April 2019, Uber scientist Raquel Urtasun offered a more cautious estimate of the company’s eventual self-driving capabilities, saying "self-driving cars are going to be in our lives. The question of when is not clear yet. To have it at scale is going to take a long time."==== Cancellation of research on autonomous trucks ====After spending $925 million to develop autonomous trucks, Uber cancelled its self-driving truck program in July 2018. Uber acquired Otto for $625 million in 2016. According to a February 2017 lawsuit filed by Waymo, owned by an affiliate of Google, ex-Google employee Anthony Levandowski allegedly "downloaded 9.7 GB of Waymo's highly confidential files and trade secrets, including blueprints, design files and testing documentation" before resigning to found Otto, which was purchased by Uber. A ruling in May 2017 required Uber to return documents to Waymo. The trial began February 5, 2018. A settlement was announced on February 8, 2018 in which Uber gave Waymo $244 million in Uber equity and agreed not to infringe on Waymo's intellectual property.== Criticism ===== Criticism by the taxi industry ===The taxi industry has claimed that ridesharing companies skirt regulations that apply to passenger transport and ridesharing companies are therefore illegal taxicab operations. This has resulted in additional regulations imposed on ridesharing companies and, in some jurisdictions, certain ridesharing companies are banned from operating.In New York City, use of ridesharing companies has reduced the value of taxi medallions, transferable permits or licenses authorizing the holder to pick up passengers for hire. After soaring in value after the Great Recession due to their perceived safety, New York City taxi medallions were again trading for around $170,000 each in 2018. Annual rental rates were $30,000. A couple of credit unions that lent money secured by medallions suffered from bank failure.=== Classification of drivers as independent contractors ===Unless otherwise required by law, rideshare drivers are generally independent contractors and not employees. This designation may affect taxation, work hours, and overtime benefits and lawsuits have been filed by drivers alleging that they are entitled to the rights and remedies of being considered "employees" under employment law. In response, ridesharing companies say they provide "flexible and independent jobs" for drivers.In O'Connor v. Uber Technologies, a lawsuit filed in the United States District Court for the Northern District of California on August 16, 2013, Uber drivers pleaded that according to the California Labor Code they should be classified as employees and receive reimbursement of business expenses such as gas and vehicle maintenance costs. In March 2019, Uber agreed to pay $20 million to settle the case.On October 28, 2016, in the case of Aslam v Uber BV, the Central London Employment tribunal ruled that Uber drivers are "workers", rather than self-employed individuals, and are entitled to the minimum wage under the National Minimum Wage Act 1998, paid holiday, and other normal worker entitlements. Two Uber drivers had brought the test case to the employment tribunal with the assistance of the GMB Union, on behalf of a group of drivers in London. Uber appealed the decision. In December 2018, Uber lost an appeal of the case at the Court of Appeal, but was granted permission to appeal to the Supreme Court of the United Kingdom.In March 2018, the Federal Department of Economic Affairs, Education and Research of Switzerland, gave the legal opinion that under the conditions that bind drivers to Uber that they should be classified as employees.In April 2018, the California Supreme Court ruled in Dynamex Operations v. Superior Court that document delivery company Dynamex has misclassified its delivery drivers as independent contractors rather than employees. This ultimately led to California passing Assembly Bill 5 on September 11, 2019, which would require many jobs, to be classified as employees, with the according minimum wage protections and unemployment benefits, beginning in 2020. Uber and Lyft both pledged to keep drivers classified as contractors, saying they could meet the requirements of the new test, and both pledged $30 million on a 2020 ballot initiative against AB 5.=== Driver criticism of compensation ===Drivers have complained that in some cases, after expenses, they earn less than minimum wage. As a result, in some jurisdictions, such as New York City, drivers are guaranteed a minimum wage. The New York City minimum wage was set at $26.51 before expenses or $17.22 after expenses in 2019, and an analysis by the NYC Taxi and Limousine Commission revealed that 85% of drivers made less than the minimum wage prior to the law.In May 2018, a unanimous panel of the United States Court of Appeals for the Ninth Circuit found that the City of Seattle's attempt to engage in collective bargaining on behalf of ridesharing company workers was not entitled to state action immunity from the Sherman Antitrust Act.=== Dynamic pricing ===Ridesharing companies use dynamic pricing models; prices for the same route vary based on the supply and demand for rides at the time the ride is requested. When rides are in high demand in a certain area and there are not enough drivers in such area, fares increase to get more drivers to that area. The rate quoted to the rider reflects such dynamic pricing.Ridesharing companies were criticized for extreme surcharges during emergencies such as Hurricane Sandy, the 2014 Sydney hostage crisis, and the June 2017 London Bridge attack, especially when taxis offered to transport riders for free; however, in many cases, the surcharges were refunded by the ridesharing companies and the companies later agreed to either not charge surcharges, or in some cases, offer free rides, during certain emergencies.=== Increased traffic congestion ===Ridesharing companies have been criticized for increasing traffic congestion in New York City and San Francisco. A report published by Schaller Consulting in July 2018 showed that, as a result of ridesharing companies, traffic congestion increased both cities, which already had comprehensive public transport systems in place. A main reason was that a large number of people, who would otherwise have used public transport, shifted to services offered by transportation network companies.=== Reduced usage of public transportation ===Studies have shown that ridesharing companies have led to a reduction in use of public transportation.=== Lack of wheelchair accessible vans ===In some areas, ridesharing companies are required by law to have a certain amount of wheelchair accessible vans (WAVs) on the road at any given time. This can be a difficult requirement for ridesharing companies to meet because the companies don't provide vehicles and most drivers do not own a WAV, causing a shortage.=== Drivers using their phones while driving ===When a customer makes a pick-up request, a driver is notified via mobile app and is provided the customer's location. The driver has approximately 15 seconds to tap the phone to accept the request. In many jurisdictions, tapping a phone while driving is against the law as it could result in distracted driving.=== Driver status and earnings ===While Uber is viewing drivers as contractors, courts in the United Kingdom and Switzerland  consider their drivers to be employees of the company.  The Guardian quoted a driver in March 2019 who said he was earning $3.75 per hour after expenses. A report published by the Economic Policy Institute in 2018 found the average wage for drivers to be $9.21. Austrian weekly papers Profil and Trend found the hourly wage of drivers to be at €4 and claimed a high incidence of tax evasion, social fraud and circumvention of labour laws by the companies employing drivers on Ubers behalf. A 2017 report claimed that only 4 percent of all drivers were still working as such one year after entering the company.=== Misleading drivers of potential earnings ===In January 2017, Uber agreed to pay $20 million to the US government to resolve accusations by the Federal Trade Commission of having misled drivers about potential earnings.=== Alleged short-changing of drivers ===In 2017, lawyers for drivers filed a class action lawsuit that alleged that Uber did not provide drivers with the 80% of collections they were entitled to.In May 2017, after the New York Taxi Workers Alliance (NYTWA) filed a class action lawsuit in federal court in New York, Uber admitted to underpaying New York City drivers tens of millions of dollars over 2.5 years by calculating driver commissions on a net amount. Uber agreed to pay the amounts owed plus interest.=== Driver refusal to transport a service animal ===In March 2018, a lawsuit filed against Uber in the United States accused the company's drivers of not serving a woman with cerebral palsy due to her service dog, in violation of the Americans with Disabilities Act and the Texas Human Resources Code.=== Criticism for collecting fares during a taxi strike ===In late January 2017, Uber was targeted by GrabYourWallet for collecting fares during a taxi strike in New York City in protest of Trump travel ban Executive Order 13769. The Order had triggered a taxi strike in New York City, to which Uber responded by removing surge pricing from JFK airport, where Muslim refugees had been detained upon entry. Uber was also targeted because then-CEO Travis Kalanick joined an Economic Advisory Council with Donald Trump. A social media campaign known as #deleteuber was formed in protest, resulting in approximately 200,000 users deleting the app. Uber added user account deletion to meet the resulting surge in requests. Statements were e-mailed to former users who had deleted their accounts, asserting that the company would assist refugees, and that CEO Kalanick joining the Council was not an endorsement of President Trump. On February 2, 2017, Kalanick resigned from the business advisory council.=== Aggressive strategy for dealing with regulators ===When Uber was led by Travis Kalanick, the company took an aggressive strategy in dealing with obstacles, including regulators. In 2014, Kalanick said "You have to have what I call principled confrontation." Uber's strategy was generally to commence operations in a city, then, if it faced regulatory opposition, Uber mobilized public support for its service and mounted a political campaign, supported by lobbyists, to change regulations.In 2014, while in the midst of a regulatory battle, Portland, Oregon's transportation commissioner called Uber management "a bunch of thugs".In June 2014, Uber distributed to its riders the personal contact information of a commissioner in Virginia who opposed the company, and told riders to flood his inbox with complaints.In November 2017, CEO Dara Khosrowshahi dropped the "win at all costs" strategy and implemented new values for the company, including "we do the right thing".=== Alleged cancellation of ride requests to disrupt competitors ===Uber issued an apology on January 24, 2014, after documents were leaked to Valleywag and TechCrunch saying that, earlier in the month, Uber employees in New York City deliberately ordered rides from Gett, a competitor, only to cancel them later. The purpose of the fake orders was two-fold: wasting drivers' time to obstruct legitimate customers from securing a car, and offering drivers incentives—including cash—to join Uber.==== Operation SLOG plan to disrupt Lyft ====Following Lyft's expansion into New York City in July 2014, Uber, with the assistance of TargetCW, a San Diego, California-based employment agency, sent emails offering a "huge commission opportunity" to several contractors based on the "personal hustle" of the participants. Those who responded to the solicitation were offered a meeting with Uber marketing managers who attempted to create a "street team" to gather intelligence about Lyft's launch plans in New York City and recruit their drivers to Uber. Recruits were given two Uber-branded iPhones (one a backup in case the person was identified by Lyft) and a series of valid credit card numbers to create dummy Lyft accounts. Participants were also required to sign non-disclosure agreements.In August 2014, Lyft reported that 177 Uber employees had ordered and canceled approximately 5,560 Lyft rides since October 2013, and that it had found links to Uber recruiters by cross-referencing the phone numbers involved. The report identified one Lyft passenger who canceled 300 rides from May 26 to June 10, 2014, and who was identified as an Uber recruiter by seven different Lyft drivers. Uber did not apologize, but suggested that the recruitment attempts were possibly independent parties trying to make money.=== Evasion of law enforcement operations ======= Greyball ====Uber developed an internal software tool called Greyball, which uses data collected from the Uber mobile app and other means, to avoid giving rides to certain individuals. The tool was used starting in 2014. By showing "ghost cars" driven by fake drivers to the targeted individuals in the Uber mobile app, and by giving real drivers a means to cancel rides requested by those individuals, Uber was able to avoid giving rides to known law enforcement officers in areas where its service is illegal. Investigative journalism by The New York Times and the resulting report, published on March 3, 2017, made public Uber's use of Greyball since 2014, describing it as a way to evade city code enforcement officials in Portland, Oregon, Australia, South Korea, and China. At first, in response to the report, Uber stated that Greyball was designed to deny rides to users who violate Uber's terms of service, including those involved in sting operations. According to Uber, Greyball can "hide the standard city app view for individual riders, enabling Uber to show that same rider a different version". Uber reportedly used Greyball to identify government officials through factors such as whether a user frequently opens the app near government offices, a review of social media profiles by Uber employees to identify law enforcement personnel, and the credit cards associated with the Uber account.On March 6, 2017, the City of Portland, Oregon announced an investigation into whether Uber had used its Greyball software tool to obstruct the enforcement of city regulations. The investigation by the Portland Bureau of Transportation (PBOT) found that: "Uber used Greyball software to intentionally evade PBOT’s officers from December 5 to December 19, 2014 and deny 29 separate ride requests by PBOT enforcement officers." Following the release of the audit, Portland's commissioner of police suggested that the city subpoena Uber to force the company to turn over information on how Uber used software to evade regulatory officials.On March 8, 2017, Uber admitted that it had used Greyball to thwart government regulators and pledged to stop using the service for that purpose.In May 2017, the United States Department of Justice opened a criminal investigation into Uber's use of Greyball to avoid local law enforcement operations.==== Ripley ====After a police raid in Uber's Brussels office, a January 2018 report by Bloomberg News stated that "Uber routinely used Ripley to thwart police raids in foreign countries." Developed as a type of secret "panic button" system, initially called "unexpected visitor protocol", then nicknamed "Ripley", to disrupt government raids on Uber's offices by locking, shutting off, and changing passwords on staff computers upon a raid; Uber likely used this button at least 24 times, from spring 2015 until late 2016.=== User privacy and data breaches ======= God view ====On November 19, 2014, then U.S. Senator Al Franken, Chairman of the United States Senate Judiciary Subcommittee on Privacy, Technology and the Law, sent a letter to Kalanick regarding privacy. Concerns were raised about internal misuse of the company's data, in particular the ability of Uber staff to track the movements of its customers, known as "God View". In 2011, a venture capitalist disclosed that Uber staff members were using the function to track journalists and politicians as well as using the feature recreationally. Staff members viewed being tracked by Uber as a positive reflection on the subject's character. An Uber job interviewee said that he was given unrestricted access to Uber's customer tracking function as part of the interview process, and that he retained that access for several hours after the interview ended.==== Data breaches ====On February 27, 2015, Uber admitted that it had suffered a data breach more than nine months earlier. Names and license plate information of approximately 50,000 drivers were inadvertently disclosed. Uber discovered this leak in September 2014 but waited more than 5 months to notify the affected people.An announcement in November 2017 revealed that in 2016, a separate data breach disclosed personal information on 600,000 drivers and 57 million customers—including names, email addresses, phone numbers, and driving license information. Using employees' usernames and passwords that had been compromised in previous breaches (a "credential stuffing" method), attackers gained access to a private GitHub repository used by Uber developers. The hackers subsequently located credentials for the company's Amazon Web Services datastore in the repository files, and were therefore able to obtain access to the account records of users and drivers, as well as other data contained in over 100 Amazon S3 buckets. Uber paid a $100,000 ransom to the hackers on the promise they would delete the stolen data. The company was subsequently criticized for concealing the loss of data. CEO Dara Khosrowshahi apologized. Uber's British divisions were fined £385,000 (reduced to £308,000) by the Information Commissioner's Office.In September 2018, Uber settled with the Federal Trade Commission for $148 million and admitted that its claim that internal access to consumers' personal information was closely monitored on an ongoing basis was false. Uber also stated that it had failed to live up to its promise to provide reasonable security for consumer data. It was the largest multi-state settlement related to a data breach.=== Safety concerns ===It is unclear if Uber is less or more safe than taxicabs, as major cities don't have much data on taxi-related incidents.==== Allegations of inadequate background checks and vetting of drivers ====Concerns regarding Uber's background checks were raised after reports of sexual abuse of passengers by Uber drivers. Sexual assaults in relation to Uber are most often committed by either Uber drivers themselves  or by individuals posing as Uber drivers. In the latter case, imposters have lured unsuspecting passengers to their vehicles by placing an Uber sticker on their dashboard or by claiming to be a passenger's expected driver.In February 2016, Uber was criticized following the 2016 Kalamazoo shootings, a shooting spree in Kalamazoo, Michigan that left six people dead and two wounded. It was committed by Jason Dalton, who was driving for Uber while conducting the shooting. During the ensuing seven-hour manhunt, authorities believe that Dalton continued to drive and accept fares. Uber was aware of issues with Dalton's driving skills, having received multiple complaints, though critics agree that Dalton would not have raised any red flags since he did not have a criminal record.In November 2017, The Colorado Public Utilities Commission fined Uber $8.9 million after discovering that 57 drivers in the state had violations in their background checks. The fine amount equaled $2,500 per day that an unqualified driver worked.=== Sexual harassment allegations and management shakeup (2017) ===On February 20, 2017, former Uber engineer Susan Fowler stated that she was subjected to sexual harassment by a manager and subsequently threatened with termination of employment by another manager if she continued to report the incident. Kalanick was reportedly aware of the harassment issues.CTO Thuan Pham was alleged to have had knowledge of and to ignore Susan Fowler's sexual harassment allegations; however, investigations by TheInformation and Buzzfeed showed this to not be the case, allowing Pham to keep his job.Uber hired former attorney general Eric Holder to investigate the claims. Arianna Huffington, a member of Uber's board of directors, also oversaw the investigation. Fowler likened Uber's culture to A Game of Thrones, in which rivals vie for the throne the same way Uber employees were encouraged to vie for power and aggression and betrayal was common. On February 20, 2017, Kalanick led a meeting with employees that was described by the participants as honest and raw.On February 27, 2017, Amit Singhal, Uber's Senior Vice President of Engineering, was forced to resign after it came to light that he failed to disclose a sexual harassment claim against him that occurred while he was the Vice President of Google Search.On June 6, 2017, Uber announced that it fired over 20 employees as a result of the investigation. On June 13, 2017, Kalanick took an indefinite leave of absence from Uber. On June 20, 2017, after multiple shareholders reportedly demanded his resignation, Kalanick resigned as CEO.==== Scandals and departure of Emil Michael ====At a private dinner in November 2014, Emil Michael, senior vice president of Uber, suggested that Uber hire a team of opposition researchers and journalists, with a million-dollar budget, to "dig up dirt" on the personal lives and backgrounds of media figures who reported negatively about Uber. Specifically, he targeted Sarah Lacy, editor of PandoDaily, who, in an article published in October 2014, accused Uber of sexism and misogyny in its advertising. Michael issued a public apology and apologized to Lacy in a personal email, claiming that Uber would never actually undertake the plan. Several journalists deleted their Uber apps. After several additional scandals involving Emil Michael, including an escort-karaoke bar scandal in Seoul and the questioning of the medical records of a rape victim in India, he left the company in June 2017 when Kalanick, who reportedly was protecting Michael, resigned.==== Settlement with victims ====In August 2018, Uber agreed to pay a total of $7 million to 480 workers to settle claims of gender discrimination, harassment and hostile work environment.=== Use of offshore companies to minimize tax liability ===In November 2017, the Paradise Papers, a set of confidential electronic documents relating to offshore investment, revealed that Uber is one of many corporations that used an offshore company to minimize taxes.== Awards ==In 2013, USA Today named Uber its tech company of the year.== References ==== Further reading ==Scholarly papersLaurell, Christofer; Sandström, Christian (June 28, 2016). "Analysing Uber in social media – disruptive technology or institutional disruption?". International Journal of Innovation Management. 20 (5): 1640013. doi:10.1142/S1363919616400132.McGaughey, E. (2018). "Uber, the Taylor Review, mutuality, and the duty to not misrepresent employment status". Industrial Law Journal. SSRN 3018516.Petropoulos, Georgios (February 22, 2016). "Uber and the economic impact of sharing economy platforms". Bruegel.Noto La Diega, Guido (2016). "Uber law and awareness by design. An empirical study on online platforms and dehumanised negotiations" (PDF). Revue européenne de droit de la consommation/ European Journal of Consumer Law. 2016 (II): 383–413 – via Northumbria Research Link.Oitaven, Juliana Carreiro Corbal; Carelli, Rodrigo de Lacerda; Casagrande, Cássio Luís (2019). Empresas de transporte, plataformas digitais e a relação de emprego: um estudo do trabalho subordinado sob aplicativos (PDF) (in Portuguese). Brasília: Ministério Público do Trabalho. ISBN 978-85-66507-27-0.Rogers, B. (2015). "The Social Costs of Uber". University of Chicago Law Review Dialogue. 82: 85.BooksIsaac, Mike. Super Pumped: The Battle for Uber. New York, NY. ISBN 9780393652246. OCLC 1090686951.== External links ==Official websiteBusiness data for Uber Inc.:
	The following outline is provided as an overview of and topical guide to software:Software – collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. The term was coined to contrast to the old term hardware (meaning physical devices). In contrast to hardware, software "cannot be touched". Software is also sometimes used in a more narrow sense, meaning application software only. Sometimes the term includes data that has not traditionally been associated with computers, such as film, tapes, and records..== What type of thing is software? ==Software can be described as all of the following:TechnologyComputer technologyTools== Types of software ==Application software – end-user applications of computers such as word processors or video games, and ERP software for groups of users.Business softwareComputer-aided designDatabasesDecision-making softwareEducational softwareEmotion-sensitive softwareImage editingIndustrial automationMathematical softwareMedical softwareMolecular modeling softwareQuantum chemistry and solid state physics softwareSimulation softwareSpreadsheetsTelecommunications (i.e., the Internet and everything that flows on it)Video editing softwareVideo gamesWord processorsMiddleware controls and co-ordinates distributed systems.Programming languages – define the syntax and semantics of computer programs. For example, many mature banking applications were written in the language COBOL, invented in 1959. Newer applications are often written in more modern languages.System software – provides the basic functions for computer usage and helps run the computer hardware and system. It includes a combination of the following:Device driverOperating systemPackage management systemServerUtilityWindow systemTeachware – any special breed of software or other means of product dedicated to education purposes in software engineering and beyond in general education.Testware – any software for testing hardware or software.Firmware – low-level software often stored on electrically programmable memory devices. Firmware is given its name because it is treated like hardware and run ("executed") by other software programs. Firmware often is not accessible for change by other entities but the developers' enterprises.Shrinkware is the older name given to consumer-purchased software, because it was often sold in retail stores in a shrink wrapped box.Device drivers – control parts of computers such as disk drives, printers, CD drives, or computer monitors.Programming tools – assist a programmer in writing computer programs, and software using various programming languages in a more convenient way. The tools include:CompilersDebuggersInterpretersLinkersText editorsIntegrated development environment (IDE) – single application for managing all of these functions.== Software products ===== By publisher ===List of Adobe softwareList of Microsoft software=== By platform ===List of Macintosh softwareList of old Macintosh software=== By  type ===List of software categoriesList of 2D animation softwareList of 3D animation softwareList of 3D computer graphics softwareList of 3D modeling softwareList of antivirus softwareList of chess softwareList of compilersList of computer-aided design softwareList of computer algebra systemsList of computer-assisted organic synthesis softwareList of computer simulation softwareList of concept- and mind-mapping softwareList of content management systemsList of graphing softwareList of information graphics softwareList of Linux distributionsList of operating systemsList of protein structure prediction softwareList of molecular graphics systemsList of numerical analysis softwareList of optimization softwareList of quantum chemistry and solid state physics softwareList of spreadsheet softwareList of statistical packagesList of Unified Modeling Language toolsList of video editing softwareList of web browsers=== Comparisons ===Comparison of 3D computer graphics softwareComparison of accounting softwareComparison of audio player softwareComparison of computer-aided design editorsComparison of data modeling toolsComparison of database toolsComparison of desktop publishing softwareComparison of digital audio editorsComparison of DOS operating systemsComparison of email clientsComparison of force field implementationsComparison of instant messaging clientsComparison of issue tracking systemsComparison of Linux distributionsComparison of mail serversComparison of network monitoring systemsComparison of nucleic acid simulation softwareComparison of operating systemsComparison of raster graphics editorsComparison of software for molecular mechanics modelingComparison of system dynamics softwareComparison of text editorsComparison of vector graphics editorsComparison of web frameworksComparison of web server softwareComparison of word processorsComparison of deep-learning software== History of software ==History of software engineeringHistory of free and open-source softwareHistory of software configuration managementHistory of programming languagesTimeline of programming languagesHistory of operating systemsHistory of Mac OS XHistory of Microsoft WindowsTimeline of Microsoft WindowsHistory of the web browserWeb browser history== Software development ==Software development  (outline) – development of a software product, which entails computer programming (process of writing and maintaining the source code), but also encompasses a planned and structured process from the conception of the desired software to its final manifestation. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.=== Computer programming ===Computer programming  (outline) –=== Software engineering ===Software engineering  (outline) –== Software distribution ==Software distribution –Software licensesBeerwareFreeFree and open source softwareFreely redistributable softwareOpen-source softwareProprietary softwarePublic domain softwareRevenue modelsAdwareDonationwareFreemiumFreewareCommercial softwareNagwarePostcardwareSharewareDelivery methodsDigital distributionList of mobile software distribution platformsOn-premises softwarePre-installed softwareProduct bundlingSoftware as a serviceSoftware plus servicesScamsScarewareMalwareEnd of software life cycleAbandonware== Software industry ==Software industry== Software publications ==Free Software MagazineInfoWorldPC MagazineSoftware MagazineWired (magazine)== Persons influential in software ==Bill GatesSteve JobsJonathan SachsWayne Ratliff== See also ==Outline of information technologyOutline of computersOutline of computingList of computer hardware termsBachelor of Science in Information TechnologyCustom softwareFunctional specificationMarketing strategies for product softwareService-Oriented Modeling FrameworkBus factorCapability Maturity ModelSoftware publisherUser experience== References ==== External links ==
	Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems. The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation.A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software. An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires is punctured, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue, corrosion, manufacturing flaws, or impact.Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop.== History ==The first known fault-tolerant computer was SAPO, built in 1951 in Czechoslovakia by Antonín Svoboda. Its basic design was magnetic drums connected via relays, with a voting method of memory error detection (triple modular redundancy). Several other machines were developed along this line, mostly for military use. Eventually, they separated into three distinct categories: machines that would last a long time without any maintenance, such as the ones used on NASA space probes and satellites; computers that were very dependable but required constant monitoring, such as those used to monitor and control nuclear power plants or supercollider experiments; and finally, computers with a high amount of runtime which would be under heavy use, such as many of the supercomputers used by insurance companies for their probability monitoring.Most of the development in the so-called LLNM (Long Life, No Maintenance) computing was done by NASA during the 1960s, in preparation for Project Apollo and other research aspects. NASA's first machine went into a space observatory, and their second attempt, the JSTAR computer, was used in Voyager. This computer had a backup of memory arrays to use memory recovery methods and thus it was called the JPL Self-Testing-And-Repairing computer. It could detect its own errors and fix them or bring up redundant modules as needed. The computer is still working today.Hyper-dependable computers were pioneered mostly by aircraft manufacturers, nuclear power companies, and the railroad industry in the USA. These needed computers with massive amounts of uptime that would fail gracefully enough with a fault to allow continued operation, while relying on the fact that the computer output would be constantly monitored by humans to detect faults. Again, IBM developed the first computer of this kind for NASA for guidance of Saturn V rockets, but later on BNSF, Unisys, and General Electric built their own.The 1970 F14 CADC had built-in self-test and redundancy.In general, the early efforts at fault-tolerant designs were focused mainly on internal diagnosis, where a fault would indicate something was failing and a worker could replace it. SAPO, for instance, had a method by which faulty memory drums would emit a noise before failure. Later efforts showed that, to be fully effective, the system had to be self-repairing and diagnosing – isolating a fault and then implementing a redundant backup while alerting a need for repair. This is known as N-model redundancy, where faults cause automatic fail safes and a warning to the operator, and it is still the most common form of level one fault-tolerant design in use today.Voting was another initial method, as discussed above, with multiple redundant backups operating constantly and checking each other's results, with the outcome that if, for example, four components reported an answer of 5 and one component reported an answer of 6, the other four would "vote" that the fifth component was faulty and have it taken out of service. This is called M out of N majority voting.Historically, motion has always been to move further from N-model and more to M out of N due to the fact that the complexity of systems and the difficulty of ensuring the transitive state from fault-negative to fault-positive did not disrupt operations.Tandem and Stratus were among the first companies specializing in the design of fault-tolerant computer systems for online transaction processing.== Terminology ==A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed.  For example, a building with a backup electrical generator will provide the same voltage to wall outlets even if the grid power fails.A system that is designed to fail safe, or fail-secure, or fail gracefully, whether it functions at a reduced level or fails completely, does so in a way that protects people, property, or data from injury, damage, intrusion, or disclosure.  In computers, a program might fail-safe by executing a graceful exit (as opposed to an uncontrolled crash) in order to prevent data corruption after experiencing an error.  A similar distinction is made between "failing well" and "failing badly".Fail-deadly is the opposite strategy, which can be used in weapon systems that are designed to kill or injure targets even if part of the system is damaged or destroyed.A system that is designed to experience graceful degradation, or to fail soft (used in computing, similar to "fail safe") operates at a reduced level of performance after some component failures.  For example, a building may operate lighting at reduced levels and elevators at reduced speeds if grid power fails, rather than either trapping people in the dark completely or continuing to operate at full power.  In computing an example of graceful degradation is that if insufficient network bandwidth is available to stream an online video, a lower-resolution version might be streamed in place of the high-resolution version.  Progressive enhancement is an example in computing, where web pages are available in a basic functional format for older, small-screen, or limited-capability web browsers, but in an enhanced version for browsers capable of handling additional technologies or that have a larger display available.In fault-tolerant computer systems, programs that are considered robust are designed to continue operation despite an error, exception, or invalid input, instead of crashing completely.  Software brittleness is the opposite of robustness.  Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.A system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with full performance, so that failure can be repaired or imminent complete failure anticipated.  Likewise, a fail-fast component is designed to report at the first point of failure, rather than allow downstream components to fail and generate reports then.  This allows easier diagnosis of the underlying problem, and may prevent improper operation in a broken state.=== Single fault condition ===A single fault condition is a situation where one means for protection against a hazard is defective. If a single fault condition results unavoidably in another single fault condition, the two failures are considered as one single fault condition. A source offers the following example:A single-fault condition is a condition when a single means for protection against hazard in equipment is defective or a single external abnormal condition is present, e.g. short circuit between the live parts and the applied part.== Redundancy ==Redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment.This can consist of backup components that automatically "kick in" if one component fails. For example, large cargo trucks can lose a tire without any major consequences. They have many tires, and no one tire is critical (with the exception of the front tires, which are used to steer, but generally carry less load, each and in total, than the other four to 16, so are less likely to fail).The idea of incorporating redundancy in order to improve the reliability of a system was pioneered by John von Neumann in the 1950s.Two kinds of redundancy are possible: space redundancy and time redundancy. Space redundancy provides additional components, functions, or data items that are unnecessary for fault-free operation.  Space redundancy is further classified into hardware, software and information redundancy, depending on the type of redundant resources added to the system.  In time redundancy the computation or data transmission is repeated and the result is compared to a stored copy of the previous result. The current terminology for this kind of testing is referred to as 'In Service Fault Tolerance Testing or ISFTT for short.== Criteria ==Providing fault-tolerant design for every component is normally not an option. Associated redundancy brings a number of penalties: increase in weight, size, power consumption, cost, as well as time to design, verify, and test. Therefore, a number of choices have to be examined to determine which components should be fault tolerant:How critical is the component? In a car, the radio is not critical, so this component has less need for fault tolerance.How likely is the component to fail? Some components, like the drive shaft in a car, are not likely to fail, so no fault tolerance is needed.How expensive is it to make the component fault tolerant? Requiring a redundant car engine, for example, would likely be too expensive both economically and in terms of weight and space, to be considered.An example of a component that passes all the tests is a car's occupant restraint system. While we do not normally think of the primary occupant restraint system, it is gravity. If the vehicle rolls over or undergoes severe g-forces, then this primary method of occupant restraint may fail. Restraining the occupants during such an accident is absolutely critical to safety, so we pass the first test. Accidents causing occupant ejection were quite common before seat belts, so we pass the second test. The cost of a redundant restraint method like seat belts is quite low, both economically and in terms of weight and space, so we pass the third test. Therefore, adding seat belts to all vehicles is an excellent idea. Other "supplemental restraint systems", such as airbags, are more expensive and so pass that test by a smaller margin.Another excellent and long-term example of this principle being put into practice is the braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components and they would add considerable weight. However, the similarly critical systems for actuating the brakes under driver control are inherently less robust, generally using a cable (can rust, stretch, jam, snap) or hydraulic fluid (can leak, boil and develop bubbles, absorb water and thus lose effectiveness). Thus in most modern cars the footbrake hydraulic brake circuit is diagonally divided to give two smaller points of failure, the loss of either only reducing brake power by 50% and not causing as much dangerous brakeforce imbalance as a straight front-back or left-right split, and should the hydraulic circuit fail completely (a relatively very rare occurrence), there is a failsafe in the form of the cable-actuated parking brake that operates the otherwise relatively weak rear brakes, but can still bring the vehicle to a safe halt in conjunction with transmission/engine braking so long as the demands on it are in line with normal traffic flow. The cumulatively unlikely combination of total foot brake failure with the need for harsh braking in an emergency will likely result in a collision, but still one at lower speed than would otherwise have been the case.In comparison with the foot pedal activated service brake, the parking brake itself is a less critical item, and unless it is being used as a one-time backup for the footbrake, will not cause immediate danger if it is found to be nonfunctional at the moment of application. Therefore, no redundancy is built into it per se (and it typically uses a cheaper, lighter, but less hardwearing cable actuation system), and it can suffice, if this happens on a hill, to use the footbrake to momentarily hold the vehicle still, before driving off to find a flat piece of road on which to stop. Alternatively, on shallow gradients, the transmission can be shifted into Park, Reverse or First gear, and the transmission lock / engine compression used to hold it stationary, as there is no need for them to include the sophistication to first bring it to a halt.On motorcycles, a similar level of fail-safety is provided by simpler methods; firstly the front and rear brake systems being entirely separate, regardless of their method of activation (that can be cable, rod or hydraulic), allowing one to fail entirely whilst leaving the other unaffected. Secondly, the rear brake is relatively strong compared to its automotive cousin, even being a powerful disc on sports models, even though the usual intent is for the front system to provide the vast majority of braking force; as the overall vehicle weight is more central, the rear tyre is generally larger and grippier, and the rider can lean back to put more weight on it, therefore allowing more brake force to be applied before the wheel locks up. On cheaper, slower utility-class machines, even if the front wheel should use a hydraulic disc for extra brake force and easier packaging, the rear will usually be a primitive, somewhat inefficient, but exceptionally robust rod-actuated drum, thanks to the ease of connecting the footpedal to the wheel in this way and, more importantly, the near impossibility of catastrophic failure even if the rest of the machine, like a lot of low-priced bikes after their first few years of use, is on the point of collapse from neglected maintenance.== Requirements ==The basic characteristics of fault tolerance require:No single point of failure – If a system experiences a failure, it must continue to operate without interruption during the repair process.Fault isolation to the failing component – When a failure occurs, the system must be able to isolate the failure to the offending component. This requires the addition of dedicated failure detection mechanisms that exist only for the purpose of fault isolation. Recovery from a fault condition requires classifying the fault or failing component. The National Institute of Standards and Technology (NIST) categorizes faults based on locality, cause, duration, and effect.Fault containment to prevent propagation of the failure – Some failure mechanisms can cause a system to fail by propagating the failure to the rest of the system. An example of this kind of failure is the "rogue transmitter" that can swamp legitimate communication in a system and cause overall system failure. Firewalls or other mechanisms that isolate a rogue transmitter or failing component to protect the system are required.Availability of reversion modesIn addition, fault-tolerant systems are characterized in terms of both planned service outages and unplanned service outages. These are usually measured at the application level and not just at a hardware level. The figure of merit is called availability and is expressed as a percentage. For example, a five nines system would statistically provide 99.999% availability.Fault-tolerant systems are typically based on the concept of redundancy.== Fault tolerance research ==Research into the kinds of tolerances needed for critical systems involves a large amount of interdisciplinary work. The more complex the system, the more carefully all possible interactions have to be considered and prepared for. Considering the importance of high-value systems in transport, public utilities and the military, the field of topics that touch on research is very wide: it can include such obvious subjects as software modeling and reliability, or hardware design, to arcane elements such as stochastic models, graph theory, formal or exclusionary logic, parallel processing, remote data transmission, and more.=== Replication ===Spare components address the first fundamental characteristic of fault tolerance in three ways:Replication: Providing multiple identical instances of the same system or subsystem, directing tasks or requests to all of them in parallel, and choosing the correct result on the basis of a quorum;Redundancy: Providing multiple identical instances of the same system and switching to one of the remaining instances in case of a failure (failover);Diversity: Providing multiple different implementations of the same specification, and using them like replicated systems to cope with errors in a specific implementation.All implementations of RAID, redundant array of independent disks, except RAID 0, are examples of a fault-tolerant storage device that uses data redundancy.A lockstep fault-tolerant machine uses replicated elements operating in parallel. At any time, all the replications of each element should be in the same state. The same inputs are provided to each replication, and the same outputs are expected. The outputs of the replications are compared using a voting circuit. A machine with two replications of each element is termed dual modular redundant (DMR). The voting circuit can then only detect a mismatch and recovery relies on other methods. A machine with three replications of each element is termed triple modular redundant (TMR). The voting circuit can determine which replication is in error when a two-to-one vote is observed. In this case, the voting circuit can output the correct result, and discard the erroneous version. After this, the internal state of the erroneous replication is assumed to be different from that of the other two, and the voting circuit can switch to a DMR mode. This model can be applied to any larger number of replications.Lockstep fault-tolerant machines are most easily made fully synchronous, with each gate of each replication making the same state transition on the same edge of the clock, and the clocks to the replications being exactly in phase. However, it is possible to build lockstep systems without this requirement.Bringing the replications into synchrony requires making their internal stored states the same. They can be started from a fixed initial state, such as the reset state. Alternatively, the internal state of one replica can be copied to another replica.One variant of DMR is pair-and-spare. Two replicated elements operate in lockstep as a pair, with a voting circuit that detects any mismatch between their operations and outputs a signal indicating that there is an error. Another pair operates exactly the same way. A final circuit selects the output of the pair that does not proclaim that it is in error. Pair-and-spare requires four replicas rather than the three of TMR, but has been used commercially.=== Failure-oblivious computing ===Failure-oblivious computing is a technique that enables computer programs to continue executing despite memory errors.  The technique handles attempts to read invalid memory by returning a manufactured value to the program, which in turn, makes use of the manufactured value and ignores the former memory value it tried to access.  This is a great contrast to typical memory checkers, which inform the program of the error or abort the program.  In failure-oblivious computing, no attempt is made to inform the program that an error occurred. Furthermore, it happens that the execution is modified several times in a row, in order to prevent cascading failures.The approach has performance costs: because the technique rewrites code to insert dynamic checks for address validity, execution time will increase by 80% to 500%.=== Recovery shepherding ===Recovery shepherding is a lightweight technique to enable software programs to recover from otherwise fatal errors such as null pointer dereference and divide by zero. Comparing to the failure oblivious computing technique, recovery shepherding works on the compiled program binary directly and does not need to recompile to program. It uses the just-in-time binary instrumentation framework Pin. It attaches to the application process when an error occurs, repairs the execution, tracks the repair effects as the execution continues, contains the repair effects within the application process, and detaches from the process after all repair effects are flushed from the process state. It does not interfere with the normal execution of the program and therefore incurs negligible overhead. For 17 of 18 systematically collected real world null-dereference and divide-by-zero errors, a prototype implementation enables the application to continue to execute to provide acceptable output and service to its users on the error-triggering inputs.== Disadvantages ==Fault-tolerant design's advantages are obvious, while many of its disadvantages are not:Interference with fault detection in the same component. To continue the above passenger vehicle example, with either of the fault-tolerant systems it may not be obvious to the driver when a tire has been punctured. This is usually handled with a separate "automated fault-detection system". In the case of the tire, an air pressure monitor detects the loss of pressure and notifies the driver. The alternative is a "manual fault-detection system", such as manually inspecting all tires at each stop.Interference with fault detection in another component. Another variation of this problem is when fault tolerance in one component prevents fault detection in a different component. For example, if component B performs some operation based on the output from component A, then fault tolerance in B can hide a problem with A. If component B is later changed (to a less fault-tolerant design) the system may fail suddenly, making it appear that the new component B is the problem. Only after the system has been carefully scrutinized will it become clear that the root problem is actually with component A.Reduction of priority of fault correction. Even if the operator is aware of the fault, having a fault-tolerant system is likely to reduce the importance of repairing the fault. If the faults are not corrected, this will eventually lead to system failure, when the fault-tolerant component fails completely or when all redundant components have also failed.Test difficulty. For certain critical fault-tolerant systems, such as a nuclear reactor, there is no easy way to verify that the backup components are functional. The most infamous example of this is Chernobyl, where operators tested the emergency backup cooling by disabling primary and secondary cooling. The backup failed, resulting in a core meltdown and massive release of radiation.Cost. Both fault-tolerant components and redundant components tend to increase cost. This can be a purely economic cost or can include other measures, such as weight. Manned spaceships, for example, have so many redundant and fault-tolerant components that their weight is increased dramatically over unmanned systems, which don't require the same level of safety.Inferior components. A fault-tolerant design may allow for the use of inferior components, which would have otherwise made the system inoperable.  While this practice has the potential to mitigate the cost increase, use of multiple inferior components may lower the reliability of the system to a level equal to, or even worse than, a comparable non-fault-tolerant system.== Examples ==Hardware fault tolerance sometimes requires that broken parts be taken out and replaced with new parts while the system is still operational (in computing known as hot swapping). Such a system implemented with a single backup is known as single point tolerant, and represents the vast majority of fault-tolerant systems. In such systems the mean time between failures should be long enough for the operators to have time to fix the broken devices (mean time to repair) before the backup also fails. It helps if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.Fault tolerance is notably successful in computer applications. Tandem Computers built their entire business on such machines, which used single-point tolerance to create their NonStop systems with uptimes measured in years.Fail-safe architectures may encompass also the computer software, for example by process replication.Data formats may also be designed to degrade gracefully. HTML for example, is designed to be forward compatible, allowing new HTML entities to be ignored by Web browsers that do not understand them without causing the document to be unusable.== Related terms ==There is a difference between fault tolerance and systems that rarely have problems. For instance, the Western Electric crossbar systems had failure rates of two hours per forty years, and therefore were highly fault resistant. But when a fault did occur they still stopped operating completely, and therefore were not fault tolerant.== See also ==Byzantine fault toleranceControl reconfigurationDamage toleranceData redundancyDefence in depthElegant degradationError detection and correctionError-tolerant design (human error-tolerant design)Failure semanticsFall back and forwardFault-tolerant computer systemGraceful exitIntrusion toleranceList of system quality attributesResilience (ecology)Progressive enhancementResilience (network)Robustness (computer science)Rollback (data management)Safe-life designSoftware diversity== References ==== Further reading ==
	A computer program is a collection of instructions that performs a specific task when executed by a computer. Most computer devices require programs to function properly.A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler or assembler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software.  The underlying method used for some calculation or manipulation is known as an algorithm.== History ==Code-breaking algorithms have existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.=== Early programmable machines ===The earliest programmable machines preceded the invention of the digital computer. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices. In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where musical mechanical automata could be made to play different rhythms and drum patterns. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns could be woven and repeated by arranging the cards.=== Analytical Engine ===In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine.The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a "store"—memory to hold 1,000 numbers of 40 decimal digits each.  Numbers from the "store" would then have then been transferred to the "mill" (analogous to the CPU of a modern machine), for processing. And a "thread" being the execution of programmed instructions by the device. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables. However, after more than 17,000 pounds of the British government's money, the thousands of cogged wheels and gears never fully worked together.During a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.=== Universal Turing machine ===In 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine.It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state.This machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the "Electronic Computing Instrument" that now bears the von Neumann architecture name.=== Early programmable computers ===The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer. A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together. Its 40 units weighed 30 tons, occupied 1,800 square feet (167 m2), and consumed $650 per hour (in 1940s currency) in electricity when idle. It had 20 base-10 accumulators. Programming the ENIAC took up to two months. Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week.  The programmers of the ENIAC were women who were known collectively as the "ENIAC girls." The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns. The Manchester Baby (June 1948) was a stored-program computer. Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.=== Later computers ===Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.In 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.In 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software. Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.== Computer programming ==Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.=== Programming languages ===Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.==== Imperative languages ====Imperative programming languages specify a sequential algorithm using declarations, expressions, and statements:A declaration couples a variable name to a datatype – for example:  var x: integer; An expression yields a value – for example:  2 + 2  yields 4A statement might assign an expression to a variable or use the value of a variable to alter the program's control flow – for example: x := 2 + 2; if x = 4 then do_something();One criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.==== Declarative languages ====Declarative programming languages describe what computation should be performed and not how to compute it. Declarative programs omit the control flow and are considered sets of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions. The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself. The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.=== Compilation and interpretation ===A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or assembler, or executed immediately with the aid of an interpreter.Compilers are used to translate source code from a programming language into either object code or machine code.  Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.Some compiled and assembled object programs need to be combined as modules with a linker utility in order to produce an executable program.Interpreters are used to execute source code from a programming language line-by-line. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is that they can easily be extended to an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.Just in time compilers pre-compile computer programs just before execution. For example, the Java virtual machine Hotspot contains a Just In Time Compiler which selectively compiles Java bytecode into machine code – but only code which Hotspot predicts is likely to be used many times.Either compiled or interpreted programs might be executed in a batch process without human interaction.Scripting languages are often used to create batch processes. One common scripting language is Unix shell, and its executing environment is called the command-line interface.No properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, Java is thought of as an interpreted language and C a compiled language, despite the existence of Java compilers and C interpreters.== Storage and execution ==Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes ("runs") the program, instruction by instruction, until termination. A program in execution is called a process. Termination is either by normal self-termination, by user intervention, or by error – software or hardware error.=== Simultaneous execution ===Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer.  Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.=== Self-modifying programs ===A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.== Functional categories ==Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software. The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner. In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.=== Application software ===There are many types of application software:The word app came to being in 21st century. It is a clipping of the word "application". They have been designed for many platforms, but the word was first used for smaller mobile apps. Desktop apps are traditional computer programs that run on desktop computers. Mobile apps run on mobile devices. Web apps run inside a web browser. Both mobile and desktop apps may be downloaded from the developers' website or purchased from app stores such as Microsoft Store, Apple App Store, Mac App Store, Google Play or Intel AppUp.An application suite consists of multiple applications bundled together. Examples include Microsoft Office, LibreOffice, and iWork. They bundle a word processor, spreadsheet, and other applications.Enterprise applications bundle accounting, personnel, customer, and vendor applications. Examples include enterprise resource planning, customer relationship management, and supply chain management software.Enterprise infrastructure software supports the enterprise's software systems. Examples include databases, email servers, and network servers.Information worker software are designed for workers at the departmental level. Examples include time management, resource management, analytical, collaborative and documentation tools. Word processors, spreadsheets, email and blog clients, personal information system, and individual media editors may aid in multiple information worker tasks.Media development software generates print and electronic media for others to consume, most often in a commercial or educational setting. These produce graphics, publications, animations, and videos.Product engineering software is used to help develop large machines and other application software. Examples includes computer-aided design (CAD), computer-aided engineering (CAE), and integrated development environments.Entertainment Software can refer to video games, movie recorders and players, and music recorders and players.=== Utility programs ===Utility programs are application programs designed to aid system administrators and computer programmers.=== Operating system ===An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware.In the 1950s, the programmer, who was also the operator, would write a program and run it.After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.More often than not the program did not work.The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study.In the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an operating system was kept in the computer at all times.Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.=== Boot program ===A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents. Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.=== Embedded programs ===Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.=== Microcode programs ===Microcode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU.  Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.== See also ==Artificial intelligenceAutomatic programmingComputer virusFirmwareKiller applicationSoftwareSoftware bug== References ==== Further reading ==Knuth, Donald E. (1997). The Art of Computer Programming, Volume 1, 3rd Edition. Boston: Addison-Wesley. ISBN 978-0-201-89683-1.Knuth, Donald E. (1997). The Art of Computer Programming, Volume 2, 3rd Edition. Boston: Addison-Wesley. ISBN 978-0-201-89684-8.Knuth, Donald E. (1997). The Art of Computer Programming, Volume 3, 3rd Edition. Boston: Addison-Wesley. ISBN 978-0-201-89685-5.
	Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or is interrupted by the operating system. As of 2015, most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.== History ==An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli Numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.The first theory about software—prior to creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay On Computable Numbers, with an Application to the Entscheidungsproblem (decision problem).This eventually led to the creation of the academic fields of computer science and software engineering; Both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering and development of software.However, prior to 1946, software was not yet the programs stored in the memory of stored-program digital computers, as we now understand it. The first electronic computing devices were instead rewired in order to "reprogram" them.In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper "The Teaching of Concrete Mathematics" contained the earliest known usage of the term "software" found in a search of JSTOR's electronic archives, predating the OED's citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term "software" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.== Types ==On virtually all computer platforms, software can be grouped into a few broad categories.=== Purpose, or domain of use ===Based on the goal, computer software can be divided into:Application software  which is software that uses the computer system to perform special functions or provide entertainment functions beyond the basic operation of the computer itself. There are many different types of application software, because the range of tasks that can be performed with a modern computer is so large—see list of software.System software  which is software for managing computer hardware behaviour, as to provide basic functionalities that are required by users, or for other software to run properly, if at all. System software is also designed for providing a platform for running application software,  and it includes the following:Operating systems  which are essential collections of software that manage resources and provides common services for other software that runs "on top" of them. Supervisory programs, boot loaders, shells and window systems are core parts of operating systems. In practice, an operating system comes bundled with additional software (including application software) so that a user can potentially do some work with a computer that only has one operating system.Device drivers  which operate or control a particular type of device that is attached to a computer. Each device needs at least one corresponding device driver; because a computer typically has at minimum at least one input device and at least one output device, a computer typically needs more than one device driver.Utilities  which are computer programs designed to assist users in the maintenance and care of their computers.Malicious software or malware  which is software that is developed to harm and disrupt computers. As such, malware is undesirable. Malware is closely associated with computer-related crimes, though some malicious programs may have been designed as practical jokes.=== Nature or domain of execution ===Desktop applications such as web browsers and Microsoft Office, as well as smartphone and tablet applications (called "apps"). (There is a push in some parts of the software industry to merge desktop applications with mobile apps, to some extent. Windows 8, and later Ubuntu Touch, tried to allow the same style of application user interface to be used on desktops, laptops and mobiles.)JavaScript scripts are pieces of software traditionally embedded in web pages that are run directly inside the web browser when a web page is loaded without the need for a web browser plugin. Software written in other programming languages can also be run within the web browser if the software is either translated into JavaScript, or if a web browser plugin that supports that language is installed; the most common example of the latter is ActionScript scripts, which are supported by the Adobe Flash plugin.Server software, including:Web applications, which usually run on the web server and output dynamically generated web pages to web browsers, using e.g. PHP, Java, ASP.NET, or even JavaScript that runs on the server. In modern times these commonly include some JavaScript to be run in the web browser as well, in which case they typically run partly on the server, partly in the web browser.Plugins and extensions are software that extends or modifies the functionality of another piece of software, and require that software be used in order to function;Embedded software resides as firmware within embedded systems, devices dedicated to a single use or a few uses such as cars and televisions (although some embedded devices such as wireless chipsets can themselves be part of an ordinary, non-embedded computer system such as a PC or smartphone). In the embedded system context there is sometimes no clear distinction between the system software and the application software. However, some embedded systems run embedded operating systems, and these systems do retain the distinction between system software and application software (although typically there will only be one, fixed, application which is always run).Microcode is a special, relatively obscure type of embedded software which tells the processor itself how to execute machine code, so it is actually a lower level than machine code. It is typically proprietary to the processor manufacturer, and any necessary correctional microcode software updates are supplied by them to users (which is much cheaper than shipping replacement processor hardware). Thus an ordinary programmer would not expect to ever have to deal with it.=== Programming tools ===Programming tools are also software in the form of programs or applications that software developers (also known asprogrammers, coders, hackers or software engineers) use to create, debug, maintain (i.e. improve or fix), or otherwise support software.Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.== Topics ===== Architecture ===Users often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.Platform software  The Platform includes the firmware, device drivers, an operating system, and typically a graphical user interface which, in total, allow a user to interact with the computer and its peripherals (associated equipment). Platform software often comes bundled with the computer. On a PC one will usually have the ability to change the platform software.Application software  Application software or Applications are what most people think of when they think of software. Typical examples include office suites and video games. Application software is often purchased separately from computer hardware. Sometimes applications are bundled with the computer, but that does not change the fact that they run as independent applications. Applications are usually independent programs from the operating system, though they are often tailored for specific platforms. Most users think of compilers, databases, and other "system software" as applications.User-written software  End-user development tailors systems to meet users' specific needs. User software include spreadsheet templates and word processor templates. Even email filters are a kind of user software. Users create this software themselves and often overlook how important it is. Depending on how competently the user-written software has been integrated into default application packages, many users may not be aware of the distinction between the original packages, and what has been added by co-workers.=== Execution ===Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using "pointers" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.=== Quality and reliability ===Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.Many bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that "every program has at least one more bug" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together.=== License ===The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.Proprietary software can be divided into two types:freeware, which includes the category of "free trial" software or "freemium" software (in the past, the term shareware was often used for free trial/freemium software). As the name suggests, freeware can be used free, although in the case of free trials or freemium software, this is sometimes only true for a limited period of time or with limited functionality.software available for a fee, often inaccurately termed "commercial software", which can only be legally used on purchase of a license.Open source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software.=== Patents ===Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a detailed idea (e.g. an algorithm) on how to implement a piece of software, or a component of a piece of software. Ideas for useful things that software could do, and user requirements, are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since all useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over any programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.== Design and implementation ==Design and implementation of software varies depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the latter has much more basic functionality.Software is usually designed and created (aka coded/written/programmed) in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software (if applicable). As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like Form1.Close() and Form1.Show() to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as "coder" and "hacker" – although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.== Industry and organizations ==A great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the co-founder of Microsoft was the richest person in the world in 2009, largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products - both market leaders in their respective product categories.Non-profit software organizations include the Free Software Foundation, GNU Project and the Mozilla Foundation. Software standard organizations like the W3C, IETF develop recommended software standards such as XML, HTTP and HTML, so that software can interoperate through these standards.Other well-known large software companies include Google, IBM, TCS, Infosys, Wipro, HCL Technologies, Oracle, Novell, SAP, Symantec, Adobe Systems, Sidetrade and Corel, while small companies often provide innovation.== See also ==Software release life cycleIndependent software vendorList of softwareSoftware asset managementOpen-source software== References ===== Sources ===Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.== External links ==Software at Curlie
	SPADE (SMART Process Acceleration Development Environment) is a software development productivity and quality tool used to create professional software in a short time and with little effort. As seen in the diagram SPADE (green icon) automates many manual activities of the Software development process. It therefore takes less time and effort to perform the full software development cycle.With SPADE the remaining manual steps are:  Reqs: gathering the wishes of the customer and documenting them in clear requirements, user stories or similar.Test cases: creating integration tests cases that will be run automatically during Test.Test: usability testing and testing integration with external (non-SPADE) systems.Accept: accepting the created solutionAutomation of the other steps is possible because the (business) requirements are specified clearly by using the method SMART Requirements 2.0. SPADE then uses intelligent algorithms that incorporate dependency analysis and a set of design patterns to transform these requirements to an optimized business process design, including the flow of user interactive and fully automated steps that are part of this business process.SPADE is a domain specific model based software development tool that is well suited for creating both complex as well as simple information processing systems. It is currently less suitable for creating software that can control hardware in real time or advanced graphical user interfaces. One can however add plug-ins for accessing functionality that is not created by SPADE.== Details ==We will explain creating clear requirements using the language SMART notation which is part of the method SMART Requirements 2.0 followed by explaining how and what SPADE will automatically create from these requirements. We will also explain creating and running test cases and the typical architecture of the software solution that is created by SPADE.=== Creating clear requirements ===The input of SPADE are end result oriented business requirement specifications. These explain:This information is placed in a document, usually a file of some sort and is written down using a formal specification language. Below is an example in gray with explanation in italic text.Start by naming the process and its most important piece of information as its subjectProcess 'Order products' with subject #('Order': ORDER)Sum up the high level results. Double quotes are used to define requirements and help to create a result oriented break down structure.  The following applies:     "Customer has ordered products"     and     "Customer has an invoice if approved"     and     "The order needs to be approved if needed"Define the requirements clearly. Use if-then-else to define 'When' results should apply or should be produced. 'Where' the information is coming from is defined using references. For instance ORDER.approved is a piece of available information that is either produced during the process or is already an available piece of information. Some requirements (results) can be specified visually. To your right the "Customer has an invoice" is specified as an e-mail.  "Customer has an invoice if approved" =     if ORDER.approved then "Customer has an invoice" "The order needs to be approved if needed" =     if   "too expensive"     then "The order needs to be approved"     else ORDER.approved = true "too expensive" =     ORDER.total > 500A person can also be a source of information by stating 'input from' followed by a name that identifies the role of this person or the actual user. In the example below a person with the role CONTROLLER. If this persons in turn need information to be able to give this input, you need to state that this input can be given 'based on' certain other information. In this case the date, the BUYER and the LINES of the ORDER.  "The order needs to be approved" =     ORDER.approved = input from CONTROLLER based on #(ORDER.date, ORDER.BUYER, ORDER.LINES)The actual person that is giving the input at the time the system is used (the current user), can also be used as a piece of information. The example below defines the ORDER and its attributes. One if the attributes is called BUYER and this is filled with the actual CUSTOMER that (at the time the actual process runs) is playing that role, in other words giving the input.  "Customer has ordered products" =     One ORDER exists in ORDERS with:         date        = currentDate()         BUYER       = CUSTOMER         LINES       = "order lines" "order lines" =     Several LINE exist in ORDER_LINES with:         PRODUCT = input from CUSTOMER         number  = input from CUSTOMERThe requirements also require a business or logical data model. Most of the logical data model can be derived from the requirements. For instance it knows which entities are needed (ORDERS, ORDER_LINES and PRODUCST) and in some cases it also can derive the type of an attribute. For instance __approved__ can only be true or false because it is used as a condition and LINES should be a relation to ORDER_LINES. Some types however cannot be derived and need to be defined explicitly in this data model. Below is an example of this data model.   ORDERS =       date                 : date       BUYER                : USERS(1)       LINES                : ORDER_LINES(*) opposite of ORDER       approved             : boolean       total                : decimal(10,2) = sum(LINES.total)       summary              : text displayed = '{total} euro by {BUYER.firstName} {BUYER.lastName} on {date}'   ORDER_LINES =       PRODUCT     : PRODUCTS(1)       number      : integer       ORDER       : ORDERS(1) opposite of LINES       total       : decimal(10,2) = number * PRODUCT.price   PRODUCTS =       name         : text       price        : decimal(10,2)       summary      : text displayed = '{name} ({price} euro)'Most of this data model is pretty straight forward and resemble other data modelling techniques. Some things stand out:Relational attributes: relations are specified using relational attributes. For instance BUYER, which contains 1 instance in the standard entity USERS and LINES which contain multiple (*) instances of the entity ORDER_LINES and is the opposite of the relation ORDER (which is a relational attribute of the entity ORDER_LINES).Calculated attributes: attributes can be calculated which means they are not stored but calculated when needed. For instance the total of one instance of ORDERS is the sum of the total of its LINES. The summary is a textual value that is a template text with some placeholders inside that contain total, the first and last name of the BUYER and the date.Displayed: which means that if the system needs to render instances from ORDERS and it doesn't know how to do that, it will use the attribute marked with displayed.=== SPADE automates design and the creation of code ===SPADE perform the following steps: Parse: in other words read the business requirementsAnalyse dependencies: the dependencies between the different parts of the business requirements are analysed.Create process designs: an intelligent algorithm transform dependencies to process designs. It uses a set of design patterns and several optimization techniques to create an optimized process design that has no waste in it. The design is both a high level design (e.g. chains of business processes) as well as a low level design (e.g. at statement level).Generate sources: for the work flow and all the screens and steps in the process design.To your right is an example process design that was created by SPADE. The whole process is the business process, the yellow steps are the user interactive steps or the steps in which the system interacts with an external actor, for instance an external system. The blue steps are the fully automated steps. Example screen captures of the forms are added below the process diagram.=== Creating and running test cases ===When you are using the created solution, you are also recording test cases at the same time. Those test cases are then expanded with asserts that verify the outcome of the process. Below is an example in gray with explanation in italic text.Each test scenario starts with stating which process is started by which user. In this case process 'Order products' for user 'edwinh'.START_PROCESS = Order products, edwinhThe next part describes which roles and users will claim and then enter data in which task. In this case a customer with user name marcusk will enter 2 LINEs and each line will have a selected product and a number of products. The second task is for the manager with user name edwinh and he will fill approved with true.   # -------- FIRST CLAIM AND ENTER THE 1ST TASK ----------   task01.CLAIM_NEXT_GROUP_TASK = customer, marcusk   task01.LINEs = 2   task01.LINEs[0]-c-product = 1   task01.LINEs[0]-c-number = 10   task01.LINEs[1]-c-product = 2   task01.LINEs[1]-c-number = 20      # -------- FIRST CLAIM AND ENTER THE 2ND TASK ----------   task02.CLAIM_NEXT_GROUP_TASK = manager, edwinh   task02.approved = trueThe next part are the asserts the check if the process achieved the predicted end result. These are not recorded and need to be added manually. In this example we have added 2 asserts. The first checks if there is +1 more instance of ORDERS with attribute approved filled with TRUE. The second checks if +2 new instances of ORDER_LINES have been added.   ASSERT_PROCESS_VALUE_COUNT_01 = ORDERS.approved = TRUE, +1   ASSERT_PROCESS_ROW_COUNT_02   = ORDER_LINES, +2=== Deploying the solution ===SPADE can run on its own but it often runs as an Apache Maven plugin and is therefore part of a Maven build cycle. This build cycle also includes running the test scenarios, which in turndeploys the generated functionality as a .jar file,loads tests data,executes the test scenario's andverifies the result.The Maven build cycle can be used in daily builds all the way up to continuous delivery / deployment. For demo purposes, the steps mentioned can also be executed in the standard front-end of the resulting software solution. With the standard front end it is also possible to automate the following:analyze the existing database to check if the database already complies to the generated functionality;if there is no database present, a compliant database can be created automatically;if the database does not yet comply, tables and relations can be create or updated automatically.Migrating data from an old database or from the old release to the new release is also performed automated. However, the migration software (e.g. by using SQL or ETL) is created manually.Note that automation that SPADE provides during deployment is often used for smaller systems and for sprint demos. For deploying bigger projects, other more advanced deployment tools are more commonly used. === The resulting software solution ===The diagram to your right shows how SPADE relates to the created solution, as well as a global architecture of this solution. Below the different elements of the diagram are explained:SMART Business requirements: are (manually) gathered and documented using the requirements specification language SMART notation. This is a Domain-specific language that can be used to define information based end results that business or organizations would want to produce.Automatically creates designs, docs, source code: from the requirements SPADE then automatically creates designs, documentation and source code that can be compiled to the software solution.Users and GUI: the solution can interact with role based authorized users by different GUI's. The solution will already have standard GUI's for all functionality but can be expanded with Custom GUI's. GUI's of both types can be mixed if needed.REST/SOAP: all functionality will always have a matching REST or SOAP service that are used by the different GUI's but can also be used by authorized external systems.DBAL: the server also has a hibernate or similar database abstraction layer to communicate with the database.Plug-ins: can be used or added to the server to communicate with either external systems or with external devices. This enables solution is also able to use devices from the Internet Of Things domain. All plug-ins can be called upon from the business requirements but always in a non-technical way. For instance, if you define a DOCUMENT as a result, SPADE will know to call the plug-in associated with the entity DOCUMENTS. The plug-in will actually create and store a physical document.Specific functionality: this is the functionality that is created based upon the business requirements. With it you can create a wide variety of functionality. SPADE users can use a library of off-the-shelf requirements for example CRM, HR, profile matching and financial functionality. This can be inserted and adjusted to fit the specific needs of the client. The specific functionality can use all plug-ins as well as all generic functionality to extend the domain of available functionality.Generic functionality: by default, the solution is already equipped with a lot of standard generic functionality. For instance DMS, document generation, auto e-mails, SMS, messaging, advanced search, browse through data and export.=== Which software development activities are automated and which are manual? ===The next table shows which software development activities are automated by SPADE and which exceptions apply.== History ==2000: in 2000 Edwin Hendriks of the company CMG (company) (now part of CGI Group) developed a process improvement method called Process Acceleration. At the core of this method was a way to define the desired end result of a business process fully unambiguous as well as a structured approach to deduce the most optimal business process that would achieve this end result. This was the birth of the first version of SMART notation (at that time called PA notation) which is a formal language that can be used to specify end results of entire process chains (versus specifying the process chain itself). CMG (company) used this method and SMART notation for several of their projects and their clients.2007: although successful, CMG (company) at that time was not known for delivering process improvement consultancy. That was the reason for CMG (company) (at that time merged with Logica) to focus on the core of Process Acceleration, thus resulting in 2007 in a method that improves software development called PA SMART Requirements (now called SMART Requirements 2.0). Since that time SMART Requirements 2.0 has been used by CGI Group and their customers as well as other companies and organizations.2008: having an unambiguous definition of the end result of a process chain, and having a structured approach to deduce the most optimal process from this end result, sprung the idea to create a tool that could read the end result, deduce the most optimal process from it, and generate the software for each step in the process. Edwin Hendriks, Marcus Klimstra and Niels Bergsma developed a first working prototype of the SPADE (at that time called the PA Generator) using [.NET] and also producing systems using a [.NET] architecture.2010: Logica decided to start the development of a commercial usable version of the SPADE.2011: version 2.12 of the SPADE was used to create the first 2 systems that were made operational. Being a cross departmental time tracking system and an anonymous voting system both used by Logica itself.2012: version 3 of the SPADE was created. This was the first commercial usable version of the SPADE. From that time SPADE was used to create solutions for the clients. It was often used to recreate existing legacy systems because of the short time and cost associated when creating solutions using SPADE. Despite the increased development speed and low costs, SPADE still had teething problems. This made it difficult to estimate the actual time needed to (re)create solutions making it hard to plan projects. This was the same year that Logica was acquired by CGI Group.2015: version 4 of SPADE was used for the first time by elementary school children to create an exam system. It showed that creating SMART requirements and then asking SPADE to create a professional system for them was relatively easy when compared to other ways of creating software. In the same year a small rocket was launched which interacted with SPADE created ground station software. It showed that SPADE could in fact interact with external devices pretty fast (but still not yet fast enough to be usable to create real-time systems).2016: in version 4.4 SPADE most teething problems were solved making it possible to (re)create large and complex systems in a short time. SPADE is currently being expanded to provide an easier way to create and change requirements as well as an easier way to customize the standard GUI. This will make it possible for more non-developers to use SPADE to create solutions.== Advantages, disadvantages and considerations ==On the upside SPADE shows remarkable development speeds. International benchmarks show that the complete development cycle will be completed on average 20 times faster when compared to conventional techniques and in many cases it is even faster to completely recreate the functionality of existing software solutions compared to buying and configuring them. This development speed of course makes it easier for clients to see and try out the newly created solution. Of course by automating design and coding there will be almost no design and coding errors. The fact that the resulting solutions has no vendor-lock and is completely based on free to use open source components is also a big plus. Of course SPADE is also easy to learn.On the downside SPADE will remains a domain specific language and will therefore not be suitable for any type of functionality. This will require conventional development or other tools. Besides this real-time performance and the ability to change the GUI more easily is something that needs extra development. SPADE is also rather new and is not yet considered a mainstream development tool. Of course creating SMART requirements takes more effort and skill compared to just describing them in a couple of sentences.One should always consider that in normal software development the requirements define a fixed "contract" of the functionality that should be created. For instance the user story in a Scrum development team should also be fixed before the user story can be developed during a sprint. This is the same for SPADE projects. However, when the requirements or the user stories are ready to be developed, the sprint will be performed by SPADE and this will take only a couple of minutes. This has resulted in the tendency to move the requirements phase (the creation of the user stories) to the sprint. This is therefore considered to be a bad practice in both normal Agile development as well as Agile development using SPADE.Another consideration is that it is so easy to large and complex functionality. Although this poses no problem for SPADE, it does make it hard for certain people to handle the sheer size and complexity of the functionality of system. It is therefore advisable to still tackle size and complexity in the same way as you would in normal system development. By chopping up and structuring functionality in comprehensible pieces.== See also ==Disciplined Agile Delivery== References ==== External links ==
	Zet is a clone x86 processor where its machine code compatible with x86 processors developed as an effort to make open-hardware processor.The hardware design can be synthesized in a configurable device such an FPGA, CPLD, or on custom ASIC, and is considered as SoCAs of now, the project only supports 16-bit and able to run DOS-compatible or Windows 3.x operating systems.There has been no activity in the Zet project since 2013. == References ==== External links ==Zet Processorzet86 at OpenCores
	Software is programmed instructions stored in the memory of stored-program digital computers for execution by the processor.  Software is a recent development in human history, and it is fundamental to the Information Age.Charles Babbage's programs for his Analytical Engine in the 19th century is often considered the founder of the discipline, though both mathematicians' efforts remained theoretical only, as the technology of Babbage's day proved insufficient to build his computer. Alan Turing is credited with being the first person to come up with a theory for software in 1935, which led to the two academic fields of computer science and software engineering.The first generation of software for early stored-program digital computers in the late 1940s had its instructions written directly in binary code, generally written for mainframe computers.  Later, the development of modern programming languages alongside the advancement of the home computer would greatly widen  the scope and breadth of available software, beginning with assembly language, and continuing on through functional programming and object-oriented programming paradigms.== Before stored-program digital computers ===== Origins of computer science ===Computing as a concept goes back to ancient times, with devices such as the abacus, the Antikythera mechanism, and Al-Jazari's programmable castle clock. However, these devices were pure hardware and had no software - their computing powers were directly tied to their specific form and engineering.Software requires the concept of a general-purpose processor - what is now described as a Turing machine - as well as computer memory in which reusable sets of routines and mathematical functions comprising programs can be stored, started, and stopped individually, and only appears recently in human history.The first known computer algorithm was written by Charles Babbage in the 19th century for his planned Analytical Engine, to translate Luigi Menabrea's work on Bernoulli numbers for machine instruction. However, this remained theoretical only - the lesser state of engineering in the lifetime of these two mathematicians proved insufficient to construct the Analytical Engine.The first modern theory of software was proposed by Alan Turing in his 1935 essay Computable numbers with an application to the Entscheidungsproblem (decision problem).This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering is focused on more practical concerns.However, prior to 1946, software as we now understand it –  programs stored in the memory of stored-program digital computers –  did not yet exist. The very first electronic computing devices were instead rewired in order to "reprogram" them. The ENIAC, one of the first electronic computers, was programmed largely by women who had been previously working as human computers.  Engineers would give the programmers blueprints of the ENIAC wiring and expected them to figure out how to program the machine. The women who worked as programmers prepped the ENIAC for its first public reveal, wiring the patch panels together for the demonstrations.  Kathleen Booth developed Assembly Language in 1950 to make it easier to program the computers she worked on at Birkbeck College.Grace Hopper worked as one of the first programmers of the Harvard Mark I. She later created a 500 page manual for the computer. Hopper is often falsely credited with coining the terms "bug" and "debugging," when she found a moth in the Mark II, causing a malfunction; however, the term was in fact already in use when she found the moth. Hopper developed the first compiler and brought her idea from working on the Mark computers to working on UNIVAC in the 1950s. Hopper also developed the programming language FLOW-MATIC to program the UNIVAC. Frances E. Holberton, also working at UNIVAC, developed a code, C-10, which let programmers use keyboard inputs and created the Sort-Merge Generator in 1951. Adele Mildred Koss and Hopper also created the precursor to a report generator.== Early days of computer software (1948–1979) ==In his manuscript "A Mathematical Theory of Communication", Claude Shannon (1916–2001) provided an outline for how binary logic could be implemented to program a computer.  Subsequently, the first computer programmers used binary code to instruct computers to perform various tasks.  Nevertheless, the process was very arduous. Computer programmers had to provide long strings of binary code to tell the computer what data to store. Code and data had to be loaded onto computers using various tedious mechanisms, including flicking switches or punching holes at predefined positions in cards and loading these punched cards into a computer. With such methods, if a mistake was made, the whole program might have to be loaded again from the beginning.The very first time a stored-program computer held a piece of software in electronic memory and executed it successfully, was 11 am 21 June 1948, at the University of Manchester, on the Manchester Baby computer.  It was written by Tom Kilburn, and calculated the highest factor of the integer 2^18 = 262,144. Starting with a large trial divisor, it performed division of 262,144 by repeated subtraction then checked if the remainder was zero. If not, it decremented the trial divisor by one and repeated the process.  Google released a tribute to the Manchester Baby, celebrating it as the "birth of software". In the late 1950s and early 1960s, a popular innovation was the development of computer languages such as Fortran, COBOL and BASIC. These languages allowed programs to be specified in an abstract way, independent of the precise details of the hardware architecture of the computer. The languages were primarily intended only for specifying numerical calculations.COBOL was first conceived of when Mary K. Hawes convened a meeting (which included Grace Hopper) in 1959 to discuss how to create a computer language to be shared between businesses. Hopper's innovation with COBOL was developing a new symbolic way to write programming. Her programming was self-documenting. Betty Holberton helped edit the language which was submitted to the Government Printing Office in 1960. FORMAC was developed by Jean E. Sammet in the 1960s. Her book, Programming Languages: History and Fundamentals (1969), became an influential text.=== Apollo Mission ===The Apollo Mission to the moon depended on software to program the computers in the landing modules.  The computers were programmed with a language called "Basic" (no relation with the BASIC  programming language developed at Dartmouth at about the same time). The software also had an interpreter which was made up of a series of routines and an executive (like a modern-day operating system), which specified which programs to run and when. Both were designed by Hal Laning. Margaret Hamilton, who had previously been involved with software reliability issues when working on the US SAGE air defense system, was also part of the Apollo software team. Hamilton was in charge of the onboard flight software for the Apollo computers. Hamilton felt that software operations were not just part of the machine, but also intricately involved with the people who operated the software. Hamilton also coined the term "software engineering" while she was working at NASA.The actual "software" for the computers in the Apollo missions was made up of wires that were threaded through magnetic cores. Where the wire went through a magnetic core, that represented a "1" and where the wire went around the core, that represented a "0." Each core stored 64 bits of information. Hamilton and others would create the software by punching holes in punch cards, which were then later processed on a Honeywell mainframe where the software could be simulated. When the code was "solid," then it was sent to be woven into the magnetic cores at Raytheon, where women known as "Little Old Ladies" worked on the wires. The program itself was "indestructible" and could even withstand lightning strikes, which happened to Apollo 12. Wiring the computers took several weeks to do, freezing software development during that time.While using the simulators to test the programming, Hamilton discovered ways that code could produce dangerous errors when human mistakes were made while using it. NASA believed that the astronauts would not make mistakes due to their training. Hamilton was not allowed to program code to prevent errors that would lead to system crash, so she annotated the code in the program documentation. Her ideas to add error-checking code was rejected as "excessive." However, exactly what Hamilton predicted would happen occurred on the Apollo 8 flight, when human error caused the computer to wipe out all of the navigational data.=== Bundling of software with hardware and its legal issues ===Later, software was sold to multiple customers by being bundled with the hardware by original equipment manufacturers (OEMs) such as Data General, Digital Equipment and IBM. When a customer bought a minicomputer, at that time the smallest computer on the market, the computer did not come with Pre-installed software, but needed to be installed by engineers employed by the OEM.This bundling attracted the attention of US antitrust regulators, who sued IBM for improper "tying" in 1969, alleging that it was an antitrust violation that customers who wanted to obtain its software had to also buy or lease its hardware in order to do so. However, the case was dropped by the US Justice Department, after many years of attrition, as it concluded it was "without merit".Data General also encountered legal problems related to bundling –  although in this case, it was due to a civil suit from a would-be competitor. When Data General introduced the Data General Nova, a company called Digidyne wanted to use its RDOS operating system on its own hardware clone. Data General refused to license their software and claimed their "bundling rights". The US Supreme Court set a precedent called Digidyne v. Data General in 1985 by letting a 9th circuit appeal court decision on the case stand, and Data General was eventually forced into licensing the operating system because it was ruled that restricting the license to only DG hardware was an illegal tying arrangement. Even though the District Court noted that "no reasonable juror could find that within this large and dynamic market with much larger competitors", Data General "had the market power to restrain trade through an illegal tie-in arrangement", the tying of the operating system to the hardware was ruled as per se illegal on appeal.In 2008, Psystar Corporation was sued by Apple Inc. for distributing unauthorized Macintosh clones with OS X preinstalled, and countersued. One of the arguments in the countersuit - citing the Data General case - was that Apple dominates the market for OS X compatible computers by illegally tying the operating system to Apple computers. District Court Judge William Alsup rejected this argument, saying, as the District Court had ruled in the Data General case over 20 years prior, that the relevant market was not simply one operating system (Mac OS) but all PC operating systems, including Mac OS, and noting that Mac OS did not enjoy a dominant position in that broader market. Alsup's judgement also noted that the surprising Data General precedent that tying of copyrighted products was always illegal had since been "implicitly overruled" by the verdict in the Illinois Tool Works Inc. v. Independent Ink, Inc. case.=== Packaged software (Late 1960s-present) ===An industry producing independently packaged software - software that was neither produced as a "one-off" for an individual customer, nor "bundled" with computer hardware - started to develop in the late 1960s.=== Unix (1970s–present) ===Unix was an early operating system which became popular and very influential, and still exists today. The most popular variant of Unix today is macOS (previously called OS X and Mac OS X), while Linux is closely related to Unix.=== The rise of Microcomputers ===In January 1975, Micro Instrumentation and Telemetry Systems began selling its Altair 8800 microcomputer kit by mail order.  Microsoft released its first product Altair BASIC later that year, and hobbyists began developing programs to run on these kits.  Tiny BASIC was published as a type-in program in Dr. Dobb's Journal, and developed collaboratively.in 1976, Peter R. Jennings for instance created his Microchess program for MOS Technology's KIM-1 kit, but since it did not come with a tape drive, he would send the source code in a little booklet to his mail-order customers, and they would have to type the whole program in by hand.  In 1978, Kathe and Dan Spracklen released the source of their Sargon (chess) program in a computer magazine.  Jennings later switched to selling paper tape, and eventually compact cassettes with the program on it.It was an inconvenient and slow process to type in source code from a computer magazine, and a single mistyped –  or worse, misprinted –  character could render the program inoperable, yet people still did so. (Optical character recognition technology, which could theoretically have been used to scan in the listings rather than transcribe them by hand, was not yet in wide use.)Even with the spread of cartridges and cassette tapes in the 1980s for distribution of commercial software, free programs (such as simple educational programs for the purpose of teaching programming techniques) were still often printed, because it was cheaper than making and attaching cassette tapes to magazines.However, eventually a combination of four factors brought this practice of printing complete source code listings of entire programs in computer magazines to an end:programs started to become very largefloppy discs started to be used for distributing software, and then came down in priceregular people started to use computers –  and wanted a simple way to run a programcomputer magazines started to include cassette tapes or floppy discs with free or trial versions of software on themVery quickly, commercial software started to be pirated, and commercial software producers were very unhappy at this. Bill Gates, cofounder of Microsoft, was an early moraliser against software piracy with his famous Open Letter to Hobbyists in 1976.== 1980s–present ==Before the microcomputer, a successful software program typically sold up to 1,000 units at $50,000–60,000 each. By the mid-1980s, personal computer software sold thousands of copies for $50–700 each. Companies like Microsoft, MicroPro, and Lotus Development had tens of millions of dollars in annual sales. They similarly dominated the European market with localized versions of already successful products.A pivotal moment in computing history was the publication in the 1980s of the specifications for the IBM Personal Computer published by IBM employee Philip Don Estridge, which quickly led to the dominance of the PC in the worldwide desktop and later laptop markets –  a dominance which continues to this day. Microsoft, by successfully negotiating with IBM to develop the first operating system for the PC (MS-DOS), profited enormously from the PC's success over the following decades, via the success of MS-DOS and its add-on-cum-successor, Microsoft Windows. Winning the negotiation was a pivotal moment in Microsoft's history.=== Free and open source software ====== Recent developments ======= App stores ====Applications for mobile devices (cellphones and tablets) have been termed "apps" in recent years. Apple chose to funnel iPhone and iPad app sales through their App Store, and thus both vet apps, and get a cut of every paid app sold. Apple does not allow apps which could be used to circumvent their app store (e.g. virtual machines such as the Java or Flash virtual machines).The Android platform, by contrast, has multiple app stores available for it, and users can generally select which to use (although Google Play requires a compatible or rooted device).This move was replicated for desktop operating systems with GNOME Software (for Linux), the Mac App Store (for macOS), and the Windows Store (for Windows). All of these platforms remain, as they have always been, non-exclusive: they allow applications to be installed from outside the app store, and indeed from other app stores.The explosive rise in popularity of apps, for the iPhone in particular but also for Android, led to a kind of "gold rush", with some hopeful programmers dedicating a significant amount of time to creating apps in the hope of striking it rich. As in real gold rushes, not all of these hopeful entrepreneurs were successful.== Formalization of software development ==The development of curricula in computer science has resulted in improvements in software development. Components of these curricula include:Structured and Object Oriented programmingData structuresAnalysis of AlgorithmsFormal languages and compiler constructionComputer Graphics AlgorithmsSorting and SearchingNumerical Methods, Optimization and StatisticsArtificial Intelligence and Machine Learning== How software has affected hardware ==As more and more programs enter the realm of firmware, and the hardware itself becomes smaller, cheaper and faster as predicted by Moore's law, an increasing number of types of functionality of computing first carried out by software, have joined the ranks of hardware, as for example with graphics processing units. (However, the change has sometimes gone the other way for cost or other reasons, as for example with softmodems and microcode.)Most hardware companies today have more software programmers on the payroll than hardware designers, since software tools have automated many tasks of printed circuit board (PCB) engineers.== Computer software and programming language timeline ==The following tables include year by year development of many different aspects of computer software including:High level languagesOperating systemsNetworking software and applicationsComputer graphics hardware, algorithms and applicationsSpreadsheetsWord processingComputer aided design=== 1971–1974 ====== 1975–1978 ====== 1979–1982 ====== 1983–1986 ====== 1987–1990 ====== 1991–1994 ====== 1995–1998 ====== 1999–2002 ====== 2003–2006 ====== 2007–2010 ====== 2011–2014 ===== See also ==Forensic software engineeringHistory of computing hardwareHistory of operating systemsHistory of software engineeringList of failed and overbudget custom software projectsWomen in computingTimeline of women in computing== References ===== Sources ===Ceruzzi, Paul E. (1998). History of Computing. Cambridge, Massachusetts: MIT Press. ISBN 9780262032551 – via EBSCOhost.Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.Gürer, Denise (1995). "Pioneering Women in Computer Science" (PDF). Communications of the ACM. 38 (1): 45–54. doi:10.1145/204865.204875.Light, Jennifer S. (1999). "When Computers Were Women". Technology and Culture. 40 (3): 455–483. JSTOR 25147356.Mindell, David A. (2008). Digital Apollo: Human and Machine in Spaceflight. Cambridge, Massachusetts: The MIT Press. ISBN 9780262266680.Smith, Erika E. (2013). "Recognizing a Collective Inheritance through the History of Women in Computing". CLCWeb: Comparative Literature & Culture: A WWWeb Journal. 15 (1): 1–9. doi:10.7771/1481-4374.1972 – via EBSCOhost.
	Wide Angle Software is a United Kingdom-based software development company, headquartered in Stafford, England. The company was founded in 2006. Wide Angle Software is known for producing media management software and applications including software for PC, Mac, iOS and Android. The company develops software for manipulating digital media on portable media devices such as iPods, iPhones and iPads. In particular, Wide Angle Software's TouchCopy and iBackup Extractor which offer solutions for backup and retrieval of lost or deleted iPhone music and messages have been highly rated and reviewed by multiple technology industry magazine and websites, including PCWorld, Lifewire, CNET and Standard-Examiner. Another Mac and Windows App developed by Wide Angle Software is TuneSweeper, which enables users to clean their iTunes libraries of duplicate and missing music. TuneSweeper was featured in Macworld and MacUpdate.== See also ==SymantecTouchCopyUltraBac== References ==== External links ==Official website
	Critical code studies (CCS) is an emerging academic subfield, related to software studies, digital humanities, cultural studies, computer science, human-computer interface, and the do-it-yourself maker culture. Its primary focus is on the cultural significance of computer code, without excluding or focusing solely upon the code's functional purpose.As introduced by Mark C. Marino, critical code studies was initially a method by which scholars "can read and explicate code the way we might explicate a work of literature," but the concept also draws upon Espen Aarseth's conception of a cybertext as a "mechanical device for the production and consumption of verbal signs" (Cybertext, 21), arguing that in order to understand a digital artifact we must also understand the constraints and capabilities of the authoring tools used by the creator of the artifact, as well as the memory storage and interface required for the user to experience the digital artifact.Evidence that critical code studies has gained momentum since 2006 include an article by Matthew Kirschenbaum in the Chronicle of Higher Education, CCS sessions at the Modern Language Association in 2011 that were "packed" with attendees, several academic conferences devoted wholly to critical code studies, and a book devoted to the explication of a single line of computer code, titled 10 PRINT CHR$(205.5+RND(1)); : GOTO 10.== See also ==Software studies== References ==== Bibliography ==Black, M. J, (2002) The Art of Code. PhD dissertation, University of Pennsylvania.Berry, D. M. (2011) The Philosophy of Software: Code and Mediation in the Digital Age, Basingstoke: Palgrave Macmillan. [1]Berry, D. M. (2008) Copy, Rip, Burn: The Politics of Copyleft and Open Source, London: Pluto Press.Chopra, S. and Dexter, S. (2008) Decoding Liberation: The Promise of Free and Open Source Software. Oxford: Routledge.Chun, W. H. K. (2008) ‘On “Sourcery,” or Code as Fetish’, Configurations, 16:299–324.Chun, W. H. K. (2011) Programmed Visions: Software and Memory, MIT Press.Fuller, M. (2003) Behind the Blip: Essays on the Culture of Software. London: Autonomedia.Fuller, M. (2008) Software Studies\A Lexicon. London: MIT Press.Hayles, N. K. (2004) ‘Print Is Flat, Code Is Deep: The Importance of Media-Specific Analysis’, Poetics Today, 25(1): 67–90.Heim, M. (1987) Electric Language: A Philosophical Discussion of Word Processing. London: Yale University Press.Kirschenbaum, M. (2004) ‘Extreme Inscription: Towards a Grammatology of the Hard Drive’, TEXT Technology, No. 2, pp. 91–125.Kirschenbaum, M. (2008) Mechanisms: New Media and the Forensic Imagination, MIT Press.Kitchin, R. and Dodge, M. (2011) Code/Space: Software and Everyday Life, MIT Press.Kittler, F. (1997). Literature, Media, Information Systems, Johnston, J. (ed.). Amsterdam: OPA.Kittler, F. (1999) Gramophone, Film, Typewriter. Stanford: Stanford University Press.Mackenzie, A. (2003) The problem of computer code: Leviathan or common power, retrieved 13/03/2010 from http://www.lancs.ac.uk/staff/mackenza/papers/code-leviathan.pdfMackenzie, A. (2006) Cutting Code: Software and Sociality, Oxford: Peter Lang.Manovich, L. (2001) The Language of New Media. London: MIT Press.Manovich, L. (2008) Software takes Command, retrieved 03/05/2010 from https://web.archive.org/web/20110127183751/http://lab.softwarestudies.com/2008/11/softbook.htmlManovich, L. and Douglas, J. (2009) Visualizing Temporal Patterns In Visual Media: Computer Graphics as a Research Method, retrieved 10/10/09 from http://softwarestudies.com/cultural_analytics/visualizing_temporal_patterns.pdfMarino, M. C. (2006) Critical Code Studies, Electronic Book Review, accessed 16 Sept 2011, http://www.electronicbookreview.com/thread/electropoetics/codologyMontfort, N. and Bogost, I. (2009) Racing the Beam: The Atari Video Computer System, London: MIT Press.Wardrip-Fruin, N. (2011) Expressive Processing. London: MIT Press.
	Software studies is an emerging interdisciplinary research field, which studies software systems and their social and cultural effects.== Overview ==The implementation and use of software has been studied in recent fields such as cyberculture, Internet studies, new media studies, and digital culture, yet prior to software studies, software was rarely ever addressed as a distinct object of study.Software studies is an interdisciplinary field. To study software as an artifact, it draws upon methods and theory from the digital humanities and from computational perspectives on software. Methodologically, software studies usually differs from the approaches of computer science and software engineering, which concern themselves primarily with software in information theory and in practical application; however, these fields all share an emphasis on computer literacy, particularly in the areas of programming and source code. This emphasis on analyzing software sources and processes (rather than interfaces) often distinguishes software studies from new media studies, which is usually restricted to discussions of interfaces and observable effects.== History ==The conceptual origins of software studies include Marshall McLuhan's focus on the role of media in themselves, rather than the content of media platforms, in shaping culture. Early references to the study of software as a cultural practice appear in Friedrich Kittler's essay, "Es gibt keine Software," Lev Manovich's Language of New Media, and Matthew Fuller's Behind the Blip: Essays on the culture of software. Much of the impetus for the development of software studies has come from videogame studies, particularly platform studies, the study of videogames and other software artifacts in their hardware and software contexts. New media art, software art, motion graphics, and computer-aided design are also significant software-based cultural practices, as is the creation of new protocols and platforms.The first conference events in the emerging field were Software Studies Workshop 2006 and SoftWhere 2008.In 2008, MIT Press launched a Software Studies book series with an edited volume of essays (Matthew Fuller's "Software Studies: a Lexicon"), and the first academic program was launched, (Lev Manovich, Benjamin H. Bratton and Noah Wardrip-Fruin's "Software Studies Initiative" at U. California San Diego).In 2011, a number of mainly British researchers established Computational Culture, an open-access peer-reviewed journal. The journal provides a platform for "inter-disciplinary enquiry into the nature of the culture of computational objects, practices, processes and structures."== Related fields ==Software studies is closely related to a number of other emerging fields in the digital humanities that explore functional components of technology from a social and cultural perspective.  Software studies' focus is at the level of the entire program, specifically the relationship between interface and code.  Notably related are critical code studies, which is more closely attuned to the code rather than the program, and platform studies, which investigates the relationships between hardware and software.== See also ==Critical code studiesDigital HumanitiesCultural studiesNew mediaComputer scienceSoftware engineeringDigital sociology== References ==== Bibliography ==Bassett, C. (2007) The Arc and the Machine: Narrative and New Media. Manchester:Manchester University Press.Black, M. J, (2002) The Art of Code. PhD dissertation, University ofPennsylvania.Berry, D. M. (2011) The Philosophy of Software: Code and Mediation in the Digital Age, Basingstoke: Palgrave Macmillan.Berry, D. M. (2008) Copy, Rip, Burn: The Politics of Copyleft and Open Source, London: Pluto Press.Chopra, S. and Dexter, S. (2008) Decoding Liberation: The Promise of Free and Open Source Software. Oxford: Routledge.Chun, W. H. K. (2008) ‘On “Sourcery,” or Code as Fetish’, Configurations, 16:299–324.Fuller, M. (2003) Behind the Blip: Essays on the Culture of Software. London: Autonomedia.Fuller, M. (2006) Software Studies Workshop, retrieved 13/04/2010Fuller, M. (2008) Software Studies: A Lexicon. London: MIT Press.Hayles, N. K. (2004) ‘Print Is Flat, Code Is Deep: The Importance of Media-Specific Analysis’, Poetics Today, 25(1): 67–90.Heim, M. (1987) Electric Language: A Philosophical Discussion of Word Processing. London: Yale University Press.Kirschenbaum, M. (2004) ‘Extreme Inscription: Towards a Grammatology of the Hard Drive’, TEXT Technology, No. 2, pp. 91–125.Kittler, F. (1997). Literature, Media, Information Systems, Johnston, J. (ed.). Amsterdam: OPA.Kittler, F. (1999) Gramophone, Film, Typewriter. Stanford: Stanford University Press.Mackenzie, A. (2003) The problem of computer code: Leviathan or common power, retrieved 13/03/2010 from http://www.lancs.ac.uk/staff/mackenza/papers/code-leviathan.pdfMackenzie, A. (2006) Cutting Code: Software and Sociality, Oxford: Peter Lang.Manovich, L. (2001) The Language of New Media. London: MIT Press.Manovich, L. (2008) Software takes Command, first draft released under CC license, retrieved 03/05/2010Manovich, L. (2013) Software takes Command, London and New York: Bloomsbury Academic.Manovich, L. and Douglas, J. (2009) Visualizing Temporal Patterns In Visual Media: Computer Graphics as a Research Method, retrieved 10/10/09Marino, M. C. (2006) Critical Code Studies, Electronic Book Review, accessed 16 Sept 2011Montfort, N. and Bogost, I. (2009) Racing the Beam: The Atari Video Computer System, London: MIT Press.Wardrip-Fruin, N. (2011) Expressive Processing. London: MIT Press.== External links ==Software studies bibliography at Monoskop.org
	Precobs is a predictive policing-software using algorithms and knowledge about crimes committed in the past to predict the commitment of so-called "near repeat"-crimes. Precobs is an abbreviation and stands for Pre Crime Observation System. It is developed and sold by the Institut für musterbasierte Prognosetechnik (Institute for pattern-based Prediction Technique) – IfmPt – located in Oberhausen, Germany.== The concept of near repeat-prediction ==Precobs is used to forecast the commitment of "near repeat crimes", at the moment basically for the burglary prevention. The knowledge about near repeats bases on the experience that crimes of the below mentioned categories are often not committed only once, but several times within a close geographical and temporal context, the so-called spatiotemporal proximity  (see also Crime Contagion Models). Near repeat crimes are typically repeated within 72 hours.Apart from burglary, those crimes can be:Street robbery,armed robbery andmotor vehicle theft.Behind the concept of near repeats stands the empirical observation that "crime clusters in space and time".  Different international studies about near repeat burglary have revealed patterns in the geographical and temporal connection of committed break-in´s.  The highest risk of a near repeat exists within 48 hours after the first crime. Afterwards, it remains for approximately one month. Johnsons and Bowers concluded after the analysis of burglary data from Merseyside, UK:“The central conclusion is that a burglary event is a predictor of significantly elevated rates of burglary within 1–2 months and within a range of up to 300–400 metres of a burgled home."Two approaches try to explain this phenomenon: According to the "boost hypothesis", a past victimization "boosts" the probability of becoming a victim again. It is argued that the perpetrator returns to the place of the first crime to make use of his knowledge about this area.  In contrast, the "flag hypothesis" refers to the attractiveness of the target. Perpetrators will generally focus on attractive targets. == Application and advantages ==The Precobs-software is based on the existing knowledge about burglaries of the past. It uses so-called "triggers" and "anti-triggers" to determine the probability of near repeats. Trigger criteria are characteristics about the site of crime, the way of how the crime was committed (modus operandi), the stolen items and the date of the crime.  If certain trigger criteria are fulfilled, the system qualifies the crime as a potential near repeat. The existence of anti-triggers, in contrast, prevents the system from near repeat alerts.  This can be, for example, the intentional breaking of window glass. Such a modus operandi would indicate a non-professional background which lowers the likelihood of a near repeat significantly. Furthermore, past success rates of the software play an important role.The software analyses this data and predicts future crimes in a geographical radius of 250m and in a time window of between 24 hours and 7 days.The forecast is depicted by a map which contains spatial and temporal information.  In past applications, Precobs could reach a prediction accuracy of around 80%.  Correspondingly, authorities which use Precobs, do not have to deploy staff for the time-consuming research for near repeat crimes or burglary series anymore.First, the Precobs-warning is reassessed by a human police officer. Then, authorities can use this information to send out patrols much more specifically to prevent expected future crimes. The police forces can operate undercover to observe suspects or avoid crimes by patrolling the relevant areas. Precobs warnings can be subject of police internal use only. However, the data can also be used to inform the public about areas in which future crimes are expected. The Police in Aargau, Switzerland, for example, uses a mobile app to publish certain Precobs warnings.  The so-called KaPo-App provides the users with push messages which speak out alerts for specific areas or villages.  The KaPo-App can be downloaded by everyone.Precobs was tested first in the Swiss cantons Zurich, Baselland and Aargau and in the Bavarian cities Munich and Nuremberg. Its use is also considered by the German state of Berlin.  According to statements of the Zurich Police authority, numbers of burglary could be reduced by 30% compared to the previous year.  Similar results could be achieved in certain areas of Bavaria, where the numbers decreased by between 17.5 and 42%.On 24 June 2015, the Bavarian Ministry of the Interior, for Building and Transport announced the permanent use of predictive policing-software.== Criticism and disadvantages ==The use of Precobs has risen different questions and concerns.One of the main concerns of predictive policing software in general and hence of Precobs refers to the use of data the software needs to forecast any crimes. Since the software does not use any personal information and the final decision is always met by a police officer, the Bavarian Data Security Officer has qualified Precobs as unproblematic.  However, critics of Precobs refer to the possibility of enriching the near repeat prediction process with data deriving from the internet of things and social networks.  For example, a 2014 published study suggests that the use of real time Twitter messages could improve automated crime prediction significantly.Other authors believe that the use of Precobs or comparable systems could be a justification for authorities to broadly expand the collection of data.Furthermore, critics question the reliability of the data produced by predictive policing systems. Generally, the causal link between the reduction in crime numbers and the use of predictive policing software has been doubted. When tested in Nuremberg, Germany, for example, numbers of break-ins first decreased in the typically burglary-intensive autumn, but then went up again in December. One of the often articulated concerns in this context is that authorities might tend to rely too easily on software predictions instead of their grown experience.Moreover, Precobs´ predictions base on the assumption that crimes which were committed incidentally do not possess a relevant probability of a near repeat.  Accordingly, the software can only forecast burglaries committed by professionally acting criminals. Hence, the scope of the system is limited.The system depends on the data fed in by the authorities. With regard to a high dark figure of burglaries, the use of systems like Precobs might narrow down the focus of the authorities too much on a specific area. In addition, especially professional criminals could adapt to the system. Knowing that authorities use a predictive policing software, criminals could switch to other geographical areas or adopt a specific behavior to mislead the software. This could finally lead to a "run" of mutual behavior prediction.A further problem arises, when the data produced by the software is published like in the case of the Aargau Police App. The publication of such information could alienate the population and create a mood of fear.== References ==
	Software visualization or software visualisation refers to the visualization of information of and related to software systems—either the architecture of its source code or metrics of their runtime behavior- and their development process by means of static, interactive or animated 2-D or 3-D visual representations of their structure, execution, behavior, and evolution.== Software system information ==Software visualization uses a variety of information available about software systems. Key information categories include: implementation artifacts such as source codes,software metric data from measurements or from reverse engineering,traces that record execution behavior,software testing data (e.g., test coverage)software repository data that tracks changes.== Objectives ==The objectives of software visualization are to support the understanding of software systems (i.e., its structure) and algorithms (e.g., by animating the behavior of sorting algorithms) as well as the analysis and exploration of software systems and their anomalies (e.g., by showing classes with high coupling) and their development and evolution. One of the strengths of software visualization is to combine and relate information of software systems that are not inherently linked, for example by projecting code changes onto software execution traces.Software visualization can be used as tool and technique to explore and analyze software system information, e.g., to discover anomalies similar to the process of visual data mining. For example, software visualization is used to monitoring activities such as for code quality or team activity. Visualization is not inherently a method for software quality assurance. Software visualization participates to Software Intelligence in allowing to discover and take advantage of mastering inner components of software systems.== Types ==Tools for software visualization might be used to visualize source code and quality defects during software development and maintenance activities. There are different approaches to map source code to a visual representation such as by software maps Their objective includes, for example, the automatic discovery and visualization of quality defects in object-oriented software systems and services. Commonly, they visualize the direct relationship of a class and its methods with other classes in the software system and mark potential quality defects. A further benefit is the support for visual navigation through the software system.More or less specialized graph drawing software is used for software visualization. A small-scale 2003 survey of researchers active in the reverse engineering and software maintenance fields found that a wide variety of visualization tools were used, including general purpose graph drawing packages like GraphViz and GraphEd, UML tools like Rational Rose and Borland Together, and more specialized tools like Visualization of Compiler Graphs (VCG) and Rigi. The range of UML tools that can act as a visualizer by reverse engineering source is by no means short; a 2007 book noted that besides the two aforementioned tools, ESS-Model, BlueJ, and Fujaba also have this capability, and that Fujaba can also identify design patterns.== See also ==ProgramsImagix 4DNDependSotoarcSourcetrail[1]Softagram[2]Getaviz[3]SonarGraph[4]Related conceptsApplication discovery and understandingSoftware maintenanceSoftware mapsSoftware diagnosisCognitive dimensions of notationsSoftware archaeology== References ==== Further reading ==Roels, R., Mestereaga, P., and Signer, B. (2016). "An Interactive Source Code Visualisation Plug-in for the MindXpres Presentation Platform". Communications in Computer and Information Science (CCIS), 583, 2016Burch, M., Diehl, S., and Weißgerber, P. (2005). Visual data mining in software archives. Proceedings of the 2005 ACM symposium on Software visualization (SoftVis '05). ACM, New York, NY, USA, 37-46. doi:10.1145/1056018.1056024Diehl, S. (2002). Software Visualization. International Seminar. Revised Papers (LNCS Vol. 2269), Dagstuhl Castle, Germany, 20–25 May 2001 (Dagstuhl Seminar Proceedings).Diehl, S. (2007). Software Visualization — Visualizing the Structure, Behaviour, and Evolution of Software. Springer, 2007, ISBN 978-3-540-46504-1Eades, P. and Zhang, K. (1996). "Software Visualisation", Series on Software Engineering and Knowledge Engineering, Vol.7, World Scientific Co., Singapore, 1996, ISBN 981-02-2826-0, 268 pages.Gîrba, T., Kuhn, A., Seeberger, M., and Ducasse, S., "How Developers Drive Software Evolution," Proceedings of International Workshop on Principles of Software Evolution (IWPSE 2005), IEEE Computer Society Press, 2005, pp. 113–122. PDFKeim, D. A. (2002). Information visualization and visual data mining. IEEE Transactions on Visualization and Computer Graphics, USA * vol 8 (Jan. March 2002), no 1, p 1 8, 67 refs.Knight, C. (2002). System and Software Visualization. In Handbook of software engineering & knowledge engineering. Vol. 2, Emerging technologies (Vol. 2): World Scientific Publishing Company.Kuhn, A., and Greevy, O., "Exploiting the Analogy Between Traces and Signal Processing," Proceedings IEEE International Conference on Software Maintenance (ICSM 2006), IEEE Computer Society Press, Los Alamitos CA, September 2006. PDFLanza, M. (2004). CodeCrawler — polymetric views in action. Proceedings. 19th International Conference on Automated Software Engineering, Linz, Austria, 20 24 Sept. 2004 * Los Alamitos, CA, USA: IEEE Comput. Soc, 2004, p 394 5.Lopez, F. L., Robles, G., & Gonzalez, B. J. M. (2004). Applying social network analysis to the information in CVS repositories. "International Workshop on Mining Software Repositories (MSR 2004)" W17S Workshop 26th International Conference on Software Engineering, Edinburgh, Scotland, UK, 25 May 2004 * Stevenage, UK: IEE, 2004, p 101 5.Marcus, A., Feng, L., & Maletic, J. I. (2003). 3D representations for software visualization. Paper presented at the Proceedings of the 2003 ACM symposium on Software visualization, San Diego, California.Soukup, T. (2002). Visual data mining : techniques and tools for data visualization and mining. New York: Chichester.Staples, M. L., & Bieman, J. M. (1999). 3-D Visualization of Software Structure. In Advances in Computers (Vol. 49, pp. 96–143): Academic Press, London.Stasko, J. T., Brown, M. H., & Price, B. A. (1997). Software Visualization: MIT Press.Van Rysselberghe, F. (2004). Studying Software Evolution Information By Visualizing the Change History. Proceedings. 20th International Conference On Software Maintenance. pp 328–337, IEEE Computer Society Press, 2004Wettel, R., and Lanza, M., Visualizing Software Systems as Cities. In Proceedings of VISSOFT 2007 (4th IEEE International Workshop on Visualizing Software For Understanding and Analysis), pp. 92 – 99, IEEE Computer Society Press, 2007.Zhang, K. (2003). "Software Visualization - From Theory to Practice". Kluwer Academic Publishers, Boston, April 2003, ISBN 1-4020-7448-4, 468 pages.== External links ==SoftVis the ACM Symposium on Software VisualizationVISSOFT 2nd IEEE Working Conference on Software VisualizationEPDV Eclipse Project Dependencies Viewer
	7z is a compressed archive file format that supports several different data compression, encryption and pre-processing algorithms.  The 7z format initially appeared as implemented by the 7-Zip archiver.  The 7-Zip program is publicly available under the terms of the GNU Lesser General Public License.  The LZMA SDK 4.62 was placed in the public domain in December 2008.  The latest stable version of 7-Zip and LZMA SDK is version 19.00.The 7z file format specification is distributed with 7-Zip's source code. The specification can be found in plain text format in the 'doc' sub-directory of the source code distribution.== Features and enhancements ==The 7z format provides the following main features:Open, modular architecture that allows any compression, conversion, or encryption method to be stacked.High compression ratios (depending on the compression method used)AES-256 encryption.Large file support (up to approximately 16 exbibytes, or 264 bytes).Unicode file names.Support for solid compression, where multiple files of like type are compressed within a single stream, in order to exploit the combined redundancy inherent in similar files.Compression and encryption of archive headers.Support for multi-part archives : e.g. xxx.7z.001, xxx.7z.002, ... (see the context menu items Split File... to create them and Combine Files... to re-assemble an archive from a set of multi-part component files)Support for custom codec plugin DLLs.The format's open architecture allows additional future compression methods to be added to the standard.=== Compression methods ===The following compression methods are currently defined:LZMA – A variation of the LZ77 algorithm, using a sliding dictionary up to 4 GB in length for duplicate string elimination.  The LZ stage is followed by entropy coding using a Markov chain-based range coder and binary trees.LZMA2 – modified version of LZMA providing better multithreading support and less expansion of incompressible data.Bzip2 – The standard Burrows–Wheeler transform algorithm.  Bzip2 uses two reversible transformations; BWT, then Move to front with Huffman coding for symbol reduction (the actual compression element).PPMd – Dmitry Shkarin's 2002 PPMdH (PPMII/cPPMII) with small changes: PPMII is an improved version of the 1984 PPM compression algorithm (prediction by partial matching).DEFLATE – Standard algorithm based on 32 kB LZ77 and Huffman coding.  Deflate is found in several file formats including ZIP, gzip, PNG and PDF.  7-Zip contains a from-scratch DEFLATE encoder that frequently beats the de facto standard zlib version in compression size, but at the expense of CPU usage.A suite of recompression tools called AdvanceCOMP contains a copy of the DEFLATE encoder from the 7-Zip implementation; these utilities can often be used to further compress the size of existing gzip, ZIP, PNG, or MNG files.=== Pre-processing filters ===The LZMA SDK comes with the BCJ and BCJ2 preprocessors included, so that later stages are able to achieve greater compression: For x86, ARM, PowerPC (PPC), IA-64 Itanium, and ARM Thumb processors, jump targets are 'normalized'  before compression by changing relative position into absolute values. For x86, this means that near jumps, calls and conditional jumps (but not short jumps and conditional jumps) are converted from the machine language "jump 1655 bytes backwards" style notation to normalized "jump to address 5554" style notation; all jumps to 5554, perhaps a common subroutine, are thus encoded identically, making them more compressible.BCJ – Converter for 32-bit x86 executables. Normalise target addresses of near jumps and calls from relative distances to absolute destinations.BCJ2 – Pre-processor for 32-bit x86 executables.  BCJ2 is an improvement on BCJ, adding additional x86 jump/call instruction processing.  Near jump, near call,  conditional near jump targets are split out and compressed separately in another stream.Delta encoding – delta filter, basic preprocessor for multimedia data.Similar executable pre-processing technology is included in other software; the RAR compressor features displacement compression for 32-bit x86 executables and IA-64 executables, and the UPX runtime executable file compressor includes support for working with 16-bit values within DOS binary files.=== Encryption ===The 7z format supports encryption with the AES algorithm with a 256-bit key.  The key is generated from a user-supplied passphrase using an algorithm based on the SHA-256 hash function. The SHA-256 is executed 218 (262144) times, which causes a significant delay on slow PCs before compression or extraction starts. This technique is called key stretching and is used to make a brute-force search for the passphrase more difficult.  Current GPU-based, and custom hardware attacks limit the effectiveness of this particular method of key stretching, so it is still important to choose a strong password.The 7z format provides the option to encrypt the filenames of a 7z archive.=== Limitations ===The 7z format does not store filesystem permissions (such as UNIX owner/group permissions or NTFS ACLs), and hence can be inappropriate for backup/archival purposes. A workaround on UNIX-like systems for this is to convert data to a tar bitstream before compressing with 7z. But it is worth noting that GNU tar (common in many UNIX environments) can also compress with the LZMA algorithm natively, without the use of 7z, and that in this case the suggested file extension for the archive is ".tar.lzma" (or just ".tlz"), and not ".tar.7z". On the other hand, it is important to note, that tar does not save the filesystem encoding, which means that tar compressed filenames can become unreadable if decompressed on a different computer. It is also possible to use LZMA2 by running it through the xz tool. Recent versions of GNU tar support the -J switch, which runs TAR through XZ. The file extension is ".tar.xz" or ".txz". This method of compression has been adopted with many distributions for packaging, such as Arch, Debian (deb), Fedora (rpm) and Slackware.The 7z format does not allow extraction of some "broken files"—that is (for example) if one has the first segment of a series of 7z files, 7z cannot give the start of the files within the archive—it must wait until all segments are downloaded. The 7z format also lacks recovery records, making it vulnerable to data degradation unless used in conjunction with external solutions, like parchives, or within filesystems with robust error-correction. By way of comparison, zip files also lack a recovery feature.== See also ==Comparison of archive formatsList of archive formatsFree file formatOpen format== References ==== Further reading ==Salomon, David (2007). Data compression: the complete reference. Springer. p. 241. ISBN 1-84628-602-6.== External links ==Official website7z on SourceForge.net
	Genesis3D is a game engine created by Eclipse Entertainment. It was distributed as free open-source software and later under an open-source license. In 1999, Eclipse had been acquired by WildTangent, which overtook development. The engine aims at rendering indoor scenes with moderate polygon counts at high performances. The first beta version of the engine was released on July 30, 1998. Release Candidate 1 was shipped on March 2, 1999.== Games developed using Genesis3D ==Extreme Boards & Blades (1999), a sports game developed by Silverfish Studios and published by Head Games PublishingExtreme Paintbrawl 2 (1999), a first-person shooter developed by Hoplite Research and published by Head Games PublishingG-Sector (2000), a third-person shooter developed by Freeform InteractiveKingsborn (2000), a first-person shooter developed by Mili SoftwareAmsterDoom (2000), a first-person shooterThe Galaxian (2000), a space combat game developed by the Hass Tech Interactive EntertainmentCatechumen (2000), a first-person shooter developed and published by N'Lightning SoftwareOminous Horizons: A Paladin's Calling (2001), a first-person shooter developed and published by N'Lightning SoftwareNeocron (2002), a first-person shooter developed by Reakktor.com and published by CDV SoftwareDragon's Lair 3D: Return to the Lair (2002), an action-adventure game developed by Dragonstone Software and published by Ubi SoftF.D.N.Y. Firefighter: American Hero (2002), a first-person shooter developed by Mekada and published by Activision ValueEthnic Cleansing (2002), a first-person shooter developed by the National Alliance and published by Resistance RecordsSpecial Force (2003), a first-person shooter developed and published by Hezbollah== References ==== External links ==Official website
	The CODY Assessment (Computer aided Dyscalculia test and training) is a diagnostic screener for elementary school children from 2nd to 4th grade used to determine math weakness or dyscalculia. It also generates a detailed report evaluating each child's mathematical skills. It was developed in 2013 as a part of the CODY Project, which partnered psychologists at the University of Münster with technology experts at Kaasa health, a German software company.== Application ==The CODY Assessment is part of the mathematical training software Meister Cody ‒ Talasia. Children take the assessment, which creates a detailed report evaluating their math skills, when they begin the program and again 30 days later. Additionally, the CODY Project used the assessment in its research with several elementary schools in order to evaluate the mathematical skills of children before and after various instructional/ intervention methods.== Set-up ==The CODY Assessment takes approximately 30–40 minutes and detects four aspects: core markers (dot enumeration & magnitude comparison), number processing, calculation and working memory skills. It's comprised several subtests (listed below), which evaluate both mathematical and cognitive skills:Reaction Time TestDot enumerationMagnitude Comparison (symbolic and mixed)TranscodingCalculationNumber SetsNumber LineMatrix SpanMissing NumberThe subtests were inspired by the scientific findings of Brian Butterworth, who developed the background of a computer-based screening-test for detecting a dyscalculia.== Validation ==University of Münster validated the CODY Assessment. The validity and reliability of the test procedure were elaborately tested with a sample of more than 600 elementary school children from the second to fourth grade. The specificity of the CODY Assessment is 81 and the sensitivity is 76. The Ratz-Index is 0,68, which shows a good level of reliability.== References ==== External links ==Dyscalculia diagnosis websiteCODY assessment
	GraphQL is an open-source data query and manipulation language for APIs, and a runtime for fulfilling queries with existing data. GraphQL was developed internally by Facebook in 2012 before being publicly released in 2015.  On 7 November 2018, the GraphQL project was moved from Facebook to the newly-established GraphQL Foundation, hosted by the non-profit Linux Foundation. Since 2012, GraphQL's rise has followed the adoption timeline as set out by Lee Byron, GraphQL's creator, with accuracy. Byron's goal is to make GraphQL omnipresent across web platforms.It provides an efficient, robust, and flexible approach to developing web APIs, and has been compared and contrasted with REST and other web service architectures. It allows clients to define the structure of the data required, and the same structure of the data is returned from the server, therefore preventing excessively large amounts of data from being returned, but this has implications for how effective web caching of query results can be. The flexibility and richness of the query language also adds complexity that may not be worthwhile for simple APIs. It consists of a type system, query language and execution semantics, static validation, and type introspection.GraphQL supports reading, writing (mutating), and subscribing to changes to data (realtime updates - most commonly implemented using WebHooks).Major GraphQL clients include Apollo Client and Relay. GraphQL servers are available for multiple languages, including Haskell, JavaScript, Perl, Python, Ruby, Java, C#, Scala, Go, Elixir, Erlang, PHP, R, and Clojure.On 9 February 2018, the GraphQL Schema Definition Language (SDL) became part of the specification.== See also ==Query by ExampleOpenAPI Specification== References ==== External links ==Official website
	Diviac is an online booking website for scuba holidays. Started as a digital logbook for divers, the company focus changed in 2015 to scuba holidays.== History ==Diviac was founded in December 2012 in Zumikon, Switzerland by Joel Perrenoud and Thomas Achhorner. The company was officially incorporated in April 2013.In November 2013 the first version of Diviac Logbook was launched.In December 2014 Diviac closed CHF 1 million in a financing round led by the StartAngels Network, a Swiss angel investor network.In March 2015 Diviac Logbook releases two mobile apps for iOS and Android to access the logbook from phones and tablets.In April 2015 the first version of the Diviac Travel website was launched. In the same month Diviac also acquired Scubadviser.com, a website collecting dive centres and liveaboards reviews.In December 2015 Trekksoft and Diviac announced a partnership to offer a one-stop solution to dive centers for managing their bookings and increase their online distribution.In November 2016, Diviac Eco Travel was introduced. This division of Diviac partners with major NGOs and research centers to offer eco-friendly holidays to divers and travelers. During these vacations, guests participate alongside marine experts in field studies and research activities. As of December 2016, trips are offered in partnership with the Bimini Biological Field Station Foundation, Manta Trust and the Marine Wildlife Watch of the Philippines.In January 2018, Diviac announced the acquisition of the company by PADI.== Diviac Logbook ==Diviac Logbook is a digital logbook that allows divers to keep track of their dives and trips, it serves as proof of dive experience and as a memoir of journeys and trips.Diviac Logbook is one of the few third-party digital logbooks endorsed by Scubapro.In June 2015 Diviac and Suunto announce a partnership where Suunto users can connect to Diviac through Suunto’s DM5 software and the Movescount sports community, and upload their dives directly to Diviac’s cloud-logbook.In January 2016 the training agency IANTD made Diviac Logbook its official digital logbook and became the first agency to acknowledge digitally validated logs as official proof of diving experience.== Diviac Travel ==Diviac Travel is an online booking website specialized in scuba holidays. The website allows users to research and book diving and accommodation from liveaboards, dive resorts and dive centers worldwide. It claims to have one of the broadest offering on the market with thousands of destinations and dive operators in the platform.== References ==
	VSDC Free Video Editor is a non-linear editing (NLE) application developed by Flash-Integro, LLC. The program is capable of processing high-resolution footage including 4K UHD, 3D and VR 360-degree videos. VSDC allows for applying professional post production effects and live color correction, supports VirtualDub plug-ins as well as the ability to capture video from screen, record voice, save multimedia files to numerous formats including those pre-configured for publishing on Facebook, Vimeo, YouTube, Instagram, and Twitter.== Overview ==VSDC runs on Windows 2000/XP/Vista/7/8/10. The editor supports video and audio files recorded on smartphones, action cameras, professional cameras, drones, and can be used for all common video editing tasks necessary for producing broadcast-quality, high-definition video. == Video processing ===== Basic video editing features ===Cutting, splitting into parts, merging, trimming, cropping, rotating, flipping, playing backwards, volume changingResizing, quality and resolution settingsVideo stabilizationSpeed changeText and subtitles insertionSlideshow wizard offering 70+ transition effectsSnapshotsDeLogo filter automatically hiding unwanted elements in a video with a blurred or pixelated mask360-degree video to 2D video conversion3D video to 2D video conversionQuick Instagram-like filtersFull-featured text editor for titles and text-related effects.Embedded video converter supporting more than 20 formatsBuilt-in screen recorderBuilt-in voice recorder=== Advanced post-production ===Color correctionApart from the standard tools of automatic contrast, brightness, and temperature adjustment, VSDC offers professional color correction solutions:Color correction tables (LUTs)RGB curve modifying the aspect of the whole video or image per selected color (red, green, blue or white).Hue Saturation curves defining color region per six colors and enables to modify the aspect of a video or an image based on the selected color.Gradient tool allowing for creating a gradual blend between multiple colors20+ standard color adjustmentsMask tool – enables to apply one or several filters to a certain part of a video or an image30+ blending modesMovement – the objects on the scene can move following a chosen trajectoryAnimation – illusion of motion and change of any static objects in the scene by means of the rapid display of a sequence of these objectsVideo effects:Chroma key  - a tool that allows for replacing a solid color background (typically green or blue) in a video or an image.15 filters including Deinterlacing, Pixelize, Delogo, Blur, and more8 transformation effects including Zoom, Mirror, Resample, and more5 transition types with transparency featureDynamic TV effects (Aging TV, Broken TV, Noise TV)VirtualDub plugins supportCharts and diagrams - 3D charts including Pie, Radar, Torus, Bar, Bubble, Spline, Step Line, Spline area, Funnel, Pyramid, etc. for optimized display of any complex data360-degree and 3D video editing== Audio processing ==VSDC allows for splitting a video into audio and video layers and editing them as separate elements: as waveforms and video tracks.Audio editing tools and effects:The Audio Spectrum tool animates a waveform to the rhythm of music or any other sounds.The built-in Voice over feature allows for recording voice and adding it to the footage.Audio amplitude effects (normalization, fading in and out, amplification) help to correct an imperfect soundtrack.Delay, time-stretch and reverse effects are tailored to give audio tracks relevant sounding: as if sung by a chorus, stretched in time or played backward.DeNoise tools (Median filter and Audio gate) for audio noise reduction.Simultaneous work with several audio tracks== Formats and codecs ==== References ==== External links ==Official website
	Kroen Group C.V. is the first Georgian software company (Georgian: კრუნ გრუფ), previously known as Kroen V.O.F., is a Georgian software publisher. The company was founded in the Rotterdam in 2010. Kroen Group C.V. operates under five main brands KROEN & Co, Georgian Commercial Data Systems, Queron, Finnion, Educatief Centrum Wegtransport & Logistiek.== References ==== External links ==Official website
	Yvette Lundy (22 April 1916 – 3 November 2019) was a member of the French Resistance during World War II. She provided the inspiration for the character of "Mademoiselle Lise Lundi" in Tony Gatlif's 2009 film, Korkoro.== Early life ==She was born at Oger, the youngest of seven siblings, into a family of agricultural workers originating from the Reims area. In 1938 she began working as a teacher at Gionges. During May 1940, as the Battle of France began, she fled the area, but returned two months later.== Wartime activities ==As a Resistance worker in occupied France, Lundy began supplying official documents to escapees from the camp at Bazancourt and to Jewish families. She assisted the Communist Marcel Nautré, and others involved in the Possum network, in avoiding detection by the authorities, as well as sheltering Free French fighters parachuted into the region.Lundy was arrested on 19 June 1944 in her classroom at Gionges and was interrogated by the Gestapo at Châlons-sur-Marne, where she was subsequently imprisoned. From there she was taken to Romainville and, on 18 July 1944, was deported, first to Sarrebruck Neue Bremm and then to Ravensbrück concentration camp in transport no 47360. On 16 November of the same year, she was transferred to Buchenwald concentration camp. Her sister Berthe was also imprisoned in Germany and her older brother Lucien was interned at Auschwitz concentration camp; both survived. Her other brother, Georges, did not survive his internment and died at Schörzingen.Yvette Lundy was freed from Schlieben by the Red Army on 20 April 1945 and was flown back to France within a month.== Post-war ==Lundy remained silent about her war experiences until 1959, for her family's sake. After that date, she began going into schools to share her testimony. Her visits proved extremely popular with pupils.Lundy's memoir, Le Fil de l'araignée, co-written with Laurence Barbarot-Boisson, was published in 2012.At the age of 101, she was awarded the honour of Grand Officier de la Légion d'honneur.She died in November 2019 at the age of 103.== References ==
	ProGet is a Package management system, designed by the Inedo software company. It allows users to host and manage personal or enterprise-wide packages, applications, and components.It was originally designed as a private NuGet (the package manager for the Microsoft development platform) manager and symbol and source server. Beginning in 2015, ProGet has expanded support, added enterprise grade features, and is targeted to fit into a DevOps methodology.Enterprises utilize ProGet to “package applications and components” with the aim of ensuring software is built only once, and deployed consistently across environments. Gartner lists ProGet as a tool aligned to the “Preprod” section of a DevOps toolchain being used to “hold/stage the software ready for release”. ProGet currently supports a growing list of package managers, including NuGet, Chocolatey, Bower, npm, Maven, PowerShell, RubyGems, Helm for Kubernetes, Debian, Python, and Visual Studio Extensions (.vsix).ProGet also supports  Docker containers, Jenkins build artifacts (through a plugin) and vulnerability scanning. It is possible to monitor feeds from the ProGet interface; these features are also available to be managed from a number of the clients with which it interfaces.== Features ==Some of ProGet's main features include:== Platform and Users ==Originally, ProGet was designed to run on Microsoft Windows but beginning in Version 4.4, it can also run on Linux via Docker.ProGet is used by Asos.com, Cellenza, Abanca WebMD Health Services, SiteCore and Infragistics, among others. == References ==== External links ==Official website
	Prodapt Solutions Private Limited is an Indian multinational software services, and operations company headquartered in Chennai, India. The company focuses on the digital service provider ecosystem and delivers products and services in O/BSS, SDN/NFV, robotic process automation (RPA), business process services, infrastructure management, AI/ML, and product lifecycle services.Prodapt is part of the Jhaver Group of companies, which is an Indian business conglomerate concentrating on healthcare, chemicals, coated fabrics, zip fasteners, protective garments, and software.Prodapt has delivery centers in India, the United States, the Netherlands, and South Africa. Prodapt is an ISO 9001:2008, ISO 27001:2015, CMMI Level 3 and SSAE16 certified company.== History ==Prodapt was established in 1999 in Chennai, India. In 2010, Prodapt expanded its operations in the North American region with the acquisition of Pacific Crest Technology, a North American company focused on telecom.Prodapt created its in-house IoT framework known as Synapt in 2013. The Synapt encompasses the IoT device enablement framework (Connect), IoT middleware platform (Middleware), service management framework (Service Manager), enterprise data management framework (Data Lake), and machine learning (SMILE – Synapt Machine Intelligence & Learning Engine).In 2015, Prodapt acquired a Netherlands-based telecom consulting company known as VDVL Consultants, which is now rebranded as Prodapt Consulting.== Partnerships ==Prodapt has several industry partnerships, including Sprint, Gemalto, and Aeris. In 2016, Prodapt partnered with Blue Prism to deliver robotic process automation services.== Awards & accolades ==In 2011, Prodapt became part of the Deloitte Technology Fast 50 India and Technology Fast 500 APAC.In 2015, Prodapt was again recognized by Deloitte in Technology Fast 50 India and Technology Fast 500 APAC.In 2015, Prodapt was named by Liberty Global as the Best Vendor in Support & After Sales.In 2016, Prodapt has been awarded three key HR awards (Innovation in Recruitment, Talent Management, and Fun at Work) by the World HRD congress.In 2016, the Process Experience Lab at Prodapt received the "Outsourcing Innovation of the Year" award by Asia Outsourcing Congress.== References ==
	Relative pitch is the ability of a person to identify or re-create a given musical note by comparing it to a reference note and identifying the interval between those two notes. Relative pitch implies some or all of the following abilities:Determine the distance of a musical note from a set point of reference, e.g. "three octaves above middle C"Identify the intervals between given tones, regardless of their relation to concert pitch (A = 440 Hz)the skill used by singers to correctly sing a melody, following musical notation, by pitching each note in the melody according to its distance from the previous note. Alternatively, the same skill which allows someone to hear a melody for the first time and name the notes relative to some known reference pitch.This last definition, which applies not only to singers but also to players of instruments who rely on their own skill to determine the precise pitch of the notes played (wind instruments, fretless string instruments like violin or viola, etc.), is an essential skill for musicians in order to play successfully with others. As an example, think of the different concert pitches used by orchestras playing music from different styles (a baroque orchestra using period instruments might decide to use a higher-tuned pitch).Unlike absolute pitch (sometimes called "perfect pitch"), relative pitch is quite common among musicians, especially musicians who are used to "playing by ear", and a precise relative pitch is a constant characteristic among good musicians. Unlike perfect pitch, relative pitch can be developed through ear training. Computer-aided ear training is becoming a popular tool for musicians and music students, and various software is available for improving relative pitch.Some music teachers teach their students relative pitch by having them associate each possible interval with the first two notes of a popular song. (See ear training.) Another method of developing relative pitch is playing melodies by ear on a musical instrument, especially one which, unlike a piano or other fingered instrument, requires a specific manual adjustment for each particular tone. Indian musicians learn relative pitch by singing intervals over a drone, which is also described by W. A. Mathieu using Western just intonation terminology. Many Western ear training classes use solfège to teach students relative pitch, while others use numerical sight-singing.Compound intervals (intervals greater than an octave) can be more difficult to detect than simple intervals (intervals less than an octave).Interval recognition is used to identify chords, and can be applied to accurately tune an instrument with respect to a given reference tone, even if the tone is not in concert pitch.== See also ==Tonal memory== References ==
	Usher is an enterprise security platform released by Microstrategy, Inc. in 2015. The technology is designed to replace user-entered passwords with biometric identity and multi-step authentication methods, and features digital badges and geo-fencing administration options. The service takes the form of a mobile application that allows users to access both physical and digital space based on more passive identification methods (facial recognition, Bluetooth discovery, etc).== Overview ==Usher has been positioned as a "password killer" that is based on moving towards security credentials more tied to identity and place. Applications that can be managed via Usher include Salesforce.com, Google Apps, and select Microsoft software.The security features of the product are backed up by Usher Analytics, which allows administrators to assess data in real time to identify threats and behavioral abnormalities.== Clients ==Organizations that use Usher include: Georgetown University1776Saudi Arabia Ministry of Foreign AffairsBlackbox BI Consultancy== References ==== External links ==Official website
	In fluid dynamics, a vortex (plural vortices/vortexes) is a region in a fluid in which the flow revolves around an axis line, which may be straight or curved. Vortices form in stirred fluids, and may be observed in smoke rings, whirlpools in the wake of a boat, and the winds surrounding a tropical cyclone, tornado or dust devil. Vortices are a major component of turbulent flow. The distribution of velocity, vorticity (the curl of the flow velocity), as well as the concept of circulation are used to characterize vortices. In most vortices, the fluid flow velocity is greatest next to its axis and decreases in inverse proportion to the distance from the axis. In the absence of external forces, viscous friction within the fluid tends to organize the flow into a collection of irrotational vortices, possibly superimposed to larger-scale flows, including larger-scale vortices. Once formed, vortices can move, stretch, twist, and interact in complex ways.  A moving vortex carries with it some angular and linear momentum, energy, and mass.== Properties ===== Vorticity ===A key concept in the dynamics of vortices is the vorticity, a vector that describes the local rotary motion at a point in the fluid, as would be perceived by an observer that moves along with it.  Conceptually, the vorticity could be observed by placing a tiny rough ball at the point in question, free to move with the fluid, and observing how it rotates about its center.   The direction of the vorticity vector is defined to be the direction of the axis of rotation of this imaginary ball (according to the right-hand rule) while its length is twice the ball's angular velocity.  Mathematically, the vorticity is defined as the curl (or rotational) of the velocity field of the fluid, usually denoted by                                                         ω              →                                            {\displaystyle {\vec {\omega }}}   and expressed by the vector analysis formula                     ∇        ×                                                            u                            →                                            {\displaystyle \nabla \times {\vec {\mathit {u}}}}  , where                     ∇              {\displaystyle \nabla }   is the nabla operator and                                                                         u                            →                                            {\displaystyle {\vec {\mathit {u}}}}   is the local flow velocity.The local rotation measured by the vorticity                                                         ω              →                                            {\displaystyle {\vec {\omega }}}   must not be confused with the angular velocity vector of that portion of the fluid with respect to the external environment or to any fixed axis. In a vortex, in particular,                                                         ω              →                                            {\displaystyle {\vec {\omega }}}   may be opposite to the mean angular velocity vector of the fluid relative to the vortex's axis.=== Vortex types ===In theory, the speed u of the particles (and, therefore, the vorticity) in a vortex may vary with the distance r from the axis in many ways.  There are two important special cases, however:If the fluid rotates like a rigid body – that is, if the angular rotational velocity Ω is uniform, so that u increases proportionally to the distance r from the axis – a tiny ball carried by the flow would also rotate about its center as if it were part of that rigid body.  In such a flow, the vorticity is the same everywhere: its direction is parallel to the rotation axis, and its magnitude is equal to twice the uniform angular velocity Ω of the fluid around the center of rotation.                                                        Ω              →                                      =        (        0        ,        0        ,        Ω        )        ,                                                    r              →                                      =        (        x        ,        y        ,        0        )        ,              {\displaystyle {\vec {\Omega }}=(0,0,\Omega ),\quad {\vec {r}}=(x,y,0),}                                                          u              →                                      =                                            Ω              →                                      ×                                            r              →                                      =        (        −        Ω        y        ,        Ω        x        ,        0        )        ,              {\displaystyle {\vec {u}}={\vec {\Omega }}\times {\vec {r}}=(-\Omega y,\Omega x,0),}                                                          ω              →                                      =        ∇        ×                                            u              →                                      =        (        0        ,        0        ,        2        Ω        )        =        2                                            Ω              →                                      .              {\displaystyle {\vec {\omega }}=\nabla \times {\vec {u}}=(0,0,2\Omega )=2{\vec {\Omega }}.}  If the particle speed u is inversely proportional to the distance r from the axis, then the imaginary test ball would not rotate over itself; it would maintain the same orientation while moving in a circle around the vortex axis. In this case the vorticity                                                         ω              →                                            {\displaystyle {\vec {\omega }}}   is zero at any point not on that axis, and the flow is said to be irrotational.                                                        Ω              →                                      =        (        0        ,        0        ,        α                  r                      −            2                          )        ,                                                    r              →                                      =        (        x        ,        y        ,        0        )        ,              {\displaystyle {\vec {\Omega }}=(0,0,\alpha r^{-2}),\quad {\vec {r}}=(x,y,0),}                                                          u              →                                      =                                            Ω              →                                      ×                                            r              →                                      =        (        −        α        y                  r                      −            2                          ,        α        x                  r                      −            2                          ,        0        )        ,              {\displaystyle {\vec {u}}={\vec {\Omega }}\times {\vec {r}}=(-\alpha yr^{-2},\alpha xr^{-2},0),}                                                          ω              →                                      =        ∇        ×                                            u              →                                      =        0.              {\displaystyle {\vec {\omega }}=\nabla \times {\vec {u}}=0.}  ==== Irrotational vortices ====In the absence of external forces, a vortex usually evolves fairly quickly toward the irrotational flow pattern, where the flow velocity u is inversely proportional to the distance r. Irrotational vortices are also called free vortices.For an irrotational vortex, the circulation is zero along any closed contour that does not enclose the vortex axis; and has a fixed value, Γ, for any contour that does enclose the axis once. The tangential component of the particle velocity is then                               u                      θ                          =                                            Γ                              2                π                r                                                          {\displaystyle u_{\theta }={\tfrac {\Gamma }{2\pi r}}}  . The angular momentum per unit mass relative to the vortex axis is therefore constant,                     r                  u                      θ                          =                                            Γ                              2                π                                                          {\displaystyle ru_{\theta }={\tfrac {\Gamma }{2\pi }}}  .However, the ideal irrotational vortex flow is not physically realizable, since it would imply that the particle speed (and hence the force needed to keep particles in their circular paths) would grow without bound as one approaches the vortex axis.  Indeed, in real vortices there is always a core region surrounding the axis where the particle velocity stops increasing and then decreases to zero as r goes to zero.  Within that region, the flow is no longer irrotational: the vorticity                                                         ω              →                                            {\displaystyle {\vec {\omega }}}   becomes non-zero, with direction roughly parallel to the vortex axis. The Rankine vortex is a model that assumes a rigid-body rotational flow where r is less than a fixed distance r0, and irrotational flow outside that core regions. The Lamb–Oseen vortex model is an exact solution of the Navier–Stokes equations governing fluid flows and assumes cylindrical symmetry, for which                              u                      θ                          =                  (                      1            −                          e                                                                    −                                          r                                              2                                                                                                  4                    ν                    t                                                                                )                                      Γ                          2              π              r                                      .              {\displaystyle u_{\theta }=\left(1-e^{\frac {-r^{2}}{4\nu t}}\right){\frac {\Gamma }{2\pi r}}.}  ==== Rotational vortices ====A rotational vortex – one which has non-zero vorticity away from the core – can be maintained indefinitely in that state only through the application of some extra force, that is not generated by the fluid motion itself.For example, if a water bucket is spun at constant angular speed w about its vertical axis, the water will eventually rotate in rigid-body fashion. The particles will then move along circles, with velocity u equal to wr.  In that case, the free surface of the water will assume a parabolic shape.In this situation, the rigid rotating enclosure provides an extra force, namely an extra pressure gradient in the water, directed inwards, that prevents evolution of the rigid-body flow to the irrotational state.=== Vortex geometry ===In a stationary vortex, the typical streamline (a line that is everywhere tangent to the flow velocity vector) is a closed loop surrounding the axis; and each vortex line (a line that is everywhere tangent to the vorticity vector) is roughly parallel to the axis.  A surface that is everywhere tangent to both flow velocity and vorticity is called a vortex tube.  In general, vortex tubes are nested around the axis of rotation.  The axis itself is one of the vortex lines, a limiting case of a vortex tube with zero diameter.According to Helmholtz's theorems, a vortex line cannot start or end in the fluid – except momentarily, in non-steady flow, while the vortex is forming or dissipating. In general, vortex lines (in particular, the axis line) are either closed loops or end at the boundary of the fluid.  A whirlpool is an example of the latter, namely a vortex in a body of water whose axis ends at the free surface.  A vortex tube whose vortex lines are all closed will be a closed torus-like surface.A newly created vortex will promptly extend and bend so as to eliminate any open-ended vortex lines. For example, when an airplane engine is started, a vortex usually forms ahead of each propeller, or the turbofan of each jet engine. One end of the vortex line is attached to the engine, while the other end usually stretches out and bends until it reaches the ground.When vortices are made visible by smoke or ink trails, they may seem to have spiral pathlines or streamlines.  However, this appearance is often an illusion and the fluid particles are moving in closed paths.  The spiral streaks that are taken to be streamlines are in fact clouds of the marker fluid that originally spanned several vortex tubes and were stretched into spiral shapes by the non-uniform flow velocity distribution.=== Pressure in a vortex ===The fluid motion in a vortex creates a dynamic pressure (in addition to any hydrostatic pressure) that is lowest in the core region, closest to the axis, and increases as one moves away from it, in accordance with Bernoulli's Principle.  One can say that it is the gradient of this pressure that forces the fluid to follow a curved path around the axis.In a rigid-body vortex flow of a fluid with constant density, the dynamic pressure is proportional to the square of the distance r from the axis.  In a constant gravity field, the free surface of the liquid, if present, is a concave paraboloid.In an irrotational vortex flow with constant fluid density and cylindrical symmetry, the dynamic pressure varies as P∞ − K/r2, where P∞ is the limiting pressure infinitely far from the axis.  This formula provides another constraint for the extent of the core, since the pressure cannot be negative. The free surface (if present) dips sharply near the axis line, with depth inversely proportional to r2.  The shape formed by the free surface is called a hyperboloid, or "Gabriel's Horn" (by Evangelista Torricelli).The core of a vortex in air is sometimes visible because of a plume of water vapor caused by condensation in the low pressure and low temperature of the core; the spout of a tornado is an example.  When a vortex line ends at a boundary surface, the reduced pressure may also draw matter from that surface into the core.  For example, a dust devil is a column of dust picked up by the core of an air vortex attached to the ground. A vortex that ends at the free surface of a body of water (like the whirlpool that often forms over a bathtub drain) may draw a column of air down the core.  The forward vortex extending from a jet engine of a parked airplane can suck water and small stones into the core and then into the engine.=== Stability in a vortex ===The vortices that you create becomes more stable after you stop shaking the container because as you shake the forces acting on the whole fluid are uneven. When you stop shaking the cup or put it down on a surface, the vortex is able to evenly distribute force to the liquid. === Evolution ===Vortices need not be steady-state features; they can move and change shape. In a moving vortex, the particle paths are not closed, but are open, loopy curves like helices and cycloids. A vortex flow might also be combined with a radial or axial flow pattern.  In that case the streamlines and pathlines are not closed curves but spirals or helices, respectively.  This is the case in tornadoes and in drain whirlpools.  A vortex with helical streamlines is said to be solenoidal.As long as the effects of viscosity and diffusion are negligible, the fluid in a moving vortex is carried along with it.  In particular, the fluid in the core (and matter trapped by it) tends to remain in the core as the vortex moves about. This is a consequence of Helmholtz's second theorem. Thus vortices (unlike surface and pressure waves) can transport mass, energy and momentum over considerable distances compared to their size, with surprisingly little dispersion.  This effect is demonstrated by smoke rings and exploited in vortex ring toys and guns.Two or more vortices that are approximately parallel and circulating in the same direction will attract and eventually merge to form a single vortex, whose circulation will equal the sum of the circulations of the constituent vortices. For example,  an airplane wing that is developing lift will create a sheet of small vortices at its trailing edge. These small vortices merge to form a single wingtip vortex, less than one wing chord downstream of that edge.  This phenomenon also occurs with other active airfoils, such as propeller blades.  On the other hand, two parallel vortices with opposite circulations (such as the two wingtip vortices of an airplane) tend to remain separate.Vortices contain substantial energy in the circular motion of the fluid.  In an ideal fluid this energy can never be dissipated and the vortex would persist forever. However, real fluids exhibit viscosity and this dissipates energy very slowly from the core of the vortex. It is only through dissipation of a vortex due to viscosity that a vortex line can end in the fluid, rather than at the boundary of the fluid.== Further examples ==In the hydrodynamic interpretation of the behaviour of electromagnetic fields, the acceleration of electric fluid in a particular direction creates a positive vortex of magnetic fluid. This in turn creates around itself a corresponding negative vortex of electric fluid. Exact solutions to classical nonlinear magnetic equations include the Landau-Lifshitz equation, the continuum Heisenberg model, the Ishimori equation, and the nonlinear Schrödinger equation.Bubble rings are underwater vortex rings whose core traps a ring of bubbles, or a single donut-shaped bubble. They are sometimes created by dolphins and whales.The lifting force of aircraft wings, propeller blades, sails, and other airfoils can be explained by the creation of a vortex superimposed on the flow of air past the wing.Aerodynamic drag can be explained in large part by the formation of vortices in the surrounding fluid that carry away energy from the moving body.Large whirlpools can be produced by ocean tides in certain straits or bays. Examples are Charybdis of classical mythology in the Straits of Messina, Italy; the Naruto whirlpools of Nankaido, Japan; and the Maelstrom at Lofoten, Norway.Vortices in the Earth's atmosphere are important phenomena for meteorology. They include mesocyclones on the scale of a few miles, tornados, waterspouts, and hurricanes.  These vortices are often driven by temperature and humidity variations with altitude.  The sense of rotation of hurricanes is influenced by the Earth's rotation.  Another example is the Polar vortex, a persistent, large-scale cyclone centered near the Earth's poles, in the middle and upper troposphere and the stratosphere.Vortices are prominent features of the atmospheres of other planets. They include the permanent Great Red Spot on Jupiter, the intermittent Great Dark Spot on Neptune, the polar vortices of Venus, the Martian dust devils and the North Polar Hexagon of Saturn.Sunspots are dark regions on the Sun's visible surface (photosphere) marked by a lower temperature than its surroundings, and intense magnetic activity.The accretion disks of black holes and other massive gravitational sources.Taylor-Couette flow occurs in a fluid between two nested cylinders, one rotating, the other fixed.=== Summary ===In the dynamics of fluid, a vortex is fluid that revolves around the axis line. This fluid might be curved or straight. Vortices form from stirred liquids they might be observed in smoke rings, whirlpools, in the wake of a boat or the winds around a tornado or dust devil.Vortices are an important part of turbulent flow. Vortices can otherwise be known as a circular motion of a liquid. In the cases of the absence of forces, the liquid settles. This makes the water stay still instead of moving.When they are created, vortices can move, stretch, twist and interact in complicated ways. When a vortex is moving, sometimes, it can affect an angular position.For an example, if a water bucket is rotated or spun constantly, it will rotate around an invisible line called the axis line. The rotation moves around in circles. In this example the rotation of the bucket creates extra force.The reason that the vortices can change shape is the fact that they have open particle paths. This can create a moving vortex. Examples of this fact are the shapes of tornadoes and drain whirlpools.When two or more vortices are close together they can merge to make a vortex. Vortices also hold energy in its rotation of the fluid. If the energy is never removed, it would consist of circular motion forever.== See also ==== References ===== Notes ====== Other ===Loper, David E. (November 1966). An analysis of confined magnetohydrodynamic vortex flows (PDF) (NASA contractor report NASA CR-646). Washington: National Aeronautics and Space Administration. LCCN 67060315.Batchelor, G.K. (1967). An Introduction to Fluid Dynamics. Cambridge Univ. Press. Ch. 7 et seq. ISBN 9780521098175.Falkovich, G. (2011). Fluid Mechanics, a short course for physicists. Cambridge University Press. ISBN 978-1-107-00575-4.Clancy, L.J. (1975). Aerodynamics. London: Pitman Publishing Limited. ISBN 978-0-273-01120-0.De La Fuente Marcos, C.; Barge, P. (2001). "The effect of long-lived vortical circulation on the dynamics of dust particles in the mid-plane of a protoplanetary disc". Monthly Notices of the Royal Astronomical Society. 323 (3): 601–614. Bibcode:2001MNRAS.323..601D. doi:10.1046/j.1365-8711.2001.04228.x.== External links ==Optical VorticesVideo of two water vortex rings colliding (MPEG)Chapter 3 Rotational Flows: Circulation and TurbulenceVortical Flow Research Lab (MIT) – Study of flows found in nature and part of the Department of Ocean Engineering.
	VSDX Annotator is a software application to view and annotate Microsoft Visio documents on the Apple Mac OS X  operating system. There are some Visio file Viewers or Visio alternatives for creating flowcharts, diagrams, mind-maps, concept-maps and other graphical data. VSDX Annotator is the first application that allows you to comment and annotate MS Visio files on OS X.== Capabilities ==VSDX Annotator allows annotating Visio drawings - flowcharts, diagrams, schemes,  and other visual graphic files on a Mac. It makes it possible to view multi-page drawings while displaying shape data, layers, and hyperlinks. The Annotation tab includes 16 tools that can be used to edit files. The Viewing tab contains 12 tools to navigate drawings. VSDX Annotator provides an ability to export modified Visio files in a PDF, send the PDF via email and save VSDX files in the same extension.== See also ==Alternative software for Visio on OS X system:Lucidchart (online tool)OmniGraffle (application)ConceptDraw (a suite of applications)== References ==== External links ==Official websiteVisio Viewer Mac softwareApple World Today - Notable apps and app updates for Aug. 8, 2016
	WiredTiger is a NoSQL, Open Source extensible platform for data management. WiredTiger uses MultiVersion Concurrency Control (MVCC) architecture.MongoDB acquired WiredTiger Inc. on December 16, 2014. The WiredTiger storage engine is the default storage engine starting in MongoDB version 3.2. It provides a document-level concurrency model, checkpointing, and compression, among other features. In MongoDB Enterprise, WiredTiger also supports Encryption At Rest.== References ==== Further reading ==Wolpe, Toby (February 3, 2015). "MongoDB 3.0 gets ready to roll with WiredTiger engine onboard". ZDNet. Retrieved July 8, 2016.Wolpe, Toby (December 16, 2014). "MongoDB snaps up WiredTiger and its storage expert team". ZDNet. Accessed July 8, 2016."MongoDB Buys WiredTiger: Database Move". Information Week. January 16, 2015. Retrieved July 8, 2016."MongoDB 3.0 NoSQL database offers WiredTiger data compression". The Inquirer. February 3, 2015. Retrieved July 8, 2016."MongoDB 3.0 NoSQL database integrates WiredTiger engine and Ops Manager tool". v3.co. February 3, 2015. Retrieved July 8, 2016. (subscription required)Plugge, E.; Hows, D.; Membrey, P.; Hawkins, T. (2015). The Definitive Guide to MongoDB: A complete guide to dealing with Big Data using MongoDB. Apress. p. 10. ISBN 978-1-4842-1182-3. Retrieved July 8, 2016.Wolpe, Toby (November 21, 2014). "MongoDB CTO: How our new WiredTiger storage engine will earn its stripes". ZDNet. Retrieved July 8, 2016.
	LiSiCA (Ligand Similarity using Clique Algorithm) is a ligand-based virtual screening software that searches for 2D and 3D similarities between a reference compound and a database of target compounds which should be represented in a Mol2 format. The similarities are expressed using the Tanimoto coefficients and the target compounds are ranked accordingly. LiSiCA is also available as LiSiCA PyMOL plugin both on Linux and Windows operating systems.== Description ==As an input LiSiCA requires at least one reference compound and database of target compounds. For 3D screening this database has to be a pregenerated database of conformations of target and for 2D screening a topology, that is, a list of atoms and bonds, for each target compound. On each step the algorithm compares reference compound to one of the compounds from target compounds based on their 2D or 3D representation. Both compounds(molecules) are converted to molecular graphs. In 2D and 3D screening the molecular graph vertices represent atoms. In 2D screening edges of molecular graph represent covalent bonds while in 3D screening edges are drawn between every pair of vertices and have no chemical meaning. A product graph generated from molecular graphs is then searched using fast maximum clique algorithm to find the largest substructure common to both compounds. The similarity between compounds is calculated using Tanimoto coefficients and target compounds are ranked according to their Tanimoto coefficients.== Feature overview ==LiSiCA can search 2D and 3D similarities between a reference compound and a database of target compounds. It takes as an input at least one reference compound and a database of target compounds. By default it returns only the compound most similar to the reference compound out of all compounds in database of target compounds.Other optional parameters LiSiCA uses are:Number of CPU threads to useDefault value: the default is to try to detect the number of CPUs and use all of them or, failing that, use 1Product graph dimensionPossible input: 2, 3Default value: 2Maximum allowed atom spatial distance difference for the 3D product graph measured in angstromsDefault value: 1.0Maximum allowed shortest path difference for the 2D product graph measured in the number of covalent bonds between atomsDefault value: 1Consider hydrogensDefault value: FalseNumber of highest ranked molecules to write to outputDefault value: 0Maximum allowed number of highest scoring conformations to be outputtedDefault value: 1In addition LiSiCA PyMOL plugin also offers to load saved results.== History ==LiSiCA Software (March 2015) LiSiCA PyMOL plugin (March 2016) == Interesting fact ==The word lisica in Slovenian language means a fox that is why the logo of LiSiCA software is a fox holding two molecules.== References ==== External links ==LiSiCA softwareLiSiCA plugin
	In-house software is a software that is produced by a corporate entity for purpose of using it within the organization. In-house software however may later become available for commercial use upon sole discretion of the developing organization.  The need to develop such software may arise depending on many circumstances which may be non-availability of the software in the market, potentiality or ability of the corporation to develop such software or to customize a software based on the corporate organization's need.
	Tunnelblick is a free, open source graphic user interface for OpenVPN, a Virtual Private Network (VPN), on OS X and macOS. It provides easy control of OpenVPN client and/or server connections.== History ==The first stable release was version 3.0 in March 2010. Since then, a new version has been released on a regular basis, the current stable version as of June 2019 is 3.7.9a.== Issues ==In January 2016, the Sparkle Updater component used by Tunnelblick was found to be vulnerable to a man-in-the-middle attack. This security flaw has since been patched.== References ==== External links ==Official websiteGitHub page
	Business software (or a business application) is any software or set of computer programs used by business users to perform various business functions. These business applications are used to increase productivity, to measure productivity and to perform other business functions accurately.By and large, business software is likely to be developed to meet the needs of a specific business, and therefore is not easily transferable to a different business  environment, unless its nature and operation is identical. Due to the unique requirements of each business, off-the-shelf software is unlikely to completely address a company's needs. However, where an on-the-shelf solution is necessary, due to time or monetary considerations, some level of customization is likely to be required. Exceptions do exist, depending on the business in question, and thorough research is always required before committing to bespoke or off-the-shelf solutions.Some business applications are interactive, i.e., they have a graphical user interface or user interface and users can query/modify/input data and view results instantaneously. They can also run reports instantaneously. Some business applications run in batch mode: they are set up to run based on a predetermined event/time and a business user does not need to initiate them or monitor them.Some business applications are built in-house and some are bought from vendors (off the shelf software products). These business applications are installed on either desktops or big servers. Prior to the introduction of COBOL (a universal compiler) in 1965, businesses developed their own unique machine language.  RCA's language consisted of a 12-position instruction. For example, to read a record into memory, the first two digits would be the instruction (action) code. The next four positions of the instruction (an 'A' address) would be the exact leftmost memory location where you want the readable character to be placed. Four positions (a 'B' address) of the instruction would note the very rightmost memory location where you want the last character of the record to be located. A two digit 'B' address also allows a modification of any instruction. Instruction codes and memory designations excluded the use of 8's or 9's. The first RCA business application was implemented in 1962 on a 4k RCA 301.  The RCA 301, mid frame 501, and large frame 601 began their marketing in early 1960.             Many kinds of users are found within the business environment, and can be categorized by using a small, medium and large matrix:The small business market generally consists of home accounting software, and office suites such as LibreOffice, Microsoft Office or GSuite.The medium size, or small and medium-sized enterprise (SME), has a broader range of software applications, ranging from accounting, groupware, customer relationship management, human resource management systems, outsourcing relationship management, loan origination software, shopping cart software, field service software, and other productivity enhancing applications.The last segment covers enterprise level software applications, such as those in the fields of enterprise resource planning, enterprise content management (ECM), business process management (BPM) and product lifecycle management. These applications are extensive in scope, and often come with modules that either add native functions, or incorporate the functionality of third-party computer programs.Technologies that previously only existed in peer-to-peer software applications, like Kazaa and Napster, are starting to appear within business applications.== Types of business tools ==Enterprise software application (Esa)Resource ManagementEnterprise Resource Planning (ERP)Digital dashboards, also known as business intelligence dashboards, enterprise dashboards, or executive dashboards.  These are visually based summaries of business data that show at-a-glance understanding of conditions through metrics and key performance indicators (KPIs). Dashboards are a very popular  tools that have arisen in the last few years.Online analytical processing (OLAP),  (which include HOLAP, ROLAP and MOLAP) - are a capability of some management, decision support, and executive information systems that support interactive examination of large amounts of data from many perspectives.Reporting software generates aggregated views of data to keep the management informed about the state of their business.Procurement software is business software that helps to automate the purchasing function of organizations.Data mining is the extraction of consumer information from a database by utilizing software that can isolate and identify previously unknown patterns or trends in large amounts of data. There is a variety of data mining techniques that reveal different types of patterns. Some of the techniques that belong here are statistical methods (particularly business statistics) and neural networks, as very advanced means of analyzing data.Business performance management (BPM)Document management software is made for organizing and managing multiple documents of various types. Some of them have storage functions for security and back-up of valuable business information.Employee scheduling software- used for creating and distributing employee schedules, as well as for tracking employee hours.== Brief history ==The essential motivation for business software is to increase profits by cutting costs or speeding the productive cycle. In the earliest days of white-collar business automation, large mainframe computers were used to tackle the most tedious jobs, like bank cheque clearing and factory accounting.Factory accounting software was among the most popular of early business software tools, and included the automation of general ledgers, fixed assets inventory ledgers, cost accounting ledgers, accounts receivable ledgers, and accounts payable ledgers (including payroll, life insurance, health insurance, federal and state insurance and retirement).The early use of software to replace manual white-collar labor was extremely profitable, and caused a radical shift in white-collar labor. One computer might easily replace 100 white-collar 'pencil pushers', and the computer would not require any health or retirement benefits.Building on these early successes with IBM, Hewlett-Packard and other early suppliers of business software solutions, corporate consumers demanded business software to replace the old-fashioned drafting board. CAD-CAM software (or computer-aided drafting for computer-aided manufacturing) arrived in the early 1980s. Also, project management software was so valued in the early 1980s that it might cost as much as $500,000 per copy (although such software typically had far fewer capabilities than modern project management software such as Microsoft Project, which one might purchase today for under $500 per copy.)In the early days, perhaps the most noticeable, widespread change in business software was the word processor. Because of its rapid rise, the ubiquitous IBM typewriter suddenly vanished in the 1980s as millions of companies worldwide shifted to the use of Word Perfect business software, and later, Microsoft Word software. Another vastly popular computer program for business were mathematical spreadsheet programs such as Lotus 1-2-3, and later Microsoft Excel.In the 1990s business shifted massively towards globalism with the appearance of SAP software which coordinates a supply-chain of vendors, potentially worldwide, for the most efficient, streamlined operation of factory manufacture.Yet nothing in the history of business software has had the global impact of the Internet, with its email and websites that now serve commercial interests worldwide. Globalism in business fully arrived when the Internet became a household word.The next phase in the evolution of business software is being led by the emergance of Robotic Process Automation (RPA), which involves identifying and automating highly repetitive tasks and processes, with an aim to drive operational efficiency, reduce costs and limit human error. Industries that have been in the forefront of RPA adoption include the Insurance industry, Banking and Financial Services, the Legal industry and the Healthcare industry.== Application support ==Business applications are built based on the requirements from the business users. Also, these business applications are built to use certain kind of Business transactions or data items. These business applications run flawlessly until there are no new business requirements or there is no change in underlying Business transactions. Also, the business applications run flawlessly if there are no issues with computer hardware, computer networks (Internet/intranet), computer disks, power supplies, and various software components (middleware, database, computer programs, etc.).Business applications can fail when an unexpected error occurs. This error could occur due to a data error (an unexpected data input or a wrong data input), an environment error (an in frastructure related error), a programming error, a human error or a work flow error. When a business application fails one needs to fix the business application error as soon as possible so that the business users can resume their work. This work of resolving business application errors is known as business application support.=== Reporting errors ===The Business User calls the business application support team phone number or sends an e-mail to the business application support team. The business application support team gets all the details of the error from the business user on the phone or from the e-mail. These details are then entered in a tracking software. The tracking software creates a request number and this request number is given to the business user. This request number is used to track the progress on the support issue. The request is assigned to a support team member.=== Notification of errors ===For critical business application errors (such as an application not available or an application not working correctly), an e-mail is sent to the entire organization or impacted teams so that they are aware of the issue. They are also provided with an estimated time for application availability.=== Investigation or analysis of application errors ===The business application support team member collects all the necessary information about the business software error. This information is then recorded in the support request. All of the data used by the business user is also used in the investigation. The application program is reviewed for any possible programming errors.=== Error resolution ===If any similar business application errors occurred in the past then the issue resolution steps are retrieved from the support knowledge base and the error is resolved using those steps. If it is a new support error, then new issue resolution steps are created and the error is resolved. The new support error resolution steps are recorded in the knowledge base for future use. For major business application errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error.=== Code correction ===If the business application error occurred due to programming errors, then a request is created for the application development team to correct programming errors. If the business user needs new features or functions in the business application, then the required analysis/design/programming/testing/release is planned and a new version of the business software is deployed.=== Business process correction ===If the business application error occurred due to a work flow issue or human errors during data input, then the business users are notified. Business users then review their work flow and revise it if necessary. They also modify the user guide or user instructions to avoid such an error in the future.=== Infrastructure issue correction ===If the business application error occurred due to infrastructure issues, then the specific infrastructure team is notified. The infrastructure team then implements permanent fixes for the issue and monitors the infrastructure to avoid the re-occurrence of the same error.== Support follow up and internal reporting ==The business application error tracking system is used to review all issues periodically (daily, weekly and monthly) and reports are generated to monitor the resolved issues, repeating issues, and pending issues. Reports are also generated for the IT/IS management for improvement and management of business applications.== See also ==== References ==== External links ==Bizex - Business Software
	Ad blocking or ad filtering is a software capability for removing or altering online advertising in a web browser or an application. The most popular ad blocking tools are browser extensions. Other methods are also available.== Technologies and native countermeasures ==Online advertising exists in a variety of forms, including web banners, pictures, animations, embedded audio and video, text, or pop-up windows, and can employ audio and video autoplay. All browsers offer some ways to remove or alter advertisements: either by targeting technologies that are used to deliver ads (such as embedded content delivered through browser plug-ins or via HTML5), targeting URLs that are the source of ads, or targeting behaviors characteristic to ads (such as the use of HTML5 autoplay of both audio and video).== Reasons for blocking ads ==From the standpoint of an Internet user, there are various fundamental reasons why one would want to use ad blocking, in addition to not being manipulated by brands: Protecting their privacyReduces the number of HTTP cookiesProtecting themselves from malvertisingAny intrusive actions from the ads, including but not limited to: drive-by downloads, invisible overlay click areas (such as a regular link that opens an unexpected external website), opening in a new tab, popups and auto-redirects. These often lead to scam sites (tech support and “you won a prize”).Save bandwidth (and by extension, money)Better user experienceSome ads cover the text making it partly illegible, making the site unusableLess cluttered pagesFaster page loading timesFewer distractionsAccessibility reasonsAnimations in some ads are distracting to the point of making the site unusableThe motion in some ads is nauseating for some usersSave battery on mobile devices or laptopsPrevent undesirable websites from making ad revenue out of the user's visitPublishers and their representative trade bodies, on the other hand, argue that web ads provide revenue to website owners, which enable the website owners to create or otherwise purchase content for the website. Publishers claim that the prevalent use of ad blocking software and devices could adversely affect website owner revenue and thus in turn lower the availability of free content on websites.== Benefits ==For users, the benefits of ad blocking software include quicker loading and cleaner looking web pages with fewer distractions, lower resource waste (bandwidth, CPU, memory, etc.), and privacy benefits gained through the exclusion of the tracking and profiling systems of ad delivery platforms. Blocking ads can also save substantial amounts of electrical energy and lower users' power bills, and additional energy savings can also be expected at the grid level because fewer data packets need to be transmitted between the user's machine and the website server.=== User experience ===Ad blocking software may have other benefits to users' quality of life, as it decreases Internet users' exposure to advertising and marketing industries, which promote the purchase of numerous consumer products and services that are potentially harmful or unhealthy and on creating the urge to buy immediately. The average person sees more than 5000 advertisements daily, many of which are from online sources. Each ad promises viewers that their lives will be improved by purchasing the item that is being promoted (e.g., fast food, soft drinks, candy, expensive consumer electronics) or encourages users to get into debt or gamble. Additionally, if Internet users buy all of these items, the packaging and the containers (in the case of candy and soda pop) end up being disposed of, leading to negative environmental impacts of waste disposal. Advertisements are very carefully crafted to target weaknesses in human psychology; as such, a reduction in exposure to advertisements could be beneficial for users' quality of life.Unwanted advertising can also harm the advertisers themselves if users become annoyed by the ads.  Irritated users might make a conscious effort to avoid the goods and services of firms which are using annoying "pop-up" ads which block the Web content the user is trying to view. For users not interested in making purchases, the blocking of ads can also save time.  Any ad that appears on a website exerts a toll on the user's "attention budget", since each ad enters the user's field of view and must either be consciously ignored or closed, or dealt with in some other way.  A user who is strongly focused on reading solely the content that they are seeking, likely has no desire to be diverted by advertisements that seek to sell unneeded or unwanted goods and services.  In contrast, users who are actively seeking items to purchase, might appreciate advertising, in particular targeted ads.=== Security ===Another important aspect is improving security; online advertising subjects users to a higher risk of infecting their devices with computer viruses than surfing pornography websites. In a high-profile case, malware was distributed through advertisements provided to YouTube by a malicious customer of Google's Doubleclick. In August 2015, a 0-day exploit in the Firefox browser was discovered in an advertisement on a website. When Forbes required users to disable ad blocking before viewing their website, those users were immediately served with pop-under malware. The Australian Signals Directorate recommends individuals and organizations block advertisements to improve their information security posture and mitigate potential malvertising attacks and machine compromise. The information security firm Webroot also note employing ad blockers provide effective countermeasures against malvertising campaigns for less technically-sophisticated computer users.=== Monetary ===Ad blocking can also save money for the user.  If a user's personal time is worth one dollar per minute, and if unsolicited advertising adds an extra minute to the time that the user requires for reading the webpage (i.e. the user must manually identify the ads as ads, and then click to close them, or use other techniques to either deal with them, all of which tax the user's intellectual focus in some way), then the user has effectively lost one dollar of time in order to deal with ads that might generate a few fractional pennies of display-ad revenue for the website owner.  The problem of lost time can rapidly spiral out of control if malware accompanies the ads.Ad blocking also reduces page load time and saves bandwidth for the users.  Users who pay for total transferred bandwidth ("capped" or pay-for-usage connections) including most mobile users worldwide, have a direct financial benefit from filtering an ad before it is loaded. Analysis of the 200 most popular news sites (as ranked by Alexa) in 2015 showed that Mozilla Firefox Tracking Protection lead to 39% reduction in data usage and 44% median reduction in page load time. According to research performed by The New York Times, ad blockers reduced data consumption and sped up load time by more than half on 50 news sites, including their own. Journalists concluded that "visiting the home page of Boston.com (the site with most ad data in the study) every day for a month would cost the equivalent of about $9.50 in data usage just for the ads". Streaming audio and video, even if they are not presented to the user interface, can rapidly consume gigabytes of transfer especially on a faster 4G connection. Even fixed connections are often subject to usage limits, especially the faster connections (100Mbit/s and up) which can quickly saturate a network if filled by streaming media.It is a known problem with most web browsers, including Firefox, that restoring sessions often plays multiple embedded ads at once. However, this annoyance can easily be averted simply by setting the web browser to clear all cookies and browsing-history information each time the browser software is closed.  Another preventive option is to use a script blocker, which enables the user to disable all scripts and then to selectively re-enable certain scripts as desired, in order to determine the role of each script.  The user thus can very quickly learn which scripts are truly necessary (from the standpoint of webpage functionality) and consequently which sources of scripts are undesirable, and this insight is helpful in visiting other websites in general.  Thus by precisely controlling which scripts are run in each webpage viewed, the user retains full control over what happens on his/her computer CPU and computer screen.== Popularity ==Use of mobile and desktop ad blocking software designed to remove traditional advertising grew by 41% worldwide and by 48% in the U.S. between Q2 2014 and Q2 2015. As of Q2 2015, 45 million Americans were using ad blockers. In a survey research study released Q2 2016, MetaFacts reported 72 million Americans, 12.8 million adults in the UK, and 13.2 million adults in France were using ad blockers on their PCs, smartphones, or tablet computers. In March 2016, the Internet Advertising Bureau reported that UK adblocking was already at 22% among people over 18 years old.== Methods ==One method of filtering is simply to block (or prevent autoplay of) Flash animation or image loading or Microsoft Windows audio and video files. This can be done in most browsers easily and also improves security and privacy. This crude technological method is refined by numerous browser extensions. Every web browser handles this task differently, but, in general, one alters the options, preferences or application extensions to filter specific media types. An additional add-on is usually required to differentiate between ads and non-ads using the same technology, or between wanted and unwanted ads or behaviors.The more advanced ad blocking filter software allow fine-grained control of advertisements through features such as blacklists, whitelists, and regular expression filters. Certain security features also have the effect of disabling some ads. Some antivirus software can act as an ad blocker. Filtering by intermediaries such as ISP providers or national governments is increasingly common.=== Browser integration ===As of 2015, many web browsers block unsolicited pop-up ads automatically. Current versions of Konqueror and Internet Explorer also include content filtering support. Content filtering can be added to Firefox, Chromium-based browsers, Opera, Safari, and other browsers with extensions such as AdBlock, Adblock Plus, and uBlock Origin, and a number of sources provide regularly updated filter lists. Adblock Plus (provided by the German software house Eyeo GmbH) is included in the freeware browser Maxthon from the People's Republic of China by default. Another method for filtering advertisements uses Cascading Style Sheets (CSS) rules to hide specific HTML and XHTML elements.At the beginning of 2018, Google confirmed that the built-in ad blocker for the Chrome/Chromium browsers would go live on the 15th of February: this ad blocker only blocks certain ads as specified by the Better Ads Standard (defined by the Coalition for Better Ads, in which Google itself is a board member). This built-in ad blocking mechanism is disputed because it could unfairly benefit Google's advertising itself.In 2019, both Apple and Google began to make changes to their web browsers' extension systems which encourage the use of declarative content blocking using pre-determined filters processed by the web browser, rather than filters processed at runtime by the extension. Both vendors have imposed limits on the number of entries that may be included in these lists, which have led to (especially in the case of Chrome) allegations that these changes are being made to inhibit the effectiveness of ad blockers..=== External programs ===A number of external software applications offer ad filtering as a primary or additional feature. A traditional solution is to customize an HTTP proxy (or web proxy) to filter content. These programs work by caching and filtering content before it is displayed in a user's browser. This provides an opportunity to remove not only ads but also content which may be offensive, inappropriate, or even malicious (Drive-by download). Popular proxy software which blocks content effectively include Netnanny, Privoxy, Squid, and some content-control software. The main advantage of the method is freedom from implementation limitations (browser, working techniques) and centralization of control (the proxy can be used by many users). Proxies are very good at filtering ads, but they have several limitations compared to browser-based solutions. For proxies, it is difficult to filter Transport Layer Security (SSL) (https://) traffic and full webpage context is not available to the filter. As well, proxies find it difficult to filter JavaScript-generated ad content.=== Hosts file and DNS manipulation ===Most operating systems, even those which are aware of the Domain Name System (DNS), still offer backward compatibility with a locally administered list of foreign hosts. This configuration, for historical reasons, is stored in a flat text file that by default contains very few hostnames and their associated IP addresses. Editing this hosts file is simple and effective because most DNS clients will read the local hosts file before querying a remote DNS server. Storing black-hole entries in the hosts file prevents the browser from accessing an ad server by manipulating the name resolution of the ad server to a local or nonexistent IP address (127.0.0.1 or 0.0.0.0 are typically used for IPv4 addresses). While simple to implement, these methods can be circumvented by advertisers, either by hard-coding the IP address of the server that hosts the ads (this, in its turn, can be worked around by changing the local routing table by using for example iptables or other blocking firewalls), or by loading the advertisements from the same server that serves the main content; blocking name resolution of this server would also block the useful content of the site.Using a DNS sinkhole by manipulating the hosts file exploits the fact that most operating systems store a file with IP address, domain name pairs which is consulted by most browsers before using a DNS server to look up a domain name. By assigning the loopback address to each known ad server, the user directs traffic intended to reach each ad server to the local machine or to a virtual black hole of /dev/null or bit bucket.=== DNS filtering ===Advertising can be blocked by using a DNS server which is configured to block access to domains or hostnames which are known to serve ads by spoofing the address. Users can choose to use an already modified DNS server or set up a dedicated device such as a Raspberry Pi themselves. Manipulating DNS is a widely employed method to manipulate what the end user sees from the Internet but can also be deployed locally for personal purposes. China runs its own root DNS and the EU has considered the same. Google has required their Google Public DNS be used for some applications on its Android devices. Accordingly, DNS addresses / domains used for advertising may be extremely vulnerable to a broad form of ad substitution whereby a domain that serves ads is entirely swapped out with one serving more local ads to some subset of users. This is especially likely in countries, notably Russia, India and China, where advertisers often refuse to pay for clicks or page views. DNS-level blocking of domains for non-commercial reasons is already common in China.=== Recursive Local VPN ===On Android, apps can run a local VPN connection with its own host filtering ability and DNS address without requiring root access. This approach allows adblocking app to download adblocking host files and use them to filter out ad networks throughout the device. Blokada, DNS66, AdGuard are few of the popular apps which accomplish adblocking without root permission. The adblocking is only active when the local VPN is turned on, and it completely stops when the VPN connection is disconnected. The convenience makes it easy to access content blocked by anti-adblock scripts.This approach optimizes battery usage, reduces internet slowdown caused by using external DNS or VPN adblocking and needs overall less configuration.=== Hardware devices ===Devices such as AdTrap use hardware to block Internet advertising. Based on reviews of AdTrap, this device uses a Linux Kernel running a version of PrivProxy to block ads from video streaming, music streaming, and any web browser. Another such solution is provided for network-level ad blocking for telcos by Israeli startup Shine.=== By external parties and internet providers ===Internet providers, especially mobile operators, frequently offer proxies designed to reduce network traffic. Even when not targeted specifically at ad filtering, these proxy-based arrangements will block many types of advertisements that are too large or bandwidth-consuming, or that are otherwise deemed unsuited for the specific internet connection or target device. Many internet operators block some form of advertisements while at the same time injecting their own ads promoting their services and specials.== Economic consequences for online business ==Some content providers have argued that widespread ad blocking results in decreased revenue to a website sustained by advertisements and e-commerce-based businesses, where this blocking can be detected. Some have argued that since advertisers are ultimately paying for ads to increase their own revenues, eliminating ad blocking would only dilute the value per impression and drive down the price of advertising, arguing that like click fraud, impressions served to users who use ad blockers are of little to no value to advertisers. Consequently, they argue, eliminating ad blocking would not increase overall ad revenue to content providers in the long run.== Response from publishers ===== Countermeasures ===Some websites have taken countermeasures against ad blocking software, such as attempting to detect the presence of ad blockers and informing users of their views, or outright preventing users from accessing the content unless they disable the ad blocking software, whitelist the website, or buy an "ad-removal pass". There have been several arguments supporting and opposing the assertion that blocking ads is wrong.Some publisher companies have taken steps to protect their rights to conduct their business according to prevailing law.It has been suggested that in the European Union, the practice of websites scanning for ad blocking software may run afoul of the E-Privacy Directive. This claim was further validated by IAB Europe's guidelines released in June 2016 stating that there indeed may be a legal issue in ad blocker detection. While some anti-blocking stakeholders have tried to refute this it seems safe to assume that Publishers should follow the guidelines provided by the main Publisher lobby IAB. The joint effort announced by IAB Sweden prior to IAB Europe's guideline on the matter never materialized, and would have most likely been found against European anti-competition laws if it did.In August 2017, a vendor of such counter-measures issued a demand under section 1201 of the U.S. Digital Millennium Copyright Act, to demand the removal of a domain name associated with their service from an ad-blocking filter list. The vendor argued that the domain constituted a component of a technological protection measure designed to protect a copyrighted work, and thus made it a violation of anti-circumvention law to frustrate access to it.=== Alternatives ===As of 2015, advertisers and marketers look to involve their brands directly into the entertainment with native advertising and product placement (also known as brand integration or embedded marketing). An example of product placement would be for a soft drink manufacturer to pay a reality TV show producer to have the show's cast and host appear onscreen holding cans of the soft drink. Another common product placement is for an automotive manufacturer to give free cars to the producers of a TV show, in return for the show's producer depicting characters using these vehicles during the show.Some digital publications turned to their customers for help. For example, the Guardian is asking its readers for donations to help offset falling advertising revenue. According to the newspaper's editor-in-chief, Katharine Viner, the newspaper gets about the same amount of money from membership and paying readers as it does from advertising. The newspaper considered preventing readers from accessing its content if usage of ad-blocking software becomes widespread, but so far it keeps the content accessible for readers who employ ad-blockers.== See also ==== References ==
	The Goo Goo Dolls are an American rock band formed in 1986 in Buffalo, New York, by guitarist/vocalist Johnny Rzeznik, bassist/vocalist Robby Takac, and drummer George Tutuska. Mike Malinin was the band's drummer from December 1994 until December 27, 2013 (but not made an official member until 1998). The band are renowned for their commercially successful 1998 singles "Iris" and "Slide". Other notable singles include "Name" and "Naked" from 1995's A Boy Named Goo; "Black Balloon", "Dizzy", and "Broadway" from 1998's Dizzy Up the Girl; "Here Is Gone" from 2002's Gutterflower, "Better Days", "Give a Little Bit", and "Stay with You" from 2006's Let Love In, and "Home" from 2010's Something for the Rest of Us. The band have had 19 top ten singles on various charts, and have sold more than 12 million albums worldwide.In October 2012, "Iris" was ranked #1 on Billboard's "Top 100 Pop Songs 1992–2012" chart, which also featured "Slide" (ranking at #9) and "Name" (ranking at #24). The song spent nearly 12 straight months on the Billboard charts, and held the number one position on the Hot 100 Airplay chart for 18 weeks. "Home" extended the band's record to 14 top ten hits at the Hot AC radio format (more than any other artist in the history of that format).== History ===== Origins and early music (1986–1993) ===The band's original lineup included Johnny Rzeznik (guitar, vocals), Robby Takac (bass, vocals), and George Tutuska (drums, percussion). Takac and Tutuska had been long-time friends in school and met Rzeznik while he was playing in the band The Beaumonts with Takac's cousin, Paul Takac and close friend Michael Harvey who was the inspiration for the band.  The trio picked their name from a True Detective ad for a toy called a Goo Goo Doll. "We were young and we were a garage band not trying to get a deal. We had a gig that night and needed a name. It's the best we came up with, and for some reason it stuck. If I had five more minutes, I definitely would have picked a better name", John stated. With Takac as their lead singer, the band released their first album, Goo Goo Dolls in 1987 on Mercenary Records, but was picked up in 1988 by Celluloid Records, a larger record company. They played around Buffalo's underground music circuit and across the country opening for punk bands such as Gang Green, SNFU, Dag Nasty, Bad Religion, Motorhead, ALL, The Dead Milkmen, Doughboys, Big Drill Car, The Gun Club, Uniform Choice, The Dickies, and DRI and playing with fellow Buffalo bands. The band released its second album Jed in 1989.The band released its third album, Hold Me Up, in 1990 and featured Rzeznik as the lead vocalist on five tracks, including the single, "There You Are"—as well as their then concert favorite, "Two Days in February". After being embraced by local college radio and punk scenes (including playing such venues as CBGB), the Goo Goo Dolls' third release incorporated elements of heavy metal, pop rock, and punk rock. In 1991, the song "I'm Awake Now" was recorded for the soundtrack of Freddy's Dead: The Final Nightmare.Superstar Car Wash, released in 1993, received significant media attention.  The critical success and encouraging sales of their last album resulted in a larger budget from Metal Blade Records.  The album was partially recorded at Metalworks Studios in Mississauga, Ontario. "We Are the Normal" (the single for which Rzeznik asked frontman Paul Westerberg of The Replacements to write the lyrics) received a major push toward play on college and independent radio, while its video was displayed on MTV's 120 Minutes program. "Fallin' Down" made it onto the 1993 soundtrack of Pauly Shore's hit film Son in Law.=== Mainstream success (1993–2005) ======= A Boy Named Goo, legal issues, and Tutuska's departure (1993–1997) ====Shortly after recording the band's fifth album A Boy Named Goo, Tutuska was kicked out of the band. The band moved forward from this December 31, 1994, incident, hiring Mike Malinin, and toured rigorously.A Boy Named Goo, released in 1995, had a catchy rock sound, and became one of the most successful alternative rock albums of the mid-1990s. It sold modestly in this time; however, it was not until the release of the single "Name" that the band experienced any viable commercial success. A Boy Named Goo became the first album in Metal Blade history to achieve double-platinum status. This success, however, proved bittersweet, as the band found themselves in a legal battle with Metal Blade records. The band filed suit against Metal Blade, claiming they had not earned any royalties from their album's sales, which was attributed to a "grossly unfair, one-sided and unenforceable contract" which had been signed by the band in 1987. The two sides reached a settlement which had the band signed to the Metal Blade's distributing label, Warner Bros. Records, under which the band released their sixth album, Dizzy Up the Girl, in 1998. The undisputed success of "Name" marked a fundamental change in the band's sound from alternative rock to a more mainstream "adult orientated rock" direction which disappointed many of the original fans of the band. It was "Name" that had made the band popular and they were able to make guest appearances on Beverly Hills, 90210, Charmed, and even presented an award to Michael Jackson.==== Dizzy Up the Girl and Gutterflower (1997–2005) ====Rzeznik was approached to write a song for the City of Angels soundtrack, and the end product was "Iris".  This song continued the band's fame, as it stayed on top of Billboard Hot 100 Airplay charts for a record-breaking 18 weeks and spent 4 weeks at No. 1 on Billboard's Pop Songs chart. Later that year, it was nominated for three Grammys. According to several interviews with Rzeznik, he was experiencing serious bouts of writer's block when he was approached, and was on the verge of quitting the band days before he wrote the song."Iris" was included on the triple-platinum Dizzy Up the Girl, and was among top-ten hits "Slide", "Black Balloon", "Broadway", and "Dizzy" from the same album. In 2001, the Goos released their first ever compilation CD, What I Learned About Ego, Opinion, Art & Commerce. Next, Gutterflower (2002) achieved gold certification, producing the hits "Here Is Gone", "Sympathy", and "Big Machine". On July 4, 2004, the band performed a free concert in their hometown of Buffalo, playing through a deluge of rain that can be seen on the DVD released later that year. The DVD also contained a studio version of the Goo Goo Dolls' cover of "Give a Little Bit" by Supertramp. The single reached the top of the Adult Top 40 chart in 2005.July 4, 2004 has been proclaimed "Goo Goo Dolls Day" in their native Buffalo, New York.=== Let Love In (2005–2007) ===In 2006, the Goo Goo Dolls marked their 20th anniversary with their new album Let Love In, which included the studio recording of "Give a Little Bit" as well as other top 10 radio singles "Better Days", "Stay with You", and "Let Love In".  With their third consecutive single ("Let Love In") from the album, the Goo Goo Dolls hit a record 12 top 10 hits in Adult Top 40 history, beating Matchbox Twenty and Sheryl Crow until Matchbox Twenty's release of Exile on Mainstream and the Goo Goo Dolls' release of "Before It's Too Late" from the Transformers Soundtrack, which left both groups with 13 top 10 hits in the Adult Top 40. Goo Goo Dolls planned to release another single from Let Love In, "Without You Here", as well as a song from the July 2007 Transformers movie called "Before It's Too Late", originally titled "Fiction". To promote the new single, the Goo Goo Dolls performed "Before It's Too Late" at both The Tonight Show with Jay Leno on June 8, 2007, and again at The Late Late Show with Craig Ferguson on June 22, 2007. In July 2007 the band discussed their career as a whole and gave a live performance on A&E's Private Sessions. Rzeznik stated that after the release of "Without You Here" and their summer tour with Lifehouse and Colbie Caillat, the band would return to the studio to begin work on their next album, their ninth overall.On June 27, 2007, the Goo Goo Dolls performed to a sold out crowd at Red Rocks Amphitheatre in Morrison, Colorado.  The performance premiered on HDNet in high–definition on Sunday, September 30. The entire concert was released as a DVD on the limited edition version of their 2008 release, Vol.2.The Goo Goo Dolls and the NHL Buffalo Sabres came together to create a video for the Sabres 2007 playoff run. The video was a compilation of shots from the Buffalo area and Sabres players played to the song "Better Days". It was played on jumbotron and at the HSBC Arena before every playoff game.Though not certified by the Recording Industry Association of America (RIAA), the album is said to have gone Gold by various music sites.The song "Better Days" was used in the trailer for the 2009 film Love Happens. It was also used in the pilot episode of the CBS TV show, Jericho and in a promo for WGRZ aired during Super Bowl XLVI.=== Greatest hits albums (2007–2008) ===On November 13, 2007, the Goo Goo Dolls released a greatest hits album entitled Greatest Hits Volume One: The Singles, which includes a new version of "Name" recorded and mixed by Paul David Hager, and a remix of "Feel the Silence" by Michael Brauer. On August 19, 2008, a second greatest hits album was released entitled Vol.2, which includes b-sides, rarities, and a live performance at Red Rocks Amphitheatre from their "Let Love In" tour.The band performed "Better Days" and "Stay With You" at the halftime of the Detroit Lions' 2007 Thanksgiving Day game at Ford Field, which focused on the United Way and the NFL's commitment to youth health and fitness.=== Something for the Rest of Us (2008–2011) ===The Goo Goo Dolls announced recording sessions for a new album, on their official website, unrelated to their Volume Two in 2008.  In addition, the band performed as part of the O2 Wireless Festival in London's Hyde Park in the summer of 2008, at the Miller Lite Rock 'N Racing show at the Indianapolis Motor Speedway during qualifying for the 2008 Allstate 400 at the Brickyard NASCAR Sprint Cup Series race, as well as performing a four-show tour across England.On June 19, 2008, at a ceremony in New York City, the band's singer and principal songwriter John Rzeznik was named that year's recipient of the Hal David Starlight Songwriter Award. Past recipients of the prestigious award include: Rob Thomas, Alicia Keys, John Mayer and John Legend.On June 12, 2009, the band mentioned on their Facebook page that the producer for their new album was Tim Palmer. On August 15, 2009, Robby Takac posted an update on his Twitter account stating that the recording for the new album had been completed, and the mixing of the album was yet to be done. Then on September 18, 2009, John Rzeznik stated in an interview on Good Day L.A., that the new album had tentatively been titled Something for the Rest of Us.Something for the Rest of Us was released on August 31, 2010. While the album was full of the pop-rock that has sustained the band for decades, lead singer/guitarist Rzeznik's songwriting took on a more somber, serious tone.The band performed a live mini concert at the Apple Store in Manhattan, New York on December 2, 2010. The show was recorded and was released in 2011 as a part of Apple's "Live at SoHo" collection which is sold online at the iTunes Store. On January 3, 2011, the band performed the halftime show at the Orange Bowl played between the Stanford Cardinal and the Virginia Tech Hokies. On January 30, 2011, they performed before the start of the Pro Bowl in Hawaii. On February 7, 2011, they performed at the Delta Ballroom in St. John's Newfoundland, beginning their 2011 Canadian tour.=== Magnetic and Malinin's departure (2011–2015) ===During an interview with UpVenue on February 16, 2011, Rzeznik confirmed that he was writing new material for a new album: "I've actually been experimenting, in this last week, while we've been out on this tour, just writing lyrics and then figuring out the melodic structures", said Rzeznik. "We can't wait another four years to put another album out; that's just a ridiculous waste of time."On May 23, 2011, it was announced that the band had recorded a song for the 2011 film Transformers: Dark of the Moon called "All That You Are", and was released on the film's soundtrack on June 14, 2011. In September 2011, "Iris" reached the number three spot in the Top 40 in the UK, 13 years after its original release.On August 9, 2012, the band announced that they had started recording their tenth studio album. The new album, titled Magnetic, was recorded using multiple producers. On January 18, 2013, the band released the first single off of Magnetic titled "Rebel Beat". Magnetic was released on June 11, 2013  and debuted at #8 on the Billboard Top 200 Albums chart.  On July 19, 2013, the band released the second single off of Magnetic, "Come to Me".The band supported the album with a co-headlining tour with Matchbox Twenty, a Canadian tour, an acoustic tour with support from Run River North, and a co-headlining tour with Daughtry.On December 27, 2013, Malinin announced his departure from the band via Twitter and Facebook.The Goo Goo Dolls contributed to the soundtrack of the broadway musical Finding Neverland in 2015. The soundtrack, released June 9, 2015, contained the song "If the World Turned Upside Down."=== Boxes, vinyl albums, and 20th anniversary of A Boy Named Goo and touring (2015–2017) ===The band spent the majority of 2015 in the studio writing and recording their eleventh studio album, Boxes, at Bear Creek Studio in Woodinville, Washington. Robby Takac revealed that there were eleven tracks, two of which were songs he sang lead vocals on, and that three out of eleven tracks were already completed. The album was released on May 6, 2016.The Goo Goo Dolls celebrated the 20th anniversary of the release of A Boy Named Goo by releasing a special edition of the album on November 27, 2015. The special edition contains the original track listing of the album, along with seven unreleased tracks. In addition, Dizzy Up the Girl and A Boy Named Goo were released for the first time on vinyl on October 8 and November 27, respectively.On February 22, 2016, the band announced a 2016 summer tour with Collective Soul and Tribe Society to promote Boxes.Goo Goo Dolls released an exclusive vinyl box set for Record Store Day on April 22, 2017 entitled Pick Pockets, Petty Thieves, and Tiny Victories (1987-1995) on Warner Bros. Records. The five vinyl disc box set features their first five studio albums, including their first album, Goo Goo Dolls on vinyl for the first time since 1987, Hold Me Up for the first time on vinyl domestically, Superstar Car Wash and Jed for the first time ever on vinyl and A Boy Named Goo. There were just 1,000 copies of the box set pressed.=== You Should Be Happy EP, touring, 20th Anniversary of Dizzy Up the Girl (2017–2018) ===On May 12, 2017, Goo Goo Dolls released a five-song EP entitled You Should Be Happy. The EP features four original songs, as well as a remix of the title track from their last album, "Boxes". In support of the EP, the band toured throughout the summer of 2017 on the "Long Way Home" tour with Phillip Phillips. The tour included a performance in their hometown, Buffalo, on August 12. With 2018 marking the 20th anniversary of Dizzy Up the Girl, the band embarked on a commemorative tour where they performed the album in its entirety. The tour began on September 30, and wrapped that November. 2018 also saw the release of two live albums for the band: The Audience is This Way in June, and The Audience is That Way in November.=== Second vinyl box set and Miracle Pill (2019-present) ===On April 13, 2019, (Record Store Day), the Goo Goo Dolls released an exclusive vinyl box set entitled Topography (1998-2013) on Warner Bros. Records. The five vinyl box set features five of the band's studio albums, from Dizzy Up the Girl to Magnetic. In April 2019, the band's website stated they were working on their next studio album, their twelfth, with videos showing updates of recording it at Capitol Records.In May 2019, they announced the album's title will be Miracle Pill. On June 7, the band kicked off a North American co-headlining tour with Train. On June 21, 2019, the album's first single and title track, "Miracle Pill," was released. On July 16, the music video for "Miracle Pill" was released. Late in the day on July 18, the lyric video for the album's second single, "Money, Fame & Fortune," was uploaded to YouTube. On the same day, it was confirmed that the album will be released on September 13, 2019. "Money, Fame & Fortune" was released as an immediate download for people who pre-order the album. A third single, "Indestructible," was released on August 9, 2019.== Philanthropy ==The Goo Goo Dolls have been closely involved with several charities over the years. Through the organization USA Harvest, millions of meals have been distributed to people in need by generous fans bringing nonperishable goods to the band's concerts over the course of more than a decade. A scholarship fund was created by the band for St. Joseph's Collegiate Institute in honor of Nicholas Orrange, John Rzeznik's nephew. It is a high school with higher academic requirements. The purpose of this scholarship is to aid people who carry the grades but cannot afford the school.Over the years the Goo Goo Dolls have also been involved with the Food Bank of Western NY and Compass House. The band has also volunteered their time to perform at events for ECHO, Light of Day, among others. They have lent a hand for causes from Autism research, to scholastic musical programs as well as participating in, and donating to innumerable other charities.John Rzeznik is an ambassador for VH1's Save the Music Foundation. Robby Takac is the founder of the Music is Art Foundation.== Band members ===== Timeline ===== Discography ==Studio albumsGoo Goo Dolls (1987)Jed (1989)Hold Me Up (1990)Superstar Car Wash (1993)A Boy Named Goo (1995)Dizzy Up the Girl (1998)Gutterflower (2002)Let Love In (2006)Something for the Rest of Us (2010)Magnetic (2013)Boxes (2016)Miracle Pill (2019)== Awards and nominations ===== Billboard Music Awards ====== Other Awards ===== References ==== External links ==Official website Goo Goo Dolls at Curlie
	Optimove is a privately held company that develops and markets a Relationship Marketing software as a service (SaaS). Optimove's product has a Customer Data Platform at its core and applies algorithmic optimization to autonomously improve multichannel campaigns. The company serves various industries, including retail, eCommerce, travel and hospitality, gaming, and financial services.== Corporate History ==Optimove (initially named Mobius Solutions) was founded in 2009 by Pini Yakuel and Shachar Cohen, and released the first version of its software (initially named Customer Value Maximizer) in 2010.The company is headquartered in Tel Aviv, Israel, with additional offices in London (opened in 2015) and New York City (opened in 2016) and employs more than 200 people.Optimove partners include IBM, Internet gaming platform provider iSoftBet, India's largest poker site, Adda52.com, binary options trading platform provider, TradoLogic, and social gaming operators Crazy Panda, Jelly Button Games and LuckyFish Games.In September 2016, Optimove raised its first outside funding, a $20 million round from Israel Growth Partners that valued the company at $100 million.== Software and Technology ==Optimove is a SaaS application that implements a systematic approach to planning, executing, measuring and optimizing a company's customer marketing plan with the goal of maximizing customer lifetime value. The software models each customer's behavior and preferences in order to predict which marketing campaigns will be most relevant for each individual.The marketing channels supported by Optimove include email, SMS, mobile push notification, website/app pop-ups, Facebook Custom Audiences and Google Ads.The software is based on a combination of technologies, including predictive customer modeling, customer micro-segmentation, multi-channel campaign automation, real-time campaign triggers, and systematic campaign optimization using scientific control methodologies. The application's primary interface is a calendar-based marketing management tool that helps users track and optimize campaigns. Campaigns are analyzed as measurable marketing experiments so that users can determine the financial uplift that each marketing campaign generated.The Optimove product doesn't collect or use any customer-identifying demographic data, which eases privacy concerns.== Critical reception ==Deloitte Consulting included Optimove in the Deloitte EMEA 2016 Technology Fast 500, 8 December 2016Deloitte Consulting included Optimove in the Deloitte Israel 2016 Technology Fast 50 list, 29 November 2016Israel21c included Optimove in its list of 11 Israeli companies to watch in marketing technology, May 4, 2016.Killer Startups included Optimove in its list of 14 Marketing Automation Tools That Can Save Hundreds Of Hours, January 15, 2015.The Huffington Post included Optimove in its list of 10 Impressive Tel Aviv Tech Startups, August 6, 2013.Forbes included Optimove in its list of 15 Marketing Software That Can Boost Your Business, July 28, 2013.Silverpop selected Optimove as its 2013 EMEA Partner of the Year, July 2, 2013.== References ==== External links ==Company Website
	ReplayGain is a proposed standard published by David Robinson in 2001 to measure the perceived loudness of audio in computer audio formats such as MP3 and Ogg Vorbis. It allows media players to normalize loudness for individual tracks or albums. This avoids the common problem of having to manually adjust volume levels between tracks when playing audio files from albums that have been mastered at different loudness levels.Although this de facto standard is now formally known as ReplayGain, it was originally known as Replay Gain and is sometimes abbreviated RG.ReplayGain is supported in a large number of media software and portable devices.== Operation ==ReplayGain works by first performing a psychoacoustic analysis of an entire audio track or album to measure peak level and perceived loudness. The difference between the measured perceived loudness and the desired target loudness is calculated; this is considered the ideal replay gain value. Typically, the replay gain and peak level values are then stored as metadata in the audio file. ReplayGain-capable audio players use the replay gain metadata to automatically attenuate or amplify the signal on a per-track or per-album basis such that tracks or albums play at a similar loudness level. The peak level metadata can be used to prevent gain adjustments from inducing clipping in the playback device.=== Metadata ===The original ReplayGain proposal specified an 8-byte field in the header of any file. Most implementations now use tags for ReplayGain information. FLAC and Ogg Vorbis use the REPLAYGAIN_* Vorbis comment fields. MP3 files usually use ID3v2.  Other formats such as AAC and WMA use their native tag formats with a specially formatted tag entry listing the track's replay gain and peak loudness.ReplayGain utilities usually add metadata to the audio files without altering the original audio data. Alternatively, a tool can amplify or attenuate the data itself and save the result to another, gain-adjusted audio file; this is not perfectly reversible in most cases. Some lossy audio formats, such as MP3, are structured in a way that they encode the volume of each compressed frame in a stream, and tools such as MP3Gain take advantage of this for directly applying the gain adjustment to MP3 files, adding undo information so that the process is reversible.=== Target loudness ===The target loudness of ReplayGain utilities is 89 dB sound pressure level. The SPL reference comes from a SMPTE recommendation used to calibrate playback levels in movie theaters.A more common means of specifying a reference level is relative to a full-scale signal. ReplayGain nominally plays at -14 dB relative to full-scale leaving 14 dB of headroom for reproduction of dynamic material. In contrast, the SMPTE RP 200:2002, on which the ReplayGain reference was originally based, recommends 20 dB of headroom. The more recent EBU R 128 suggests 23 dB.=== Track-gain and album-gain ===ReplayGain analysis can be performed on individual tracks, so that all tracks will be of equal volume on playback. Analysis can also be performed on a per-album basis. In album-gain analysis an additional peak-value and gain-value, which will be shared by the whole album, is calculated. Using the album-gain values during playback will preserve the volume differences among tracks on an album.On playback, listeners may decide if they want all tracks to sound equally loud or if they want all albums to sound equally loud with different tracks having different loudness. In album-gain mode, when album-gain data is missing, players should use track-gain data instead.== Alternatives ==Peak amplitude is not a reliable indicator of loudness, so consequently peak normalization does not offer reliable normalization of perceived loudness. RMS normalization is a little more accurate, but care must be taken not to introduce clipping, either by guaranteeing appropriate headroom or by using hard or soft limiting. (ReplayGain itself is an elaboration on RMS normalization.)With audio level compression, volume may be altered on-the-fly on playback (meaning "variable gain", as opposed to the "constant gain" as rendered by ReplayGain), but the dynamic range will be compressed. Although this is beneficial in keeping volume constant at all times, it is not always desirable.Sound Check is a proprietary Apple Inc. technology similar in function to ReplayGain. It is available in iTunes and on the iPod.Standard measurement algorithms for broadcast loudness monitoring applications have recently been developed by the International Telecommunication Union (ITU-R BS.1770) and the European Broadcasting Union (EBU R128).== Implementations ===== Audio players ====== Portable media players ===All devices with a working Rockbox portSandisk Sansa Fuze and Sansa Clip+iPod through other programs that convert ReplayGain data to the Apple proprietary Sound Check format (e.g. iPod Manager for foobar2000, other alternatives elsewhere on this page)Typical CD players and other legacy audio players do not support ReplayGain.==== Android compatible players ====DeaDBeeFfoobar2000 for AndroidGoneMAD Music PlayerNeutron Music PlayerMyTunesPowerAMPVanilla MusicWinamp PRO for AndroidXenoAmp Music Player=== Scanners ===beaTunes: Writes the standard replaygain_track_gain/replaygain_track_peak tags and replaces the iTunNORM metadata tag value, which is used by iTunes software and iPod music players for Sound Check volume normalization.Ex Falso: Included plugin scans files on a per-album base, writes the standard tags into metadata.EZ CD Audio Converter: Music converter, CD ripper and metadata editor.FLAC and metaflac: Encoder can optionally generate metadata. Tagger generates metadata.foobar2000: Generates metadata through included plugin using EBU R128 (but at old 89 dB levels) for all supported tag formats.iVolume: Replaces the iTunNORM metadata tag value (optionally on a per-album basis), which is used by iTunes software and iPod music players for Sound Check volume normalization.LAME: Encoder writes metadata to LAME tagMediaMonkey: Analyze Volume calculates RG values and writes them into the files as tags and into its library databaseMP3Gain: (open source) generates metadata. Can directly modify original file and write undo information as metadata.QuuxPlayer for Windows: calculates gain values and saves them in its library database; optionally writes ReplayGain tags to files with ID3v2 tags.Quod Libet: Based on Ex Falso. Generates metadata through included plugin to analyze and write ReplayGain informationRapid Evolution: Generates metadatasoundKonverter: frontend for various audio conversion tools. Is built using KDE Development Platform and has a ReplayGain tool.Winamp: Generates metadatargain: Generates metadataSound Normalizer== Notes ==== See also ==Alignment levelDialnormLoudness war== References ==== External links ==ReplayGain specificationReplayGain at Hydrogenaudio wikiReplay Gain – A Proposed Standard, the original proposal, now out of date with respect to current practiceReplay Gain in Linux — guide to using graphical and command line ReplayGain tools in Linux.
	Kitura is a free and open-source web framework written in Swift, developed by IBM and licensed under Apache 2.0.It’s an HTTP server and web framework for writing Swift server applications.== Features ==URL routing (GET, POST, PUT, DELETE)URL parametersStatic file servingFastCGI supportJSON parsingPluggable middleware== References ==== External links ==Official website
	A privacy platform often refers to software or systems that are designed in such a way to retain the privacy of its users and operators.== Projects ==Private.MeGeens.com
	XPLM Publisher is a commercial authoring and publishing software developed and sold by XPLM. It combines the Oxygen XML Editor with Oracle Agile PLM and helps technical writers to create, manage, and publish technical product documentation (e.g., user guides) invarious formats and layouts. The XPLM Publisher follows the Darwin Information Typing Architecture (DITA) and the methods of single source publishing. This allows the same source content to be (re-)used across multiple forms of media and more than one time.The main aspect of the XPLM Publisher is the tight integration with a PLM system as content management system and "single source of truth". Thus the technical writers are only provided with always valid and released engineering information.== References ==== External links ==XPLM product list
	The Vexi project is an international effort to create an easy-to-use platform for the development and delivery of Internet application interfaces outside of the standard browser stack.  It has similarities with XUL but runs on top of the Java stack, making it browser independent.== Origins ==Sometime in late 2001, Adam Megacz released the XWT project as open source software.  XWT eventually evolved into the Ibex project as the technology underpinning the project got overhauled and enhanced, but the upheaval would eventually lead to discontent among contributors upset at the extended development period of several years without a stable release.  Eventually, in April 2004, core contributor David Crawshaw called for a stable release to avoid a fork, which Adam declined instead encouraging the fork as he felt community pressure was compromising technical decision making.== Vexi 1.0 ==In April 2004, announcements of Vexi began to appear, and a project website became available on the since-defunct www.vexi.org domain.  The initial principal goal of the Vexi project was to create a stable release based on the XWT/Ibex technology stack.  Whilst Vexi 1.0 was never officially declared, there were several releases and by late 2005 there was a stable version but it omitted many of the new Ibex technologies.== Vexi 2.0 ==Vexi 2.0 development focused on integrating these, but they presented problems including incompleteness, severe bugginess, and unreliability under load, making a Vexi 2.0 release look less likely as time went on.  There was never an official Vexi 2.0 release.== Vexi 3.0 ==In 2007 the principal developers of Vexi - now brothers Charles and Michael Goodwin - announced their intentions to break with the 2.0 platform API and overhaul it using the lessons they learned, refining the Ibex technology stack to replace the problematic parts.  This new version, tentatively called Vexi 3.0, arrived at feature completeness in 2009 and is stable.Vexi is still under active development.== Companies Using Vexi ==Vexi is in production use by several companies around the globe.Companies currently specialise in developing Vexi applications:Web Enable IT (UK) with their software EmanateAchievement Focused Technology, Inc. (US)Health Care Systems Corporation (US) with their software HCare1== Contributors ===== Current ===Charles Goodwin (Core layout and widget development)Michael Goodwin (New core development)Jeff Buhrt (Core debugging)=== Inactive ===Adam Megacz (Original author of XWT/Ibex)Brian Alliet (Major core contributor)David Crawshaw (Major core contributor)Tupshin Harper (Core network code)Adam Andrews (Core and widget debugging)== References ==== External links ==Official website
	MiniTool Partition Wizard is a partition management program for hard disk drives developed by MiniTool Solution.== References ==== External links ==Official website
	ProBiS is a computer software which allows prediction of binding sites and their corresponding ligands for a given protein structure. Initially ProBiS was developed as a ProBiS algorithm by Janez Konc and Dušanka Janežič in 2010 and is now available as ProBiS server, ProBiS CHARMMing server, ProBiS algorithm and ProBiS plugin. The name ProBiS originates from the purpose of the software itself, that is to predict for a given Protein structure Binding Sites and their corresponding ligands.== Description ==ProBiS software started as ProBiS algorithm that detects structurally similar sites on protein surfaces by local surface structure alignment using a fast maximum clique algorithm. The ProBiS algorithm was followed by ProBiS server which provides access to the program ProBiS that detects protein binding sites based on local structural alignments. There are two ProBiS servers available, ProBiS server and ProBiS CHARMMing server. The latter connects ProBiS and CHARMMing servers into one functional unit that enables prediction of protein−ligand complexes and allows for their geometry optimization and interaction energy calculation. The ProBiS CHARMMing server with these additional functions can only be used at National Institutes of Health, USA. Otherwise it acts as a regular ProBiS server. Additionally a ProBiS PyMOL plugin and ProBiS UCSF Chimera plugin have been made. Both plugins are connected via the internet to a newly prepared database of pre-calculated binding site comparisons to allow fast prediction of binding sites in existing proteins from the Protein Data Bank. They enable viewing of predicted binding sites and ligands poses in three-dimensional graphics.== Protein building sites tools ==Detect structurally similar binding sitesThis tool takes as an input a query protein or a binding site. The ProBiS algorithm structurally compares the query independently of sequence or fold with a database of non-redundant protein structures. The output of this tool are a 3D query protein colored by degrees of structural conservation from blue (unconserved) to red (structurally conserved) in Jmol viewer and a table of similar proteins.Pairwise local structural alignmentThis tool takes as an input two proteins or binding sites. The ProBiS algorithm compares structures based on geometry as well as physicochemical properties and returns their local structural alignment.ProBiS web server RESTful Web ServicesThe ProBiS web server features RESTful (Representational State Transfer) web services to make the binding site similarities and local pairwise alignments for any PDB protein structure easily accessible from any script.ProBiS-Database accessProBiS-Database can be accessed directly from ProBiS (CHARMMing) server, ProBiS-Database widget, which can be included in any web page to provide access to the ProBiS-Database, or RESTful Web Services, which make ProBiS-Database easily accessible from any script.== History ==ProBiS algorithm for detection of structurally similar protein binding sites by local structural alignment (March 2010)ProBiS: A web server for detection of structurally similar protein binding sites (February 2010)ProBiS-Database: Precalculated Binding Site Similarities and Local Pairwise Alignments of Pdb Structures (December 2011)ProBiS - 2012: Web server and web services for detection of structurally similar binding sites in proteins (February 2012)Parallel-ProBiS: Fast Parallel Algorithm for Local Structural Comparison of Protein Structures and Binding Sites (March 2012)ProBiS-ligands: a web server for prediction of ligands by examination of protein binding sites (February 2014)ProBiS-Charmming: Web Interface for Prediction and Optimization of Ligands in Protein Binding Sites (August 2015)== References ==== External links ==ProBiS server
	Submission software is a category of computer software that allows its users to publish their products or websites over the Internet. This software is typically used by marketing professionals who work in online marketing. It represents an electronic solution for online marketing as opposed to offline (newspapers, street banners) or media (radio, television) marketing.Usually these packages allow three types of submissions: automatic, semi-automatic and manual.== Types of submission software ==Software submission - allows to submit software products either through the use of PAD files or by filling the websites submission formsArticle submission - submits articles to article directories or online magazinesWebsite submission - submits website addresses to all kind of directoriesPress release submission - applications that allow users to submit press releases to PR websitesRSS submission - submits RSS feeds to RSS publishing sites== References ==== See also ==Portable Application DescriptionSearch engine submission
	In distributed computing, code mobility is the ability for running programs, code or objects to be migrated (or moved) from one machine or application to another. This is the process of moving mobile code across the nodes of a network as opposed to distributed computation where the data is moved.It is common practice in distributed systems to require the movement of code or processes between parts of the system, instead of data.Examples of code mobility include scripts downloaded over a network (for example JavaScript, VBScript), Java applets, ActiveX controls, Flash animations, Shockwave movies (and Xtras), and macros embedded within Microsoft Office documents.== Overview ==The purpose of code mobility is to support sophisticated operations. For example, an application can send an object to another machine, and the object can resume executing inside the application on the remote machine with the same state as it had in the originating application.According to a classification proposed by Fuggetta, Picco and Vigna, code mobility can be either strong or weak:strong code mobility involves moving both the code, data and the execution state from one host to another, notably via a process image (this is important in cases where the running application needs to maintain its state as it migrates from host to host), while weak code mobility involves moving the code and the data only. Therefore, it may be necessary to restart the execution of the program at the destination host.Several paradigms, or architectural styles, exist within code mobility:Remote evaluation — A client sends code to a remote machine for execution.Code on demand — A client downloads code from a remote machine to execute locally.Mobile agents — Objects or code with the ability to migrate between machines autonomously.== Implementations ==Within code mobility, the Mobile Agent paradigm has conventionally attracted the most interest and research, however some recent work has produced general purpose implementations.Mobile agent frameworksAglets — Mobile agent framework, JavaJava Agent Development Framework — Mobile agent framework, JavaMobile-C — Mobile agent platform, C/C++ Mobile code can also be encapsulated or embedded in other file formats not traditionally associated with executable code.  An example of this form of encapsulation is the presence of JavaScript in a PDF.== Viruses ==Mobile code can also download and execute in the client workstation via email.  Mobile code may download via an email attachment (e.g., macro in a Word file) or via an HTML email body (e.g., JavaScript).  For example, the ILOVEYOU, TRUELOVE, and AnnaK email viruses/worms all were implemented as mobile code (VBScript in a .vbs email attachment that executed in Windows Scripting Host). In almost all situations, the user is not aware that mobile code is downloading and executing in their workstation.== Renting code ==Mobile code also refers to code "used for rent", a way of making software packages more affordable.  i.e. to use on demand.  This is specially relevant to the mobile devices being developed which are cellular phones, PDAs, etc. all in one.  Instead of installing software packages, they can be "leased" and paid for on a per-usage basis.== See also ==Code on demandMobile agentRemote evaluation== References ==
	High availability software is software used to ensure that systems are running and available most of the time. High availability is a high percentage of time that the system is functioning.  It can be formally defined as (1 – (down time/ total time))*100%.  Although the minimum required availability varies by task, systems typically attempt to achieve 99.999% (5-nines) availability.  This characteristic is weaker than fault tolerance, which typically seeks to provide 100% availability, albeit with significant price and performance penalties.High availability software is measured by its performance when a subsystem fails, its ability to resume service in a state close to the state of the system at the time of the original failure, and its ability to perform other service-affecting tasks (such as software upgrade or configuration changes) in a manner that eliminates or minimizes down time.  All faults that affect availability – hardware, software, and configuration need to be addressed by High Availability Software to maximize availability.== Features ==Typical high availability software provides features that:Enable hardware and software redundancy:These features include:The discovery of hardware and software entities,The assignment of active/standby roles to these entities,Detection of failed components,Notification to redundant components that they should become active, andThe ability to scale the system.A service is not available if it cannot service all the requests being placed on it. The “scale-out” property of a system refers to the ability to create multiple copies of a subsystem to address increasing demand, and to efficiently distribute incoming work to these copies (Load balancing (computing)) preferably without shutting down the system.  High availability software should enable scale-out without interrupting service.Enable active/standby communication (notably Checkpointing):Active subsystems need to communicate to standby subsystems to ensure that the standby is ready to take over where the active left off.  High Availability Software can provide communications abstractions like redundant message and event queues to help active subsystems in this task.  Additionally, an important concept called “checkpointing” is exclusive to highly available software.  In a checkpointed system, the active subsystem identifies all of its critical state and periodically updates the standby with any changes to this state.  This idea is commonly abstracted as a distributed hash table – the active writes key/value records into the table and both the active and standby subsystems read from it.  Unlike a “cloud” distributed hash table (Chord (peer-to-peer), Kademlia, etc.) a checkpoint is fully replicated.  That is, all records in the “checkpoint” hash table are readable so long as one copy is running.  Another technique, called an [application checkpoint], periodically saves the entire state of a program.Enable in-service upgrades:In Service Software Upgrade is the ability to upgrade software without degrading service.  It is typically implemented in redundant systems by executing what is called a “rolling” upgrade—upgrading the standby while the active provides service, failing over, and then upgrading the old active.  Another important feature is the ability to rapidly fall back to an older version of the software and configuration if the new version fails.Minimize standby latency and ensure standby correctness:Standby latency is defined as the time between when a standby is told to become active and when it is actually providing service.   “Hot” standby systems are those that actively update internal state in response to active system checkpoints, resulting in millisecond down times.  “Cold” standby systems are offline until the active fails and typically restart from a “baseline” state.  For example, many cloud solutions will restart a virtual machine on another physical machine if the underlying physical machine fails.  “Cold” fail over standby latency can range from 30+ seconds to several minutes.  Finally, “warm” standby is an informal term encompassing all systems that are running yet must do some internal processing before becoming active.  For example, a warm standby system might be handling low priority jobs – when the active fails it aborts these jobs and reads the active's checkpointed state before resuming service.  Warm standby latencies depend on how much data is checkpointed but typically have a few seconds latency.== System architecture ==High availability software can help engineers create complex system architectures that are designed to minimize the scope of failures and to handle specific failure modes.   A “normal” failure is defined as one which can be handled by the software architecture's, while a “catastrophic” failure is defined as one which is not handled.  A catastrophic failure therefore causes a service outage.  However, the software can still greatly increase availability by automatically returning to an in-service state as soon as the catastrophic failure is remedied.The simplest configuration (or “redundancy model”) is 1 active, 1 standby, or 1+1.  Another common configuration is N+1 (N active, 1 standby), which reduces total system cost by having fewer standby subsystems.  Some systems use an all-active model, which has the advantage that “standby” subsystems are being constantly validated.Configurations can also be defined with active, hot standby, and cold standby (or idle) subsystems, extending the traditional “active+standby” nomenclature to “active+standby+idle” (e.g. 5+1+1).  Typically, “cold standby” or “idle” subsystems are active for lower priority work.  Sometimes these systems are located far away from their redundant pair in a strategy called geographic redundancy.  This architecture seeks to avoid loss of service from physically-local events (fire, flood, earthquake) by separating redundant machines.Sophisticated policies can be specified by high availability software to differentiate software from hardware faults, and to attempt time-delayed restarts of individual software processes, entire software stacks, or entire systems.== Use in industry ==In the past 20 years telecommunication networks and other complex software systems have become essential parts of business and recreational activities.“At the same time [as the economy is in a downturn], 60% almost -- that's six out of 10 businesses -- require 99.999.   That's four nines or five nines of availability and uptime for their mission-critical line-of-business applications.And 9% of the respondents, so that's almost one out of 10 companies, say that they need greater than five nines of uptime. So what that means is, no downtime. In other words, you have got to really have bulletproof, bombproof applications and hardware systems. So you know, what do you use? Well one thing you have high-availability clusters or you have the more expensive and more complex fault-tolerance servers.”Telecommunications:  High Availability Software is an essential component of telecommunications equipment since a network outage can result in significant loss in revenue for telecom providers and telephone access to emergency services is an important public safety issue.Defense/Military:  Recently High Availability Software has found its way into defense projects as an inexpensive way to provide availability for manned and unmanned vehiclesSpace:  High Availability Software is proposed for use of non-radiation hardened equipment in space environments.  Radiation hardened electronics is significantly more expensive and lower performance than off-the-shelf equipment.  But High Availability Software running on a single or pair of rad-hardened controllers can manage many redundant high performance non-rad-hard computers, potentially failing over and resetting them in the event of a fault.== Use in the cloud ==Typical cloud services provide a set of networked computers (typical a virtual machine) running a standard server OS like Linux.  Computers can often communicate with other instances within the same data center for free (tenant network) and to outside computers for fee.  The cloud infrastructure may provide simple fault detection and restart at the virtual machine level.  However, restarts can take several minutes resulting in lower availability.  Additionally, cloud services cannot detect software failures within the virtual machines.  High Availability Software running inside the cloud virtual machines can detect software (and virtual machine) failures in seconds and can use checkpointing to ensure that standby virtual machines are ready to take over service.== Standards ==The Service Availability Forum defines standards for application-aware High Availability.== See also ==Computer cluster== References ==== External links ==OpenClovis SAFplus High Availability SoftwareLinux-HA SoftwareKeepalived for LinuxEvidian SafeKit Software for Windows and Linux
	This is a list of the men's American weightlifting champions since 1928.== Champions by name (since 1928) ==As of 13 May 2016== Multiple champions ==As of 13 May 2016== See also ==List of United States women's national weightlifting champions== References ==
	Windward Studios is a software development company based in Boulder, Colorado.  The company began in 1996 with the release of Enemy Nations, a popular Real-Time strategy game, before focusing on its reporting software program Windward Reports.== 1996-2003: Enemy Nations ==Windward Studios was founded by David Thielen as a software gaming company. Enemy Nations is a real-time strategy game that received very high rankings in video game magazines. The publisher went out of business shortly after the game's release, forcing developer Windward Studios to sell the game exclusively from its website. Since October 2005 the game is non-commercial Freeware and the game and the source code was made available to download.== 2003-Present: Windward Reports / Windward Studios ==In 2003, the company realigned and focused on offering an enterprise-level suite of reporting and document generation tools for business teams called Windward Reports.  Windward Reports is comparable to other reporting software such as Crystal Reports and Pentaho.  Windward Reports’ customers include GE Industrial Equipment Services, Fidelity Investments and Pfizer Inc.Windward's solution includes a .NET or Java Engine, a set of code libraries for integrating reporting and document generation into both internal and commercial software applications, and AutoTag, a template design tool that allows technical and non-technical users create report and document templates in Microsoft Office. Windward's products have been positively reviewed in eWeek  and DevSource.== 2011-Present: Code Wars ==Windward Studios sponsors the annual International Collegiate Programming Championship, aka "Code Wars," "a huge student hackathon where teams from top universities around the world have 8 hours to analyze a problem, create a solution, and test it against the entries of the other programming experts." == Media coverage ==Cubicle Wars, an early marketing video by the company, received an overwhelming response when it debuted in 2006. With over 2 million views on YouTube and Digg.com the video became an internet phenomenon large enough to merit a response in BusinessWeek’s August 2007 SmallBiz publication.== References ==== External links ==Official website
	Software categories are groups of software. They allow software to be understood in terms of those categories. Instead of the particularities of each package. Different classification schemes consider different aspects of software.== Computer Software ==Computer software can be put into categories based on common function, type, or field of use. There are three broad classifications:Application software is the general designation of computer programs for performing tasks. Application software may be general purpose (word processing, web browsers, etc.) or have a specific purpose (accounting, truck scheduling, etc.). Application software contrasts with system software.System software is a generic term referring to the computer programs used to start and run computer systems including diverse application software and networks.Computer programming tools, such as compilers and linker, are used to translate and combine computer program source code and libraries into executable RAMs (programs that will belong to one of the three said)=== Copyright status ===The GNU Project categorizes software by copyright status: free software, open source software, public domain software, copylefted software, noncopylefted free software, lax permissive licensed software, GPL-covered software, the GNU operating system, GNU programs, GNU software, FSF-copyrighted GNU software, nonfree software, proprietary software, freeware, shareware, private software and commercial software.==== Free software ====Free software is software that comes with permission for anyone to use, copy and distribute, either verbatim or with modifications, either gratis or for a fee. In particular, this means that source code must be available. "If it's not source, it's not software." If a program is free, then it can potentially be included in a free operating system such as GNU, or free versions of the Linux system.Free software in the sense of copyright license (and the GNU project) is a matter of freedom, not price. But proprietary software companies typically use the term "free software" to refer to price. Sometimes this means a binary copy can be obtained at no charge; sometimes this means a copy is bundled with a computer for sale at no additional charge.==== Open source software ====Open source software is software with its source code made available under a certain license to its licensees. It can be used and disseminated at any point, the source code is open and can be modified as required. The one condition with this type of software is that when changes are made users should make these changes known to others. One of the key characteristics of open source software is that it is the shared intellectual property of all developers and users. The Linux operating system is one of the best known examples of a collection of open source software.==== Copylefted software ====Copylefted software is free software whose distribution terms ensure that all copies of all versions carry more or less the same distribution terms. This means, for instance, that copyleft licenses generally disallow others to add additional requirements to the software (though a limited set of safe added requirements can be allowed) and require making source code available. This shields the program, and its modified versions, from some of the common ways of making a program proprietary. Some copyleft licenses block other means of turning software proprietary.Copyleft is a general concept. Copylefting an actual program requires a specific set of distribution terms. Different copyleft licenses are usually “incompatible” due to varying terms, which makes it illegal to merge the code using one license with the code using the other license. If two pieces of software use the same license, they are generally mergeable.==== Non-copylefted free software ====Noncopylefted free software comes from the author with permission to redistribute and modify and to add license restrictions.If a program is free but not copylefted, then some copies or modified versions may not be free. A software company can compile the program, with or without modifications, and distribute the executable file as a proprietary software product. The X Window System illustrates this approach. The X Consortium releases X11 with distribution terms that make it noncopylefted free software. If you wish, you can get a copy which has those distribution terms and is free. However, nonfree versions are available and workstations and PC graphics boards for which nonfree versions are the only ones that work. The developers of X11 made X11 nonfree for a while; they were able to do this because others had contributed their code under the same noncopyleft license.==== Shareware ====Shareware is software that comes with permission to redistribute copies, but says that anyone who continues to use a copy is required to pay. Shareware is not free software, or even semi-free. For most shareware, source code is not available; thus, the program cannot be modified. Shareware does not come with permission to make a copy and install it without paying a license fee, including for nonprofit activity.==== Freeware ====Like shareware, freeware is software available for download and distribution without any initial payment. Freeware never has an associated fee. Things like minor program updates and small games are commonly distributed as freeware. Though freeware is cost free, it is copyrighted, so other people can't market the software as their own.== Microsoft TechNet and AIS Software categories ==This classification has seven major elements. They are: platform and management, education and reference, home and entertainment, content and communication, operations and professional, product manufacturing and service delivery, and line of business.Platform and management—Desktop and network infrastructure and management software that allows users to control the computer operating environment, hardware components and peripherals and infrastructure services and security.Education and reference—Educational software that does not contain resources, such as training or help files for a specific application.Home and entertainment—Applications designed primarily for use in or for the home, or for entertainment.Content and communications—Common applications for productivity, content creation, and communications. These typically include office productivity suites, multimedia players, file viewers, Web browsers, and collaboration tools.Operations and professional—Applications designed for business uses such as enterprise resource management, customer relations management, supply chain and manufacturing tasks, application development, information management and access, and tasks performed by both business and technical equipment.Product manufacturing and service delivery—Help users create products or deliver services in specific industries. Categories in this section are used by the North American Industry Classification System (NAICS).=== Vertical applications ===Agriculture, Forestry and HuntingMining, Quarrying, and Oil and Gas ExtractionUtilitiesConstructionManufacturingWholesale TradeRetail TradeTransportation and WarehousingInformationFinance and InsuranceReal Estate and Rental and LeasingProfessional, Scientific,and Technical ServicesPostal and MailingManagement of Companies and EnterprisesAdministrative and Support and Waste Management and Remediation ServicesEducational ServicesHealth Care and Social AssistanceArts, Entertainment, and RecreationAccommodation and Food ServicesPublic AdministrationOther Services (except Public Administration)Internal and proprietary line-of-business applications== References ==== External links ==
	MaSMT is a free, lightweight Multi-agent system development framework, design through the Java environment. The MaSMT3 framework provides three types of agents, namely ordinary agent and managing agent and root agent. The managing agent capable to handle set of ordinary agent and the root agent capable to handle set of manager agents. MaSMT3.0 includes few features than the previous versions. MaSMT 3.0 includes root agent to handle swam of agents, Environment handling features to dynamically store agent's ontology, and notice board has been introducing to see required messages and events.  In addition to these main features, agent status monitor has been introducing to view transporting messages. Multi-agent technology is modern software palindrome that capable of handling the complexity of a software system and providing intelligent solutions through the power of agent communication. A framework is a useful tool to develop multi-agent system and it saves lot of programmer's time and provides standards for the agent development.== About MaSMT ==Multi-agent technology is a modern software palindrome that capable of handling the complexity of a software system and providing intelligent solutions through the power of agent communication. A framework is a useful tool to develop a multi-agent system and it saves much programmer's time and provides standards for the agent development. MaSMT (Multi-Agent System for Machine Translation) released as Open Source under the GNU General Public License (GPL). Hence, the license allows using the software to examine and modify the source code and to develop applications based on the platform. The framework has been completely developed through the Java and provides Cross-platform capabilities. There is no prerequisite on MaSMT agents.First Open source version of the MaSMT was released on 2 March 2016 for to provide general infrastructure for the multi agent system development. The new version of the MaSMT includes some modification of the MaSMT to archive best performance. MaSMT3.0 includes few features than the previous version including root agent to handle swam of agents, Environment handling features to store agent's action and mail transport agent to send and reserving messages as emails. In addition to these main features, activity monitor has been introducing to view transporting messages.== Infrastructure ==This section briefly describes the structure of the MaSMT agents. The MaSMT framework provides three types of agents, namely ordinary agent, manager agent and root agent. The regular agents are the action holding agents of the framework; precede relevant tasks according to the given messages. A manager agent consists of some ordinary agents within its control. The root agent is the top level agent in the system which is capable to handle set of manager agents. Through the root agent, it is much easy to manage a swarm of agents. Further, manager agents can directly communicate with other manager agents as well as its root agent. Each and every ordinary agent in the system assigns to a particular manager agent. and manager can assign to a particular root agent.  A regular agent (ordinary agent) in a swarm can directly communicate only with the agents in its swarm and its manager. The framework primarily implements infrastructure of the agents, required actions to handle agent's environment, message parsing activities to agent communication and action monitoring facilities to view an agent's action for implement multi-agent system easily.== Agent Model ==MaSMT agents consist of an abstract model of the agents. Each agent in the system consists of group, role, and agent id. According to the MaSMT architecture, any group consists of one-to-many roles and any role includes some MaSMT agents with independent agent id. Further, MaSMT agents are capable of changing their role or group at run-time. Therefore, an agent can appear in the different swam as it required. However, only one place at a time   Figure 1 shows the abstract agent model of the MaSMT system. MaSMTAbstractAgent class is used to model the entire requirement of the abstract model.== MaSMTAbstractAgent ==MaSMTAbstractAgent is an abstract model of the each agent in the MaSMT framework and which is used to identify each agent independently. The abstract agent consists of a group, a role, and an agent id. In another word, each agent of the MaSMT system has an id and assigns for the particular group and a role. For instance, an agent in a group ‘communicate’ and role is read_message and id is1 then complete agent name of the abstract agent is donated as; read_message.1@communicate== MaSMT Agent ==MaSMTAgent is the ordinary agent in the framework, normally called as an action agent. MasMT agent consists of an abstract agent, two message queues (in-queue and out-queue), access rule, communication module, Environment controller, Status monitor and notice-board reader to handle the agent's functionalities.== MaSMT Agent’s Life circle ==The MaSMT agent is a Java thread consists of three sections on its life circle such as active, live and end. The active section starts when the agent is going to activate. After completing the active section, the agent moves to its live section. The agent is in the live section while its live property is true. That can be done through the method ‘setLive(false)’ The agent does it all the actions such as read messages reply for the messages are doing in the live section. In addition, a developer can change the delay of the agent's live circle. The end section of the MaSMT agent appears when the agent is going to die. Figure 3 shows the life-circle of the MaSMT agent.== MaSMT Manager ==The MasMT Manager is a controlling agent of the system with additional features than the MaSMT ordinary agents. According to the MaSMT architecture, The manager agent consists of an abstract agent, two message queues (in-queue and out-queue), access rule, communication module, Environment controller, Status monitor, notice-board reader, network access agent and Message transport agent to archive managers functionalities. The MaSMT manager can fully control its client agent(s). The Manager agent in the MaSMT creates all its client-agent automatically (as required) at the initialization stage or whenever its use. The manager can directly access its client agents and send messages to them or control them as required. Further, manager agent can control the priority of the agents and the state of the agents. This facility removes the unnecessary workload from its client agents.  The in-Queue of the managers is a queue used to store incoming messages. The manager adds messages for the out-queue, where the messages need to send to others group. In Addition to the above basic module, MaSMT manager agent consists of net access agent and message transport agent. The net access agent supports client-server communication for the system. This agent provides facilities to connect any managers on the net through the client-server networking. Message transport agent provides message transport facilities to the manager. Figure 4 shows the architecture of the MaSMT Manager. Compare with MaSMT agent, the manager agent has full control of the notice board. Therefore, message transport agent capable to handle peer-to-peer broadcast and notice-board based agent communications.== MaSMT Root Agent ==MasMT Root is a special type of Manager agent (MaSMT Manager) that capable to handles managers in the multi agent system. This MaSMT Root agent can fully control its client managers same as the manager handle its client agents. Further, the root agent can create, remove or control (handle agent priority) its client manager(s).  The in-Queue of the root agent is a message queue that uses to store incoming messages (messages are coming from another machine). The Root agent also adds messages on out-queue where messages need to send to others machines. Same as MaSMT Managers The Root agent consists of Message transport agent and net access agent to handle messages. Theoretically, Root agent is the top level manager of MaSMT system. Therefore, only one root agent available for a multi agent system in a machine. However, the root agent able to communicate with another root agents through the Net access agent.== MaSMT Messages ==Multi-agent systems are distributed systems that normally run concurrently. Communication among others is the hidden factor behind the success of the multi-agent systems. This agent communication is done through the messages. Therefore, number of standard available for the agent communication including FIFA-ACL. With considering all the above, MaSMT uses a message type named MaSMTMessages.  MaSMTMessage is a type of agent's communication message, uses to communicate with agents. The MaSMT Messages design by using FIPA-ACL message standards. MaSMT uses MaSMTMessage (a message)  to communicate between MaSMT agents as well as another agent through the FIPA-ACL message standard.== MaSMT Message Parsing ==This section briefly describes message parsing methods used in the MaSMT system. MaSMT support peer-to-peer, broadcast and noticeboard methods to message parsing.== Application of MaSMT ==Using the framework number of the multi-agent system has been developed including Natural Language Processing applications such as English to Sinhala Agent-based Multi-agent system, Sinhala Ontology Generator and Multi-agent based Morphological Analyzer. In addition to the above Intelligent chatbot and a communication platform for Agricultural domain, File sharing application through the distributed environment has already developed through the MaSMT.== References ==== External links ==MaSMT on SourceForge.net
	Optima is a humanist sans-serif typeface designed by Hermann Zapf and released by the D. Stempel AG foundry, Frankfurt, Germany.Though classified as a sans-serif, Optima has a subtle swelling at the terminals suggesting a glyphic serif. Optima was inspired by classical Roman capitals and the stonecarving on Renaissance-period tombstones Zapf saw in Florence on a 1950 holiday to Italy.Zapf intended Optima to be a typeface that could serve for both body text and titling. To prove its versatility, Zapf set his entire book About Alphabets in the regular weight. Zapf retained an interest in the design, collaborating on variants and expansions into his eighties.== History ==Interested in calligraphy and the history of Italian printing and lettering, Zapf first visited Italy in 1950. While in Florence, Zapf was particularly interested in the design of the lettering in tombstones of the cemetery of the Basilica di Santa Croce in Florence, in which the strokes subtly widen as they reach stroke terminals without ending in a serif. He quickly sketched an early draft of the design on a 1000 lira banknote. Zapf was to work on the development of Optima during most of the following decade.In his book About Alphabets, Zapf commented that his key aim in designing Optima's capitals, inspired by the Roman capital model, was the desire to avoid the monotony of all capital letters having a roughly square footprint, as he felt was true of some early sans-serif designs. Like the Roman capitals, Optima's 'E' and 'R' occupy about a half-square, the 'M' is wide and its sides are splayed.Upon the suggestion of Monroe Wheeler of the Museum of Modern Art in New York, Zapf decided to adapt his typeface to be used as a book type. “He thereupon changed the proportions of the lowercase, and by means of photography, he tested the suitability of the design for continuous reading application.” Zapf designed the capital letters of Optima after the inscriptions on the Trajan Column (A.D. 113). Optima is the first German typeface not based on the standard baseline alignment that had been used up until this point in time. Zapf states “This base line is not ideal for a roman, as it was designed for the high x-height of the Fraktur and Textura letters. Thus, too many German types have ascenders which are too long and descenders which are too short. The proportions of Optima Roman are now in the Golden Section: lowercase x-height equalling the minor and ascenders-descenders the major. However, the curved lines of the stems of each letter result from technical considerations of type manufacturing rather than purely esthetic considerations.”The development of Optima took place over the period 1955-1958. Optima was first manufactured as a foundry version in 1958 by Stempel of Frankfurt, and by Mergenthaler in America shortly thereafter. It was released to the public at an exhibition in Düsseldorf in that same year. If it had been up to Zapf, Optima would have been named New Roman, but the marketing staff insisted that it be named Optima.Zapf wrote later in his life of his preference for Optima over all of his other typefaces, but he also mentioned “a father should not have a favorite among his daughters.”== Structure ==Optima’s design follows humanist lines; its capitals (like those of Palatino, Hans Eduard Meier’s Syntax and Carol Twombly's Trajan) originate from the classic Roman monumental capital model, reflecting a reverence for Roman capitals as an ideal form.Optima is an example of a modulated-stroke sans-serif, a design type where the strokes are variable in width. The design style has been intermittently popular since the late nineteenth century; Optima is one of the most lastingly popular examples of the genre. Optima was originally targeted by Stempel's Walter Cunz as a competitor to Ludwig & Mayer's Colonia design, which has not been digitised. Shaw also suggests the little-known 1948 design Romann Antiqua, as well as Stellar by Robert Hunter Middleton as predecessors, and notes the existence of Pascal by José Mendoza y Almeida (1962) as a design with a similar set of influences. Optima is however quite restrained in stroke width variation; more display-oriented predecessors such as Britannic show far more differentiation in stroke width than Optima does.Optima's sloped version was originally an oblique or sloped roman, in which the letters do not take on handwriting characteristics. For Optima nova (discussed below) Zapf decided to create a new true italic with a greater slant angle.At the same time as the late development of Optima, Zapf was also working on a non-modulated sans for Linotype, to be named Magnus and intended to compete with Gill Sans. It was ultimately never released.== Optima Greek (1973) ==It is a Greek variant designed by Matthew Carter, based on sketches from Hermann Zapf. Digital version has not been produced.== Optima Classified (1976) ==It is a variant designed by Matthew Carter, based closely on Optima Medium. No digital versions have been produced.== Optima nova (2002) ==Optima nova is a redesign of the original font family, designed by Hermann Zapf and Linotype GmbH type director Akira Kobayashi. The new family contains seven font weights, adding light, demi, and heavy font weights, but removing extra black weight. Medium weight is readjusted to between medium and bold weights in the old family scale. Glyph sets are expanded to include Adobe CE and Latin Extended characters, with light to bold weight fonts supporting proportional lining figures, old style figures, and small caps.The initial and most common release of Optima, like many sans-serif fonts, has an oblique style instead of an italic: the shapes are merely tilted to the right. In Optima nova, this is replaced by a true italic. (In interviews, Zapf has said that this was his original goal from the beginning, but the need to release Optima quickly forced him to settle for an oblique.)Even in Roman fonts, letters such as Q, a, f are redesigned. The overall bounding boxes were widened in Optima nova.Reviewing it, John Berry wrote that "its 'color' on the page comes much closer to that of the original metal version than any of the earlier photo/digital versions did" but that "ends of the strokes in the letters 'a', 'c', and 's' flare much more dramatically than they ever did in the older Optima — so much so that these letters almost look as though they have serifs...It’s a subtle difference, but it’s disturbing if you’re used to the understated elegance of Optima’s letterforms."=== Optima nova Condensed ===It is a condensed variant which consists of light to bold weights, but no italic fonts. The glyph set does not support proportional lining figures, old style figures, or small caps.=== Optima nova Titling ===It is a titling capitals variant, which contains only capital letters, with restyled letterform. The glyph set is the same as Optima nova Condensed, but also includes extra ligatures. Berry writes in his review of the nova release: "it has softly curved joins and interior angles. Instead of the added crispness of detail that you might expect of a face designed for display use, this one looks more sculptural."In the tradition of hand lettering and lapidary inscription, the titling face shares similarities with the work of Zapf's friend Herb Lubalin, especially the exuberant ligatures (for which Lubalin's ITC Lubalin Graph and ITC Avant Garde are notable). Further influence of A.M. Cassandre and Rudolf Koch, whose work greatly inspired the young Zapf, can also be seen in Optima.== Optima Pro Cyrillic (2010) ==In April 2010, Linotype announced the release of Cyrillic version of the original Optima family, in OpenType Pro font formats. Released fonts include Optima Pro Cyrillic Roman, Oblique, Bold, Bold Oblique.== Derivatives ==As with many popular fonts, knockoff designs and re-releases under different names are common, some created by Zapf himself. These all tend to copy the original release, rather than the Optima nova design which represents Zapf's final thoughts on his design. In the Bitstream font collection, Zapf Humanist 601 is provided as an Optima clone. Other Optima clones include Optane from the WSI Fonts collection, Opulent by Rubicon Computer Labs Inc., Ottawa from Corel, CG Omega and Eterna. Freely available implementations include URW Classico (available with URW Font package from Ghostscript). Linux Biolinum is a libre font inspired by it. Zapf's Palatino Sans is a more informal typeface in the same style, with a design reminiscent of brushstrokes or calligraphy.In a memoir written for Linotype, Zapf commented: The name "Optima" was not my idea at all. It is for me too presumptuous and was the invention of the sales people at Stempel.== Notes ==== References ==Margaret Re, Johanna Drucker, Matthew Carter, James Mosley. Typographically Speaking: The Art of Matthew Carter. Princeton Architectural Press: 2003. ISBN 1-56898-427-8, ISBN 978-1-56898-427-8.Blackwell, Lewis. 20th Century Type. Yale University Press: 2004. ISBN 0-300-10073-6.Fiedl, Frederich, Nicholas Ott and Bernard Stein. Typography: An Encyclopedic Survey of Type Design and Techniques Through History. Black Dog & Leventhal: 1998. ISBN 1-57912-023-7.Jaspert, W. Pincus, W. Turner Berry and A.F. Johnson. The Encyclopedia of Type Faces. Blandford Press Lts.: 1953, 1983. ISBN 0-7137-1347-X.Lawson, Alexander S., Anatomy of a Typeface. Godine: 1990. ISBN 978-0-87923-333-4.Macmillan, Neil. An A–Z of Type Designers. Yale University Press: 2006. ISBN 0-300-11151-7.Zapf, Hermann. Manuale Typographicum. The MIT Press: 19534, 1970. ISBN 0-262-24011-4.== External links ==Typowiki: OptimaThe story of Palatino and Optima from Hermann ZapfLinotype: Optima from Hermann ZapfLearn more about Optima nova from Hermann Zapf and Akira KobayashiThe Story of Linotype's OptimaAlternatives to Optima - Stephen Coles
	rpix86 is a DOS emulator for the Raspberry Pi created by Patrick Aalto. rpix86 emulates an Intel 80486 x86 CPU running at 20MHz with 640kB of memory, 256-color Super VGA graphics at 640x480, and a Sound Blaster 2.0 sound card.The latest version is 0.19, which was released in June 2015.rpix86 does not have an inbuilt command-line interpreter. The user needs to have a DOS program that provides the command shell features. Currently the only supported shell is 4DOS version 7.50.== See also ==DOSBoxVirtual DOS machine== References ==== External links ==rpix86 by Patrick Aalto
	AdTruth is a software product and the digital media division of 41st Parameter, a company headquartered in Scottsdale, Arizona with regional offices in San Jose, California; London, England; and Munich, Germany.  AdTruth allows marketers to recognize and reach target audiences across online devices.AdTruth software identifies users for targeting, tracking, performance tracking across digital media, including mobile and desktop, by analysing patterns in large numbers of advertisements served over the internet, rather than through the use of cookies.== History ==AdTruth was founded in 2011 by Ori Eisen of 41st Parameter, to repurpose the company's fraud detection and prevention technology, for use within the advertising industry to accurately target intended audiences, particularly in mobile. Eisen was joined by James Lamberti in the role of vice president and general manager.  In 2012 41st Parameter raised $13 million in Series D financing from Norwest Venture Partners, Kleiner Perkins Caufield & Byers, JAFCO Ventures and Georgian Partners, bringing total funding to about $35 million.In May 2012, AdTruth hosted a meeting of digital media executives to discuss  Apple’s UDID deprecation, with the intent of developing a device-neutral replacement standard.AdTruth joined the World Wide Web Consortium's  Tracking Protection Working Group, which provides guidance for implementing and adhering to Do Not Track policies.  AdTruth also worked with privacy firm Truste to create a privacy compliant Do Not Track-style mechanism for mobile.In 2013, the company Experian purchased 41st Parameter, acquiring AdTruth as part of the deal.== Product ==AdTruth software helps marketers track, target and retarget consumers using more than 100 parameters, including milliseconds in differences in the internal clock setting, to recognize a particular device anonymously.  AdTruth’s technology uses non-UDID information to identify a wide range of devices for cookieless ad targeting. Its technology currently has about a 90 percent accuracy rate on iOS, higher on Android and desktop. AdTruth also has mobile web to app bridging capabilities as well as DeviceInsight technology, enabling marketers to identify users across mobile web and app content.41st Parameter’s patented AdTruth technology is being used by MdotM, in response to the deprecation of the UDID that included tracking and targeting capabilities.== Competitors ==AdTruth’s main competitor is BlueCava, which deploys a similar device-fingerprinting technology.== References ==== External links ==https://web.archive.org/web/20120920040354/http://adtruth.com/download/adtruthaudiencerecognition.pdfhttp://www.admonsters.com/blog/consumer-tracking-respect-device-recognition-and-next-leap-forward-digital-mediahttp://www.slideshare.net/morr41https://www.scribd.com/41stParameter
	A fitness app is an application that can be downloaded on any mobile device and used anywhere to get fit. As of 2015, the number of health-related apps released on the two leading platforms, iPhone operating system (iOS) and Android, had reached more than 165,000. Apps can perform various functions such as allowing users to set fitness goals, tracking caloric intake, gathering workout ideas, and sharing progress on social media to facilitate healthy behavior change. They can be used as a platform to promote healthy behavior change with personalized workouts, fitness advice and nutrition plans. Fitness apps can work in conjunction with wearable devices to synchronize their health data to third-party devices for easier accessibility. Through using gamification elements and creating competition among friends and family, fitness apps can help incentive users to be more motivated. Running and workout apps such as RockMyRun allow users to run or work out to music in the form of DJ mixes that can be personalized based on the user's steps per minute, heart rate or ideal cadence thus boosting and enhancing performance during exercise.== Notable apps ==RockMyRunRuntasticRunkeeperC25KMyFitnessPalFitWellKidfitFIT Radio== See also ==Exercise and music== References ==
	There are various implementations of the Advanced Encryption Standard, also known as Rijndael.== Libraries ==Rijndael is free for any use public or private, commercial or non-commercial. The authors of Rijndael used to provide a homepage for the algorithm. Care should be taken when implementing AES in software. Like most encryption algorithms, Rijndael was designed on big-endian systems. For this reason, little-endian systems, which include the common PC, return correct test vector results only through swapping bytes of the input and output words.The algorithm operates on plaintext blocks of 16 bytes. Encryption of shorter blocks is possible only by padding the source bytes, usually with null bytes. This can be accomplished via several methods, the simplest of which assumes that the final byte of the cipher identifies the number of null bytes of padding added.Careful choice must be made in selecting the mode of operation of the cipher. The simplest mode encrypts and decrypts each 128-bit block separately. In this mode, called electronic code book (ECB), blocks that are identical will be encrypted identically; this is entirely insecure. It makes some of the plaintext structure visible in the ciphertext. Selecting other modes, such as using a sequential counter over the block prior to encryption (i.e., CTR mode) and removing it after decryption avoids this problem.Current list of FIPS 197 validated cryptographic modules (hosted by NIST)Current list of FIPS 140 validated cryptographic modules with validated AES implementations (hosted by NIST) – Most of these involve a commercial implementation of AES algorithms. Look for "FIPS-approved algorithms" entry in the "Level / Description" column followed by "AES" and then a specific certificate number.=== C/ASM library ===LibgcryptwolfSSL (previously CyaSSL)GnuTLSNetwork Security ServicesOpenSSLLibreSSLmbed TLS (previously PolarSSL)Reference original implementationaxTLSMicrosoft CryptoAPI uses Cryptographic Service Providers to offer encryption implementations. The Microsoft AES Cryptographic Provider was introduced in Windows XP and can be used with any version of the Microsoft CryptoAPI.tiny-AES-c Small portable AES128/192/256 in C (suitable for embedded systems)AES-256 a byte-oriented portable AES-256 implementation in CSolaris Cryptographic Framework offers multiple implementations, with kernel providers for hardware acceleration on x86 (using the Intel AES instruction set) and on SPARC (using the SPARC AES instruction set). It is available in Solaris and derivatives, as of Solaris 10.OpenAES portable C cryptographic libraryLibTomCrypt is a modular and portable cryptographic toolkit that provides developers with well known published block ciphers, one-way hash functions, chaining modes, pseudo-random number generators, public key cryptography and other routines.libSodium API for NaClAES Dust Compact implementation of AES-128 encryption in C, x86, AMD64, ARM32 and ARM64 assembly.=== C++ library ===Botan has implemented Rijndael since its very first release in 2001Crypto++ A comprehensive C++ public-domain implementation of encryption and hash algorithms. FIPS validated=== C# /.NET ===As of version 3.5 of the .NET Framework, the System.Security.Cryptography namespace contains both a fully managed implementation of AES and a managed wrapper around the CAPI AES implementation.Bouncy Castle Crypto Library=== Go ===The crypto/aes package in standard library=== Java ===Java Cryptography Extension, integrated in the Java Runtime Environment since version 1.4.2IAIK JCEBouncy Castle Crypto Library=== Python ===PyCrypto – The Python Cryptography Toolkit PyCrypto, extended in PyCryptoDomekeyczar – Cryptography Toolkit keyczarM2Crypto – M2Crypto is the most complete OpenSSL wrapper for Python.Cryptography – Python library which exposes cryptographic recipes and primitives.PyNaCl – Python binding for libSodium (NaCl)=== JavaScript ===SJCL library – contains JavaScript implementations of AES in CCM, CBC, OCB and GCM modesAES-JS – portable JavaScript implementation of AES ECB and CTR modesForge – JavaScript implementations of AES in CBC, CTR, OFB, CFB, and GCM modesasmCrypto – JavaScript implementation of popular cryptographic utilities with focus on performance. Supports CBC, CFB, CCM modes.pidCrypt – open source JavaScript library. Only supports the CBC and CTR modes.=== LabVIEW ===AES LabVIEW – LabVIEW implementation.== Applications ===== Archive and compression tools ===7zAmanda BackupPeaZipPKZIPRARWinZipUltraISO=== File encryption ===Gpg4winNcrypt=== Encrypting file systems ===Most encrypting file systems use AES, e.g. NTFS=== Disk / partition encryption ===BitLocker (part of certain editions of Windows operating systems)CipherShedDiskCryptorFileVault (part of the Mac OS X operating system, and also the included Disk Utility makes AES-encrypted drive images)GBDEGeli (software)LibreCryptLUKSPrivate DiskTrueCrypt (discontinued)VeraCrypt=== Security for communications in local area networks ===IEEE 802.11i, an amendment to the original IEEE 802.11 standard specifying security mechanisms for wireless networks, uses AES-128 in CCM mode (CCMP).The ITU-T G.hn standard, which provides a way to create a high-speed (up to 1 Gigabit/s) local area network using existing home wiring (power lines, phone lines and coaxial cables), uses AES-128 for encryption.=== Miscellaneous ===DataLocker Uses AES 256-bit CBC and XTS mode hardware encryptionGet Backup Pro uses AES-128 and AES-256GPG, GPL-licensed, includes AES, AES-192, and AES-256 as options.IPsecIronKey Uses AES 128-bit and 256-bit CBC-mode hardware encryptionKeePass Password SafeLastPassLinux kernel's Crypto API, now exposed to userspaceNetLib Encryptionizer supports AES 128/256 in CBC, ECB and CTR modes for file and folder encryption on the Windows platform.Pidgin (software), has a plugin that allows for AES EncryptionPyEyeCrypt Free open-source text encryption tool/GUI with user-selectable AES encryption methods and PBKDF2 iterations.Signal ProtocolGoogle Allo (optional)Facebook Messenger (optional)SignalTextSecureWhatsAppSocialDocs file encryption uses AES256 to provide a free-online file encryption toolXFire uses AES-128, AES-192 and AES 256 to encrypt usernames and passwordsCertain games and engines, such as the Rockstar Advanced Game Engine used in Grand Theft Auto IV, use AES to encrypt game assets in order to deter hacking in multiplayer.== Hardware ==Intel and AMD processors include the AES instruction set.On IBM zSeries mainframes, AES is implemented as the KM series of assembler opcodes when various Message Security Assist facilities are installed.SPARC S3 core processors include the AES instruction set, which is used with SPARC T4 and SPARC T5 systems.== References ==
	Skylum (formerly Macphun) is a software developing company based in Bellevue, Washington. It is most known for its photo editing software Aurora HDR and Luminar. Skylum is also the developer of Snapheal, Focus, Tonality, Intensify, Noiseless and FX Photo Studio.Founded as Macphun in 2008, the company decided to change its name to Skylum in 2017, following the decision to develop its Mac only software for Windows as well.== History ==Skylum was founded as Macphun in 2008 by two gaming developers and amateur photographers. Initially the company developed applications for iOS. One of its first applications was Vintage Video Maker, which was later named Vintagio. In 2009, Apple named Vintagio among Best iPhone apps of the year. In 2010, Alex Tsepko joined the team. In total, the company released over 60 applications in the first three years. However, it saw the greatest number of downloads in the photography applications. Skylum thus decided to develop the same photography applications for MacOS.In early 2010, Skylum launched its first MacOS application, FX Photo Studio Pro, which was earlier available for iOS only. Several other applications were also developed for MacOS including Snapheal.In order to tap the North Americas, the company moved its headquarters to San Diego, United States in 2013. A great number of employees came from the Nik Collection, which was earlier acquired by Google. Later that year, the company launched Intensify, a fully featured photo editing software, that was named among 2013 Best Mac App Store apps. In 2014, Skylum launched Tonality, a black-and-white photo editor software, that won Apple’s Editors’ Choice of the year. The same year, Skylum hired a team in Europe to develop localized versions of its software and start European expansion.In 2015, Skylum released a new image noise reduction application called Noiseless. Same year Skylum partnered with Trey Ratcliff to develop an HDR program. Through the collaboration Aurora HDR, a High Dynamic Range editing and processing tool, was released in November. A year later, Skylum developed Luminar, an all-in-one photo editing software as an alternative to Adobe’s Lightroom. Both the software became the most known applications by the company. In 2017, the company released Aurora HDR and Luminar for Windows - software that previously was available for MacOS only. At the same time, it was also announced that Macphun would change its name to Skylum.== Products ==Skylum is most known for its photo editing software, Aurora HDR launched in 2015, and Luminar launched in 2016. Other notable software by the company include: Snapheal, Focus, Tonality, Intensify, Noiseless, FX Photo Studio, and Photolemur.== References ==
	Computer engineering is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work but also how they integrate into the larger picture.Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.In many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.== History ==Computer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world's first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took 5 years to complete.While the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947, the silicon surface passivation process (via thermal oxidation) by Mohamed Atalla at Bell Labs in 1957, the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959, the metal-oxide-semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.=== History of computer engineering education ===The first computer engineering degree program in the United States was established in 1971 at Case Western Reserve University in Cleveland, Ohio. As of 2015, there were 250 ABET-accredited computer engineering programs in the U.S. In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor's degree generally called computer engineering.  Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.== Education ==Computer engineering is referred to as computer science and engineering at some universities. Most entry-level computer engineering jobs require at least a bachelor's degree in computer engineering (or computer science and engineering). Typically one must learn an array of mathematics such as calculus, algebra and trigonometry and some computer science classes. Sometimes a degree in electronic engineering is accepted, due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a strong background in computer programming is necessary. According to BLS, "a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum". Some large firms or specialized jobs require a master's degree.It is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers. This can be helpful, especially when it comes to learning new skills or improving existing ones. For example, as the relative cost of fixing a bug increases the further along it is in the software development cycle, there can be greater cost savings attributed to developing and testing for quality code as soon as possible in the process, and particularly before release.== Applications and practice ==There are two major specialties in computer engineering: hardware and software.=== Computer hardware engineering ===According to the BLS, Job Outlook employment for computer hardware engineers, the expected ten-year growth from 2014 to 2024 for computer hardware engineering was an estimated 3% and there was a total of 77,700 jobs that same year. ("Slower than average" in their own words when compared to other occupations)" and is down from 7% for the 2012 to 2022 BLS estimate and is further down from 9% in the BLS 2010 to 2020 estimate." Today, computer hardware is somehow equal to electronic and computer engineering (ECE) and has been divided into many subcategories; the most significant is embedded system design.=== Computer software engineering ===According to the U.S. Bureau of Labor Statistics (BLS), "computer applications software engineers and computer systems software engineers are projected to be among the faster than average growing occupations" The expected ten-year growth as of 2014 for computer software engineering was an estimated seventeen percent and there was a total of 1,114,000 jobs that same year. This is down from the 2012 to 2022 BLS estimate of 22% for software developers. And, further down from the 30% 2010 to 2020 BLS estimate. In addition, growing concerns over cybersecurity add up to put computer software engineering high above the average rate of increase for all fields. However, some of the work will be outsourced in foreign countries. Due to this, job growth will not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead go to computer software engineers in countries such as India. In addition, the BLS Job Outlook for Computer Programmers, 2014–24 has an −8% (a decline, in their words) for those who program computers (i.e. embedded systems) who are not computer application developers.== Specialty areas ==There are many specialty areas in the field of computer engineering.=== Coding, cryptography, and information protection ===Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.=== Communications and wireless networks ===Those focusing on communications and wireless networks, work advancements in telecommunications systems and networks (especially wireless networks), modulation and error-control coding, and information theory. High-speed network design, interference suppression and modulation, design, and analysis of fault-tolerant system, and storage and transmission schemes are all a part of this specialty.=== Compilers and operating systems ===This specialty focuses on compilers and operating systems design and development. Engineers in this field develop new operating system architecture, program analysis techniques, and new techniques to assure quality. Examples of work in this field include post-link-time code transformation algorithm development and new operating system development.=== Computational science and engineering ===Computational Science and Engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, "computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more".=== Computer networks, mobile computing, and distributed systems ===In this specialty, engineers build integrated environments for computing, communications, and information access. Examples include shared-channel wireless networks, adaptive resource management in various systems, and improving the quality of service in mobile and ATM environments. Some other examples include work on wireless network systems and fast Ethernet cluster wired systems.=== Computer systems: architecture, parallel processing, and dependability ===Engineers working in computer systems work on research projects that allow for reliable, secure, and high-performance computer systems. Projects such as designing processors for multi-threading and parallel processing are included in this field. Other examples of work in this field include development of new theories, algorithms, and other tools that add performance to computer systems.Computer architecture includes CPU design, cache hierarchy layout, memory organization and load balancing.=== Computer vision and robotics ===In this specialty, computer engineers focus on developing visual sensing technology to sense an environment, representation of an environment, and manipulation of the environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.=== Embedded systems ===Individuals working in this area design technology for enhancing the speed, reliability, and performance of systems. Embedded systems are found in many devices from a small FM radio to the space shuttle. According to the Sloan Cornerstone Career Center, ongoing developments in embedded systems include "automated vehicles and equipment to conduct search and rescue, automated transportation systems, and human-robot coordination to repair equipment in space." As of 2018, computer embedded computer engineering specializations include system-on-chip design, architecture of edge computing and the Internet of things.=== Integrated circuits, VLSI design, testing and CAD ===This specialty of computer engineering requires adequate knowledge of electronics and electrical systems. Engineers working in this area work on enhancing the speed, reliability, and energy efficiency of next-generation very-large-scale integrated (VLSI) circuits and microsystems. An example of this specialty is work done on reducing the power consumption of VLSI algorithms and architecture.=== Signal, image and speech processing ===Computer engineers in this area develop improvements in human-computer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.=== Quantum computing ===== See also ===== Related fields ====== Associations ===== References ==17. ^ https://www.ece.iastate.edu/the-department/history/history-of-computing18. ^ https://collegegrad.com/careers/computer-hardware-engineers19. ^ https://www.sokanu.com/careers/software-engineer/
	Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.== What is computer science? ==Computer science can be described as all of the following:Academic disciplineScienceApplied science== Subfields ===== Mathematical foundations ===Coding theory – Useful in networking and other areas where computers communicate with each other.Game theory – Useful in artificial intelligence and cybernetics.Graph theory – Foundations for data structures and searching algorithms.Mathematical logic – Boolean logic and other ways of modeling logical queries; the uses and limitations of formal proof methodsNumber theory – Theory of the integers.  Used in cryptography as well as a test domain in artificial intelligence.=== Algorithms and data structures ===Algorithms – Sequential and parallel computational procedures for solving a wide range of problems.Data structures – The organization and manipulation of data.=== Artificial intelligence ===Outline of artificial intelligenceArtificial intelligence – The implementation and study of systems that exhibit an autonomous intelligence or behavior of their own.Automated reasoning – Solving engines, such as used in Prolog, which produce steps to a result given a query on a fact and rule database, and automated theorem provers that aim to prove mathematical theorems with some assistance from a programmer.Computer vision – Algorithms for identifying three-dimensional objects from a two-dimensional picture.Soft computing, the use of inexact solutions for otherwise extremely difficult problems:Machine learning - Automated creation of a set of rules and axioms based on input.Evolutionary computing - Biologically inspired algorithms.Natural language processing - Building systems and algorithms that analyze, understand, and generate natural (human) languages.Robotics – Algorithms for controlling the behaviour of robots.=== Communication and security ===Networking – Algorithms and protocols for reliably communicating data across different shared or dedicated media, often including error correction.Computer security – Practical aspects of securing computer systems and computer networks.Cryptography – Applies results from complexity, probability, algebra and number theory to invent and break codes, and analyze the security of cryptographic protocols.=== Computer architecture ===Computer architecture – The design, organization, optimization and verification of a computer system, mostly about CPUs and Memory subsystem (and the bus connecting them).Operating systems – Systems for managing computer programs and providing the basis of a usable system.=== Computer graphics ===Computer graphics – Algorithms both for generating visual images synthetically, and for integrating or altering visual and spatial information sampled from the real world.Image processing – Determining information from an image through computation.Information visualization – Methods for representing and displaying abstract data to facilitate human interaction for exploration and understanding.=== Concurrent, parallel, and distributed systems ===Parallel computing - The theory and practice of simultaneous computation; data safety in any multitasking or multithreaded environment.Concurrency (computer science) – Computing using multiple concurrent threads of execution, devising algorithms for solving problems on multiple processors to achieve maximal speed-up compared to sequential execution.Distributed computing – Computing using multiple computing devices over a network to accomplish a common objective or task and thereby reducing the latency involved in single processor contributions for any task.=== Databases ===Relational databases – the set theoretic and algorithmic foundation of databases.Structured Storage - non-relational databases such as NoSQL databases.Data mining – Study of algorithms for searching and processing information in documents and databases; closely related to information retrieval.=== Programming languages and compilers ===Compiler theory – Theory of compiler design, based on Automata theory.Programming language pragmatics – Taxonomy of programming languages, their strength and weaknesses. Various programming paradigms, such as object-oriented programming.Programming language theoryFormal semantics – rigorous mathematical study of the meaning of programs.Type theory – Formal analysis of the types of data, and the use of these types to understand properties of programs — especially program safety.=== Scientific computing ===Computational science – constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems.Numerical analysis – Approximate numerical solution of mathematical problems such as root-finding, integration, the solution of ordinary differential equations; the approximation of special functions.Symbolic computation – Manipulation and solution of expressions in symbolic form, also known as Computer algebra.Computational physics – Numerical simulations of large non-analytic systemsComputational chemistry – Computational modelling of theoretical chemistry in order to determine chemical structures and propertiesBioinformatics and Computational biology – The use of computer science to maintain, analyse, store biological data and to assist in solving biological problems such as Protein folding, function prediction and Phylogeny.Computational neuroscience – Computational modelling of neurophysiology.=== Software engineering ===Formal methods – Mathematical approaches for describing and reasoning about software design.Software engineering – The principles and practice of designing, developing, and testing programs, as well as proper engineering practices.Algorithm design – Using ideas from algorithm theory to creatively design solutions to real tasks.Computer programming – The practice of using a programming language to implement algorithms.Human–computer interaction – The study and design of computer interfaces that people use.Reverse engineering – The application of the scientific method to the understanding of arbitrary existing software.=== Theory of computation ===Automata theory – Different logical structures for solving problems.Computability theory – What is calculable with the current models of computers. Proofs developed by Alan Turing and others provide insight into the possibilities of what may be computed and what may not.List of unsolved problems in computer scienceComputational complexity theory – Fundamental bounds (especially time and storage space) on classes of computations.Quantum computing theory – Explores computational models involving quantum superposition of bits.== History ==History of computer scienceList of pioneers in computer science== Professions ==ProgrammerTeacher/ProfessorSoftware engineerSoftware architectSoftware developerSoftware testerHardware engineerData analystInteraction designerNetwork administrator== Data and data structures ==Data structureData typeAssociative array and Hash tableArrayListTreeStringMatrix (computer science)Database== Programming paradigms ==Imperative programming/Procedural programmingFunctional programmingLogic programmingObject oriented programmingClassInheritanceObject== See also ==AbstractionBig O notationClosureCompilerCognitive science== External links ==Outline of computer science at CurlieACM report on a recommended computer science curriculum (2008)Directory of free university lectures in Computer ScienceCollection of Computer Science BibliographiesPhotographs of computer scientists (Bertrand Meyer's gallery)
	The Australian Centre for Robotic Vision is an unincorporated collaborative venture with funding of $25.6m over seven years to pursue a research agenda tackling the critical and complex challenge of applying robotics in the real world.The Centre won the 2017 Amazon Robotics Challenge with their robot Cartman.== Research organisations ==The Center is made up of an interdisciplinary team from four Australian universities:Queensland University of Technology (QUT),The University of Adelaide (UoA),The Australian National University (ANU),Monash University, andresearch organisations and international universitiesData61 (previously known as NICTA or National ICT Australia Ltd),INRIA Rennes Bretagne,Georgia Institute of Technology,Imperial College London,The Swiss Federal Institute of Technology Zurich, andThe University of Oxford.== Goals ==The Centre aims to achieve breakthrough science and technology in robotic vision by addressing four key research objectives: robust vision, vision and action, semantic vision, and algorithms and architecture. Together the four research objectives form the Centre’s research themes, which serve as organisational groupings of the Centre’s research projects.=== Robust vision ===Will develop new sensing technologies and robust algorithms that allow robots to use visual perception in all viewing conditions: night and day, rain or shine, summer or winter, fast moving or static.=== Vision and action ===Will create new theory and methods for using image data for control of robotic systems that navigate through space, grasp objects, interact with humans and use motion to assist in seeing.=== Semantic vision ===Will produce novel learning algorithms that can both detect and recognise a large, and potentially ever increasing, number of object classes from robotically acquired images, with increasing reliability over time.=== Algorithms and architectures ===Will create novel technologies and techniques to ensure that the algorithms developed across the themes can be run in real-time on robotic systems deployed in large-scale real-world applications.== References ==== External links ==Official website
	Nearline storage (a portmanteau of "near" and "online storage") is a term used in computer science to describe an intermediate type of data storage that represents a compromise between online storage (supporting frequent, very rapid access to data) and offline storage/archiving (used for backups or long-term storage, with infrequent access to data).Nearline storage dates back to the IBM 3850 Mass Storage System (MSS) tape library, which was announced in 1974.== Overview ==The formal distinction between online, nearline, and offline storage is:Online storage is immediately available for I/O.Nearline storage is not immediately available, but can be made online quickly without human intervention.Offline storage is not immediately available, and requires some human intervention to become online.For example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage.== Robotic nearline storage ==The nearline storage system knows on which volume (cartridge) the data resides, and usually asks a robot to retrieve it from this physical location (usually: a tape library or optical jukebox) and put it into a tape drive or optical disc drive to enable access by bringing the data it contains online. This process is not instant, but it only requires a few seconds.Nearline tape and optical storage has the advantage of relatively longer lifespans compared to spinning hard drives, simply due to the storage media being idle and usually stored in protected dust-free enclosures when not in use. In a robotic tape loading system, the tape drive used for accessing data experiences the most wear and may need occasional replacement, but the tapes themselves can last for years to decades. If there are sealable access doors between the access mechanism and the media, it is possible for the idle media storage enclosure to survive fire, floods, lightning strikes, and other disasters.== Hard disk drive nearline storage ==MAID (massive array of idle drives) systems archive data in an array of hard disk drives, with the most drives in a MAID usually stopped. The MAID system spins up each drive on demand when necessary to read (or in some cases to write) data on that drive. For a given amount of storage capacity, MAID systems have higher densities and lower power and cooling requirements than "hot" storage systems that keep all the disks spinning at full speed at all times.Some hard drive and storage systems vendors and suppliers use the term in reference to low-rotational speed hard drives, that are built to be more reliable than generic desktop and laptop computer hard drives, and are intended to be operational continuously for 24 hours a day, seven days a week, possibly for several years.Nearline hard drives may be used in personal or small business network-attached storage (NAS) systems, or as non-critical moderate-performance data storage on servers, where greater durability is required for the drive to operate continuously.By comparison, standard hard drives are assumed to only be in operation for a few hours each day, and are not spinning when the computer is either turned off or in sleep mode. Standard hard drives may also use data caching methods that can improve single-drive performance, but would interfere with the operation of multi-drive RAID storage systems, potentially causing data loss or corruption.Specifically the term nearline hard drive is being used to refer to high-capacity Serial ATA drives that work with Serial Attached SCSI storage devices. Presumably this usage is by analogy to the high-capacity and low-access speed tape systems.== References ==
	Visual computing is a generic term for all computer science disciplines handling with images and 3D models, i.e. computer graphics, image processing, visualization, computer vision, virtual and augmented reality, video processing, but also includes aspects of pattern recognition, human computer interaction, machine learning and digital libraries. The core challenges are the acquisition, processing, analysis and rendering of visual information (mainly images and video). Application areas include industrial quality control, medical image processing and visualization, surveying, robotics, multimedia systems, virtual heritage, special effects in movies and television, and computer games.== History and overview ==Visual computing is a relatively newly coined term, which got its current meaning around 2005, when the established computer science disciplines computer graphics, image processing, computer vision and others noticed that their methods and applications overlapped more and more, so that a new generic term was needed. Many of the used mathematical and algorithmic methods are the same in all areas dealing with images: image formats, filtering methods, color models, image metrics and others. And also the programming methods on graphics hardware, the manipulation tricks to handle huge data, textbooks and conferences, the scientific communities of these disciplines and working groups at companies intermixed more and more.Furthermore, applications increasingly needed techniques from more than one of these fields concurrently. To generate very detailed models of complex objects you need image recognition, 3D sensors and reconstruction algorithms, and to display these models believably you need realistic rendering techniques with complex lighting simulation. Real-time graphics is the basis for usable virtual and augmented reality software. A good segmentation of the organs is the basis for interactive manipulation of 3D visualizations of medical scans. Robot control needs the recognition of objects just as a model of its environment. And all devices (computers) need ergonomic graphical user interfaces.Although many problems are considered solved within the scientific communities of the sub-disciplines making up visual computing (mostly under idealistic assumptions), one major challenge of visual computing as a whole is the integration of these partial solutions into applicable products. This includes dealing with many practical problems like addressing a multitude of hardware, the use of real data (that is often erroneous and/or gigantic in size), and the operation by untrained users. In this respect, Visual computing is more than just the sum of its sub-disciplines, it is the next step towards systems fit for real use in all areas using images or 3D objects on the computer.== Visual computing disciplines ==At least the following disciplines are sub-fields of visual computing. More detailed descriptions of each of these fields can be found on the linked special pages.Computer graphics and computer animationComputer graphics is a general term for all techniques that produce images as result with the help of a computer. To transform the description of objects to nice images is called rendering which is always a compromise between image quality and run-time.Image analysis and computer visionTechniques that can extract content information from images are called image analysis techniques. Computer vision is the ability of computers (or of robots) to recognize their environment and to interpret it correctly.Visualization and visual analyticsVisualization is used to produce images that shall communicate messages. Data may be abstract or concrete, often with no a priori geometrical components. Visual analytics describes the discipline of interactive visual analysis of data, also described as “the science of analytical reasoning supported by the interactive visual interface”.Geometric modeling and 3D-printingTo represent objects for rendering it needs special methods and data structures, which subsumed with the term geometric modeling. In addition to describing and interactive geometric techniques, sensor data are more and more used to reconstruct geometrical models. Algorithms for the efficient control of 3D printers also belong to the field of visual computing.Image processing and image editingIn contrast to image analysis image processing manipulates images to produce better images. “Better” can have very different meanings subject to the respective application. Also, it has to be discriminated from image editing which describes interactive manipulation of images based on human validation.Virtual and augmented realityTechniques that produce the feeling of immersion into a fictive world are called virtual reality (VR). Requirements for VR include head-mounted displays, real-time tracking, and high-quality real-time rendering. Augmented reality enables the user to see the real environment in addition to the virtual objects, which augment this reality. Accuracy requirements on rendering speed and tracking precision are significantly higher here.Human computer interactionThe planning, design and uses of interfaces between people and computers is not only part of every system involving images. Due to the high bandwidth of the human visual channel (eye), images are also a preferred part of ergonomic user interfaces in any system, so that human-computer interaction is also an integral part of visual computing.== Footnotes ==== External links ==Microsoft Research Group Visual ComputingVisual Computing at NVidiaVisual Computing Group at Harvard UniversityVisual Computing Center at KAUSTApplied Research in Visual Computing (Fraunhofer IGD)Institute of Visual Computing (Hochschule Bonn-Rhein-Sieg, Sankt Augustin)VRVis Research Center for Virtual Reality and Visualisation (Vienna, Austria)Visual Computing Group @ HTW Berlin (Germany)
	Computer science (sometimes called computation science or computing science, but not to be confused with computational science or software engineering) is the study of processes that interact with data and that can be represented as data in the form of programs. It enables the use of algorithms to manipulate, store, and communicate digital information. A computer scientist studies the theory of computation and the practice of designing software systems.Its fields can be divided into theoretical and practical disciplines. Computational complexity theory is highly abstract, while computer graphics emphasizes real-world applications. Programming language theory considers approaches to the description of computational processes, while computer programming itself involves the use of programming languages and complex systems. Human–computer interaction considers the challenges in making computers useful, usable, and accessible.== History ==The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer". "A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published  the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".During the 1940s, as new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC were developed, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population. It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704 and later the IBM 709 computers, which were widely used during the exploration period of such devices. "Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again". During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, called the Transistor Computer. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications. The metal–oxide–silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. The MOSFET made it possible to build high-density integrated circuit chips, leading to what is known as the computer revolution or microcomputer revolution.Time has seen significant improvements in the usability and effectiveness of computing technology. Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of humanitarian aid was needed for efficient use—in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.=== Contributions ===Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society—in fact, along with electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the information revolution, seen as the third major leap in human technological progress after the Industrial Revolution (1750–1850 CE) and the Agricultural Revolution (8000–5000 BCE).These contributions include:The start of the "Digital Revolution", which includes the current Information Age and the Internet.A formal definition of computation and computability, and proof that there are computationally unsolvable and intractable problems.The concept of a programming language, a tool for the precise expression of methodological information at various levels of abstraction.In cryptography, breaking the Enigma code was an important factor contributing to the Allied victory in World War II.Scientific computing enabled practical evaluation of processes and situations of great complexity, as well as experimentation entirely by software. It also enabled advanced study of the mind, and mapping of the human genome became possible with the Human Genome Project. Distributed computing projects such as Folding@home explore protein folding.Algorithmic trading has increased the efficiency and liquidity of financial markets by using artificial intelligence, machine learning, and other statistical and numerical techniques on a large scale. High frequency algorithmic trading can also exacerbate volatility.Computer graphics and computer-generated imagery have become ubiquitous in modern entertainment, particularly in television, cinema, advertising, animation and video games. Even films that feature no explicit CGI are usually "filmed" now on digital cameras, or edited or post-processed using a digital video editor.Simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.Artificial intelligence is becoming increasingly important as it gets more efficient and complex. There are many applications of AI, some of which can be seen at home, such as robotic vacuum cleaners. It is also present in video games and on the modern battlefield in drones, anti-missile systems, and squad support robots.Human–computer interaction combines novel algorithms with design strategies that enable rapid human performance, low error rates, ease in learning, and high satisfaction. Researchers use ethnographic observation and automated data collection to understand user needs, then conduct usability tests to refine designs. Key innovations include the direct manipulation, selectable web links, touchscreen designs, mobile applications, and virtual reality.== Etymology ==Although first proposed in 1956, the term "computer science" appears in a 1959 article in Communications of the ACM,in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.Also, in the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics of the University of Edinburgh)."In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain."A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes." The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, statistics, and logic.Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.== Philosophy ==A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.== Fields ==Computer science is no more about computers than astronomy is about telescopes.As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.=== Theoretical computer science ===Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies. All studies related to mathematical, logic and formal concepts and methods could be considered as theoretical computer science, provided that the motivation is clearly drawn from the field of computing.==== Data structures and algorithms ====Data structures and algorithms are the study of commonly used computational methods and their computational efficiency.==== Theory of computation ====According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated?" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.==== Information and coding theory ====Information theory is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.==== Programming language theory ====Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.==== Formal methods ====Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.=== Computer systems ======= Computer architecture and computer engineering ====Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.==== Computer performance analysis ====Computer performance analysis is the study of work flowing through computers with the general goals of improving throughput, controlling response time, using resources efficiently, eliminating bottlenecks, and predicting performance under anticipated peak loads.Benchmarks are used to compare the performance of systems carrying different chips and/or system architectures.==== Concurrent, parallel and distributed systems ====Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.==== Computer networks ====This branch of computer science aims to manage networks between computers worldwide.==== Computer security and cryptography ====Computer security is a branch of computer technology with an objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.==== Databases ====A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages.=== Computer applications ======= Computer graphics and visualization ====Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.==== Human–computer interaction ====Research that develops theories, principles, and guidelines for user interface designers, so they can create satisfactory user experiences with desktop, laptop, and mobile devices.==== Scientific computing ====Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. In practical use, it is typically the application of computer simulation and other forms of computation to problems in various scientific disciplines.==== Artificial intelligence ====Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.=== Software engineering ===Software engineering is the study of designing, implementing, and modifying software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal maintenance and arrangement.== Discoveries ==The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent "anything".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as "on/off", "magnetized/de-magnetized", "high-voltage/low-voltage", etc.).Alan Turing's insight: there are only five actions that a computer has to perform in order to do "anything".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;move right one location;read symbol at current location;print 0 at current location;print 1 at current location.Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do "anything".Only three rules are needed to combine any set of basic instructions into more complex ones:sequence: first do this, then do that; selection: IF such-and-such is the case, THEN do this, ELSE do that;repetition: WHILE such-and-such is the case DO this.Note that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).== Programming paradigms ==Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.Object-oriented programming, a programming paradigm based on the concept of "objects", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus Object-oriented computer programs are made out of objects that interact with one another.Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.== Academia ==Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.== Education ==Computer Science, known by its near synonyms, Computing, Computer Studies, Information Technology (IT) and Information and Computing Technology (ICT), has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all 7,000,000 pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.=== Challenges ===In many countries, there is a significant gender gap in computer science education. In 2012, only 20 percent of computer science degrees in the United States were awarded to women. The gender gap is also a problem in other western countries. The gap is smaller, or nonexistent, in some parts of the world. In 2011, women earned half of the computer science degrees in Malaysia. In 2001, 55 percent of computer science graduates in Guyana were women.== See also ==== Notes ==== References ==== Further reading ==== External links ==Computer science at CurlieScholarly Societies in Computer ScienceWhat is Computer Science?Best Papers Awards in Computer Science since 1996Photographs of computer scientists by Bertrand MeyerEECS.berkeley.edu=== Bibliography and academic search engines ===CiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.DBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universität Trier, in Germany.The Collection of Computer Science Bibliographies (article)=== Professional organizations ===Association for Computing MachineryIEEE Computer SocietyInformatics EuropeAAAIAAAS Computer Science=== Misc ===Computer Science—Stack Exchange: a community-run question-and-answer site for computer scienceWhat is computer scienceIs computer science science?Computer Science (Software) Must be Considered as an Independent Discipline.
	Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or is interrupted by the operating system. As of 2015, most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.== History ==An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli Numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.The first theory about software—prior to creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay On Computable Numbers, with an Application to the Entscheidungsproblem (decision problem).This eventually led to the creation of the academic fields of computer science and software engineering; Both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering and development of software.However, prior to 1946, software was not yet the programs stored in the memory of stored-program digital computers, as we now understand it. The first electronic computing devices were instead rewired in order to "reprogram" them.In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper "The Teaching of Concrete Mathematics" contained the earliest known usage of the term "software" found in a search of JSTOR's electronic archives, predating the OED's citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term "software" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.== Types ==On virtually all computer platforms, software can be grouped into a few broad categories.=== Purpose, or domain of use ===Based on the goal, computer software can be divided into:Application software  which is software that uses the computer system to perform special functions or provide entertainment functions beyond the basic operation of the computer itself. There are many different types of application software, because the range of tasks that can be performed with a modern computer is so large—see list of software.System software  which is software for managing computer hardware behaviour, as to provide basic functionalities that are required by users, or for other software to run properly, if at all. System software is also designed for providing a platform for running application software,  and it includes the following:Operating systems  which are essential collections of software that manage resources and provides common services for other software that runs "on top" of them. Supervisory programs, boot loaders, shells and window systems are core parts of operating systems. In practice, an operating system comes bundled with additional software (including application software) so that a user can potentially do some work with a computer that only has one operating system.Device drivers  which operate or control a particular type of device that is attached to a computer. Each device needs at least one corresponding device driver; because a computer typically has at minimum at least one input device and at least one output device, a computer typically needs more than one device driver.Utilities  which are computer programs designed to assist users in the maintenance and care of their computers.Malicious software or malware  which is software that is developed to harm and disrupt computers. As such, malware is undesirable. Malware is closely associated with computer-related crimes, though some malicious programs may have been designed as practical jokes.=== Nature or domain of execution ===Desktop applications such as web browsers and Microsoft Office, as well as smartphone and tablet applications (called "apps"). (There is a push in some parts of the software industry to merge desktop applications with mobile apps, to some extent. Windows 8, and later Ubuntu Touch, tried to allow the same style of application user interface to be used on desktops, laptops and mobiles.)JavaScript scripts are pieces of software traditionally embedded in web pages that are run directly inside the web browser when a web page is loaded without the need for a web browser plugin. Software written in other programming languages can also be run within the web browser if the software is either translated into JavaScript, or if a web browser plugin that supports that language is installed; the most common example of the latter is ActionScript scripts, which are supported by the Adobe Flash plugin.Server software, including:Web applications, which usually run on the web server and output dynamically generated web pages to web browsers, using e.g. PHP, Java, ASP.NET, or even JavaScript that runs on the server. In modern times these commonly include some JavaScript to be run in the web browser as well, in which case they typically run partly on the server, partly in the web browser.Plugins and extensions are software that extends or modifies the functionality of another piece of software, and require that software be used in order to function;Embedded software resides as firmware within embedded systems, devices dedicated to a single use or a few uses such as cars and televisions (although some embedded devices such as wireless chipsets can themselves be part of an ordinary, non-embedded computer system such as a PC or smartphone). In the embedded system context there is sometimes no clear distinction between the system software and the application software. However, some embedded systems run embedded operating systems, and these systems do retain the distinction between system software and application software (although typically there will only be one, fixed, application which is always run).Microcode is a special, relatively obscure type of embedded software which tells the processor itself how to execute machine code, so it is actually a lower level than machine code. It is typically proprietary to the processor manufacturer, and any necessary correctional microcode software updates are supplied by them to users (which is much cheaper than shipping replacement processor hardware). Thus an ordinary programmer would not expect to ever have to deal with it.=== Programming tools ===Programming tools are also software in the form of programs or applications that software developers (also known asprogrammers, coders, hackers or software engineers) use to create, debug, maintain (i.e. improve or fix), or otherwise support software.Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.== Topics ===== Architecture ===Users often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.Platform software  The Platform includes the firmware, device drivers, an operating system, and typically a graphical user interface which, in total, allow a user to interact with the computer and its peripherals (associated equipment). Platform software often comes bundled with the computer. On a PC one will usually have the ability to change the platform software.Application software  Application software or Applications are what most people think of when they think of software. Typical examples include office suites and video games. Application software is often purchased separately from computer hardware. Sometimes applications are bundled with the computer, but that does not change the fact that they run as independent applications. Applications are usually independent programs from the operating system, though they are often tailored for specific platforms. Most users think of compilers, databases, and other "system software" as applications.User-written software  End-user development tailors systems to meet users' specific needs. User software include spreadsheet templates and word processor templates. Even email filters are a kind of user software. Users create this software themselves and often overlook how important it is. Depending on how competently the user-written software has been integrated into default application packages, many users may not be aware of the distinction between the original packages, and what has been added by co-workers.=== Execution ===Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using "pointers" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.=== Quality and reliability ===Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.Many bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that "every program has at least one more bug" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together.=== License ===The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.Proprietary software can be divided into two types:freeware, which includes the category of "free trial" software or "freemium" software (in the past, the term shareware was often used for free trial/freemium software). As the name suggests, freeware can be used free, although in the case of free trials or freemium software, this is sometimes only true for a limited period of time or with limited functionality.software available for a fee, often inaccurately termed "commercial software", which can only be legally used on purchase of a license.Open source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software.=== Patents ===Software patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a detailed idea (e.g. an algorithm) on how to implement a piece of software, or a component of a piece of software. Ideas for useful things that software could do, and user requirements, are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since all useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.Software patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over any programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.== Design and implementation ==Design and implementation of software varies depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the latter has much more basic functionality.Software is usually designed and created (aka coded/written/programmed) in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software (if applicable). As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like Form1.Close() and Form1.Show() to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as "coder" and "hacker" – although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.== Industry and organizations ==A great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the co-founder of Microsoft was the richest person in the world in 2009, largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products - both market leaders in their respective product categories.Non-profit software organizations include the Free Software Foundation, GNU Project and the Mozilla Foundation. Software standard organizations like the W3C, IETF develop recommended software standards such as XML, HTTP and HTML, so that software can interoperate through these standards.Other well-known large software companies include Google, IBM, TCS, Infosys, Wipro, HCL Technologies, Oracle, Novell, SAP, Symantec, Adobe Systems, Sidetrade and Corel, while small companies often provide innovation.== See also ==Software release life cycleIndependent software vendorList of softwareSoftware asset managementOpen-source software== References ===== Sources ===Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.== External links ==Software at Curlie
	The philosophy of computer science is concerned with the philosophical questions that arise with the study of computer science, which is understood to mean not just programming but the whole study of concepts and methods that assist in the development and maintenance of computer systems. There is still no common understanding of the content, aim, focus, or topic of the philosophy of computer science, despite some attempts to develop a philosophy of computer science like the philosophy of physics or the philosophy of mathematics.The philosophy of computer science as such deals with the meta-activity that is associated with the development of the concepts and methodologies that implement and analyze the computational systems.== See also ==Computer-assisted proof: Philosophical objectionsPhilosophy of artificial intelligencePhilosophy of informationPhilosophy of mathematicsPhilosophy of sciencePhilosophy of technology== References ==== Further reading ==Matti Tedre (2014). The Science of Computing: Shaping a Discipline. Chapman Hall.Scott Aaronson. "Why Philosophers Should Care About Computational Complexity". In Computability: Gödel, Turing, Church, and beyond.Timothy Colburn. Philosophy and Computer Science. Explorations in Philosophy. M.E. Sharpe, 1999. ISBN 1-56324-991-X.A.K. Dewdney. New Turing Omnibus: 66 Excursions in Computer ScienceLuciano Floridi (editor). The Blackwell Guide to the Philosophy of Computing and Information, 2004.Luciano Floridi (editor). Philosophy of Computing and Information: 5 Questions. Automatic Press, 2008.Luciano Floridi. Philosophy and Computing: An Introduction, Routledge, 1999.Christian Jongeneel. The informatical worldview, an inquiry into the methodology of computer science.Jan van Leeuwen. "Towards a philosophy of the information and computing sciences", NIAS Newsletter 42, 2009.Moschovakis, Y. (2001). What is an algorithm? In Enquist, B. and Schmid, W., editors, Mathematics unlimited — 2001 and beyond, pages 919–936. Springer.Alexander Ollongren, Jaap van den Herik. Filosofie van de informatica. London and New York: Routledge, 1999. ISBN 0-415-19749-XTedre, Matti (2014), The Science of Computing: Shaping a Discipline Taylor and Francis.Ray Turner and Nicola Angius. "The Philosophy of Computer Science". Stanford Encyclopedia of Philosophy.Matti Tedre (2011). Computing as a Science: A Survey of Competing Viewpoints. Minds & Machines 21, 3, 361–387.Ray Turner. Computational Artefacts-Towards a Philosophy of Computer Science. Springer. [1]== External links ==The International Association for Computing and PhilosophyPhilosophy of Computing and Information at PhilPapersA draft version of Philosophy of Computer Science by William J. RapaportPhilosophy of Computation at Berkeley
	In computing, static dispatch is a form of polymorphism fully resolved during compile time. It is a form of method dispatch, which describes how a language or environment will select which implementation of a method or function to use.Examples are templates in C++, and generic programming in other languages, in conjunction with function overloading (including operator overloading). Code is said to be monomorphised, with specific data types deduced and traced through the call graph, in order to instantiate specific versions of generic functions, and select specific function calls based on the supplied definitions.This contrasts with dynamic dispatch, which is based on runtime information (such as vtable pointers and other forms of run time type information).Static dispatch is possible because there is a guarantee of there only ever being a single implementation of the method in question.  Static dispatch is typically faster than dynamic dispatch which by nature has higher overhead.== References ==https://developer.apple.com/swift/blog/?id=27
	Information and computer science (ICS) or computer and information science (CIS) (plural forms, i.e., sciences, may also be used) is a field that emphasizes both computing and informatics, upholding the strong association between the fields of information sciences and computer sciences and treating computers as a tool rather than a field.Information science is one with a long history, unlike the relatively very young field of computer science, and is primarily concerned with gathering, storing, disseminating, sharing and protecting any and all forms of information. It is a broad field, covering a myriad of different areas but is often referenced alongside computer science because of the incredibly useful nature of computers and computer programs in helping those studying and doing research in the field – particularly in helping to analyse data and in spotting patterns too broad for a human to intuitively perceive. While information science is sometimes confused with information theory, the two have vastly different subject matter. Information theory focuses on one particular mathematical concept of information while information science is focused on all aspects of the processes and techniques of information.Computer science, in contrast, is less focused on information and its different states, but more, in a very broad sense, on the use of computers – both in theory and practice – to design and implement algorithms in order to aid the processing of information during the different states described above. It has strong foundations in the field of mathematics, as the very first recognised practitioners of the field were renowned mathematicians such as Alan Turing.Information science and computing began to converge in the 1950s and 1960s, as information scientists started to realize the many ways computers would improve information storage and retrieval.== Terminology ==Due to the distinction between computers and computing, some research groups refer to computing or datalogy. The French refer to computer science as informatique. The term information and communications technology (ICT), refers to how humans communicate with using machines and computers, making a distinction from information and computer science, which is how computers use and gain information.Informatics is also distinct from computer science, which encompasses the study of logic and low-level computing issues.== Education ==Universities may confer degrees of ICS and CIS, not to be confused with a more specific Bachelor of Computer Science or respective graduate computer science degrees.The QS World University Rankings is one of the most widely recognised and distinguished university comparisons. They ranked the top 10 universities for computer science and information systems in 2015. They are:Massachusetts Institute of Technology (MIT)Stanford UniversityUniversity of OxfordCarnegie Mellon UniversityHarvard UniversityUniversity of California, Berkeley (UCB)University of CambridgeThe Hong Kong University of Science and TechnologySwiss Federal Institute of Technology (ETH Zurich)Princeton UniversityA Computer Information Science degree gives students both network and computing knowledge which is needed to design, develop, and assist information systems which helps to solve business problems and to support business problems and to support business operations and decision making at a managerial level also.== Areas of information and computer science ==Due the nature of this field, many topics are also shared with computer science and information systems.The discipline of Information and Computer Science spans a vast range of areas from basic computer science theory (algorithms and computational logic)  to in depth analysis of data manipulation and use within technology.=== Programming theory ===The process of taking a given algorithm and encoding it into a language that can be understood and executed by a computer. There are many different types of programming languages and various different types of computers, however, they all have the same goal: to turn algorithms into machine code.Popular programming languages used within the academic study of CIS include, but are not limited to;JavaPythonC#C++PerlRubyPascalSwiftVisual Basic=== Information and information systems ===The academic study of software and hardware systems that process large quantities and data, support large scale data management and how data can be used. This is where the field is unique from the standard study of computer science. The area of information systems focuses on the networks of hardware and software that are required to process, manipulate and distribute such data.=== Computer systems and organisations ===The process of analysing computer architecture and various logic circuits. This involves looking at low level computer processes at bit level computation. This is an in-depth look into the hardware processing of a computational system, involving looking at the basic structure of a computer and designing such systems. This can also involve evaluating complex circuit diagrams, and being able to construct these to solve a main problem.The main purpose behind this area of study is to achieve an understanding of how computers function on a basic level, often through tracing machine operations.=== Machines, languages, and computation ===This is the study into fundamental computer algorithms, which are the basis to computer programs. Without algorithms, no computer programs would exist. This also involves the process of looking into various mathematical functions behind computational algorithms, basic theory and functional (low level) programming.In an academic setting, this area would introduce the fundamental mathematical theorems and functions behind theoretical computer science which are the building blocks for other areas in the field. Complex topics such as; proofs, algebraic functions and sets will be introduced during studies of CIS.== Developments ==Information and computer science is a field that is rapidly developing with job prospects for students being extremely promising with 75.7% of graduates gaining employment. Also the IT industry employs one in twenty of the workforce with it predicted to increase nearly five times faster than the average of the UK and between 2012 and 2017 more than half a million people will be needed within the industry and the fact that nine out of ten tech firms are suffering from candidate shortages which is having a negative impact on their business as it delays the creation and development of new products, and it’s predicted in the US that in the next decade there will be more than one million jobs in the technology sector than computer science graduates to fill them. Because of this programming is now being taught at an earlier age with an aim to interest students from a young age into computer and information science hopefully leading more children to study this at a higher level. For example, children in England will now be exposed to computer programming at the age of 5 due to an updated national curriculum.== Employment ==Due to the wide variety of jobs that now involve computer and information science related tasks, it is difficult to provide a comprehensive list of possible jobs in this area, but some of the key areas are artificial intelligence, software engineering and computer networking and communication. Work in this area also tends to require sufficient understanding of mathematics and science. Moreover, jobs that having a CIS degree can lead to, include: systems analyst, network administrator, system architect, information systems developer, web programmer, or software developer.The earning potential for CIS graduates is quite promising. A 2013 survey from the National Association of Colleges and Employers (NACE) found that the average starting salary for graduates who earned a degree in a computer related field was $59,977, up 4.3% from the prior year. This is higher than other popular degrees such as business ($54,234), education ($40,480) and math and sciences ($42,724). Furthermore, Payscale ranked 129 college degrees based on their graduates earning potential with engineering, math, science, and technology fields dominating the ranking. With eight computer related degrees appearing among the top 30. With the lowest starting salary for these jobs being $49,900. A Rasmussen College article describes various jobs CIS graduates may obtain with software applications developers at the top making a median income of  $98,260.According to the National Careers Service an Information Scientist can expect to earn £24,000+ per year as a starting salary.== References ==
	Linguistics is the scientific study of language. It involves analysing language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 6th-century-BC Indian grammarian Pāṇini who wrote a formal description of the Sanskrit language in his Aṣṭādhyāyī.Linguists traditionally analyse human language by observing an interplay between sound and meaning. Phonetics is the study of speech and non-speech sounds, and delves into their acoustic and articulatory properties. The study of language meaning, on the other hand, deals with how languages encode relations between entities, properties, and other aspects of the world to convey, process, and assign meaning, as well as manage and resolve ambiguity. While the study of semantics typically concerns itself with truth conditions, pragmatics deals with how situational context influences the production of meaning.Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Many modern theories that deal with the principles of grammar are based on Noam Chomsky's framework of generative linguistics.In the early 20th century, Ferdinand de Saussure distinguished between the notions of langue and parole in his formulation of structural linguistics. According to him, parole is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language. This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's langue), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., parole, in Saussurean terms).The study of parole (which manifests through cultural discourses and dialects) is the domain of sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by its own set of grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language. This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or corpus data).Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.Linguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focuses on how languages change and grow, particularly over an extended period of time.Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.Related areas of study also include the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).== Nomenclature ==Before the 20th century, the term philology, first attested in 1716, was commonly used to refer to the study of language, which was then predominantly historical in focus. Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted and the term philology is now generally used for the "study of a language's grammar, history, and literary tradition", especially in the United States (where philology has never been very popularly considered as the "science of language").Although the term "linguist" in the sense of "a student of language" dates from 1641, the term "linguistics" is first attested in 1847. It is now the usual term in English for the scientific study of language, though linguistic science is sometimes used.Linguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, and the humanities.  Many linguists, such as David Crystal, conceptualize the field as being primarily scientific.  The term linguist applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.== Variation and universality ==While some theories on linguistics focus on the different varieties that language produces, among different sections of society, others focus on the universal properties that are common to all human languages. The theory of variation therefore would elaborate on the different usages of popular languages like French and English across the globe, as well as its smaller dialects and regional permutations within their national boundaries. The theory of variation looks at the cultural stages that a particular language undergoes, and these include the following.=== Pidgin ===The pidgin stage in a language is a stage when communication occurs through a grammatically simplified means, developing between two or more groups that do not have a language in common. Typically, it is a mixture of languages at the stage when there occurs a mixing between a primary language with other language elements.=== Creole ===A creole stage in language occurs when there is a stable natural language developed from a mixture of different languages. It is a stage that occurs after a language undergoes its pidgin stage. At the creole stage, a language is a complete language, used in a community and acquired by children as their native language.=== Dialect ===A dialect is a variety of language that is characteristic of a particular group among the language speakers. The group of people who are the speakers of a dialect are usually bound to each other by social identity. This is what differentiates a dialect from a register or a discourse, where in the latter case, cultural identity does not always play a role. Dialects are speech varieties that have their own grammatical and phonological rules, linguistic features, and stylistic aspects, but have not been given an official status as a language. Dialects often move on to gain the status of a language due to political and social reasons. Other times, dialects remain marginalized, particularly when they are associated with marginalized social groups. Differentiation amongst dialects (and subsequently, languages too) is based upon the use of grammatical rules, syntactic rules, and stylistic features, though not always on lexical use or vocabulary. The popular saying that "a language is a dialect with an army and navy" is attributed as a definition formulated by Max Weinreich.Universal grammar takes into account general formal structures and features that are common to all dialects and languages, and the template of which pre-exists in the mind of an infant child. This idea is based on the theory of generative grammar and the formal school of linguistics, whose proponents include Noam Chomsky and those who follow his theory and work."We may as individuals be rather fond of our own dialect. This should not make us think, though, that it is actually any better than any other dialect. Dialects are not good or bad, nice or nasty, right or wrong – they are just different from one another, and it is the mark of a civilised society that it tolerates different dialects just as it tolerates different races, religions and sexes."=== Discourse ===Discourse is language as social practice (Baynham, 1995) and is a multilayered concept.  As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies.   Discourse influences genre, which is chosen in response to different situations and finally, at micro level, discourse influences language as text (spoken or written) at the phonological or lexico-grammatical level. Grammar and discourse are linked as parts of a system.  A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register. There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Registers and discourses therefore differentiate themselves through the use of vocabulary, and at times through the use of style too. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the "medical discourse", and so on.=== Standard language ===When a dialect is documented sufficiently through the linguistic description of its grammar, which has emerged through the consensual laws from within its community, it gains political and national recognition through a country or region's policies. That is the stage when a language is considered a standard variety, one whose grammatical laws have now stabilised from within the consent of speech community participants, after sufficient evolution, improvisation, correction, and growth. The English language, besides perhaps the French language, may be examples of languages that have arrived at a stage where they are said to have become standard varieties.The study of a language's universal properties, on the other hand, include some of the following concepts.=== Lexicon ===The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can't stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization, and the new words are called neologisms.It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.=== Relativity ===As constructed popularly through the Sapir–Whorf hypothesis, relativists believe that the structure of a particular language is capable of influencing the cognitive patterns through which a person shapes his or her world view. Universalists believe that there are commonalities between human perception as there is in the human capacity for language, while relativists believe that this varies from language to language and person to person. While the Sapir–Whorf hypothesis is an elaboration of this idea expressed through the writings of American linguists Edward Sapir and Benjamin Lee Whorf, it was Sapir's student Harry Hoijer who termed it thus. The 20th century German linguist Leo Weisgerber also wrote extensively about the theory of relativity. Relativists argue for the case of differentiation at the level of cognition and in semantic domains. The emergence of cognitive linguistics in the 1980s also revived an interest in linguistic relativity. Thinkers like George Lakoff have argued that language reflects different cultural metaphors, while the French philosopher of language Jacques Derrida's writings have been seen to be closely associated with the relativist movement in linguistics, especially through deconstruction and was even heavily criticized in the media at the time of his death for his theory of relativism.== Structures ==Linguistic structures are pairings of meaning and form.  Any particular pairing of meaning and form is a Saussurean sign.  For instance, the meaning "cat" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data.Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously).  All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis.  For instance, consider the structure of the word "tenth" on two different levels of analysis.  On the level of internal word structure (known as morphology), the word "tenth" is made up of one linguistic form indicating a number and another form indicating ordinality.  The rule governing the combination of these forms ensures that the ordinality marker "th" follows the number "ten."  On the level of sound structure (known as phonology), structural analysis shows that the "n" sound in "tenth" is made differently from the "n" sound in "ten" spoken alone.  Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of "tenth", they are less often aware of the rule governing its sound structure.  Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language.Linguistics has many sub-fields concerned with particular aspects of linguistic structure. The theory that elucidates on these, as propounded by Noam Chomsky, is known as generative theory or universal grammar. These sub-fields range from those focused primarily on form to those focused primarily on meaning. They also run the gamut of level of analysis of language, from individual sounds, to words, to phrases, up to cultural discourse.=== Grammar ===Sub-fields that focus on a grammatical study of language include the following.Phonetics, the study of the physical properties of speech sound production and perceptionPhonology, the study of sounds as abstract elements in the speaker's mind that distinguish meaning (phonemes)Morphology, the study of morphemes, or the internal structures of words and how they can be modifiedSyntax, the study of how words combine to form grammatical phrases and sentencesSemantics, the study of the meaning of words (lexical semantics) and fixed word combinations (phraseology), and how these combine to form the meanings of sentencesPragmatics, the study of how utterances are used in communicative acts, and the role played by context and non-linguistic knowledge in the transmission of meaningDiscourse analysis, the analysis of language use in texts (spoken, written, or signed)Stylistics, the study of linguistic factors (rhetoric, diction, stress) that place a discourse in contextSemiotics, the study of signs and sign processes (semiosis), indication, designation, likeness, analogy, metaphor, symbolism, signification, and communication=== Style ===Stylistics is the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric, diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.== Approaches ===== Theoretical ===One major debate in linguistics concerns the very nature of language and how it should be understood. Some linguists hypothesize that there is a module in the human brain that allows people to undertake linguistic behaviour, which is part of the formalist approach. This "universal grammar" is considered to guide children when they learn language and to constrain what sentences are considered grammatical in any human language. Proponents of this view, which is predominant in those schools of linguistics that are based on the generative theory of Noam Chomsky, do not necessarily consider that language evolved for communication in particular. They consider instead that it has more to do with the process of structuring human thought (see also formal grammar).=== Functional ===Another group of linguists, by contrast, use the term "language" to refer to a communication system that developed to support cooperative activity and extend cooperative networks. Such theories of grammar, called "functional", view language as a tool that emerged and is adapted to the communicative needs of its users, and the role of cultural evolutionary processes are often emphasized over that of biological evolution.== Methodology ==Linguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is "good" or "bad". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is "better" or "worse" than another.Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favouring a particular dialect or "acrolect". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in language instruction, like in ELT, where certain fundamental grammatical rules and lexical items need to be introduced to a second-language speaker who is attempting to acquire the language.=== Anthropology ===The objective of describing languages is often to uncover cultural knowledge about communities. The use of anthropological methods of investigation on linguistic sources leads to the discovery of certain cultural traits among a speech community through its linguistic features. It is also widely used as a tool in language documentation, with an endeavour to curate endangered languages. However, linguistic inquiry now uses the anthropological method to understand cognitive, historical, sociolinguistic and historical processes that languages undergo as they change and evolve, as well as general anthropological inquiry uses the linguistic method to excavate into culture. In all aspects, anthropological inquiry usually uncovers the different variations and relativities that underlie the usage of language.=== Sources ===Most contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is becauseSpeech appears to be universal to all human beings capable of producing and perceiving it, while there have been many cultures and speech communities that lack written communication;Features appear in speech which aren't always recorded in writing, including phonological rules, sound changes, and speech errors;All natural writing systems reflect a spoken language (or potentially a signed one), even with pictographic scripts like Dongba writing Naxi homophones with the same pictogram, and text in writing systems used for two languages changing to fit the spoken language being recorded;Speech evolved before human beings invented writing;People learnt to speak and process spoken language more easily and earlier than they did with writing.Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.=== Analysis ===Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was more geared towards analysis and comparison between different language variations, which existed at the same given point of time.At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article "the" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.== History ===== Early grammarians ===The formal study of language began in India with Pāṇini, the 6th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a non-Arab, made a detailed description of Arabic in AD 760 in his monumental work, Al-kitab fi al-nahw (الكتاب في النحو, The Book on Grammar), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system).  Western interest in the study of languages began somewhat later than in the East, but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his Cratylus dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in and taught Greek to speakers of other languages. While this school was the first to use the word "grammar" in its modern sense, Plato had used the word in its original meaning as "téchnē grammatikḗ" (Τέχνη Γραμματική), the "art of writing", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax. Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.=== Comparative philology ===In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics. Bloomfield attributes "the first great scientific linguistic work of the world" to Jacob Grimm, who wrote Deutsche Grammatik. It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts (On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race).=== Structuralism ===Early in the 20th century, Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them. By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still foundational in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the langue-parole distinction, distinguishing language as an abstract system (langue) from language as a concrete manifestation of this system (parole). Substantial additional contributions following Saussure's definition of a structural approach to language came from The Prague school, Leonard Bloomfield, Charles F. Hockett, Louis Hjelmslev, Émile Benveniste and Roman Jakobson.=== Generativism ===During the last half of the 20th century, following the work of Noam Chomsky, linguistics was dominated by the generativist school. While formulated by Chomsky in part as a way to explain how human beings acquire language and the biological constraints on this acquisition, in practice it has largely been concerned with giving formal accounts of specific phenomena in natural languages. Generative theory is modularist and formalist in character. Chomsky built on earlier work of Zellig Harris to formulate the generative theory of language. According to this theory the most basic form of language is a set of syntactic rules universal for all humans and underlying the grammars of all human languages. This set of rules is called Universal Grammar, and for Chomsky describing it is the primary objective of the discipline of linguistics. For this reason the grammars of individual languages are of importance to linguistics only in so far as they allow us to discern the universal underlying rules from which the observable linguistic variability is generated.In the classic formalization of generative grammars first proposed by Noam Chomsky in the 1950s, a grammar G consists of the following components:A finite set N of nonterminal symbols, none of which appear in strings formed from G.A finite set                     Σ              {\displaystyle \Sigma }   of terminal symbols that is disjoint from N.A finite set P of production rules, that map from one string of symbols to another.A formal description of language attempts to replicate a speaker's knowledge of the rules of their language, and the aim is to produce a set of rules that is minimally sufficient to successfully model valid linguistic forms.=== Functionalism ===Functional theories of language propose that since language is fundamentally a tool, it is reasonable to assume that its structures are best analysed and understood with reference to the functions they carry out. Functional theories of grammar differ from formal theories of grammar, in that the latter seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, whereas the former defines the functions performed by language and then relates these functions to the linguistic elements that carry them out. This means that functional theories of grammar tend to pay attention to the way language is actually used, and not just to the formal relations between linguistic elements.Functional theories describe language in term of the functions existing at all levels of language.Phonological function: the function of the phoneme is to distinguish between different lexical material.Semantic function: (Agent, Patient, Recipient, etc.), describing the role of participants in states of affairs or actions expressed.Syntactic functions: (e.g. Subject and Object), defining different perspectives in the presentation of a linguistic expressionPragmatic functions: (Theme and Rheme, Topic and Focus, Predicate), defining the informational status of constituents, determined by the pragmatic context of the verbal interaction. Functional descriptions of grammar strive to explain how linguistic functions are performed in communication through the use of linguistic forms.=== Cognitive linguistics ===Cognitive linguistics emerged as a reaction to generativist theory in the 1970s and 1980s. Led by theorists like Ronald Langacker and George Lakoff, cognitive linguists propose that language is an emergent property of basic, general-purpose cognitive processes. In contrast to the generativist school of linguistics, cognitive linguistics is non-modularist and functionalist in character. Important developments in cognitive linguistics include cognitive grammar, frame semantics, and conceptual metaphor, all of which are based on the idea that form–function correspondences based on representations derived from embodied experience constitute the basic units of language.Cognitive linguistics interprets language in terms of concepts (sometimes universal, sometimes specific to a particular tongue) that underlie its form. It is thus closely associated with semantics but is distinct from psycholinguistics, which draws upon empirical findings from cognitive psychology in order to explain the mental processes that underlie the acquisition, storage, production and understanding of speech and writing.  Unlike generative theory, cognitive linguistics denies that there is an autonomous linguistic faculty in the mind; it understands grammar in terms of conceptualization; and claims that knowledge of language arises out of language use. Because of its conviction that knowledge of language is learned through use, cognitive linguistics is sometimes considered to be a functional approach, but it differs from other functional approaches in that it is primarily concerned with how the mind creates meaning through language, and not with the use of language as a tool of communication.== Areas of research ===== Historical linguistics ===Historical linguists study the history of specific languages as well as general characteristics of language change. The study of language change is also referred to as "diachronic linguistics" (the study of how one particular language has changed over time), which can be distinguished from  "synchronic linguistics" (the comparative study of more than one language at a given moment in time without regard to previous stages). Historical linguistics was among the first sub-disciplines to emerge in linguistics, and was the most widely practised form of linguistics in the late 19th century. However, there was a shift to the synchronic approach in the early twentieth century with Saussure, and became more predominant in western linguistics with the work of Noam Chomsky.=== Ecolinguistics ===Ecolinguistics explores the role of language in the life-sustaining interactions of humans, other species and the physical environment. The first aim is to develop linguistic theories which see humans not only as part of society, but also as part of the larger ecosystems that life depends on. The second aim is to show how linguistics can be used to address key ecological issues, from climate change and biodiversity loss to environmental justice.=== Sociolinguistics ===Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and ideolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research on both style and discourse in language, and also study the theoretical factors that are at play between language and society.=== Developmental linguistics ===Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into is how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is.=== Neurolinguistics ===Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.== Applied linguistics ==Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and "applies" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. "Applied linguistics" has been argued to be something of a misnomer. Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally "applying" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim. This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international lingua franca like English. Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved. Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.== Interdisciplinary fields ==Within the broad discipline of linguistics, various emerging sub-disciplines focus on a more detailed description and analysis of language, and are often organized on the basis of the school of thought and theoretical approach that they pre-suppose, or the external factors that influence them.=== Semiotics ===Semiotics is the study of sign processes (semiosis), or signification and communication, signs, and symbols, both individually and grouped into sign systems, including the study of how meaning is constructed and understood. Semioticians often do not restrict themselves to linguistic communication when studying the use of signs but extend the meaning of "sign" to cover all kinds of cultural symbols. Nonetheless, semiotic disciplines closely related to linguistics are literary studies, discourse analysis, text linguistics, and philosophy of language. Semiotics, within the linguistics paradigm, is the study of the relationship between language and culture. Historically, Edward Sapir and Ferdinand De Saussure's structuralist theories influenced the study of signs extensively until the late part of the 20th century, but later, post-modern and post-structural thought, through language philosophers including Jacques Derrida, Mikhail Bakhtin, Michel Foucault, and others, have also been a considerable influence on the discipline in the late part of the 20th century and early 21st century. These theories emphasize the role of language variation, and the idea of subjective usage, depending on external elements like social and cultural factors, rather than merely on the interplay of formal elements.=== Language documentation ===Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal structural linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a primary focus in many university programmes in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.=== Translation ===The sub-field of translation includes the translation of written and spoken texts across mediums, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations, such as travel agencies as well as governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate for example, which is an automated, programmed facility to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Academic Translators, specialize and semi specialize on various other disciplines such as; Technology, Science, Law, Economics etc.=== Biolinguistics ===Biolinguistics is the study of the biology and evolution of language. It is a highly interdisciplinary field, including linguists, biologists, neuroscientists, psychologists, mathematicians, and others. By shifting the focus of investigation in linguistics to a comprehensive scheme that embraces natural sciences, it seeks to yield a framework by which the fundamentals of the faculty of language are understood.=== Clinical linguistics ===Clinical linguistics is the application of linguistic theory to the fields of Speech-Language Pathology. Speech language pathologists work on corrective measures to cure communication disorders and swallowing disordersChaika (1990) showed that people with schizophrenia who display speech disorders, like rhyming inappropriately, have attentional dysfunction, as when a patient, shown a colour chip and then asked to identify it, responded "looks like clay. Sounds like gray. Take you for a roll in the hay. Heyday, May Day." The color chip was actually clay-colored, so his first response was correct.'However, most people suppress or ignore words which rhyme with what they've said unless they are deliberately producing a pun, poem or rap. Even then, the speaker shows connection between words chosen for rhyme and an overall meaning in discourse. People with schizophrenia with speech dysfunction show no such relation between rhyme and reason. Some even produce stretches of gibberish combined with recognizable words.=== Computational linguistics ===Computational linguistics is the study of linguistic issues in a way that is "computationally responsible", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.=== Evolutionary linguistics ===Evolutionary linguistics is the interdisciplinary study of the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.=== Forensic linguistics ===Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates on the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also contributed expertise in criminal cases.== See also ==== References ==== Bibliography ==Akmajian, Adrian; Demers, Richard; Farmer, Ann; Harnish, Robert (2010). Linguistics: An Introduction to Language and Communication. Cambridge, MA: The MIT Press. ISBN 978-0-262-51370-8.Aronoff, Mark; Rees-Miller, Janie, eds. (2000). The handbook of linguistics. Oxford: Blackwell.Bloomfield, Leonard (1983) [1914]. An Introduction to the Study of Language: New edition. Amsterdam: John Benjamins Publishing. ISBN 978-90-272-8047-3.Chomsky, Noam (1998). On Language. The New Press, New York. ISBN 978-1-56584-475-9.Derrida, Jacques (1967). Of Grammatology. The Johns Hopkins University Press. ISBN 978-0-8018-5830-7.Hall, Christopher (2005). An Introduction to Language and Linguistics. Breaking the Language Spell. Routledge. ISBN 978-0-8264-8734-6.Isac, Daniela; Charles Reiss (2013). I-language: An Introduction to Linguistics as Cognitive Science, 2nd edition. Oxford University Press. ISBN 978-0-19-966017-9.Pinker, Steven (1994). The Language Instinct. William Morrow and Company. ISBN 978-0-14-017529-5.Crystal, David (1990). Linguistics. Penguin Books. ISBN 978-0-14-013531-2.== External links ==The Linguist List, a global online linguistics community with news and information updated dailyGlossary of linguistic terms by SIL International (last updated 2004)Glottopedia, MediaWiki-based encyclopedia of linguistics, under constructionLinguistic sub-fields – according to the Linguistic Society of AmericaLinguistics and language-related wiki articles on Scholarpedia and Citizendium"Linguistics" section – A Bibliography of Literary Theory, Criticism and Philology, ed. J.A. García Landa (University of Zaragoza, Spain)Isac, Daniela; Charles Reiss (2013). I-language: An Introduction to Linguistics as Cognitive Science, 2nd edition. Oxford University Press. ISBN 978-0-19-953420-3.Linguistics at Curlie
	Drop-in replacement is a term used in computer science and other fields. It refers to the abilityto replace one hardware (or software) component with another one without any other code or configurationchanges being required and resulting in no negative impacts. Usually, the replacement has some benefits including one or more of the following:increased securityincreased speedincreased feature setincreased compatibility (e.g. with other components or standards support)increased support (e.g. the old component may no longer be supported, maintained, or manufactured)== See also ==Pin compatibilityPlug compatibleClone (computing)Backward compatibilityKludge
	Business software (or a business application) is any software or set of computer programs used by business users to perform various business functions. These business applications are used to increase productivity, to measure productivity and to perform other business functions accurately.By and large, business software is likely to be developed to meet the needs of a specific business, and therefore is not easily transferable to a different business  environment, unless its nature and operation is identical. Due to the unique requirements of each business, off-the-shelf software is unlikely to completely address a company's needs. However, where an on-the-shelf solution is necessary, due to time or monetary considerations, some level of customization is likely to be required. Exceptions do exist, depending on the business in question, and thorough research is always required before committing to bespoke or off-the-shelf solutions.Some business applications are interactive, i.e., they have a graphical user interface or user interface and users can query/modify/input data and view results instantaneously. They can also run reports instantaneously. Some business applications run in batch mode: they are set up to run based on a predetermined event/time and a business user does not need to initiate them or monitor them.Some business applications are built in-house and some are bought from vendors (off the shelf software products). These business applications are installed on either desktops or big servers. Prior to the introduction of COBOL (a universal compiler) in 1965, businesses developed their own unique machine language.  RCA's language consisted of a 12-position instruction. For example, to read a record into memory, the first two digits would be the instruction (action) code. The next four positions of the instruction (an 'A' address) would be the exact leftmost memory location where you want the readable character to be placed. Four positions (a 'B' address) of the instruction would note the very rightmost memory location where you want the last character of the record to be located. A two digit 'B' address also allows a modification of any instruction. Instruction codes and memory designations excluded the use of 8's or 9's. The first RCA business application was implemented in 1962 on a 4k RCA 301.  The RCA 301, mid frame 501, and large frame 601 began their marketing in early 1960.             Many kinds of users are found within the business environment, and can be categorized by using a small, medium and large matrix:The small business market generally consists of home accounting software, and office suites such as LibreOffice, Microsoft Office or GSuite.The medium size, or small and medium-sized enterprise (SME), has a broader range of software applications, ranging from accounting, groupware, customer relationship management, human resource management systems, outsourcing relationship management, loan origination software, shopping cart software, field service software, and other productivity enhancing applications.The last segment covers enterprise level software applications, such as those in the fields of enterprise resource planning, enterprise content management (ECM), business process management (BPM) and product lifecycle management. These applications are extensive in scope, and often come with modules that either add native functions, or incorporate the functionality of third-party computer programs.Technologies that previously only existed in peer-to-peer software applications, like Kazaa and Napster, are starting to appear within business applications.== Types of business tools ==Enterprise software application (Esa)Resource ManagementEnterprise Resource Planning (ERP)Digital dashboards, also known as business intelligence dashboards, enterprise dashboards, or executive dashboards.  These are visually based summaries of business data that show at-a-glance understanding of conditions through metrics and key performance indicators (KPIs). Dashboards are a very popular  tools that have arisen in the last few years.Online analytical processing (OLAP),  (which include HOLAP, ROLAP and MOLAP) - are a capability of some management, decision support, and executive information systems that support interactive examination of large amounts of data from many perspectives.Reporting software generates aggregated views of data to keep the management informed about the state of their business.Procurement software is business software that helps to automate the purchasing function of organizations.Data mining is the extraction of consumer information from a database by utilizing software that can isolate and identify previously unknown patterns or trends in large amounts of data. There is a variety of data mining techniques that reveal different types of patterns. Some of the techniques that belong here are statistical methods (particularly business statistics) and neural networks, as very advanced means of analyzing data.Business performance management (BPM)Document management software is made for organizing and managing multiple documents of various types. Some of them have storage functions for security and back-up of valuable business information.Employee scheduling software- used for creating and distributing employee schedules, as well as for tracking employee hours.== Brief history ==The essential motivation for business software is to increase profits by cutting costs or speeding the productive cycle. In the earliest days of white-collar business automation, large mainframe computers were used to tackle the most tedious jobs, like bank cheque clearing and factory accounting.Factory accounting software was among the most popular of early business software tools, and included the automation of general ledgers, fixed assets inventory ledgers, cost accounting ledgers, accounts receivable ledgers, and accounts payable ledgers (including payroll, life insurance, health insurance, federal and state insurance and retirement).The early use of software to replace manual white-collar labor was extremely profitable, and caused a radical shift in white-collar labor. One computer might easily replace 100 white-collar 'pencil pushers', and the computer would not require any health or retirement benefits.Building on these early successes with IBM, Hewlett-Packard and other early suppliers of business software solutions, corporate consumers demanded business software to replace the old-fashioned drafting board. CAD-CAM software (or computer-aided drafting for computer-aided manufacturing) arrived in the early 1980s. Also, project management software was so valued in the early 1980s that it might cost as much as $500,000 per copy (although such software typically had far fewer capabilities than modern project management software such as Microsoft Project, which one might purchase today for under $500 per copy.)In the early days, perhaps the most noticeable, widespread change in business software was the word processor. Because of its rapid rise, the ubiquitous IBM typewriter suddenly vanished in the 1980s as millions of companies worldwide shifted to the use of Word Perfect business software, and later, Microsoft Word software. Another vastly popular computer program for business were mathematical spreadsheet programs such as Lotus 1-2-3, and later Microsoft Excel.In the 1990s business shifted massively towards globalism with the appearance of SAP software which coordinates a supply-chain of vendors, potentially worldwide, for the most efficient, streamlined operation of factory manufacture.Yet nothing in the history of business software has had the global impact of the Internet, with its email and websites that now serve commercial interests worldwide. Globalism in business fully arrived when the Internet became a household word.The next phase in the evolution of business software is being led by the emergance of Robotic Process Automation (RPA), which involves identifying and automating highly repetitive tasks and processes, with an aim to drive operational efficiency, reduce costs and limit human error. Industries that have been in the forefront of RPA adoption include the Insurance industry, Banking and Financial Services, the Legal industry and the Healthcare industry.== Application support ==Business applications are built based on the requirements from the business users. Also, these business applications are built to use certain kind of Business transactions or data items. These business applications run flawlessly until there are no new business requirements or there is no change in underlying Business transactions. Also, the business applications run flawlessly if there are no issues with computer hardware, computer networks (Internet/intranet), computer disks, power supplies, and various software components (middleware, database, computer programs, etc.).Business applications can fail when an unexpected error occurs. This error could occur due to a data error (an unexpected data input or a wrong data input), an environment error (an in frastructure related error), a programming error, a human error or a work flow error. When a business application fails one needs to fix the business application error as soon as possible so that the business users can resume their work. This work of resolving business application errors is known as business application support.=== Reporting errors ===The Business User calls the business application support team phone number or sends an e-mail to the business application support team. The business application support team gets all the details of the error from the business user on the phone or from the e-mail. These details are then entered in a tracking software. The tracking software creates a request number and this request number is given to the business user. This request number is used to track the progress on the support issue. The request is assigned to a support team member.=== Notification of errors ===For critical business application errors (such as an application not available or an application not working correctly), an e-mail is sent to the entire organization or impacted teams so that they are aware of the issue. They are also provided with an estimated time for application availability.=== Investigation or analysis of application errors ===The business application support team member collects all the necessary information about the business software error. This information is then recorded in the support request. All of the data used by the business user is also used in the investigation. The application program is reviewed for any possible programming errors.=== Error resolution ===If any similar business application errors occurred in the past then the issue resolution steps are retrieved from the support knowledge base and the error is resolved using those steps. If it is a new support error, then new issue resolution steps are created and the error is resolved. The new support error resolution steps are recorded in the knowledge base for future use. For major business application errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error.=== Code correction ===If the business application error occurred due to programming errors, then a request is created for the application development team to correct programming errors. If the business user needs new features or functions in the business application, then the required analysis/design/programming/testing/release is planned and a new version of the business software is deployed.=== Business process correction ===If the business application error occurred due to a work flow issue or human errors during data input, then the business users are notified. Business users then review their work flow and revise it if necessary. They also modify the user guide or user instructions to avoid such an error in the future.=== Infrastructure issue correction ===If the business application error occurred due to infrastructure issues, then the specific infrastructure team is notified. The infrastructure team then implements permanent fixes for the issue and monitors the infrastructure to avoid the re-occurrence of the same error.== Support follow up and internal reporting ==The business application error tracking system is used to review all issues periodically (daily, weekly and monthly) and reports are generated to monitor the resolved issues, repeating issues, and pending issues. Reports are also generated for the IT/IS management for improvement and management of business applications.== See also ==== References ==== External links ==Bizex - Business Software
	Computer science in sport is an interdisciplinary discipline that has its goal in combining the theoretical as well as practical aspects and methods of the areas of informatics and sport science. The main emphasis of the interdisciplinarity is placed on the application and use of computer-based but also mathematical techniques in sport science, aiming in this way at the support and advancement of theory and practice in sports. The reason why computer science has become an important partner for sport science is mainly connected with "the fact that the use of data and media, the design of models, the analysis of systems etc. increasingly requires the support of suitable tools and concepts which are developed and available in computer science".== Historical background ==Going back in history, computers in sports were used for the first time in the 1960s, when the main purpose was to accumulate sports information. Databases were created and expanded in order to launch documentation and dissemination of publications like articles or books that contain any kind of knowledge related to sports science. Until the mid-1970s also the first organization in this area called IASI (International Association for Sports Information) was formally established. Congresses and meetings were organized more often with the aim of standardization and rationalization of sports documentation. Since at that time this area was obviously less computer-oriented, specialists talk about sports information rather than sports informatics when mentioning the beginning of this field of science.Based on the progress of computer science and the invention of more powerful computer hardware in the 1970s, also the real history of computer science in sport began. This was as well the first time when this term was officially used and the initiation of a very important evolution in sports science.In the early stages of this area statistics on biomechanical data, like different kinds of forces or rates, played a major role. Scientists started to analyze sports games by collecting and looking at such values and features in order to interpret them. Later on, with the continuous improvement of computer hardware - in particular microprocessor speed – many new scientific and computing paradigms were introduced, which were also integrated in computer science in sport. Specific examples are modeling as well as simulation, but also pattern recognition, design, and (sports) data mining.As another result of this development, the term 'computer science in sport' has been added in the encyclopedia of sports science in 2004.== Areas of research ==The importance and strong influence of computer science as an interdisciplinary partner for sport and sport science is mainly proven by the research activities in computer science in sport. The following IT concepts are thereby of particular interest:Data acquisition and data processingDatabases and expert systemsModelling (mathematical, IT based, biomechanical, physiological)Simulation (interactive, animation etc.)PresentationBased on the fields from above, the main areas of research in computer science in sport include amongst others:Training and coachingBiomechanicsSports equipment and technologyComputer-aided applications (software, hardware) in sportsUbiquitous computing in sportsMultimedia and InternetDocumentationEducation== Research communities ==A clear demonstration for the evolution and propagation towards computer science in sport is also the fact that nowadays people do research in this area all over the world. Since the 1990s many new national and international organizations regarding the topic of computer science in sport were established. These associations are regularly organizing congresses and workshops with the aim of dissemination as well as exchange of scientific knowledge and information on all sort of topics regarding the interdisciplinary discipline.=== Historical survey ===As a first example, in Australia and New Zealand scientists have built up the MathSport group of ANZIAM (Australia and New Zealand Industrial and Applied Mathematics), which since 1992 organizes biennial meetings, initially under the name "Mathematics and Computers in Sport Conferences", and now "MathSport". Main topics are mathematical models and computer applications in sports, as well as coaching and teaching methods based on informatics.The European community was also among the leading motors of the emergence of the field. Some workshops on this topic were successfully organized in Germany since the late 1980s. In 1997 the first international meeting on computer science in sport was held in Cologne. The main aim was to spread out and share applications, ideas and concepts of the use of computers in sports, which should also make a contribution to the creation of internationalization and thus to boost research work in this area.Since then, such international symposia took place every two years all over Europe. As the first conferences were a raving success, it was decided to go even further and the foundation of an organization was the logical consequence. This step was accomplished in 2003, when the International Association of Computer Science in Sport (IACSS) was established during the 4th international symposium in Barcelona, when Prof. Jürgen Perl was also chosen as the first president. A few years earlier, the first international e-journal on this topic (International Journal of Computer Science in Sport) was released already. The internationalization is confirmed moreover by the fact that three conferences already took place outside of Europe - in Calgary (Canada) in 2007, Canberra (Australia) in 2009 and Shanghai (China) in 2011. During the symposium in Calgary additionally the president position changed - it has been assigned to Prof. Arnold Baca, who has been re-elected in 2009 and 2011. The following Symposia on Computer Science in Sport took place in Europe again, in Istanbul (Turkey) in 2013 and in Loughborough (UK) in 2015. In 2017 the 11th Symposium of Computer Science in Sport took place in Constance (Germany). During the conference in Istanbul Prof. Martin Lames was elected as president of the IACSS. He was re-elected in 2015, 2017 and 2019.The 12th International Symposium of Computer Science in Sports was held in Moscow (Russia) from 8 to 10 July 2019: https://iacss2019.ru/=== National organizations ===In addition to the international associations from above, currently the following national associations on computer science in sport exist (if available, the web addresses are also given):Austrian Association of Computer Science in Sport - http://www.sportinformatik.atBritish Association of Computer Science in Sport and ExerciseChinese Association of Computer Science in SportCroatian Association of Computer Science in SportSection Computer Science in Sport of the German Association of Sport Science - http://www.dvs-sportinformatik.de (in German)Swiss Association of Computer Science in Sport SACSS - http://sacss.orgIndian Federation of Computer Science in Sport - http://www.ifcss.inPortuguese Association of Computer Science in SportTurkish Association of Computer Science in SportRussian Association of Computer Science in Sport - https://www.racss.ru/== References ==== Further reading ==Dabnichki P. & Baca, A. (2008). Computers in Sport, WIT Press. ISBN 978-1-84564-064-4Baca, A. (2015). Computer Science in Sport - Research and practice, Routledge. ISBN 978-1-315-88178-2== External links ==MathSport - ANZIAM (Australia and New Zealand Industrial and Applied Mathematics)ECSS (European College of Sport Science)ISEA (International Sports Engineering Association)IACSS (International Association of Computer Science in Sport)
	Mayank Prakash (born 1973) is a business leader and a techy. Computer Weekly magazine described him as "the most influential person in UK IT" in their 2017 awards.== Qualifications ==Mayank Prakash holds a MBA from Manchester Business School, is a Wharton Fellow and an alumnus of Singularity University.== IT, eBusiness and Transformation career ==Mayank Prakash started his career as a Graduate Engineer Trainee at the Hewlett-Packard. He worked in the HCL JV starting with the entrepreneurial Frontline Solutions start-up venture and was soon after selected to join the Senior Management Trainee Programme and deputed to incubate ERP implementation capabilities working with the Big 4 Consulting firms.Prakash was the International CIO of Avaya, then the Group CIO of iSoft and later the CIO of Sage Group in UK. Prakash was hired by Morgan Stanley as a Managing Director, Tech and Data, as part of the executive team of Morgan Stanley Wealth and Asset Management.In 2014, Prakash left Morgan Stanley to join the Department for Work and Pensions as the Director General in charge of technology, replacing Andy Nelson. In this role, Prakash combines his predecessor's job as CIO with business transformation, security and data responsibilities, and works directly for Sir Robert Devereux. As of 2015, Prakash was paid a salary of between £195,000 and £199,999 for this role, making him one of the 328 most highly paid people in the British public sector at that time.The British Computer Society welcomed the appointment, and Morgan Stanley was sorry to see him leave. GDS Chief Mike Bracken described the hiring of Prakash as the turning point for Whitehall's ability to hire the best digital leaders across industry.Credited with quietly turning around one of the largest IT estates into a lean digital delivery machine on a massive scale, TechUK and DigiLeaders recently recognised the strength of industry partnerships and the scale of digital transformation being delivered at DWP resulting in DWP becoming #8 on Twitter attracting a million digital techies and leaders from all sectors.== Personal life ==Mayank is one of 6 authors of open source heuristic computer vision library.== Awards and recognition ==2015: Computer Weekly UKTech50, 5th most influential person in UK IT2016: CIO of the Year2016: Computer Weekly UKTech50, 6th most influential person in UK IT2017: Computer Weekly UKTech50, most influential person in UK IT2017: Digital Leader of the Year== References ==
	Loop perforation is an approximate computing technique that allows to regularly skip some iterations of a loop.It relies on one parameter: the skip factor. The skip factor can either be interpreted as the number of iteration to skip each time or the number of iterations to perform before skipping one.== Code examples ==The examples that follows provide the result of loop perforation applied on this C-like source code=== Skip n iterations each time ====== Skip one iteration after n ===== See also ==Approximate computingTask skippingMemoization== Notes ==== References ==
	Task skipping is an approximate computing technique that allows to skip code blocks according to a specific boolean condition to be checked at run-time.This technique is usually applied on the most computational-intensive section of the code.It relies on the fact that a tuple of values sequentially computed are going to be useful only if the whole tuple meet certain conditions. Knowing that a value of the tuple invalides or probably will invalidate the whole tuple, it is possible to avoid the computation of the rest of the tuple.== Code example ==The example that follows provides the result of task skipping applied on this C-like source code=== Skipping a task ===== See also ==Loop perforationMemoization== Notes ==== References ==
	Technology transfer in computer science refers to the transfer of technology developed in computer science or applied computing research, from universities and governments to the private sector. These technologies may be abstract, such as algorithms and data structures, or concrete, such as open source software packages.== Examples ==Notable examples of technology transfer in computer science include:== References ==
	The Fable of Oscar is a fable proposed by John L. Pollock in his book How to Build a Person (ISBN 9780262161138) to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines.== Fable ==Once in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an "intelligent machine" that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal.=== Oscar I ===The first version of the machine is called "Oscar I". It has pain sensors and "fight-or-flight" responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960.=== Oscar II ===In order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a "pain sensor sensor" was built to sense its pain sensors, thus giving it a rudimentary self-awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it.=== Oscar III ===The problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can't distinguish if a machine-eating tiger and a mirror image of such tiger. To solve such problem,  "introspective sensors" were built into Oscar II and made him "Oscar III". Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self-awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted.=== Mind/Body Problem ===Consider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are "being self-aware and being conscious".=== Conclusion ===In the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites.== See also ==Mind–body problemRobot== External links ==http://johnpollock.us/ftp/OSCAR-web-page/oscar.htmlhttp://philpapers.org/rec/POLOAC== References ==
	In functional programming, an applicative functor is a structure intermediate between functors and monads, in that they allow sequencing of functorial computations (unlike plain functors) but without deciding on which computation to perform on the basis of the result of a previous computation (unlike monads). Applicative functors are the programming equivalent of lax monoidal functors with tensorial strength in category theory.Applicative functors were introduced in 2007 by Conor McBride and Ross Paterson in their paper Functional Pearl: applicative programming with effects.Applicative functors first appeared as a library feature in Haskell, but have since spread to other languages as well, including  Idris, Agda, OCaml, and Scala. Both Glasgow Haskell and Idris now offer language features designed to ease programming with applicative functors.In Haskell, applicative functors are implemented in the Applicative type class.== See also ==Current definition of the Applicative class in Haskell== References ==
	In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.The simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.== Selection by sorting ==By sorting the list or array then selecting the desired element, selection can be reduced to sorting. This method is inefficient for selecting a single element, but is efficient when many selections need to be made from an array, in which case only one initial, expensive sort is needed, followed by many cheap selection operations – O(1) for an array, though selection is O(n) in a linked list, even if sorted, due to lack of random access. In general, sorting requires O(n log n) time, where n is the length of the list, although a lower bound is possible with non-comparative sorting algorithms like radix sort and counting sort.Rather than sorting the whole list or array, one can instead use partial sorting to select the k smallest or k largest elements. The kth smallest (resp., kth largest element) is then the largest (resp., smallest element) of the partially sorted list – this then takes O(1) to access in an array and O(k) to access in a list.=== Unordered partial sorting ===If partial sorting is relaxed so that the k smallest elements are returned, but not in order, the factor of O(k log k) can be eliminated. An additional maximum selection (taking O(k) time) is required, but since                     k        ≤        n              {\displaystyle k\leq n}  , this still yields asymptotic complexity of O(n). In fact, partition-based selection algorithms yield both the kth smallest element itself and the k smallest elements (with other elements not in order). This can be done in O(n) time – average complexity of quickselect, and worst-case complexity of refined partition-based selection algorithms.Conversely, given a selection algorithm, one can easily get an unordered partial sort (k smallest elements, not in order) in O(n) time by iterating through the list and recording all elements less than the kth element. If this results in fewer than k − 1 elements, any remaining elements equal the kth element. Care must be taken, due to the possibility of equality of elements: one must not include all elements less than or equal to the kth element, as elements greater than the kth element may also be equal to it.Thus unordered partial sorting (lowest k elements, but not ordered) and selection of the kth element are very similar problems. Not only do they have the same asymptotic complexity, O(n), but a solution to either one can be converted into a solution to the other by a straightforward algorithm (finding a max of k elements, or filtering elements of a list below a cutoff of the value of the kth element).=== Partial selection sort ===A simple example of selection by partial sorting is to use the partial selection sort.The obvious linear time algorithm to find the minimum (resp. maximum) – iterating over the list and keeping track of the minimum (resp. maximum) element so far – can be seen as a partial selection sort that selects the 1 smallest element. However, many other partial sorts also reduce to this algorithm for the case k = 1, such as a partial heap sort.More generally, a partial selection sort yields a simple selection algorithm which takes O(kn) time. This is asymptotically inefficient, but can be sufficiently efficient if k is small, and is easy to implement. Concretely, we simply find the minimum value and move it to the beginning, repeating on the remaining list until we have accumulated k elements, and then return the kth element. Here is partial selection sort-based algorithm: function select(list[1..n], k)     for i from 1 to k         minIndex = i         minValue = list[i]         for j from i+1 to n             if list[j] < minValue                 minIndex = j                 minValue = list[j]                 swap list[i] and list[minIndex]     return list[k]== Partition-based selection ==Linear performance can be achieved by a partition-based selection algorithm, most basically quickselect. Quickselect is a variant of quicksort – in both one chooses a pivot and then partitions the data by it, but while Quicksort recurses on both sides of the partition, Quickselect only recurses on one side, namely the side on which the desired kth element is. As with Quicksort, this has optimal average performance, in this case linear, but poor worst-case performance, in this case quadratic. This occurs for instance by taking the first element as the pivot and searching for the maximum element, if the data is already sorted. In practice this can be avoided by choosing a random element as pivot, which yields almost certain linear performance. Alternatively, a more careful deterministic pivot strategy can be used, such as median of medians. These are combined in the hybrid introselect algorithm (analogous to introsort), which starts with Quickselect but falls back to median of medians if progress is slow, resulting in both fast average performance and optimal worst-case performance of O(n).The partition-based algorithms are generally done in place, which thus results in partially sorting the data. They can be done out of place, not changing the original data, at the cost of O(n) additional space.=== Median selection as pivot strategy ===A median-selection algorithm can be used to yield a general selection algorithm or sorting algorithm, by applying it as the pivot strategy in Quickselect or Quicksort; if the median-selection algorithm is asymptotically optimal (linear-time), the resulting selection or sorting algorithm is as well. In fact, an exact median is not necessary – an approximate median is sufficient. In the median of medians selection algorithm, the pivot strategy computes an approximate median and uses this as pivot, recursing on a smaller set to compute this pivot. In practice the overhead of pivot computation is significant, so these algorithms are generally not used, but this technique is of theoretical interest in relating selection and sorting algorithms.In detail, given a median-selection algorithm, one can use it as a pivot strategy in Quickselect, obtaining a selection algorithm. If the median-selection algorithm is optimal, meaning O(n), then the resulting general selection algorithm is also optimal, again meaning linear. This is because Quickselect is a divide and conquer algorithm, and using the median at each pivot means that at each step the search set decreases by half in size, so the overall complexity is a geometric series times the complexity of each step, and thus simply a constant times the complexity of a single step, in fact                     2        =        1                  /                (        1        −        (        1                  /                2        )        )              {\displaystyle 2=1/(1-(1/2))}   times (summing the series).Similarly, given a median-selection algorithm or general selection algorithm applied to find the median, one can use it as a pivot strategy in Quicksort, obtaining a sorting algorithm. If the selection algorithm is optimal, meaning O(n), then the resulting sorting algorithm is optimal, meaning O(n log n). The median is the best pivot for sorting, as it evenly divides the data, and thus guarantees optimal sorting, assuming the selection algorithm is optimal. A sorting analog to median of medians exists, using the pivot strategy (approximate median) in Quicksort, and similarly yields an optimal Quicksort.== Incremental sorting by selection ==Converse to selection by sorting, one can incrementally sort by repeated selection. Abstractly, selection only yields a single element, the kth element. However, practical selection algorithms frequently involve partial sorting, or can be modified to do so. Selecting by partial sorting naturally does so, sorting the elements up to k, and selecting by partitioning also sorts some elements: the pivots are sorted to the correct positions, with the kth element being the final pivot, and the elements between the pivots have values between the pivot values. The difference between partition-based selection and partition-based sorting, as in quickselect versus quicksort, is that in selection one recurses on only one side of each pivot, sorting only the pivots (an average of log(n) pivots are used), rather than recursing on both sides of the pivot.This can be used to speed up subsequent selections on the same data; in the extreme, a fully sorted array allows O(1) selection. Further, compared with first doing a full sort, incrementally sorting by repeated selection amortizes the sorting cost over multiple selections.For partially sorted data (up to k), so long as the partially sorted data and the index k up to which the data is sorted are recorded, subsequent selections of j less than or equal to k can simply select the jth element, as it is already sorted, while selections of j greater than k only need to sort the elements above the kth position.For partitioned data, if the list of pivots is stored (for example, in a sorted list of the indices), then subsequent selections only need to select in the interval between two pivots (the nearest pivots below and above). The biggest gain is from the top-level pivots, which eliminate costly large partitions: a single pivot near the middle of the data cuts the time for future selections in half. The pivot list will grow over subsequent selections, as the data becomes more sorted, and can even be passed to a partition-based sort as the basis of a full sort.== Using data structures to select in sublinear time ==Given an unorganized list of data, linear time (Ω(n)) is required to find the minimum element, because we have to examine every element (otherwise, we might miss it). If we organize the list, for example by keeping it sorted at all times, then selecting the kth largest element is trivial, but then insertion requires linear time, as do other operations such as combining two lists.The strategy to find an order statistic in sublinear time is to store the data in an organized fashion using suitable data structures that facilitate the selection. Two such data structures are tree-based structures and frequency tables.When only the minimum (or maximum) is needed, a good approach is to use a heap, which is able to find the minimum (or maximum) element in constant time, while all other operations, including insertion, are O(log n) or better. More generally, a self-balancing binary search tree can easily be augmented to make it possible to both insert an element and find the kth largest element in O(log n) time; this is called an order statistic tree. We simply store in each node a count of how many descendants it has, and use this to determine which path to follow. The information can be updated efficiently since adding a node only affects the counts of its O(log n) ancestors, and tree rotations only affect the counts of the nodes involved in the rotation.Another simple strategy is based on some of the same concepts as the hash table. When we know the range of values beforehand, we can divide that range into h subintervals and assign these to h buckets. When we insert an element, we add it to the bucket corresponding to the interval it falls in. To find the minimum or maximum element, we scan from the beginning or end for the first nonempty bucket and find the minimum or maximum element in that bucket. In general, to find the kth element, we maintain a count of the number of elements in each bucket, then scan the buckets from left to right adding up counts until we find the bucket containing the desired element, then use the expected linear-time algorithm to find the correct element in that bucket.If we choose h of size roughly sqrt(n), and the input is close to uniformly distributed, this scheme can perform selections in expected O(sqrt(n)) time. Unfortunately, this strategy is also sensitive to clustering of elements in a narrow interval, which may result in buckets with large numbers of elements (clustering can be eliminated through a good hash function, but finding the element with the kth largest hash value isn't very useful). Additionally, like hash tables this structure requires table resizings to maintain efficiency as elements are added and n becomes much larger than h2. A useful case of this is finding an order statistic or extremum in a finite range of data. Using above table with bucket interval 1 and maintaining counts in each bucket is much superior to other methods. Such hash tables are like frequency tables used to classify the data in descriptive statistics.== Lower bounds ==In The Art of Computer Programming, Donald E. Knuth discussed a number of lower bounds for the number of comparisons required to locate the t smallest entries of an unorganized list of n items (using only comparisons). There is a trivial lower bound of n − 1 for the minimum or maximum entry. To see this, consider a tournament where each game represents one comparison. Since every player except the winner of the tournament must lose a game before we know the winner, we have a lower bound of n − 1 comparisons.The story becomes more complex for other indexes. We define                               W                      t                          (        n        )              {\displaystyle W_{t}(n)}   as the minimum number of comparisons required to find the t smallest values. Knuth references a paper published by S. S. Kislitsyn, which shows an upper bound on this value:                              W                      t                          (        n        )        ≤        n        −        t        +                  ∑                      n            +            1            −            t            <            j            ≤            n                          ⌈                              log                          2                                          j                ⌉                          for                        n        ≥        t              {\displaystyle W_{t}(n)\leq n-t+\sum _{n+1-t<j\leq n}\lceil {\log _{2}\,j}\rceil \quad {\text{for}}\,n\geq t}  This bound is achievable for t=2 but better, more complex bounds are known for larger t.== Space complexity ==The required space complexity of selection is O(1) additional storage, in addition to storing the array in which selection is being performed. Such space complexity can be achieved while preserving optimal O(n) time complexity.== Online selection algorithm ==Online selection may refer narrowly to computing the kth smallest element of a stream, in which case partial sorting algorithms (with k + O(1)) space for the k smallest elements so far) can be used, but partition-based algorithms cannot be.Alternatively, selection itself may be required to be online, that is, an element can only be selected from a sequential input at the instance of observation and each selection, respectively refusal, is irrevocable. The problem is to select, under these constraints, a specific element of the input sequence (as for example the largest or the smallest value)with largest probability. This problem can be tackled by the Odds algorithm, which yields the optimal under an independence condition; it is also optimal itself as an algorithm with the number of computations being linear in the length of input.The simplest example is the secretary problem of choosing the maximum with high probability, in which case optimal strategy (on random data) is to track the running maximum of the first n/e elements and reject them, and then select the first element that is higher than this maximum.== Related problems ==One may generalize the selection problem to apply to ranges within a list, yielding the problem of range queries. The question of range median queries (computing the medians of multiple ranges) has been analyzed.== Language support ==Very few languages have built-in support for general selection, although many provide facilities for finding the smallest or largest element of a list. A notable exception is C++, which provides a templated nth_element method with a guarantee of expected linear time, and also partitions the data, requiring that the nth element be sorted into its correct place, elements before the nth element are less than it, and elements after the nth element are greater than it. It is implied but not required that it is based on Hoare's algorithm (or some variant) by its requirement of expected linear time and partitioning of data.For Perl, the module Sort::Key::Top, available from CPAN, provides a set of functions to select the top n elements from a list using several orderings and custom key extraction procedures. Furthermore, the Statistics::CaseResampling module provides a function to calculate quantiles using quickselect.Python's standard library (since 2.4) includes heapq.nsmallest() and nlargest(), returning sorted lists, in O(n log k) time.Because language support for sorting is more ubiquitous, the simplistic approach of sorting followed by indexing is preferred in many environments despite its disadvantage in speed. Indeed, for lazy languages, this simplistic approach can even achieve the best complexity possible for the k smallest/greatest sorted (with maximum/minimum as a special case) if the sort is lazy enough.== See also ==Ordinal optimization== References ==== External links =="Lecture notes for January 25, 1996: Selection and order statistics", ICS 161: Design and Analysis of Algorithms, David Eppstein
	Load Shedding is a technique used in information systems, especially web services, to avoid overloading the system and making it unavailable for all users. The idea is to ignore some requests rather than crashing a system and making it fail to serve any request.Considerations shaping the design of load shedding algorithms include:when one of several load balanced servers becomes unavailable due to overload, all other servers will receive a higher load, potentially leading to more overload and a snow-ball effect which takes down the entire system.when one part in a system of microservices starts becoming slower due to high load, other services will have waiting requests queuing up, potentially more than fits in their memory, which could again take down the entire system.A popular open-source tool for defending against delays in downstream systems is Finagle [1].== References ==Site Reliability Engineering: How Google Runs Production Systems, Edited by Betsy Beyer, Chris Jones, Jennifer Petoff and Niall Richard Murphy, O'Reilly 2016, ISBN 9781491929124System Support for Large-scale Internet Services, Jingyu Zhou, PhD thesis, University of California, Santa Barbara, 2006Handling Overload Conditions In High Performance Trustworthy Information Retrieval Systems, Sumalatha Ramachandran, Sharon Joseph, Sujaya Paulraj, Vetriselvi Ramaraj, Journal of Computing, Volume 2, Issue 4, April 2010, ISSN 2151-9617Load Shedding Techniques for Data Stream Systems, Brian Babcock, Mayur Datar, Rajeev Motwani
	Fork and pull model refers to a software development model mostly used on GitHub, where multiple developers working on an open shared project make their own contributions by sharing a main repository and pushing changes after granted pull request by integrator users. Followed by the appearance of distributed version control systems (DVCS), Git naturally enables the usage of a pull-based development model, in which developers can copy the project onto their own repository, and push their changes to the original repository, where the integrators will determine the validity of the pull request. Ever since its appearance, pull-based development received large popularity within the open software development community. On GitHub, about more than 400000 pull-requests emerged per month on average in 2015. It is also model shared on most collaborative coding platforms, like BitBucket, Gitorious etc. More and more functionalities are added to facilitate pull-based model.== References ==
	In computer science, a quaject is an object-like data structure containing both data and code (or pointers to code), exposed as an interface in the form of callentries, and can accept a list of callentries to other quajects for callbacks and callouts. They were developed by Alexia Massalin in 1989 for the Synthesis kernel, and named for the Qua! Machine, a unique hardware platform built by Massalin. The origin of the term 'qua' is unclear; Massalin claims humorously that it is a sound made by koalas.The main purpose of quajects is to provide an abstraction to manage self-modifying code, by allowing runtime code optimizing on a per-object basis. While the original Synthesis kernel required quajects to be written in hand-developed assembly language, this was done to avoid developing a complex compiler; Massalin noted that just-in-time compilation (JIT) for a high-level programming language that permits runtime code generation, as in Lisp or Smalltalk, can also apply this approach, though she also asserted that the complexity of such a compiler was likely to be prohibitive.Quajects differ from more conventional objects in two key ways: first, they always use a form of the dependency injection pattern to manage both interfaces to other quajects, and continuations out of the quaject; the list of callentry references for this is part of quaject creation, and may be updated during the quaject's lifetime. Second, and more critically, a given quaject's set of methods can be unique to the specific quaject; methods for a type or class of quajects are stored as one or more templates, rather than as fixed code. While shared methods can be accessed through a common table of pointers, individual quajects can also have methods that are generated specifically to tailor the performance for that quaject's behavior.== References ==
	In computer science, unmarshalling or unmarshaling refers to the process of transforming a representation of an object that was used for storage or transmission to a representation of the object that is executable. A serialized object which was used for communication can not be processed by a computer program. An unmarshalling interface takes the serialized object and transforms it into an executable form. Unmarshalling (similar to deserialization) is the reverse process of marshalling.== Usage ==Usually XML objects are used when data needs to be transferred between processes, threads or systems, because this results in shorter message wire format and efficient data transfers. Once the data is transferred back to a program or an application, it needs to be converted back to an executable object for usage. Hence, unmarshalling is generally used in the receiver end of the implementations of Remote Method Invocation (RMI) and Remote Procedure Call (RPC) mechanisms to unmarshal transmitted objects in an executable form.=== JAXB ===JAXB or Java Architecture for XML Binding is the most common framework used by developers to marshal and unmarshal Java objects. JAXB provides for the interconversion between fundamental data types supported by Java and standard XML schema data types.=== XmlSerializer ===XmlSerializer is the framework used by C# developers to marshal and unmarshal C# objects. One of the advantages of C# over Java is that C# natively supports marshalling due to the inclusion of XmlSerializer class. Java, on the other hand requires a non-native glue code in the form of JAXB to support marshalling.=== XML and Executable Representation ===An example of unmarshalling is the conversion of an XML representation of an object to the default representation of the object in any programming language. Consider the following class.XML representation of Student object:Executable representation of Student object:The conversion of the XML representation of the objects created by code snippet 1 to the default executable Java representation of the objects created by code snippet 2 is called unmarshalling.== Unmarshaller in JAXB ==The process of unmarshalling the XML data into an executable Java object is taken care of by the in-built Unmarshaller class. It also validates the XML data as it gets unmarshalled. The unmarshal methods defined in the Unmarshaller class are overloaded for the different types of XML inputs. Some of the important implementations of unmarshal methods:Unmarshalling from an XML File:Unmarshalling from an XML file in InputStream:Unmarshalling from an XML file in a URL:== Unmarshalling XML Data ==Unmarshal methods can deserialize an entire XML document or a small part of it. When the XML root element is globally declared, these methods utilize the JAXBContext's mapping of XML root elements to JAXB mapped classes to initiate the unmarshalling. If the mappings are not sufficient and the root elements are declared locally, the unmarshal methods use declaredType methods for the unmarshalling process. These two approaches can be understood below.=== Unmarshal a global XML root element ===The unmarshal method uses JAXBContext to unmarshal the XML data, when the root element is globally declared. The JAXBContext object always maintains a mapping of the globally declared XML element and its name to a JAXB mapped class. If the XML element name or its @xsi:type attribute matches the JAXB mapped class, the unmarshal method transforms the XML data using the appropriate JAXB mapped class. However, if the XML element name has no match, the unmarshal process will abort and throw an UnmarshalException. This can be avoided by using the unmarshal by declaredType methods.=== Unmarshal a local XML root element ===When the root element is not declared globally, the application assists the unmarshaller by application-provided mapping using declaredType parameters. By an order of precedence, even if the root name has a mapping to an appropriate JAXB class, the declaredType overrides the mapping. However, if the @xsi:type attribute of the XML data has a mapping to an appropriate JAXB class, then this takes precedence over declaredType parameter. The unmarshal methods by declaredType parameters always return a JAXBElement<declaredType> instance. The properties of this JAXBElement instance are set as follows:== Comparison with deserialization ==An object that is serialized is in the form of a byte stream and it can eventually be converted back to a copy of the original object. Deserialization is the process of converting the byte stream data back to its original object type.An object that is marshalled, however, records the state of the original object and it contains the codebase (codebase here refers to a list of URLs where the object code can be loaded from, and not source code). Hence, in order to convert the object state and codebase(s), unmarshalling must be done. The unmarshaller interface automatically converts the marshalled data containing codebase(s) into an executable Java object in JAXB. Any object that can be deserialized can be unmarshalled. However, the converse need not be true.== See also ==Marshalling (computer science)Java Architecture for XML Binding== References ==
	This is a list of algorithm general topics. Analysis of algorithmsAnt colony algorithmApproximation algorithmBest and worst casesBig O notationCombinatorial searchCompetitive analysisComputability theoryComputational complexity theoryEmbarrassingly parallel problemEmergent algorithmEvolutionary algorithmFast Fourier transformGenetic algorithmGraph exploration algorithmHeuristicHill climbingImplementationLas Vegas algorithmLock-free and wait-free algorithmsMonte Carlo algorithmNumerical analysisOnline algorithmPolynomial time approximation schemeProblem sizePseudorandom number generatorQuantum algorithmRandom-restart hill climbingRandomized algorithmRunning timeSorting algorithmSearch algorithmStable algorithm (disambiguation)Super-recursive algorithmTree search algorithm== See also ==List of algorithms for specific algorithmsList of computability and complexity topics for more abstract theoryList of complexity classes, complexity classList of data structures.
	Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the "characterizations" of the notion of "algorithm" in more detail.== The problem of definition ==Over the last 200 years the definition of algorithm has become more complicated and detailed as researchers have tried to pin down the term. Indeed, there may be more than one type of "algorithm". But most agree that algorithm has something to do with defining generalized processes for the creation of "output" integers from other "input" integers – "input parameters" arbitrary and infinite in extent, or limited in extent but still variable—by the manipulation of distinguishable symbols (counting numbers) with finite collections of rules that a person can perform with paper and pencil.The most common number-manipulation schemes—both in formal mathematics and in routine life—are: (1) the recursive functions calculated by a person with paper and pencil, and (2) the Turing machine or its Turing equivalents—the primitive register machine or "counter machine" model, the Random Access Machine model (RAM), the Random access stored program machine model (RASP) and its functional equivalent "the computer".When we are doing "arithmetic" we are really calculating by the use of "recursive functions" in the shorthand algorithms we learned in grade-school, for example, adding and subtracting.The proofs that every "recursive function" we can calculate by hand we can compute by machine and vice versa—note the usage of the words calculate versus compute—is remarkable. But this equivalence together with the thesis (unproven assertion) that this includes every calculation/computation indicates why so much emphasis has been placed upon the use of Turing-equivalent machines in the definition of specific algorithms, and why the definition of "algorithm" itself often refers back to "the Turing machine". This is discussed in more detail under Stephen Kleene's characterization.The following are summaries of the more famous characterizations (Kleene, Markov, Knuth) together with those that introduce novel elements—elements that further expand the definition or contribute to a more precise definition.[A mathematical problem and its result can be considered as two points in a space, and the solution consists of a sequence of steps or a path linking them. Quality of the solution is a function of the path. There might be more than one attribute defined for the path, e.g. length, complexity of shape, an ease of generalizing, difficulty, and so on.]== Chomsky hierarchy ==There is more consensus on the "characterization" of the notion of "simple algorithm".All algorithms need to be specified in a formal language, and the "simplicity notion" arises from the simplicity of the language. The Chomsky (1956) hierarchy is a containment hierarchy of classes of formal grammars that generate formal languages. It is used for classifying of programming languages and abstract machines.From the Chomsky hierarchy perspective, if the algorithm can be specified on a simpler language (than  unrestricted), it can be  characterized by this kind of language, else it is a typical "unrestricted algorithm".Examples: a "general purpose" macro language, like M4 is unrestricted (Turing complete), but the C preprocessor macro language is not, so any algorithm expressed in C preprocessor is a "simple algorithm".See also Relationships between complexity classes.== Features of a Good Algorithm ==The following are the features of a good algorithm;Precision: a good algorithm must have a certain outlined steps. The steps should be exact enough, and not varying.Uniqueness: each step taken in the algorithm should give a definite result as stated by the writer of the algorithm. The results should not fluctuate by any means.Feasibility: the algorithm should be possible and practicable in real life. It should not be abstract or imaginary.Input: a good algorithm must be able to accept a set of defined input.Output: a good algorithm should be able to produce results as output, preferably solutions.Finiteness: the algorithm should have a stop after a certain number of instructions.Generality: the algorithm must apply to a set of defined inputs.== 1881 John Venn's negative reaction to W. Stanley Jevons's Logical Machine of 1870 ==In early 1870 W. Stanley Jevons presented a "Logical Machine" (Jevons 1880:200) for analyzing a syllogism or other logical form e.g. an argument reduced to a Boolean equation. By means of what Couturat (1914) called a "sort of logical piano [,] ... the equalities which represent the premises ... are "played" on a keyboard like that of a typewriter. ... When all the premises have been "played", the panel shows only those constituents whose sum is equal to 1, that is, ... its logical whole. This mechanical method has the advantage over VENN's geometrical method..." (Couturat 1914:75).For his part John Venn, a logician contemporary to Jevons, was less than thrilled, opining that "it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines" (italics added, Venn 1881:120). But of historical use to the developing notion of "algorithm" is his explanation for his negative reaction with respect to a machine that "may subserve a really valuable purpose by enabling us to avoid otherwise inevitable labor":(1) "There is, first, the statement of our data in accurate logical language",(2) "Then secondly, we have to throw these statements into a form fit for the engine to work with – in this case the reduction of each proposition to its elementary denials",(3) "Thirdly, there is the combination or further treatment of our premises after such reduction,"(4) "Finally, the results have to be interpreted or read off. This last generally gives rise to much opening for skill and sagacity."He concludes that "I cannot see that any machine can hope to help us except in the third of these steps; so that it seems very doubtful whether any thing of this sort really deserves the name of a logical engine."(Venn 1881:119–121).== 1943, 1952 Stephen Kleene's characterization ==This section is longer and more detailed than the others because of its importance to the topic: Kleene was the first to propose that all calculations/computations—of every sort, the totality of—can equivalently be (i) calculated by use of five "primitive recursive operators" plus one special operator called the mu-operator, or be (ii) computed by the actions of a Turing machine or an equivalent model.Furthermore, he opined that either of these would stand as a definition of algorithm.A reader first confronting the words that follow may well be confused, so a brief explanation is in order. Calculation means done by hand, computation means done by Turing machine (or equivalent).  (Sometimes an author slips and interchanges the words). A "function" can be thought of as an "input-output box" into which a person puts natural numbers called "arguments" or "parameters" (but only the counting numbers including 0—the nonnegative integers) and gets out a single nonnegative integer (conventionally called "the answer"). Think of the "function-box" as a little man either calculating by hand using "general recursion" or computing by Turing machine (or an equivalent machine)."Effectively calculable/computable" is more generic and means "calculable/computable by some procedure, method, technique ... whatever...".  "General recursive" was Kleene's way of writing what today is called just "recursion"; however, "primitive recursion"—calculation by use of the five recursive operators—is a lesser form of recursion that lacks access to the sixth, additional, mu-operator that is needed only in rare instances. Thus most of life goes on requiring only the "primitive recursive functions."=== 1943 "Thesis I", 1952 "Church's Thesis" ===In 1943 Kleene proposed what has come to be known as Church's thesis:"Thesis I. Every effectively calculable function (effectively decidable predicate) is general recursive" (First stated by Kleene in 1943 (reprinted page 274 in Davis, ed. The Undecidable; appears also verbatim in Kleene (1952) p.300)In a nutshell: to calculate any function the only operations a person needs (technically, formally) are the 6 primitive operators of "general" recursion (nowadays called the operators of the mu recursive functions).Kleene's first statement of this was under the section title "12. Algorithmic theories". He would later amplify it in his text (1952) as follows:"Thesis I and its converse provide the exact definition of the notion of a calculation (decision) procedure or algorithm, for the case of a function (predicate) of natural numbers" (p. 301, boldface added for emphasis)(His use of the word "decision" and "predicate" extends the notion of calculability to the more general manipulation of symbols such as occurs in mathematical "proofs".)This is not as daunting as it may sound – "general" recursion is just a way of making our everyday arithmetic operations from the five "operators" of the primitive recursive functions together with the additional mu-operator as needed. Indeed, Kleene gives 13 examples of primitive recursive functions and Boolos–Burgess–Jeffrey add some more, most of which will be familiar to the reader—e.g. addition, subtraction, multiplication and division, exponentiation, the CASE function, concatenation, etc., etc.; for a list see Some common primitive recursive functions.Why general-recursive functions rather than primitive-recursive functions?Kleene et al. (cf §55 General recursive functions p. 270 in Kleene 1952) had to add a sixth recursion operator called the minimization-operator (written as μ-operator or mu-operator) because Ackermann (1925) produced a hugely growing function—the Ackermann function—and Rózsa Péter (1935) produced a general method of creating recursive functions using Cantor's diagonal argument, neither of which could be described by the 5 primitive-recursive-function operators. With respect to the Ackermann function:"...in a certain sense, the length of the computation algorithm of a recursive function which is not also primitive recursive grows faster with the arguments than the value of any primitive recursive function" (Kleene (1935) reprinted p. 246 in The Undecidable, plus footnote 13 with regards to the need for an additional operator, boldface added).But the need for the mu-operator is a rarity. As indicated above by Kleene's list of common calculations, a person goes about their life happily computing primitive recursive functions without fear of encountering the monster numbers created by Ackermann's function (e.g. super-exponentiation ).=== 1952 "Turing's thesis" ===Turing's Thesis hypothesizes the computability of "all computable functions" by the Turing machine model and its equivalents.To do this in an effective manner, Kleene extended the notion of "computable" by casting the net wider—by allowing into the notion of "functions" both "total functions" and "partial functions". A total function is one that is defined for all natural numbers (positive integers including 0). A partial function is defined for some natural numbers but not all—the specification of "some" has to come "up front". Thus the inclusion of "partial function" extends the notion of function to "less-perfect" functions. Total- and partial-functions may either be calculated by hand or computed by machine.Examples:"Functions": include "common subtraction m − n" and "addition m + n""Partial function": "Common subtraction" m − n is undefined when only natural numbers (positive integers and zero) are allowed as input – e.g. 6 − 7 is undefinedTotal function: "Addition" m + n is defined for all positive integers and zero.We now observe Kleene's definition of "computable" in a formal sense:Definition: "A partial function φ is computable, if there is a machine M which computes it" (Kleene (1952) p. 360)"Definition 2.5. An n-ary function f(x1, ..., xn) is partially computable if there exists a Turing machine Z such thatf(x1, ..., xn) = ΨZ(n)(x1, ..., [xn)In this case we say that [machine] Z computes f. If, in addition, f(x1, ..., xn) is a total function, then it is called computable" (Davis (1958) p. 10)Thus we have arrived at Turing's Thesis:"Every function which would naturally be regarded as computable is computable ... by one of his machines..." (Kleene (1952) p.376)Although Kleene did not give examples of "computable functions" others have. For example, Davis (1958) gives Turing tables for the Constant, Successor and Identity functions, three of the five operators of the primitive recursive functions:Computable by Turing machine:Addition (also is the Constant function if one operand is 0)Increment (Successor function)Common subtraction (defined only if x ≥ y). Thus "x − y" is an example of a partially computable function.Proper subtraction x┴y (as defined above)The identity function: for each i, a function UZn =  ΨZn(x1, ..., xn) exists that plucks xi out of the set of arguments (x1, ..., xn)MultiplicationBoolos–Burgess–Jeffrey (2002) give the following as prose descriptions of Turing machines for:Doubling: 2pParityAdditionMultiplicationWith regards to the counter machine, an abstract machine model equivalent to the Turing machine:Examples Computable by Abacus machine (cf Boolos–Burgess–Jeffrey (2002))AdditionMultiplicationExponention: (a flow-chart/block diagram description of the algorithm)Demonstrations of computability by abacus machine (Boolos–Burgess–Jeffrey (2002)) and by counter machine (Minsky 1967):The six recursive function operators:Zero functionSuccessor functionIdentity functionComposition functionPrimitive recursion (induction)MinimizationThe fact that the abacus/counter machine models can simulate the recursive functions provides the proof that: If a function is "machine computable" then it is "hand-calculable by partial recursion". Kleene's Theorem XXIX :"Theorem XXIX: "Every computable partial function φ is partial recursive..." (italics in original, p. 374).The converse appears as his Theorem XXVIII. Together these form the proof of their equivalence, Kleene's Theorem XXX.=== 1952 Church–Turing Thesis ===With his Theorem XXX Kleene proves the equivalence of the two "Theses"—the Church Thesis and the Turing Thesis. (Kleene can only hypothesize (conjecture) the truth of both thesis – these he has not proven):THEOREM XXX: The following classes of partial functions ... have the same members: (a) the partial recursive functions, (b) the computable functions ..."(p. 376)Definition of "partial recursive function": "A partial function φ is partial recursive in [the partial functions] ψ1, ... ψn if there is a system of equations E which defines φ recursively from [partial functions] ψ1, ... ψn" (p. 326)Thus by Kleene's Theorem XXX: either method of making numbers from input-numbers—recursive functions calculated by hand or computated by Turing-machine or equivalent—results in an "effectively calculable/computable function". If we accept the hypothesis that every calculation/computation can be done by either method equivalently we have accepted both Kleene's Theorem XXX (the equivalence) and the Church–Turing Thesis (the hypothesis of "every").=== A note of dissent: "There's more to algorithm..." Blass and Gurevich (2003) ===The notion of separating out Church's and Turing's theses from the "Church–Turing thesis" appears not only in Kleene (1952) but in Blass-Gurevich (2003) as well. But while there are agreements, there are disagreements too:"...we disagree with Kleene that the notion of algorithm is that well understood. In fact the notion of algorithm is richer these days than it was in Turing's days. And there are algorithms, of modern and classical varieties, not covered directly by Turing's analysis, for example, algorithms that interact with their environments, algorithms whose inputs are abstract structures, and geometric or, more generally, non-discrete algorithms" (Blass-Gurevich (2003) p. 8, boldface added)== 1954 A. A. Markov Jr.'s characterization ==Andrey Markov Jr. (1954) provided the following definition of algorithm:"1. In mathematics, "algorithm" is commonly understood to be an exact prescription, defining a computational process, leading from various initial data to the desired result....""The following three features are characteristic of algorithms and determine their role in mathematics:"a) the precision of the prescription, leaving no place to arbitrariness, and its universal comprehensibility -- the definiteness of the algorithm;"b) the possibility of starting out with initial data, which may vary within given limits -- the generality of the algorithm;"c) the orientation of the algorithm toward obtaining some desired result, which is indeed obtained in the end with proper initial data -- the conclusiveness of the algorithm." (p.1)He admitted that this definition "does not pretend to mathematical precision" (p. 1). His 1954 monograph was his attempt to define algorithm more accurately; he saw his resulting definition—his "normal" algorithm—as "equivalent to the concept of a recursive function" (p. 3). His definition included four major components (Chapter II.3 pp. 63ff):"1. Separate  elementary steps, each of which will be performed according to one of [the substitution] rules... [rules given at the outset]"2. ... steps of local nature ... [Thus the algorithm won't change more than a certain number of symbols to the left or right of the observed word/symbol]"3. Rules for the substitution formulas ... [he called the list of these "the scheme" of the algorithm]"4. ...a means to distinguish a "concluding substitution" [i.e. a distinguishable "terminal/final" state or states]In his Introduction Markov observed that "the entire significance for mathematics" of efforts to define algorithm more precisely would be "in connection with the problem of a constructive foundation for mathematics" (p. 2). Ian Stewart (cf Encyclopædia Britannica) shares a similar belief: "...constructive analysis is very much in the same algorithmic spirit as computer science...". For more see constructive mathematics and Intuitionism.Distinguishability and Locality: Both notions first appeared with Turing (1936–1937) --"The new observed squares must be immediately recognizable by the computer [sic: a computer was a person in 1936]. I think it reasonable to suppose that they can only be squares whose distance from the closest of the immediately observed squares does not exceed a certain fixed amount. Let us stay that each of the new observed squares is within L squares of one of the previously observed squares." (Turing (1936) p. 136 in Davis ed. Undecidable)Locality appears prominently in the work of Gurevich and Gandy (1980) (whom Gurevich cites). Gandy's "Fourth Principle for Mechanisms" is "The Principle of Local Causality":"We now come to the most important of our principles. In Turing's analysis the requirement that the action depend only on a bounded portion of the record was based on a human limitiation. We replace this by a physical limitation which we call the principle of local causation. Its justification lies in the finite velocity of propagation of effects and signals: contemporary physics rejects the possibility of instantaneous action at a distance." (Gandy (1980) p. 135 in J. Barwise et al.)== 1936, 1963, 1964 Gödel's characterization ==1936: A rather famous quote from Kurt Gödel appears in a "Remark added in proof [of the original German publication] in his paper "On the Length of Proofs" translated by Martin Davis appearing on pp. 82–83 of The Undecidable. A number of authors—Kleene, Gurevich, Gandy etc. -- have quoted the following:"Thus, the concept of "computable" is in a certain definite sense "absolute," while practically all other familiar metamathematical concepts (e.g. provable, definable, etc.) depend quite essentially on the system with respect to which they are defined." (p. 83)1963: In a "Note" dated 28 August 1963 added to his famous paper On Formally Undecidable Propositions (1931) Gödel states (in a footnote) his belief that "formal systems" have "the characteristic property that reasoning in them, in principle, can be completely replaced by mechanical devices" (p. 616 in van Heijenoort). ". . . due to "A. M. Turing's work a precise and unquestionably adequate definition of the general notion of formal system can now be given [and] a completely general version of Theorems VI and XI is now possible." (p. 616). In a 1964 note to another work he expresses the same opinion more strongly and in more detail.1964: In a Postscriptum, dated 1964, to a paper presented to the Institute for Advanced Study in spring 1934, Gödel amplified his conviction that "formal systems" are those that can be mechanized:"In consequence of later advances, in particular of the fact that, due to A. M. Turing's work, a precise and unquestionably adequate definition of the general concept of formal system can now be given . . . Turing's work gives an analysis of the concept of "mechanical procedure" (alias "algorithm" or "computational procedure" or "finite combinatorial procedure"). This concept is shown to be equivalent with that of a "Turing machine".* A formal system can simply be defined to be any mechanical procedure for producing formulas, called provable formulas . . . ." (p. 72 in Martin Davis ed. The Undecidable: "Postscriptum" to "On Undecidable Propositions of Formal Mathematical Systems" appearing on p. 39, loc. cit.)The * indicates a footnote in which Gödel cites the papers by Alan Turing (1937) and Emil Post (1936) and then goes on to make the following intriguing statement:"As for previous equivalent definitions of computability, which however, are much less suitable for our purpose, see Alonzo Church, Am. J. Math., vol. 58 (1936) [appearing in The Undecidable pp. 100-102]).Church's definitions encompass so-called "recursion" and the "lambda calculus" (i.e. the λ-definable functions). His footnote 18 says that he discussed the relationship of "effective calculatibility" and "recursiveness" with Gödel but that he independently questioned "effectively calculability" and "λ-definability":"We now define the notion . . . of an effectively calculable function of positive integers by identifying it with the notion of a recursive function of positive integers18 (or of a λ-definable function of positive integers."It has already been pointed out that, for every function of positive integers which is effectively calculable in the sense just defined, there exists an algorithm for the calculation of its value."Conversely it is true . . ." (p. 100, The Undecidable).It would appear from this, and the following, that far as Gödel was concerned, the Turing machine was sufficient and the lambda calculus was "much less suitable." He goes on to make the point that, with regards to limitations on human reason, the jury is still out:("Note that the question of whether there exist finite non-mechanical procedures** not equivalent with any algorithm, has nothing whatsoever to do with the adequacy of the definition of "formal system" and of "mechanical procedure.") (p. 72, loc. cit.)"(For theories and procedures in the more general sense indicated in footnote ** the situation may be different. Note that the results mentioned in the postscript do not establish any bounds for the powers of human reason, but rather for the potentialities of pure formalism in mathematics.) (p. 73 loc. cit.)Footnote **: "I.e., such as involve the use of abstract terms on the basis of their meaning. See my paper in Dial. 12(1958), p. 280." (this footnote appears on p. 72, loc. cit).== 1967 Minsky's characterization ==Minsky (1967) baldly asserts that "an algorithm is "an effective procedure" and declines to use the word "algorithm" further in his text; in fact his index makes it clear what he feels about "Algorithm, synonym for Effective procedure"(p. 311):"We will use the latter term [an effective procedure] in the sequel. The terms are roughly synonymous, but there are a number of shades of meaning used in different contexts, especially for 'algorithm'" (italics in original, p. 105)Other writers (see Knuth below) use the word "effective procedure". This leads one to wonder: What is Minsky's notion of "an effective procedure"? He starts off with:"...a set of rules which tell us, from moment to moment, precisely how to behave" (p. 106)But he recognizes that this is subject to a criticism:"... the criticism that the interpretation of the rules is left to depend on some person or agent" (p. 106)His refinement? To "specify, along with the statement of the rules, the details of the mechanism that is to interpret them". To avoid the "cumbersome" process of "having to do this over again for each individual procedure" he hopes to identify a "reasonably uniform family of rule-obeying mechanisms". His "formulation":"(1) a language in which sets of behavioral rules are to be expressed, and"(2) a single machine which can interpret statements in the language and thus carry out the steps of each specified process." (italics in original, all quotes this para. p. 107)In the end, though, he still worries that "there remains a subjective aspect to the matter. Different people may not agree on whether a certain procedure should be called effective" (p. 107)But Minsky is undeterred. He immediately introduces "Turing's Analysis of Computation Process" (his chapter 5.2). He quotes what he calls "Turing's thesis""Any process which could naturally be called an effective procedure can be realized by a Turing machine" (p. 108. (Minsky comments that in a more general form this is called "Church's thesis").After an analysis of "Turing's Argument" (his chapter 5.3)he observes that "equivalence of many intuitive formulations" of Turing, Church, Kleene, Post, and Smullyan "...leads us to suppose that there is really here an 'objective' or 'absolute' notion. As Rogers [1959] put it:"In this sense, the notion of effectively computable function is one of the few 'absolute' concepts produced by modern work in the foundations of mathematics'" (Minsky p. 111 quoting Rogers, Hartley Jr (1959) The present theory of Turing machine computability, J. SIAM 7, 114-130.)== 1967 Rogers' characterization ==In his 1967 Theory of Recursive Functions and Effective Computability Hartley Rogers' characterizes "algorithm" roughly as "a clerical (i.e., deterministic, bookkeeping) procedure . . . applied to . . . symbolic inputs and which will eventually yield, for each such input, a corresponding symbolic output"(p. 1). He then goes on to describe the notion "in approximate and intuitive terms" as having 10 "features", 5 of which he asserts that "virtually all mathematicians would agree [to]" (p. 2). The remaining 5 he asserts "are less obvious than *1 to *5 and about which we might find less general agreement" (p. 3).The 5 "obvious" are:1 An algorithm is a set of instructions of finite size,2 There is a capable computing agent,3 "There are facilities for making, storing, and retrieving steps in a computation"4 Given #1 and #2 the agent computes in "discrete stepwise fashion" without use of continuous methods or analogue devices",5 The computing agent carries the computation forward "without resort to random methods or devices, e.g. , dice" (in a footnote Rogers wonders if #4 and #5 are really the same)The remaining 5 that he opens to debate, are:6 No fixed bound on the size of the inputs,7 No fixed bound on the size of the set of instructions,8 No fixed bound on the amount of memory storage available,9 A fixed finite bound on the capacity or ability of the computing agent (Rogers illustrates with example simple mechanisms similar to a Post–Turing machine or a counter machine),10 A bound on the length of the computation -- "should we have some idea, 'ahead of time', how long the computationwill take?" (p. 5). Rogers requires "only that a computation terminate after some finite number of steps; we do not insist on an a priori ability to estimate this number." (p. 5).== 1968, 1973 Knuth's characterization ==Knuth (1968, 1973) has given a list of five properties that are widely accepted as requirements for an algorithm:Finiteness: "An algorithm must always terminate after a finite number of steps ... a very finite number, a reasonable number"Definiteness: "Each step of an algorithm must be precisely defined; the actions to be carried out must be rigorously and unambiguously specified for each case"Input: "...quantities which are given to it initially before the algorithm begins. These inputs are taken from specified sets of objects"Output: "...quantities which have a specified relation to the inputs"Effectiveness: "... all of the operations to be performed in the algorithm must be sufficiently basic that they can in principle be done exactly and in a finite length of time by a man using paper and pencil"Knuth offers as an example the Euclidean algorithm for determining the greatest common divisor of two natural numbers (cf. Knuth Vol. 1 p. 2).Knuth admits that, while his description of an algorithm may be intuitively clear, it lacks formal rigor, since it is not exactly clear what "precisely defined" means, or "rigorously and unambiguously specified" means, or "sufficiently basic", and so forth. He makes an effort in this direction in his first volume where he defines in detail what he calls the "machine language" for his "mythical MIX...the world's first polyunsaturated computer" (pp. 120ff). Many of the algorithms in his books are written in the MIX language. He also uses tree diagrams, flow diagrams and state diagrams."Goodness" of an algorithm, "best" algorithms: Knuth states that "In practice, we not only want algorithms, we want good algorithms...." He suggests that some criteria of an algorithm's goodness are the number of steps to perform the algorithm, its "adaptability to computers, its simplicity and elegance, etc." Given a number of algorithms to perform the same computation, which one is "best"? He calls this sort of inquiry "algorithmic analysis: given an algorithm, to determine its performance characteristcis" (all quotes this paragraph: Knuth Vol. 1 p. 7)== 1972 Stone's characterization ==Stone (1972) and Knuth (1968, 1973) were professors at Stanford University at the same time so it is not surprising if there are similarities in their definitions (boldface added for emphasis):"To summarize ... we define an algorithm to be a set of rules that precisely defines a sequence of operations such that each rule is effective and definite and such that the sequence terminates in a finite time." (boldface added, p. 8)Stone is noteworthy because of  his detailed discussion of what constitutes an “effective” rule – his robot, or person-acting-as-robot, must have some information and abilities within them, and if not the information and the ability must be provided in "the algorithm":"For people to follow the rules of an algorithm, the rules must be formulated so that they can be followed in a robot-like manner, that is, without the need for thought... however, if the instructions [to solve the quadratic equation, his example] are to be obeyed by someone who knows how to perform arithmetic operations but does not know how to extract a square root, then we must also provide a set of rules for extracting a square root in order to satisfy the definition of algorithm" (p. 4-5)Furthermore, "...not all instructions are acceptable, because they may require the robot to have abilities beyond those that we consider reasonable.” He gives the example of a robot confronted with the question is “Henry VIII a King of England?” and to print 1 if yes and 0 if no, but the robot has not been previously provided with this information. And worse, if the robot is asked if Aristotle was a King of England and the robot only had been provided with five names, it would not know how to answer. Thus:“an intuitive definition of an acceptable sequence of instructions is one in which each instruction is precisely defined so that the robot is guaranteed to be able to obey it” (p. 6)After providing us with his definition, Stone introduces the Turing machine model and states that the set of five-tuples that are the machine’s instructions are “an algorithm ... known as a Turing machine program” (p. 9). Immediately thereafter he goes on say that a “computation of a Turing machine is described by stating:"1. The tape alphabet"2. The form in which the [input] parameters are presented on the tape"3. The initial state of the Turing machine"4. The form in which answers [output] will be represented on the tape when the Turing machine halts"5. The machine program" (italics added, p. 10)This precise prescription of what is required for "a computation" is in the spirit of what will follow in the work of Blass and Gurevich.== 1995 Soare's characterization =="A computation is a process whereby we proceed from initially given objects, called inputs, according to a fixed set of rules, called a program, procedure, or algorithm, through a series of steps and arrive at the end of these steps with a final result, called the output. The algorithm, as a set of rules proceeding from inputs to output, must be precise and definite with each successive step clearly determined. The concept of computability concerns those objects which may be specified in principle by computations . . ."(italics in original, boldface added p. 3)== 2000 Berlinski's characterization ==While a student at Princeton in the mid-1960s, David Berlinski was a student of Alonzo Church (cf p. 160). His year-2000 book The Advent of the Algorithm: The 300-year Journey from an Idea to the Computer contains the following definition of algorithm:"In the logician's voice:"an algorithm isa finite procedure,written in a fixed symbolic vocabulary,governed by precise instructions,moving in discrete steps, 1, 2, 3, . . .,whose execution requires no insight, cleverness,intuition, intelligence, or perspicuity,and that sooner or later comes to an end.'" (boldface and italics in the original, p. xviii)== 2000, 2002 Gurevich's characterization ==A careful reading of Gurevich 2000 leads one to conclude (infer?) that he believes that "an algorithm" is actually "a Turing machine" or "a pointer machine" doing a computation. An "algorithm" is not just the symbol-table that guides the behavior of the machine, nor is it just one instance of a machine doing a computation given a particular set of input parameters, nor is it a suitably programmed machine with the power off; rather an algorithm is the machine actually doing any computation of which it is capable. Gurevich does not come right out and say this, so as worded above this conclusion (inference?) is certainly open to debate:" . . . every algorithm can be simulated by a Turing machine . . . a program can be simulated and therefore given a precise meaning by a Turing machine." (p. 1)" It is often thought that the problem of formalizing the notion of sequential algorithm was solved by Church [1936] and Turing [1936]. For example, according to Savage [1987], an algorithm is a computational process defined by a Turing machine. Church and Turing did not solve the problem of formalizing the notion of sequential algorithm. Instead they gave (different but equivalent) formalizations of the notion of computable function, and there is more to an algorithm than the function it computes. (italics added p. 3)"Of course, the notions of algorithm and computable function are intimately related: by definition, a computable function is a function computable by an algorithm. . . . (p. 4)In Blass and Gurevich 2002 the authors invoke a dialog between "Quisani" ("Q") and "Authors" (A), using Yiannis Moshovakis as a foil, where they come right out and flatly state:"A: To localize the disagreement, let's first mention two points of agreement. First, there are some things that are obviously algorithms by anyone's definition -- Turing machines , sequential-time ASMs [Abstract State Machines], and the like. . . .Second, at the other extreme are specifications that would not be regarded as algorithms under anyone's definition, since they give no indication of how to compute anything . . . The issue is how detailed the information has to be in order to count as an algorithm. . . . Moshovakis allows some things that we would call only declarative specifications, and he would probably use the word "implementation" for things that we call algorithms." (paragraphs joined for ease of readability, 2002:22)This use of the word "implementation" cuts straight to the heart of the question. Early in the paper, Q states his reading of Moshovakis:"...[H]e would probably think that your practical work [Gurevich works for Microsoft] forces you to think of implementations more than of algorithms. He is quite willing to identify implementations with machines, but he says that algorithms are something more general. What it boils down to is that you say an algorithm is a machine and Moschovakis says it is not." (2002:3)But the authors waffle here, saying "[L]et's stick to "algorithm" and "machine", and the reader is left, again, confused. We have to wait until Dershowitz and Gurevich 2007 to get the following footnote comment:" . . . Nevertheless, if one accepts Moshovakis's point of view, then it is the "implementation" of algorithms that we have set out to characterize."(cf Footnote 9 2007:6)== 2003 Blass and Gurevich's characterization ==Blass and Gurevich describe their work as evolved from consideration of Turing machines and pointer machines, specifically Kolmogorov-Uspensky machines (KU machines), Schönhage Storage Modification Machines (SMM), and linking automata as defined by Knuth. The work of Gandy and Markov are also described as influential precursors.Gurevich offers a 'strong' definition of an algorithm (boldface added):"...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine....In practice, it would be ridiculous...[Nevertheless,] [c]an one generalize Turing machines so that any algorithm, never mind how abstract, can be modeled by a generalized machine?...But suppose such generalized Turing machines exist. What would their states be?...a first-order structure ... a particular small instruction set suffices in all cases ... computation as an evolution of the state ... could be nondeterministic... can interact with their environment ... [could be] parallel and multi-agent ... [could have] dynamic semantics ... [the two underpinings of their work are:] Turing's thesis ...[and] the notion of (first order) structure of [Tarski 1933]" (Gurevich 2000, p. 1-2)The above phrase computation as an evolution of the state differs markedly from the definition of Knuth and Stone—the "algorithm" as a Turing machine program. Rather, it corresponds to what Turing called the complete configuration (cf Turing's definition in Undecidable, p. 118) -- and includes both the current instruction (state) and the status of the tape. [cf Kleene (1952) p. 375 where he shows an example of a tape with 6 symbols on it—all other squares are blank—and how to Gödelize its combined table-tape status].In Algorithm examples we see the evolution of the state first-hand.== 1995 – Daniel Dennett: evolution as an algorithmic process ==Philosopher Daniel Dennett analyses the importance of evolution as an algorithmic process in his 1995 book Darwin's Dangerous Idea. Dennett identifies three key features of an algorithm:Substrate neutrality: an algorithm relies on its logical structure. Thus, the particular form in which an algorithm is manifested is not important (Dennett's example is long division: it works equally well on paper, on parchment, on a computer screen, or using neon lights or in skywriting). (p. 51)Underlying mindlessness: no matter how complicated the end-product of the algorithmic process may be, each step in the algorithm is sufficiently simple to be performed by a non-sentient, mechanical device. The algorithm does not require a "brain" to maintain or operate it. "The standard textbook analogy notes that algorithms are recipes of sorts, designed to be followed by novice cooks."(p. 51)Guaranteed results: If the algorithm is executed correctly, it will always produce the same results. "An algorithm is a foolproof recipe." (p. 51)It is on the basis of this analysis that Dennett concludes that "According to Darwin, evolution is an algorithmic process". (p. 60).However, in the previous page he has gone out on a much-further limb. In the context of his chapter titled "Processes as Algorithms", he states:"But then . . are there any limits at all on what may be considered an algorithmic process? I guess the answer is NO; if you wanted to, you can treat any process at the abstract level as an algorithmic process. . . If what strikes you as puzzling is the uniformity of the [ocean's] sand grains or the strength of the [tempered-steel] blade, an algorithmic explanation is what will satisfy your curiosity -- and it will be the truth. . . ."No matter how impressive the products of an algorithm, the underlying process always consists of nothing but a set of individualy [sic] mindless steps succeeding each other without the help of any intelligent supervision; they are 'automatic' by definition: the workings of an automaton." (p. 59)It is unclear from the above whether Dennett is stating that the physical world by itself and without observers is intrinsically algorithmic (computational) or whether a symbol-processing observer is what is adding "meaning" to the observations.== 2002 John Searle adds a clarifying caveat to Dennett's characterization ==Daniel Dennett is a proponent of strong artificial intelligence: the idea that the logical structure of an algorithm is sufficient to explain mind. John Searle, the creator of the Chinese room thought experiment, claims that "syntax [that is, logical structure] is by itself not sufficient for semantic content [that is, meaning]" (Searle 2002, p. 16). In other words, the "meaning" of symbols is relative to the mind that is using them; an algorithm—a logical construct—by itself is insufficient for a mind.Searle cautions those who claim that algorithmic (computational) processes are intrinsic to nature (for example, cosmologists, physicists, chemists, etc.):Computation [...] is observer-relative, and this is because computation is defined in terms of symbol manipulation, but the notion of a 'symbol' is not a notion of physics or chemistry. Something is a symbol only if it is used, treated or regarded as a symbol. The Chinese room argument showed that semantics is not intrinsic to syntax. But what this shows is that syntax is not intrinsic to physics. [...] Something is a symbol only relative to some observer, user or agent who assigns a symbolic interpretation to it [...] you can assign a computational interpretation to anything. But if the question asks, "Is consciousness intrinsically computational?" the answer is: nothing is intrinsically computational [italics added for emphasis]. Computation exists only relative to some agent or observer who imposes a computational interpretation on some phenomenon. This is an obvious point. I should have seen it ten years ago but I did not.== 2002: Boolos-Burgess-Jeffrey specification of Turing machine calculation ==For examples of this specification-method applied to the addition algorithm "m+n" see Algorithm examples.An example in Boolos-Burgess-Jeffrey (2002) (pp. 31–32) demonstrates the precision required in a complete specification of an algorithm, in this case to add two numbers: m+n. It is similar to the Stone requirements above.(i) They have discussed the role of "number format" in the computation and selected the "tally notation" to represent numbers:"Certainly computation can be harder in practice with some notations than others... But... it is possible in principle to do in any other notation, simply by translating the data... For purposes of framing a rigorously defined notion of computability, it is convenient to use monadic or tally notation" (p. 25-26)(ii) At the outset of their example they specify the machine to be used in the computation as a Turing machine. They have previously specified (p. 26) that the Turing-machine will be of the 4-tuple, rather than 5-tuple, variety. For more on this convention see Turing machine.(iii) Previously the authors have specified that the tape-head's position will be indicated by a subscript to the right of the scanned symbol. For more on this convention see Turing machine. (In the following, boldface is added for emphasis):"We have not given an official definition of what it is for a numerical function to be computable by a Turing machine, specifying how inputs or arguments are to be represented on the machine, and how outputs or values represented. Our specifications for a k-place function from positive integers to positive integers are as follows:"(a) [Initial number format:] The arguments m1, ... mk,  ... will be represented in monadic [unary] notation by blocks of those numbers of strokes, each block separated from the next by a single blank, on an otherwise blank tape.Example: 3+2, 111B11"(b) [Initial head location, initial state:] Initially, the machine will be scanning the leftmost 1 on the tape, and will be in its initial state, state 1.Example: 3+2, 11111B11"(c) [Successful computation -- number format at Halt:] If the function to be computed assigns a value n to the arguments that are represented initially on the tape, then the machine will eventually halt on a tape containing a block of strokes, and otherwise blank...Example: 3+2, 11111"(d) [Successful computation -- head location at Halt:] In this case [c] the machine will halt scanning the left-most 1 on the tape...Example: 3+2, 1n1111"(e) [Unsuccessful computation -- failure to Halt or Halt with non-standard number format:] If the function that is to be computed assigns no value to the arguments that are represented initially on the tape, then the machine either will never halt, or will halt in some nonstandard configuration..."(ibid)Example: Bn11111 or B11n111 or B11111nThis specification is incomplete: it requires the location of where the instructions are to be placed and their format in the machine--(iv) in the finite state machine's TABLE or, in the case of a Universal Turing machine on the tape, and(v) the Table of instructions in a specified formatThis later point is important. Boolos-Burgess-Jeffrey give a demonstration (p. 36) that the predictability of the entries in the table allow one to "shrink" the table by putting the entries in sequence and omitting the input state and the symbol. Indeed, the example Turing machine computation required only the 4 columns as shown in the table below (but note: these were presented to the machine in rows):== 2006: Sipser's assertion and his three levels of description ==For examples of this specification-method applied to the addition algorithm "m+n" see Algorithm examples.Sipser begins by defining '"algorithm" as follows:"Informally speaking, an algorithm is a collection of simple instructions for carrying out some task. Commonplace in everyday life, algorithms sometimes are called procedures or recipes (italics in original, p. 154)"...our real focus from now on is on algorithms. That is, the Turing machine merely serves as a precise model for the definition of algorithm .... we need only to be comfortable enough with Turing machines to believe that they capture all algorithms" ( p. 156)Does Sipser mean that "algorithm" is just "instructions" for a Turing machine, or is the combination of "instructions + a (specific variety of) Turing machine"? For example, he defines the two standard variants (multi-tape and non-deterministic) of his particular variant (not the same as Turing's original) and goes on, in his Problems (pages 160-161), to describes four more variants (write-once, doubly infinite tape (i.e. left- and right-infinite), left reset, and "stay put instead of left). In addition, he imposes some constraints. First, the input must be encoded as a string (p. 157) and says of numeric encodings in the context of complexity theory:"But note that unary notation for encoding numbers (as in the number 17 encoded by the uary string 11111111111111111) isn't reasonable because it is exponentially larger than truly reasonable encodings, such as base k notation for any k ≥ 2."(p. 259)van Emde Boas comments on a similar problem with respect to the random access machine (RAM) abstract model of computation sometimes used in place of the Turing machine when doing "analysis of algorithms":"The absence or presence of multiplicative and parallel bit manipulation operations is of relevance for the correct understanding of some results in the analysis of algorithms.". . . [T]here hardly exists such as a thing as an "innocent" extension of the standard RAM model in the uniform time measures; either one only has additive arithmetic or one might as well include all reasonable multiplicative and/or bitwise Boolean instructions on small operands." (van Emde Boas, 1990:26)With regards to a "description language" for algorithms Sipser finishes the job that Stone and Boolos-Burgess-Jeffrey started (boldface added). He offers us three levels of description of Turing machine algorithms (p. 157):High-level description: "wherein we use ... prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head."Implementation description: "in which we use ... prose to describe the way that the Turing machine moves its head and the way that it stores data on its tape. At this level we do not give details of states or transition function."Formal description: "... the lowest, most detailed, level of description... that spells out in full the Turing machine's states, transition function, and so on."== Notes ==== References ==David Berlinski (2000), The Advent of the Algorithm: The 300-Year Journey from an Idea to the Computer, Harcourt, Inc., San Diego, ISBN 0-15-601391-6 (pbk.)George Boolos, John P. Burgess, Richard Jeffrey (2002), Computability and Logic: Fourth Edition, Cambridge University Press, Cambridge, UK. ISBN 0-521-00758-5 (pbk).Andreas Blass and Yuri Gurevich (2003), Algorithms: A Quest for Absolute Definitions, Bulletin of European Association for Theoretical Computer Science 81, 2003. Includes an excellent bibliography of 56 references.Burgin, M. Super-recursive algorithms, Monographs in computer science, Springer, 2005. ISBN 0-387-95569-0Davis, Martin (1958). Computability & Unsolvability. New York: McGraw-Hill Book Company, Inc.. A source of important definitions and some Turing machine-based algorithms for a few recursive functions.Davis, Martin (1965). The Undecidable: Basic Papers On Undecidable Propositions, Unsolvable Problems and Computable Functions. New York: Raven Press. Davis gives commentary before each article. Papers of Gödel, Alonzo Church, Turing, Rosser, Kleene, and Emil Post are included.Dennett, Daniel (1995). Darwin's Dangerous Idea. New York: Touchstone/Simon & Schuster.Robin Gandy, Church's Thesis and principles for Mechanisms, in J. Barwise, H. J. Keisler and K. Kunen, eds., The Kleene Symposium, North-Holland Publishing Company 1980) pp. 123–148. Gandy's famous "4 principles of [computational] mechanisms" includes "Principle IV -- The Principle of Local Causality".Yuri Gurevich, Sequential Abstract State Machines Capture Sequential Algorithms, ACM Transactions on Computational Logic, Vol 1, no 1 (July 2000), pages 77–111. Includes bibliography of 33 sources.Kleene C., Stephen (1943). "Recursive Predicates and Quantifiers". American Mathematical Society Transactions. Transactions of the American Mathematical Society, Vol. 53, No. 1. 54 (1): 41–73. doi:10.2307/1990131. JSTOR 1990131. Reprinted in The Undecidable, p. 255ff. Kleene refined his definition of "general recursion" and proceeded in his chapter "12. Algorithmic theories" to posit "Thesis I" (p. 274); he would later repeat this thesis (in Kleene 1952:300) and name it "Church's Thesis"(Kleene 1952:317) (i.e., the Church Thesis).Kleene, Stephen C. (1991) [1952]. Introduction to Metamathematics (Tenth ed.). North-Holland Publishing Company. Excellent — accessible, readable — reference source for mathematical "foundations".Knuth, Donald E.. (1973) [1968]. The Art of Computer Programming Second Edition, Volume 1/Fundamental Algorithms (2nd ed.). Addison-Wesley Publishing Company. The first of Knuth's famous series of three texts.Lewis, H.R. and Papadimitriou, C.H. Elements of the Theory of Computation, Prentice-Hall, Uppre Saddle River, N.J., 1998A. A. Markov (1954) Theory of algorithms. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [i.e. Jerusalem, Israel Program for Scientific Translations, 1961; available from the Office of Technical Services, U.S. Dept. of Commerce, Washington] Description 444 p. 28 cm. Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algerifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS 60-51085.]Minsky, Marvin (1967). Computation: Finite and Infinite Machines (First ed.). Prentice-Hall, Englewood Cliffs, NJ. Minsky expands his "...idea of an algorithm — an effective procedure..." in chapter 5.1 Computability, Effective Procedues and Algorithms. Infinite machines."Hartley Rogers, Jr, (1967), Theory of Recursive Functions and Effective Computability, MIT Press (1987), Cambridge MA, ISBN 0-262-68052-1 (pbk.)Searle, John (2002). Consciousness and Language. Cambridge UK: Cambridge University Press. ISBN 0-521-59744-7.Robert Soare, (1995 to appear in Proceedings of the 10th International Congress of Logic, Methodology, and Philosophy of Science, August 19–25, 1995, Florence Italy), Computability and Recursion), on the web at ??.Michael Sipser, (2006), Introduction to the Theory of Computation: Second Edition, Thompson Course Technology div. of Thompson Learning, Inc. Boston, MA. ISBN 978-0-534-95097-2.Ian Stewart, Algorithm, Encyclopædia Britannica 2006.Stone, Harold S. Introduction to Computer Organization and Data Structures (1972 ed.). McGraw-Hill, New York. Cf in particular the first chapter titled: Algorithms, Turing Machines, and Programs. His succinct informal definition: "...any sequence of instructions that can be obeyed by a robot, is called an algorithm" (p. 4).Peter van Emde Boas (1990), "Machine Models and Simulations" pp 3–66, appearing in Jan van Leeuwen (1990), Handbook of Theoretical Computer Science. Volume A: Algorithms & Complexity, The MIT Press/Elsevier, 1990, ISBN 0-444-88071-2 (Volume A)
	In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book "Super-recursive algorithms" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.Burgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.== Definition ==Burgin (2005: 13) uses the term recursive algorithms for algorithms that can be implemented on Turing machines, and uses the word algorithm in a more general sense. Then a super-recursive class of algorithms is "a class of algorithms in which it is possible to compute functions not computable by any Turing machine" (Burgin 2005: 107).Super-recursive algorithms are closely related to hypercomputation in a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a "computational process (including processes of input and output) that cannot be realized by recursive algorithms." (Burgin 2005: 108). A more restricted definition demands that  hypercomputation solves a supertask (see Copeland 2002; Hagar and Korolev 2007).Super-recursive algorithms are also related to algorithmic schemes, which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.== Examples ==Examples of super-recursive algorithms include (Burgin 2005: 132):limiting recursive functions and limiting partial recursive functions (E.M. Gold 1965)trial and error predicates (Hilary Putnam 1965)inductive inference machines (Carl Smith)inductive Turing machines, which perform computations similar to computations of Turing machines and produce their results after a finite number of steps (Mark Burgin)limit Turing machines, which perform computations similar to computations of Turing machines but their final results are limits of their intermediate results (Mark Burgin)trial-and-error machines (Ja. Hintikka and A. Mutanen 1998)general Turing machines (J. Schmidhuber)Internet machines (van Leeuwen, J. and Wiedermann, J.)evolutionary computers, which use DNA to produce the value of a function (Darko Roglic)fuzzy computation (Jirí Wiedermann 2004)evolutionary Turing machines (Eugene Eberbach 2005)Examples of algorithmic schemes include:Turing machines with arbitrary oracles (Alan Turing)Transrecursive operators (Borodyanskii and Burgin)machines that compute with real numbers (L. Blum, F. Cucker, M. Shub, and S. Smale 1998)neural networks based on real numbers (Hava Siegelmann 1999)For examples of practical super-recursive algorithms, see the book of Burgin.== Inductive Turing machines ==Inductive Turing machines implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task which, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary Turing machine is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name calculation procedure or algorithm (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit "some object" (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.Simple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen (1998). More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the arithmetical hierarchy (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.The non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even if it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation.There are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when "computing" something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.== Schmidhuber's generalized Turing machines ==A symbol sequence is computable in the limit if there is a finite, possibly non-halting program on a universal Turing machine that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of π but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional Turing machines with a write-only output tape cannot edit their previous outputs; generalized Turing machines, according to Jürgen Schmidhuber, can edit their output tape as well as their work tape. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval.  Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive theories of everything. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms (Schmidhuber 2000).== Relation to the Church–Turing thesis ==The Church–Turing thesis in recursion theory relies on a particular definition of the term algorithm. Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as inductive Turing machines disprove the Church–Turing thesis. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using quantum algorithms.Burgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician Martin Davis, who argues that Burgin's claims have been well understood "for decades". Davis states, "The present criticism is not about the mathematical discussion of these matters but only about the misleading claims regarding physical systems of the present and future."(Davis 2006: 128)Davis disputes Burgin's claims that sets at level                               Δ                      2                                0                                {\displaystyle \Delta _{2}^{0}}   of the arithmetical hierarchy can be called computable, saying"It is generally understood that for a computational result to be useful one must be able to at least recognize that it is indeed the result sought." (Davis 2006: 128)== See also ==Interactive computation== References ==Blum, L.,  F. Cucker, M. Shub, and S. Smale, Complexity and real computation, Springer Publishing 1998Burgin, Mark (2005), Super-recursive algorithms, Monographs in computer science, Springer. ISBN 0-387-95569-0José Félix Costa, MR2246430 Review in MathSciNet.Harvey Cohn (2005), CR131542 (0606-0574) Review in Computing ReviewsMartin Davis (2007),Review  in Bulletin of Symbolic Logic, v. 13 n. 2.Marc L. Smith (2006), Review in The Computer Journal, Vol. 49 No. 6Review, Vilmar Trevisan (2005), Zentralblatt MATH, Vol. 1070. Review 1070.68038Copeland, J. (2002) Hypercomputation, Minds and Machines, v. 12, pp. 461–502Davis, Martin (2006), "The Church–Turing Thesis: Consensus and opposition". Proceedings, Computability in Europe 2006.  Lecture notes in computer science, 3988 pp. 125–132Eberbach, E. (2005) "Toward a theory of evolutionary computation", BioSystems 82, 1-19Gold, E.M. Limiting recursion. J. Symb. Log. 10 (1965), 28-48.Gold, E. Mark (1967), Language Identification in the Limit (PDF), 10, Information and Control, pp. 447–474Hagar, A. and Korolev, A. (2007) "Quantum Hypercomputation – Hype or Computation?"Hintikka, Ja. and Mutanen, A. An Alternative Concept of Computability, in “Language, Truth, and Logic in Mathematics”, Dordrecht, pp. 174–188, 1998Kleene, Stephen C. (1952), Introduction to Metamathematics (First ed.), Amsterdam: North-Holland Publishing Company.Peter Kugel,  "It's time to think outside the computational box", Communications of the ACM, Volume 48, Issue 11, November 2005Petrus H. Potgieter, "Zeno machines and hypercomputation", Theoretical Computer Science, Volume 358,  Issue 1  (July 2006) pp. 23 – 33Hilary Putnam, "Trial and Error Predicates and the Solution to a Problem of Mostowski". Journal of Symbolic Logic, Volume 30, Issue 1 (1965), 49-57Darko Roglic, "The universal evolutionary computer based on super-recursive algorithms of evolvability"Hava Siegelmann, Neural Networks and Analog Computation: Beyond the Turing Limit, Birkhäuser, 1999, ISBN 0817639497Turing, A. (1939) Systems of Logic Based on Ordinals, Proc. Lond. Math. Soc., Ser.2, v. 45: 161-228van Leeuwen, J. and Wiedermann, J. (2000a) Breaking the Turing Barrier: The case of the Internet, Techn. Report, Inst. of Computer Science, Academy of Sciences of the Czech Republic, PragueJiří Wiedermann, Characterizing the super-Turing computing power and efficiency of classical fuzzy Turing machines, Theoretical Computer Science, Volume 317, Issue 1-3, June 2004Jiří Wiedermann and Jan van Leeuwen, "The emergent computational potential of evolving artificial living systems", AI Communications, v. 15, No. 4, 2002== Further reading ==Akl, S.G., Three counterexamples to dispel the myth of the universal computer, Parallel Processing Letters, Vol. 16, No. 3, September 2006, pp. 381 – 403.Akl, S.G., The myth of universal computation, in: Parallel Numerics, Trobec, R., Zinterhof, P., Vajtersic, M., and Uhl, A., Eds., Part 2, Systems and Simulation, University of Salzburg, Salzburg, Austria and Jozef Stefan Institute, Ljubljana, Slovenia, 2005, pp. 211 – 236Angluin, D., and Smith, C. H. (1983) Inductive Inference: Theory and Methods, Comput. Surveys, v. 15, no. 3, pp. 237–269Apsïtis, K, Arikawa, S, Freivalds, R., Hirowatari, E., and Smith, C. H. (1999) On the inductive inference of recursive real-valued functions, Theoretical Computer Science, 219(1-2): 3—17Boddy, M, Dean, T.  1989.  "Solving Time-Dependent Planning Problems". Technical Report: CS-89-03, Brown UniversityBurgin, M. "Algorithmic Complexity of Recursive and Inductive Algorithms", Theoretical Computer Science, v. 317, No. 1/3, 2004, pp. 31–60Burgin, M. and Klinger, A. Experience, Generations, and Limits in Machine Learning, Theoretical Computer Science, v. 317, No. 1/3, 2004, pp. 71–91Eberbach, E., and Wegner, P., "Beyond Turing Machines", Bulletin of the European Association for Theoretical Computer Science (EATCS Bulletin), 81, Oct. 2003, 279-304S. Zilberstein, Using Anytime Algorithms in Intelligent Systems, "AI Magazine", 17(3):73-83, 1996== External links ==A New Paradigm for Computation. Los Angeles ACM Chapter Meeting, December 1, 1999.Anytime algorithm from FOLDOC
	In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.This divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).Understanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These divide-and-conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.== Divide and conquer == The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. For example, to sort a given list of n natural numbers, split it into two lists of about n/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the merge sort algorithm.The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding).  These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops.  Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm".  Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems. The name decrease and conquer has been proposed instead for the single-subproblem class.An important application of divide and conquer is in optimization, where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.== Early historical examples ==Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into single subproblems, and indeed can be solved iteratively.Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC. Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.Another notable example is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two n-digit numbers in                     O        (                  n                                    log                              2                                      ⁡            3                          )              {\displaystyle O(n^{\log _{2}3})}   operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that                     Ω        (                  n                      2                          )              {\displaystyle \Omega (n^{2})}   operations would be required for that task.As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered. This is related to a radix sort, described for punch-card sorting machines as early as 1929.== Advantages ===== Solving difficult problems ===Divide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height n to moving a tower of height n − 1.=== Algorithm efficiency ===The divide-and-conquer paradigm often helps in the discovery of efficient algorithms.  It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size n, and (b) there is a bounded number p of subproblems of size ~ n/p at each stage, then the cost of the divide-and-conquer algorithm will be O(n logpn).=== Parallelism ===Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.=== Memory access ===Divide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called cache-oblivious, because it does not contain the cache size as an explicit parameter.Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be optimal cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is blocking, as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.The same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.=== Roundoff control ===In computations with rounded arithmetic, e.g. with floating-point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add N numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.  While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.== Implementation issues ===== Recursion ===Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.=== Explicit stack ===Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue.  This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.=== Stack size ===In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow.  D&C algorithms that are time-efficient often have relatively small recursion depth.  For example, the quicksort algorithm can be implemented so that it never requires more than                               log                      2                          ⁡        n              {\displaystyle \log _{2}n}   nested recursive calls to  sort                     n              {\displaystyle n}   items.Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it.  Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure.  Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.=== Choosing the base cases ===In any recursive algorithm, there is considerable freedom in the choice of the base cases, the small subproblems that are solved directly in order to terminate the recursion.Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve.  For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small.  Note that, if the empty list were the only base case, sorting a list with n entries would entail maximally n quicksort calls that would do nothing but return immediately.  Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation).  For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.  Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.=== Sharing repeated subproblems ===For some problems, the branched recursion may end up evaluating the same sub-problem many times over.  In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization.  Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.== See also ==Akra–Bazzi methodDecomposable aggregation functionFork–join modelMaster theorem (analysis of algorithms)Mathematical inductionMapReduceHeuristic (computer science)== References ==
	Algorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering.It is a general methodology for algorithmic research.== Origins ==In 1995, a report from an NSF-sponsored workshop "with the purpose of assessing the current goals and directions of the Theory of Computing (TOC) community" identified the slow speed of adoption of theoretical insights by practitioners as an important issue and suggested measures toreduce the uncertainty by practitioners whether a certain theoretical breakthrough will translate into practical gains in their field of work, andtackle the lack of ready-to-use algorithm libraries, which provide stable, bug-free and well-tested implementations for algorithmic problems and expose an easy-to-use interface for library consumers.But also, promising algorithmic approaches have been neglected due to difficulties in mathematical analysis.The term "algorithm engineering" was first used with specificity in 1997, with the first Workshop on Algorithm Engineering (WAE97), organized by Giuseppe F. Italiano.== Difference from algorithm theory ==Algorithm engineering does not intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental algorithmics (also called empirical algorithmics).This way it can provide new insights into the efficiency and performance of algorithms in cases wherethe algorithm at hand is less amenable to algorithm theoretic analysis,formal analysis pessimistically suggests bounds which are unlikely to appear on inputs of practical interest,the algorithm relies on the intricacies of modern hardware architectures like data locality, branch prediction, instruction stalls, instruction latencies which the machine model used in Algorithm Theory is unable to capture in the required detail,the crossover between competing algorithms with different constant costs and asymptotic behaviors needs to be determined.== Methodology ==Some researchers describe algorithm engineering's methodology as a cycle consisting of algorithm design, analysis, implementation and experimental evaluation, joined by further aspects like machine models or realistic inputs.They argue that equating algorithm engineering with experimental algorithmics is too limited, because viewing design and analysis, implementation and experimentation as separate activities ignores the crucial feedback loop between those elements of algorithm engineering.=== Realistic models and real inputs ===While specific applications are outside the methodology of algorithm engineering, they play an important role in shaping realistic models of the problem and the underlying machine, and supply real inputs and other design parameters for experiments.=== Design ===Compared to algorithm theory, which usually focuses on the asymptotic behavior of algorithms, algorithm engineers need to keep further requirements in mind: Simplicity of the algorithm, implementability in programming languages on real hardware, and allowing code reuse.Additionally, constant factors of algorithms have such a considerable impact on real-world inputs that sometimes an algorithm with worse asymptotic behavior performs better in practice due to lower constant factors.=== Analysis ===Some problems can be solved with heuristics and randomized algorithms in a simpler and more efficient fashion than with deterministic algorithms. Unfortunately, this makes even simple randomized algorithms difficult to analyze because there are subtle dependencies to be taken into account.=== Implementation ===Huge semantic gaps between theoretical insights, formulated algorithms, programming languages and hardware pose a challenge to efficient implementations of even simple algorithms, because small implementation details can have rippling effects on execution behavior.The only reliable way to compare several implementations of an algorithm is to spend an considerable amount of time on tuning and profiling, running those algorithms on multiple architectures, and looking at the generated machine code.=== Experiments ===See: Experimental algorithmics=== Application engineering ===Implementations of algorithms used for experiments differ in significant ways from code usable in applications.While the former prioritizes fast prototyping, performance and instrumentation for measurements during experiments, the latter requires thorough testing, maintainability, simplicity, and tuning for particular classes of inputs.=== Algorithm libraries ===Stable, well-tested algorithm libraries like LEDA play an important role in technology transfer by speeding up the adoption of new algorithms in applications. Such libraries reduce the required investment and risk for practitioners, because it removes the burden of understanding and implementing the results of academic research.== Conferences ==Two main conferences on Algorithm Engineering are organized annually, namely:Symposium on Experimental Algorithms (SEA), established in 1997 (formerly known as WEA).SIAM Meeting on Algorithm Engineering and Experiments (ALENEX), established in 1999.The 1997 Workshop on Algorithm Engineering (WAE'97) was held in Venice (Italy) on September 11–13, 1997. The Third International Workshop on Algorithm Engineering (WAE'99) was held in London, UK in July 1999.The first Workshop on Algorithm Engineering and Experimentation (ALENEX99) was held in Baltimore, Maryland on January 15–16, 1999. It was sponsored by DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science (at Rutgers University), with additional support from SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory, and SIAM, the Society for Industrial and Applied Mathematics.== References ==
	An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.== List of paradigms ==== General ==Greedy algorithm  in optimization problemsDivide and conquer algorithmsDynamic programmingPrune and searchBrute-force searchBacktrackingBranch and bound== Parameterized complexity ==KernelizationIterative compression== Computational geometry ==Sweep line algorithmsRotating calipersRandomized incremental construction.== References ==
	Bisection is a method used in software development to identify change sets that result in a specific behavior change. It is mostly employed for finding the patch that introduced a bug. Another application area is finding the patch that indirectly fixed a bug.== Overview ==The process of locating the changeset that introduced a specific regression was described as "source change isolation" in 1997 by Brian Ness and Viet Ngo of Cray Research. Regression testing was performed on Cray's compilers in editions comprising one or more changesets. Editions with known regressions could not be validated until developers addressed the problem. Source change isolation narrowed the cause to a single changeset that could then be excluded from editions, unblocking them with respect to this problem, while the author of the change worked on a fix. Ness and Ngo outlined linear search and binary search methods of performing this isolation.Code bisection has the goal of minimizing the effort to find a specific change set.It employs a divide and conquer algorithm thatdepends on having access to the code history which is usually preserved byrevision control in a code repository.== Bisection method ===== Code bisection algorithm ===Code history has the structure of a directed acyclic graph which can be topologically sorted. This makes it possible to use a divide and conquer search algorithm which:splits up the search space of candidate revisionstests for the behavior in questionreduces the search space depending on the test resultre-iterates the steps above until a range with at most one bisectable patch candidate remains=== Algorithmic complexity ===Bisection is in LSPACE having an algorithmic complexity of                     O        (        log        ⁡        N        )              {\displaystyle O(\log N)}   with                     N              {\displaystyle N}   denoting the number of revisions in the search space, and is similar to a binary search.=== Desirable repository properties ===For code bisection it is desirable that each revision in the search space can be built and tested independently.== Automation support ==Although the bisection method can be completed manually, one of its main advantages is that it can be easily automated. It can thus fit into existing test automation processes: failures in exhaustive automated regression tests can trigger automated bisection to localize faults. Ness and Ngo focused on its potential in Cray's continuous delivery-style environment in which the automatically-isolated bad changeset could be automatically excluded from builds.The revision control systems Git and Mercurial have built-in functionality for code bisection. The user can start a bisection session with a specified range of revisions from which the revision control system proposes a revision to test, the user tells the system whether the revision tested as "good" or "bad", and the process repeats until the specific "bad" revision has been identified. Other revision control systems, such as Bazaar or Subversion, support bisection through plugins or external scripts.Phoronix Test Suite can do bisection automatically to find performance regressions.== See also ==Delta debugging (generalization of finding a minimal cause of a bug)Annotation § Source control (determining changesets that edited a line in a file)== References ==
	The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm , and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for Cluster analysis, by first representing the similarity data in a similarity graph, and afterwards finding all the highly connected subgraphs as clusters. The algorithm does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv and Ron Shamir in 1998.The HCS algorithm gives clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.== Similarity Modeling and Preprocessing ==The goal of cluster analysis is to group elements into disjoint subsets, or clusters, based on similarity between elements, so that elements in the same cluster are highly similar to each other (homogeneity), while elements from different clusters have low similarity to each other (separation). Similarity graph is one of the models to represent the similarity between elements, and in turn facilitate generating of clusters. To construct a similarity graph from similarity data, represent elements as vertices, and elicit edges between vertices when the similarity value between them is above some threshold.== Algorithm ==In the similarity graph, the more edges exist for a given number of vertices, the more similar such a set of vertices are between each other. In other words, if we try to disconnect a similarity graph by removing edges, the more edges we need to remove before the graph becomes disconnected, the more similar the vertices in this graph. Minimum cut is a minimum set of edges without which the graph will become disconnected.HCS clustering algorithm finds all the subgraphs with n vertices such that the minimum cut of those subgraphs contain more than n/2 edges, and identifies them as clusters. Such a subgraph is called a Highly Connected Subgraph (HCS). Single vertices are not considered clusters and are grouped into a singletons set S.Given a similarity graph G(V,E), HCS clustering algorithm will check if it is already highly connected, if yes, returns G, otherwise uses the minimum cut of G to partition G into two subgraphs H and H', and recursively run HCS clustering algorithm on H and H'.== Example ==The following animation shows how the HCS clustering algorithm partitions a similarity graph into three clusters.== Pseudocode ==The step of finding the minimum cut on graph G is a subroutine that can be implemented using different algorithms for this problem. See below for an example algorithm for finding minimum cut using randomization.== Complexity ==The running time of the HCS clustering algorithm is bounded by N x f(n,m). f(n,m) is the time complexity of computing a minimum cut in a graph with n vertices and m edges, and N is the number of clusters found. In many applications N << n.For fast algorithms for finding a minimum cut in an unweighted graph: == Proof of correctness ==The clusters produced by the HCS clustering algorithm possess several properties, which can demonstrate the homogeneity and the separation of the solution.Theorem 1 The diameter of every highly connect graph is at most two.Proof: We know the edges of minimum cut must be greater or equal than the minimum degree of the graph. If the graph G is highly connected, then the edges of the minimum cut must be greater than the number of vertices divided by 2. So the degree of vertices in the highly connected graph G must be greater than half the vertices. Therefore, for any two vertices in this graph G, there must be at least one common neighbor, as the distance between them is two.Theorem 2 (a) The number of edges in a highly connected subgraph is quadratic. (b) The number of edges removed by each iteration of the HCS algorithm is at most linear.Proof: (For a) From Theorem 1 we know every vertex must have more than half of the total vertices as neighbors. Therefore, the total number of edges in a highly connect subgraph must be at least (n/2) x n x 1/2, where we sum all the degrees of each vertex and divide by 2.(For b) Each iteration HCS algorithm will separate a graph containing n vertices into two subgraphs, so the number of edges between those two components is at most n/2.Theorem 1 and 2a provide a strong indication to the homogeneity, as the only better possibility in terms of the diameter is that every two vertices of a cluster are connected by an edge, which is both too stringent and also a NP-hard problem.Theorem 2b also indicates separation since the number of edges removed by each iteration of the HCS algorithm is at most linear in the size of the underlying subgraph, contrast to the quadratic number of edges within final clusters.== Variations ==Singletons adoption: Elements left as singletons by the initial clustering process can be "adopted" by clusters based on similarity to the cluster. If the maximum number of neighbors to a specific cluster is large enough, then it can be added to that cluster.Removing Low Degree Vertices: When the input graph has vertices with low degrees, it is not worthy to run the algorithm since it is computationally expensive and not informative. Alternatively, a refinement of the algorithm can first remove all vertices with a degree lower than certain threshold.== Examples of HCS usage ==Gene expression analysis The hybridization of synthetic oligonucleotides to arrayed cDNAs yields a fingerprint for each cDNA clone. Run HCS algorithm on these fingerprints can identify clones corresponding to the same gene.PPI network structure discovery Using HCS clustering to detect dense subnetworks in PPI that may have biological meaning and represent biological processes."Survey of clustering algorithms." Neural Networks, IEEE Transactions The CLICK clustering algorithm is an adaptation of HCS algorithm on weighted similarity graphs, where the weight is assigned with a probability flavor.https://www.researchgate.net/publication/259350461_Partitioning_Biological_Networks_into_Highly_Connected_Clusters_with_Maximum_Edge_Coverage  Partitioning Biological Networks into Highly Connected Clusters with Maximum Edge Coverage]R Documentation== References ==
	Algorithmics is the science of algorithms. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem.
	EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the EdgeRank system and uses a machine learning algorithm that, as of 2013, takes more than 100,000 factors into account.EdgeRank was developed and implemented by Serkan Piantino.== Formula and factors ==In 2010, a simplified version of the EdgeRank algorithm was presented as:                              ∑                                    e              d              g              e              s                                      e                                    u                      e                                    w                      e                                    d                      e                                {\displaystyle \sum _{\mathrm {edges\,} e}u_{e}w_{e}d_{e}}  where:                              u                      e                                {\displaystyle u_{e}}   is user affinity.                              w                      e                                {\displaystyle w_{e}}   is how the content is weighted.                              d                      e                                {\displaystyle d_{e}}   is a time-based decay parameter.User Affinity: The User Affinity part of the algorithm in Facebook's EdgeRank looks at the relationship and proximity of the user and the content (post/status update).Content Weight: What action was taken by the user on the content.Time-Based Decay Parameter: New or old. Newer posts tend to hold a higher place than older posts.Some of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.== Impact ==EdgeRank and its successors have a broad impact on what users actually see out of what they ostensibly follow: for instance, the selection can produce a filter bubble (if users are exposed to updates which confirm their opinions etc.) or alter people's mood (if users are shown a disproportionate amount of positive or negative updates).As a result, for Facebook pages, the typical engagement rate is less than 1 % (or less than 0.1 % for the bigger ones) and organic reach 10 % or less for most non-profits.As a consequence, for pages it may be nearly impossible to reach any significant audience without paying to promote their content.== See also ==PageRank, the ranking algorithm used by Google's search engine== References ==== External links ==edgerank.netFacebook - How News Feed Works
	In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). In most models, these algorithms have access to limited memory (generally logarithmic in the size of and/or the maximum value in the stream). They may also have limited processing time per item.These constraints may mean that an algorithm produces an approximate answer based on a summary or "sketch" of the data stream.== History ==Though streaming algorithms had already been studied by Munro and Paterson as early as 1980, as well as Philippe Flajolet and G. Nigel Martin in 1982/83, the field of streaming algorithms was first formalized and popularized in a 1996 paper by Noga Alon, Yossi Matias, and Mario Szegedy. For this paper, the authors later won the Gödel Prize in 2005 "for their foundational contribution to streaming algorithms." There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing.Semi-streaming algorithms were introduced in 2005 as a relaxation of streaming algorithms for graphs [1], in which the space allowed is linear in the number of vertices n, but only logarithmic in the number of edges m.  This relaxation is still meaningful for dense graphs, and can solve interesting problems (such as connectivity) that are insoluble in                     o        (        n        )              {\displaystyle o(n)}   space.== Models ===== Data stream model ===In the data stream model, some or all of the input is represented as a finite sequence of integers (from some finite domain) which is generally not available for random access, but instead arrives one at a time in a "stream". If the stream has length n and the domain has size m, algorithms are generally constrained to use space that is logarithmic in m and n. They can generally make only some small constant number of passes over the stream, sometimes just one.=== Turnstile and cash register models ===Much of the streaming literature is concerned with computing statistics onfrequency distributions that are too large to be stored. For this class ofproblems, there is a vector                               a                =        (                  a                      1                          ,        …        ,                  a                      n                          )              {\displaystyle \mathbf {a} =(a_{1},\dots ,a_{n})}  (initialized to the zero vector                               0                      {\displaystyle \mathbf {0} }  ) that has updatespresented to it in a stream. The goal of these algorithms is to computefunctions of                               a                      {\displaystyle \mathbf {a} }   using considerably less space than itwould take to represent                               a                      {\displaystyle \mathbf {a} }   precisely. There are twocommon models for updating such streams, called the "cash register" and"turnstile" models.In the cash register model each update is of the form                     ⟨        i        ,        c        ⟩              {\displaystyle \langle i,c\rangle }  , so that                               a                      i                                {\displaystyle a_{i}}   is incremented by some positiveinteger                     c              {\displaystyle c}  . A notable special case is when                     c        =        1              {\displaystyle c=1}  (only unit insertions are permitted).In the turnstile model each update is of the form                     ⟨        i        ,        c        ⟩              {\displaystyle \langle i,c\rangle }  , so that                               a                      i                                {\displaystyle a_{i}}   is incremented by some (possibly negative) integer                     c              {\displaystyle c}  . In the "strict turnstile" model, no                              a                      i                                {\displaystyle a_{i}}   at any time may be less than zero.=== Sliding window model ===Several papers also consider the "sliding window" model. In this model,the function of interest is computing over a fixed-size window in thestream. As the stream progresses, items from the end of the window areremoved from consideration while new items from the stream take theirplace.Besides the above frequency-based problems, some other types of problemshave also been studied. Many graph problems are solved in the settingwhere the adjacency matrix or the adjacency list of the graph is streamed insome unknown order. There are also some problems that are very dependenton the order of the stream (i.e., asymmetric functions), such as countingthe number of inversions in a stream and finding the longest increasingsubsequence.== Evaluation ==The performance of an algorithm that operates on data streams is measured by three basic factors:The number of passes the algorithm must make over the stream.The available memory.The running time of the algorithm.These algorithms have many similarities with online algorithms since they both require decisions to be made before all data are available, but they are not identical. Data stream algorithms only have limited memory available but they may be able to defer action until a group of points arrive, while online algorithms are required to take action as soon as each point arrives.If the algorithm is an approximation algorithm then the accuracy of the answer is another key factor.  The accuracy is often stated as an                     (        ϵ        ,        δ        )              {\displaystyle (\epsilon ,\delta )}   approximation meaning that the algorithm achieves an error of less than                     ϵ              {\displaystyle \epsilon }   with probability                     1        −        δ              {\displaystyle 1-\delta }  .== Applications ==Streaming algorithms have several applications in networking such asmonitoring network links for elephant flows, counting the number ofdistinct flows, estimating the distribution of flow sizes, and soon. They also have applications indatabases, such as estimating the size of a join.== Some streaming problems ===== Frequency moments ===The kth frequency moment of a set of frequencies                              a                      {\displaystyle \mathbf {a} }   is defined as                               F                      k                          (                  a                )        =                  ∑                      i            =            1                                n                                    a                      i                                k                                {\displaystyle F_{k}(\mathbf {a} )=\sum _{i=1}^{n}a_{i}^{k}}  .The first moment                               F                      1                                {\displaystyle F_{1}}   is simply the sum of the frequencies(i.e., the total count). The second moment                               F                      2                                {\displaystyle F_{2}}   is useful forcomputing statistical properties of the data, such as the Gini coefficientof variation.                               F                      ∞                                {\displaystyle F_{\infty }}   is defined as the frequency of themost frequent item(s).The seminal paper of Alon, Matias, and Szegedy dealt with theproblem of estimating the frequency moments.==== Calculating Frequency Moments ====A direct approach to find the frequency moments requires to maintain a register mi for all distinct elements ai ∈ (1,2,3,4,...,N) which requires at least memoryof order                     Ω        (        N        )              {\displaystyle \Omega (N)}  . But we have space limitations and require an algorithm that computes in much lower memory. This can be achieved by using approximations instead of exact values. An algorithm that computes an (ε,δ)approximation of Fk, where F'k is the (ε,δ)-approximated value of Fk. Where ε is the approximation parameter and δ is the confidence parameter.===== Calculating  F0 (Distinct Elements in a DataStream) =========== FM-Sketch Algorithm ======Flajolet et al. in  introduced probabilistic method of counting which was inspired from a paper by Robert Morris. Morris in his paper says that if the requirement of accuracy is dropped, a counter n can be replaced by a counter log n which can be stored in log log n bits. Flajolet et al. in  improved this method by using a hash function h which is assumed to uniformly distribute the element in the hash space (a binary string of length L).                    h        :        [        m        ]        →        [        0        ,                  2                      L                          −        1        ]              {\displaystyle h:[m]\rightarrow [0,2^{L}-1]}  Let bit(y,k) represent the kth bit in binary representation of y                    y        =                  ∑                      k            ≥            0                                    b          i          t                (        y        ,        k        )        ∗                  2                      k                                {\displaystyle y=\sum _{k\geq 0}\mathrm {bit} (y,k)*2^{k}}  Let                     ρ        (        y        )              {\displaystyle \rho (y)}   represents the position of leastsignificant 1-bit in the binary representation of yi with a suitable convention for                     ρ        (        0        )              {\displaystyle \rho (0)}  .                    ρ        (        y        )        =                              {                                                                                M                    i                    n                                    (                  k                  :                                      b                    i                    t                                    (                  y                  ,                  k                  )                  ==                  1                  )                                                                      if                                     y                  >                  0                                                                              L                                                                      if                                     y                  =                  0                                                                                      {\displaystyle \rho (y)={\begin{cases}\mathrm {Min} (k:\mathrm {bit} (y,k)==1)&{\text{if }}y>0\\L&{\text{if }}y=0\end{cases}}}  Let A be the sequence of data stream of length M whose cardinality need to be determined. Let BITMAP [0...L − 1] be thehash space where the ρ(hashedvalues) are recorded. The below algorithm then determines approximate cardinality of A.Procedure FM-Sketch:    for i in 0 to L − 1 do        BITMAP[i]:=0     end for    for x in A: do        Index:=ρ(hash(x))        if BITMAP[index]=0 then BITMAP[index]:=1        end if    end for    B:= Position of left most 0 bit of BITMAP[]     return 2^BIf there are N distinct elements in a data stream.For                     i        ≫        log        ⁡        (        N        )              {\displaystyle i\gg \log(N)}   then BITMAP[i] is certainly 0For                     i        ≪        log        ⁡        (        N        )              {\displaystyle i\ll \log(N)}   then BITMAP[i] is certainly 1For                     i        ≈        log        ⁡        (        N        )              {\displaystyle i\approx \log(N)}   then BITMAP[i] is a fringes of 0's and 1's====== K-Minimum Value Algorithm ======The previous algorithm describes the first attempt to approximate F0 in the data stream by Flajolet and Martin. Their algorithm picks a random hash function which they assume to uniformly distribute the hash values in hash space.Bar-Yossef et al. in  introduced k-minimum value algorithm for determining number of distinct elements in data stream. They used a similar hash function h which can be normalized to [0,1] as                     h        :        [        m        ]        →        [        0        ,        1        ]              {\displaystyle h:[m]\rightarrow [0,1]}  . But they fixed a limit t to number of values in hash space. The value of t is assumed of the order                     O                  (                                                    1                                  ε                                      2                                                                                )                      {\displaystyle O\left({\dfrac {1}{\varepsilon _{2}}}\right)}   (i.e. less approximation-value ε requires more t). KMV algorithm keeps only t-smallest hash values in the hash space. After all the m values of stream have arrived,                     υ        =                  M          a          x                (        h        (                  a                      i                          )        )              {\displaystyle \upsilon =\mathrm {Max} (h(a_{i}))}   is used to calculate                              F                      0                    ′                =                                            t              υ                                            {\displaystyle F'_{0}={\dfrac {t}{\upsilon }}}  . That is, in a close-to uniform hash space, they expect at-least t elements to be less than                      O                  (                                                    t                                  F                                      0                                                                                )                      {\displaystyle O\left({\dfrac {t}{F_{0}}}\right)}  .Procedure 2 K-Minimum ValueInitialize first t values of KMV for a in a1 to an doif h(a) < Max(KMV) thenRemove Max(KMV) from KMV setInsert h(a) to KMV end ifend for return t/Max(KMV)====== Complexity analysis of KMV ======KMV algorithm can be implemented in                     O                  (                                    (                                                                    1                                          ε                                              2                                                                                                        )                        ⋅            log            ⁡            (            m            )                    )                      {\displaystyle O\left(\left({\dfrac {1}{\varepsilon _{2}}}\right)\cdot \log(m)\right)}   memory bits space. Each hash value requires space of order                     O        (        log        ⁡        (        m        )        )              {\displaystyle O(\log(m))}   memory bits. There are hash values of the order                     O                  (                                                    1                                  ε                                      2                                                                                )                      {\displaystyle O\left({\dfrac {1}{\varepsilon _{2}}}\right)}  . The access time can be reduced if we store the t hash values in a binary tree. Thus the time complexity will be reduced to                     O                  (                      log            ⁡                          (                                                                    1                    ε                                                              )                        ⋅            log            ⁡            (            m            )                    )                      {\displaystyle O\left(\log \left({\dfrac {1}{\varepsilon }}\right)\cdot \log(m)\right)}  .===== Calculating Fk =====Alon et al. in  estimates Fk by defining random variables that can be computed within given space and time. The expected value of random variable gives the approximate value of Fk.Let us assume length of sequence m is known in advance.Construct a random variable X as follows:Select ap be a random member of sequence A with index at p,                               a                      p                          =        l        ∈        (        1        ,        2        ,        3        ,        …        ,        n        )              {\displaystyle a_{p}=l\in (1,2,3,\ldots ,n)}  Let                     r        =                  |                {        q        :        q        ≥        p        ,                  a                      q                          =        l        }                  |                      {\displaystyle r=|\{q:q\geq p,a_{q}=l\}|}  , represents the number of occurrences of l within the members of the sequence A following ap.Random variable                     X        =        m        (                  r                      k                          −        (        r        −        1                  )                      k                          )              {\displaystyle X=m(r^{k}-(r-1)^{k})}  .Assume S1 be of the order                     O        (                  n                      1            −            1                          /                        k                                    /                          λ                      2                          )              {\displaystyle O(n^{1-1/k}/\lambda ^{2})}   and S2 be of the order                     O        (        log        ⁡        (        1                  /                ε        )        )              {\displaystyle O(\log(1/\varepsilon ))}  . Algorithm takes S2 randomvariable Y1,Y2,...,YS2 and outputs the median Y . Where Yi  is the average of Xijwhere 1 ≤ j ≤ S1.Now  calculate expectation of random variable E(X).                                                                        E                (                X                )                                            =                                                              ∑                                      i                    =                    1                                                        n                                                                    ∑                                      i                    =                    1                                                                              m                                              i                                                                                            (                                  j                                      k                                                  −                (                j                −                1                                  )                                      k                                                  )                                                                                  =                                                                                  m                    m                                                  [                (                                  1                                      k                                                  +                (                                  2                                      k                                                  −                                  1                                      k                                                  )                +                …                +                (                                  m                                      1                                                        k                                                  −                (                                  m                                      1                                                  −                1                                  )                                      k                                                  )                )                                                                                                                +                                (                                  1                                      k                                                  +                (                                  2                                      k                                                  −                                  1                                      k                                                  )                +                …                +                (                                  m                                      2                                                        k                                                  −                (                                  m                                      2                                                  −                1                                  )                                      k                                                  )                )                +                …                                                                                                                +                                (                                  1                                      k                                                  +                (                                  2                                      k                                                  −                                  1                                      k                                                  )                +                …                +                (                                  m                                      n                                                        k                                                  −                (                                  m                                      n                                                  −                1                                  )                                      k                                                  )                )                ]                                                                                  =                                                              ∑                                      i                    =                    1                                                        n                                                                    m                                      i                                                        k                                                  =                                  F                                      k                                                                                            {\displaystyle {\begin{array}{lll}E(X)&=&\sum _{i=1}^{n}\sum _{i=1}^{m_{i}}(j^{k}-(j-1)^{k})\\&=&{\frac {m}{m}}[(1^{k}+(2^{k}-1^{k})+\ldots +(m_{1}^{k}-(m_{1}-1)^{k}))\\&&\;+\;(1^{k}+(2^{k}-1^{k})+\ldots +(m_{2}^{k}-(m_{2}-1)^{k}))+\ldots \\&&\;+\;(1^{k}+(2^{k}-1^{k})+\ldots +(m_{n}^{k}-(m_{n}-1)^{k}))]\\&=&\sum _{i=1}^{n}m_{i}^{k}=F_{k}\end{array}}}  ====== Complexity of Fk ======From the algorithm to calculate Fk discussed above, we can see that each random variable X stores value of ap and r. So, to compute X we need to maintain only log(n) bits for storing ap and log(n) bits for storing r. Total number of random variable X will be the                               S                      1                          ∗                  S                      2                                {\displaystyle S_{1}*S_{2}}  .Hence the total space complexity the algorithm takes is of the order of                     O                  (                                                                                          k                    log                    ⁡                                                                  1                        ε                                                                                                  λ                                          2                                                                                                          n                              1                −                                                      1                    k                                                                                      (                              log                ⁡                n                +                log                ⁡                m                            )                                )                      {\displaystyle O\left({\dfrac {k\log {1 \over \varepsilon }}{\lambda ^{2}}}n^{1-{1 \over k}}\left(\log n+\log m\right)\right)}  ====== Simpler approach to calculate F2 ======The previous algorithm calculates                               F                      2                                {\displaystyle F_{2}}   in order of                     O        (                              n                          (        log        ⁡        m        +        log        ⁡        n        )        )              {\displaystyle O({\sqrt {n}}(\log m+\log n))}   memory bits. Alon et al. in  simplified this algorithm using four-wise independent random variable with values mapped to                     {        −        1        ,        1        }              {\displaystyle \{-1,1\}}  .This further reduces the complexity to calculate                               F                      2                                {\displaystyle F_{2}}    to                     O                  (                                                                                          log                    ⁡                                                                  1                        ε                                                                                                  λ                                          2                                                                                                          (                              log                ⁡                n                +                log                ⁡                m                            )                                )                      {\displaystyle O\left({\dfrac {\log {1 \over \varepsilon }}{\lambda ^{2}}}\left(\log n+\log m\right)\right)}  === Frequent elements ===In the data stream model, the frequent elements problem is to output a set of elements that constitute more than some fixed fraction of the stream. A special case is the majority problem, which is to determine whether or not any value constitutes a majority of the stream.More formally, fix some positive constant c > 1, let the length of the stream be m, and let fi denote the frequency of value i in the stream. The frequent elements problem is to output the set { i | fi > m/c }.Some notable algorithms are:Boyer–Moore majority vote algorithmKarp-Papadimitriou-Shenker algorithmCount-Min sketchSticky samplingLossy countingSample and HoldMulti-stage Bloom filtersCount-sketchSketch-guided samplingMisra–Gries summary=== Event detection ===Detecting events in data streams is often done using a heavy hitters algorithm as listed above: the most frequent items and their frequency are determined using one of these algorithms, then the largest increase over the previous time point is reported as trend. This approach can be refined by using exponentially weighted moving averages and variance for normalization.=== Counting distinct elements ===Counting the number of distinct elements in a stream (sometimes called theF0 moment) is another problem that has been well studied.The first algorithm for it was proposed by Flajolet and Martin. In 2010, Daniel Kane, Jelani Nelson and David Woodruff found an asymptotically optimal algorithm for this problem. It uses O(ε2 + log d) space, with O(1) worst-case update and reporting times, as well as universal hash functions and a r-wise independent hash family where r = Ω(log(1/ε) / log log(1/ε)).=== Entropy ===The (empirical) entropy of a set of frequencies                               a                      {\displaystyle \mathbf {a} }   isdefined as                               F                      k                          (                  a                )        =                  ∑                      i            =            1                                n                                                              a                              i                                      m                          log        ⁡                                            a                              i                                      m                                {\displaystyle F_{k}(\mathbf {a} )=\sum _{i=1}^{n}{\frac {a_{i}}{m}}\log {\frac {a_{i}}{m}}}  , where                     m        =                  ∑                      i            =            1                                n                                    a                      i                                {\displaystyle m=\sum _{i=1}^{n}a_{i}}  .Estimation of this quantity in a stream has been done by:McGregor et al.Do Ba et al.Lall et al.Chakrabarti et al.=== Online learning ===Learn a model (e.g. a classifier) by a single pass over a training set.Feature hashingStochastic gradient descent== Lower bounds ==Lower bounds have been computed for many of the data streaming problemsthat have been studied. By far, the most common technique for computingthese lower bounds has been using communication complexity.== See also ==Data stream miningData stream clusteringOnline algorithmStream processingSequential algorithm== Notes ==== References ==Alon, Noga; Matias, Yossi; Szegedy, Mario (1999), "The space complexity of approximating the frequency moments", Journal of Computer and System Sciences, 58 (1): 137–147, doi:10.1006/jcss.1997.1545, ISSN 0022-0000. First published as Alon, Noga; Matias, Yossi; Szegedy, Mario (1996), "The space complexity of approximating the frequency moments", Proceedings of the 28th ACM Symposium on Theory of Computing (STOC 1996), pp. 20–29, CiteSeerX 10.1.1.131.4984, doi:10.1145/237814.237823, ISBN 978-0-89791-785-8.Babcock, Brian; Babu, Shivnath; Datar, Mayur; Motwani, Rajeev; Widom, Jennifer (2002), "Models and issues in data stream systems", Proceedings of the 21st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2002) (PDF), pp. 1–16, CiteSeerX 10.1.1.138.190, doi:10.1145/543613.543615, ISBN 978-1581135077.Gilbert, A. C.; Kotidis, Y.; Muthukrishnan, S.; Strauss, M. J. (2001), "Surfing Wavelets on Streams: One-Pass Summaries for Approximate Aggregate Queries" (PDF), Proceedings of the International Conference on Very Large Data Bases: 79–88.Kane, Daniel M.; Nelson, Jelani; Woodruff, David P. (2010), An optimal algorithm for the distinct elements problem, PODS '10, New York, NY, USA: ACM, pp. 41–52, CiteSeerX 10.1.1.164.142, doi:10.1145/1807085.1807094, ISBN 978-1-4503-0033-9.Karp, R. M.; Papadimitriou, C. H.; Shenker, S. (2003), "A simple algorithm for finding frequent elements in streams and bags", ACM Transactions on Database Systems, 28 (1): 51–55, CiteSeerX 10.1.1.116.8530, doi:10.1145/762471.762473.Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), "Data streaming algorithms for estimating entropy of network traffic", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145, doi:10.1145/1140277.1140295, hdl:1802/2537, ISBN 978-1595933195.Xu, Jun (Jim) (2007), A Tutorial on Network Data Streaming (PDF).Heath, D., Kasif, S., Kosaraju, R., Salzberg, S.,  Sullivan, G., "Learning Nested Concepts With Limited Storage",  Proceeding IJCAI'91 Proceedings of the 12th international joint conference on Artificial intelligence - Volume 2, Pages 777-782,  Morgan Kaufmann Publishers Inc. San Francisco, CA, USA ©1991Morris, Robert (1978), "Counting large numbers of events in small registers", Communications of the ACM, 21 (10): 840–842, doi:10.1145/359619.359627.
	In mathematics and computer science, an algorithm ( (listen)) is a sequence of instructions, typically to solve a class of problems or perform a computation. Algorithms are unambiguous specifications for performing calculation, data processing, automated reasoning, and other tasks.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians circa 2500 BC and Egyptian mathematicians circa 1550 BC. Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as Al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.The word algorithm itself is derived from the 9th-century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.== Etymology ==The word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to algorismus. Al-Khwārizmī (Arabic: الخوارزمي‎,  Persian: خوارزمی‎, c. 780–850) was a Persian  mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.which translates as:Algorism is the art by which at present we use those Indian figures, which number two times five.The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.== Informal definition ==An informal definition could be "a set of rules that precisely defines a sequence of operations", which would include all computer programs, including programs that do not perform numeric calculations, and (for example) any prescribed bureaucratic procedure.Generally, a program is only an algorithm if it stops eventually.A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):Precise instructions (in language understood by "the computer") for a fast, efficient, "good" process that specifies the "moves" of "the computer" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and "effectively" produce, in a "reasonable" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.== Formalization ==Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000): Minsky: "But we will also maintain, with Turing ... that any procedure which could "naturally" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute". Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage [1987], an algorithm is a computational process defined by a Turing machine".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case. This is because of a major theorem of Computability Theory known as the Halting Problem.Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.=== Expressing algorithms ===Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).Representations of algorithms can be classed into three accepted levels of Turing machine description:1 High-level description"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head."2 Implementation description"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function."3 Formal descriptionMost detailed, "lowest level", gives the Turing machine's "state table".For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.== Design ==Algorithm design refers to a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.One of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.Typical steps in the development of algorithms:Problem definitionDevelopment of a modelSpecification of the algorithmDesigning an algorithmChecking the correctness of the algorithmAnalysis of algorithmImplementation of algorithmProgram testingDocumentation preparation== Implementation ==Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.== Computer algorithms ==In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology."Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:Knuth: " ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc"Chaitin: " ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such a proof would solve the Halting problem (ibid).Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.Computers (and computors), models of computation: A computer (or human "computor") is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction " Z ← 0 "; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.== Examples ===== Algorithm example ===One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:High-level description:If there are no numbers in the set then there is no highest number.Assume the first number in the set is the largest number in the set.For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:=== Euclid's algorithm ===Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements. Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.==== Computer language for Euclid's algorithm ====Only a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.A location is symbolized by upper case letter(s), e.g. S, A, etc.The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.==== An inelegant program for Euclid's algorithm ====The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:INPUT:1 [Into two locations L and S put the numbers l and s that represent the two lengths]:  INPUT L, S2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:  R ← LE0: [Ensure r ≥ s.]3 [Ensure the smaller of the two numbers is in S and the larger in R]:  IF R > S THEN    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:    GOTO step 6  ELSE    swap the contents of R and S.4   L ← R (this first step is redundant, but is useful for later discussion).5   R ← S6   S ← LE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.7 IF S > R THEN    done measuring so    GOTO 10  ELSE    measure again,8   R ← R − S9   [Remainder-loop]:    GOTO 7.E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.10 IF R = 0 THEN     done so     GOTO step 15   ELSE     CONTINUE TO step 11,E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.11  L ← R12  R ← S13  S ← L14  [Repeat the measuring process]:    GOTO 7OUTPUT:15 [Done. S contains the greatest common divisor]:   PRINT SDONE:16 HALT, END, STOP.==== An elegant program for Euclid's algorithm ====The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.The following version can be used with Object Oriented languages:=== Testing the Euclid algorithms ===Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.But "exceptional cases" must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.=== Measuring and improving the Euclid algorithms ===Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.== Algorithmic analysis ==It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.=== Formal versus empirical ===The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.=== Execution efficiency ===To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.== Classification ==There are various ways to classify algorithms, each with its own merits.=== By implementation ===One way to classify algorithms is by implementation means.RecursionA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.LogicalAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.Serial, parallel or distributedAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms and are called inherently serial problems.Deterministic or non-deterministicDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.Exact or approximateWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.Quantum algorithmThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.=== By design paradigm ===Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:Brute-force or exhaustive searchThis is the naive method of trying every possible solution to see which is best.Divide and conquerA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.Search and enumerationMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.Randomized algorithmSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexityThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.Back trackingIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.=== Optimization problems ===For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:Linear programmingWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.Dynamic programmingWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.The greedy methodA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.The heuristic methodIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.=== By field of study ===Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.=== By complexity ===Algorithms can be classified by the amount of time they need to complete compared to their input size:Constant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.Linear time: if the time is proportional to the input size. E.g. the traverse of a list.Logarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.Polynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.Exponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.== Continuous algorithms ==The adjective "continuous" when applied to the word "algorithm" can mean:An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; orAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.== Legal issues ==Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).== History: Development of the notion of "algorithm" ===== Ancient Near East ===The earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).=== Discrete and distinguishable symbols ===Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.=== Manipulation of symbols as "place holders" for numbers: algebra ===Muhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms "algorism" and "algorithm" are derived from the name al-Khwārizmī, while the term "algebra" is derived from the book Al-jabr. In Europe, the word "algorithm" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.=== Cryptographic algorithms ===The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.=== Mechanical contrivances with discrete states ===The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.Logical machines 1870 – Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] ...". With this machine he could analyze a "syllogism or any other simple logical argument".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned."=== Mathematics during the 19th century up to the mid-20th century ===Symbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S.C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".=== Emil Post (1936) and Alan Turing (1936–37, 1939) ===Emil Post (1936) described the actions of a "computer" (human being) as follows:"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be"a two-way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke."One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes..."A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]". See more at Post–Turing machineAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers."Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite..."The behavior of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite..."Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided."Turing's reduction yields the following:"The simple operations must therefore include:"(a) Changes of the symbol on one of the observed squares"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares."It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:"(A) A possible change (a) of symbol together with a possible change of state of mind."(B) A possible change (b) of observed squares, together with a possible change of state of mind""We may now construct a machine to do the work of this computer."A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:"A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability ... ."† We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without particular identification with any one of these definitions".=== J.B. Rosser (1939) and S.C. Kleene (1943) ===J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225–226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the predicate value true?"" (Kleene 1943:273)=== History after 1950 ===A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.== See also ==== Notes ==== Bibliography ==== Further reading ==== External links ==Hazewinkel, Michiel, ed. (2001) [1994], "Algorithm", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4Algorithms at CurlieWeisstein, Eric W. "Algorithm". MathWorld.Dictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony BrookCollected Algorithms of the ACM – Association for Computing MachineryThe Stanford GraphBase – Stanford University
	The following is a list of algorithms along with one-line descriptions for each.== Automated planning ==== Combinatorial algorithms ===== General combinatorial algorithms ===Brent's algorithm: finds a cycle in function value iterations using only two iteratorsFloyd's cycle-finding algorithm: finds a cycle in function value iterationsGale–Shapley algorithm: solves the stable marriage problemPseudorandom number generators (uniformly distributed—see also List of pseudorandom number generators for other PRNGs with varying degrees of convergence and varying statistical quality):ACORN generatorBlum Blum ShubLagged Fibonacci generatorLinear congruential generatorMersenne Twister=== Graph algorithms ===Coloring algorithm: Graph coloring algorithm.Hopcroft–Karp algorithm: convert a bipartite graph to a maximum cardinality matchingHungarian algorithm: algorithm for finding a perfect matchingPrüfer coding: conversion between a labeled tree and its Prüfer sequenceTarjan's off-line lowest common ancestors algorithm: compute lowest common ancestors for pairs of nodes in a treeTopological sort: finds linear order of nodes (e.g. jobs) based on their dependencies.==== Graph drawing ====Force-based algorithms (also known as force-directed algorithms or spring-based algorithm)Spectral layout==== Network theory ====Network analysisLink analysisGirvan–Newman algorithm: detect communities in complex systemsWeb link analysisHyperlink-Induced Topic Search (HITS) (also known as Hubs and authorities)PageRankTrustRankFlow networksDinic's algorithm: is a strongly polynomial algorithm for computing the maximum flow in a flow network.Edmonds–Karp algorithm: implementation of Ford–FulkersonFord–Fulkerson algorithm: computes the maximum flow in a graphKarger's algorithm:  a Monte Carlo method to compute the minimum cut of a connected graphPush–relabel algorithm: computes a maximum flow in a graph==== Routing for graphs ====Edmonds' algorithm (also known as Chu–Liu/Edmonds' algorithm): find maximum or minimum branchingsEuclidean minimum spanning tree: algorithms for computing the minimum spanning tree of a set of points in the planeEuclidean shortest path problem: find the shortest path between two points that does not intersect any obstacleLongest path problem: find a simple path of maximum length in a given graphMinimum spanning treeBorůvka's algorithmKruskal's algorithmPrim's algorithmReverse-delete algorithmNonblocking minimal spanning switch say, for a telephone exchangeShortest path problemBellman–Ford algorithm: computes shortest paths in a weighted graph (where some of the edge weights may be negative)Dijkstra's algorithm: computes shortest paths in a graph with non-negative edge weightsFloyd–Warshall algorithm: solves the all pairs shortest path problem in a weighted, directed graphJohnson's algorithm: All pairs shortest path algorithm in sparse weighted directed graphTransitive closure problem: find the transitive closure of a given binary relationTraveling salesman problemChristofides algorithmNearest neighbour algorithmWarnsdorff's rule: A heuristic method for solving the Knight's tour problem.==== Graph search ====A*: special case of best-first search that uses heuristics to improve speedB*: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)Backtracking: abandons partial solutions when they are found not to satisfy a complete solutionBeam search: is a heuristic search algorithm that is an optimization of best-first search that reduces its memory requirementBeam stack search: integrates backtracking with beam searchBest-first search: traverses a graph in the order of likely importance using a priority queueBidirectional search: find the shortest path from an initial vertex to a goal vertex in a directed graphBreadth-first search: traverses a graph level by levelBrute-force search: An exhaustive and reliable search method, but computationally inefficient in many applications.D*: an incremental heuristic search algorithmDepth-first search: traverses a graph branch by branchDijkstra's algorithm: A special case of A* for which no heuristic function is usedGeneral Problem Solver: a seminal theorem-proving algorithm intended to work as a universal problem solver machine.Iterative deepening depth-first search (IDDFS): a state space search strategyJump point search: An optimization to A* which may reduce computation time by an order of magnitude using further heuristics.Lexicographic breadth-first search (also known as Lex-BFS): a linear time algorithm for ordering the vertices of a graphUniform-cost search: a tree search that finds the lowest-cost route where costs varySSS*: state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithmF*: Special algorithm to merge the two arrays==== Subgraphs ====CliquesBron–Kerbosch algorithm: a technique for finding maximal cliques in an undirected graphMaxCliqueDyn maximum clique algorithm: find a maximum clique in an undirected graphStrongly connected componentsPath-based strong component algorithmKosaraju's algorithmTarjan's strongly connected components algorithmSubgraph isomorphism problem=== Sequence algorithms ======= Approximate sequence matching ====Bitap algorithm: fuzzy algorithm that determines if strings are approximately equal.Phonetic algorithmsDaitch–Mokotoff Soundex: a Soundex refinement which allows matching of Slavic and Germanic surnamesDouble Metaphone: an improvement on MetaphoneMatch rating approach: a phonetic algorithm developed by Western AirlinesMetaphone: an algorithm for indexing words by their sound, when pronounced in EnglishNYSIIS: phonetic algorithm, improves on SoundexSoundex: a phonetic algorithm for indexing names by sound, as pronounced in EnglishString metrics: compute a similarity or dissimilarity (distance) score between two pairs of text stringsDamerau–Levenshtein distance compute a distance measure between two strings, improves on Levenshtein distanceDice's coefficient (also known as the Dice coefficient): a similarity measure related to the Jaccard indexHamming distance: sum number of positions which are differentJaro–Winkler distance:  is a measure of similarity between two stringsLevenshtein edit distance: compute a metric for the amount of difference between two sequencesTrigram search: search for text when the exact syntax or spelling of the target object is not precisely known==== Selection algorithms ====QuickselectIntroselect==== Sequence search ====Linear search: finds an item in an unsorted sequenceSelection algorithm: finds the kth largest item in a sequenceTernary search: a technique for finding the minimum or maximum of a function that is either strictly increasing and then strictly decreasing or vice versaSorted listsBinary search algorithm: locates an item in a sorted sequenceFibonacci search technique: search a sorted sequence using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbersJump search (or block search): linear search on a smaller subset of the sequencePredictive search: binary-like search which factors in magnitude of search term versus the high and low values in the search.  Sometimes called dictionary search or interpolated search.Uniform binary search: an optimization of the classic binary search algorithm==== Sequence merging ====Simple merge algorithmk-way merge algorithmUnion (merge, with elements on the output not repeated)==== Sequence permutations ====Fisher–Yates shuffle (also known as the Knuth shuffle): randomly shuffle a finite setSchensted algorithm: constructs a pair of Young tableaux from a permutationSteinhaus–Johnson–Trotter algorithm (also known as the Johnson–Trotter algorithm): generate permutations by transposing elementsHeap's permutation generation algorithm: interchange elements to generate next permutation==== Sequence alignment ====Dynamic time warping: measure similarity between two sequences which may vary in time or speedHirschberg's algorithm: finds the least cost sequence alignment between two sequences, as measured by their Levenshtein distanceNeedleman–Wunsch algorithm: find global alignment between two sequencesSmith–Waterman algorithm: find local sequence alignment==== Sequence sorting ====Exchange sortsBubble sort: for each pair of indices, swap the items if out of orderCocktail shaker sort or bidirectional bubble sort, a bubble sort traversing the list alternately from front to back and back to frontComb sortGnome sortOdd–even sortQuicksort: divide list into two, with all items on the first list coming before all items on the second list.; then sort the two lists. Often the method of choiceHumorous or ineffectiveBogosortStooge sortHybridFlashsortIntrosort: begin with quicksort and switch to heapsort when the recursion depth exceeds a certain levelTimsort: adaptative algorithm derived from merge sort and insertion sort. Used in Python 2.3 and up, and Java SE 7.Insertion sortsInsertion sort: determine where the current item belongs in the list of sorted ones, and insert it thereLibrary sortPatience sortingShell sort: an attempt to improve insertion sortTree sort (binary tree sort): build binary tree, then traverse it to create sorted listCycle sort: in-place with theoretically optimal number of writesMerge sortsMerge sort: sort the first and second half of the list separately, then merge the sorted listsStrand sortNon-comparison sortsBead sortBucket sortBurstsort: build a compact, cache efficient burst trie and then traverse it to create sorted outputCounting sortPigeonhole sortPostman sort: variant of Bucket sort which takes advantage of hierarchical structureRadix sort: sorts strings letter by letterSelection sortsHeapsort: convert the list into a heap, keep removing the largest element from the heap and adding it to the end of the listSelection sort: pick the smallest of the remaining elements, add it to the end of the sorted listSmoothsortOtherBitonic sorterPancake sortingSpaghetti sortTopological sortUnknown classSamplesort==== Subsequences ====Kadane's algorithm: finds maximum sub-array of any sizeLongest common subsequence problem: Find the longest subsequence common to all sequences in a set of sequencesLongest increasing subsequence problem: Find the longest increasing subsequence of a given sequenceShortest common supersequence problem: Find the shortest supersequence that contains two or more sequences as subsequences==== Substrings ====Longest common substring problem:  find the longest string (or strings) that is a substring (or are substrings) of two or more stringsSubstring searchAho–Corasick string matching algorithm: trie based algorithm for finding all substring matches to any of a finite set of stringsBoyer–Moore string-search algorithm: amortized linear (sublinear in most times) algorithm for substring searchBoyer–Moore–Horspool algorithm: Simplification of Boyer–MooreKnuth–Morris–Pratt algorithm: substring search which bypasses reexamination of matched charactersRabin–Karp string search algorithm: searches multiple patterns efficientlyZhu–Takaoka string matching algorithm: a variant of Boyer–MooreUkkonen's algorithm: a linear-time, online algorithm for constructing suffix treesMatching wildcardsRich Salz' wildmat: a widely used open-source recursive algorithmKrauss matching wildcards algorithm: an open-source non-recursive algorithm== Computational mathematics ===== Abstract algebra ===Chien search: a recursive algorithm for determining roots of polynomials defined over a finite fieldSchreier–Sims algorithm: computing a base and strong generating set (BSGS) of a permutation groupTodd–Coxeter algorithm: Procedure for generating cosets.=== Computer algebra ===Buchberger's algorithm: finds a Gröbner basisCantor–Zassenhaus algorithm: factor polynomials over finite fieldsFaugère F4 algorithm: finds a Gröbner basis (also mentions the F5 algorithm)Gosper's algorithm: find sums of hypergeometric terms that are themselves hypergeometric termsKnuth–Bendix completion algorithm: for rewriting rule systemsMultivariate division algorithm: for polynomials in several indeterminatesPollard's kangaroo algorithm (also known as Pollard's lambda algorithm ): an algorithm for solving the discrete logarithm problemPolynomial long division: an algorithm for dividing a polynomial by another polynomial of the same or lower degreeRisch algorithm: an algorithm for the calculus operation of indefinite integration (i.e. finding antiderivatives)=== Geometry ===Closest pair problem:  find the pair of points (from a set of points) with the smallest distance between themCollision detection algorithms: check for the collision or intersection of two given solidsCone algorithm: identify surface pointsConvex hull algorithms: determining the convex hull of a set of pointsGraham scanQuickhullGift wrapping algorithm or Jarvis marchChan's algorithmKirkpatrick–Seidel algorithmEuclidean distance transform: computes the distance between every point in a grid and a discrete collection of points.Geometric hashing: a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformationGilbert–Johnson–Keerthi distance algorithm: determining the smallest distance between two convex shapes.Jump-and-Walk algorithm: an algorithm for point location in triangulationsLaplacian smoothing: an algorithm to smooth a polygonal meshLine segment intersection: finding whether lines intersect, usually with a sweep line algorithmBentley–Ottmann algorithmShamos–Hoey algorithmMinimum bounding box algorithms: find the oriented minimum bounding box enclosing a set of pointsNearest neighbor search:  find the nearest point or points to a query pointPoint in polygon algorithms: tests whether a given point lies within a given polygonPoint set registration algorithms: finds the transformation between two point sets to optimally align them.Rotating calipers: determine all antipodal pairs of points and vertices on a convex polygon or convex hull.Shoelace algorithm: determine the area of a polygon whose vertices are described by ordered pairs in the planeTriangulationDelaunay triangulationRuppert's algorithm (also known as Delaunay refinement): create quality Delaunay triangulationsChew's second algorithm: create quality constrained Delaunay triangulationsMarching triangles: reconstruct two-dimensional surface geometry from an unstructured point cloudPolygon triangulation algorithms: decompose a polygon into a set of trianglesVoronoi diagrams, geometric dual of Delaunay triangulationBowyer–Watson algorithm: create voronoi diagram in any number of dimensionsFortune's Algorithm: create voronoi diagramQuasitriangulation=== Number theoretic algorithms ===Binary GCD algorithm: Efficient way of calculating GCD.Booth's multiplication algorithmChakravala method: a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equationDiscrete logarithm:Baby-step giant-stepIndex calculus algorithmPollard's rho algorithm for logarithmsPohlig–Hellman algorithmEuclidean algorithm: computes the greatest common divisorExtended Euclidean algorithm: Also solves the equation ax + by = c.Integer factorization: breaking an integer into its prime factorsCongruence of squaresDixon's algorithmFermat's factorization methodGeneral number field sieveLenstra elliptic curve factorizationPollard's p − 1 algorithmPollard's rho algorithmprime factorization algorithmQuadratic sieveShor's algorithmSpecial number field sieveTrial divisionMultiplication algorithms: fast multiplication of two numbersKaratsuba algorithmSchönhage–Strassen algorithmToom–Cook multiplicationModular square root: computing square roots modulo a prime numberTonelli–Shanks algorithmCipolla's algorithmBerlekamp's root finding algorithmOdlyzko–Schönhage algorithm: calculates nontrivial zeroes of the Riemann zeta functionLenstra–Lenstra–Lovász algorithm (also known as LLL algorithm): find a short, nearly orthogonal lattice basis in polynomial timePrimality tests: determining whether a given number is primeAKS primality testBaillie-PSW primality testFermat primality testLucas primality testMiller–Rabin primality testSieve of AtkinSieve of EratosthenesSieve of Sundaram=== Numerical algorithms ======= Differential equation solving ====Euler methodBackward Euler methodTrapezoidal rule (differential equations)Linear multistep methodsRunge–Kutta methodsEuler integrationMultigrid methods (MG methods), a group of algorithms for solving differential equations using a hierarchy of discretizationsPartial differential equation:Finite difference methodCrank–Nicolson method for diffusion equationsLax-Wendroff for wave equationsVerlet integration (French pronunciation: ​[vɛʁˈlɛ]): integrate Newton's equations of motion==== Elementary and special functions ====Computation of π:Borwein's algorithm: an algorithm to calculate the value of 1/πGauss–Legendre algorithm: computes the digits of piChudnovsky algorithm: A fast method for calculating the digits of πBailey–Borwein–Plouffe formula: (BBP formula) a spigot algorithm for the computation of the nth binary digit of πDivision algorithms: for computing quotient and/or remainder of two numbersLong divisionRestoring divisionNon-restoring divisionSRT divisionNewton–Raphson division: uses Newton's method to find the reciprocal of D, and multiply that reciprocal by N to find the final quotient Q.Goldschmidt divisionHyperbolic and Trigonometric Functions:BKM algorithm:  compute elementary functions using a table of logarithmsCORDIC:  compute hyperbolic and trigonometric functions using a table of arctangentsExponentiation:Addition-chain exponentiation: exponentiation by positive integer powers that requires a minimal number of multiplicationsExponentiating by squaring: an algorithm used for the fast computation of large integer powers of a numberMontgomery reduction: an algorithm that allows modular arithmetic to be performed efficiently when the modulus is largeMultiplication algorithms: fast multiplication of two numbersBooth's multiplication algorithm: a multiplication algorithm that multiplies two signed binary numbers in two's complement notationFürer's algorithm:  an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexityKaratsuba algorithm:  an efficient procedure for multiplying large numbersSchönhage–Strassen algorithm: an asymptotically fast multiplication algorithm for large integersToom–Cook multiplication: (Toom3) a multiplication algorithm for large integersMultiplicative inverse Algorithms: for computing a number's multiplicative inverse (reciprocal).Newton's methodRounding functions: the classic ways to round numbersSpigot algorithm: A way to compute the value of a mathematical constant without knowing preceding digitsSquare and Nth root of a number:Alpha max plus beta min algorithm: an approximation of the square-root of the sum of two squaresMethods of computing square rootsnth root algorithmShifting nth-root algorithm: digit by digit root extractionSummation:Binary splitting:  a divide and conquer technique which speeds up the numerical evaluation of many types of series with rational termsKahan summation algorithm: a more accurate method of summing floating-point numbersUnrestricted algorithm==== Geometric ====Filtered back-projection: efficiently compute the inverse 2-dimensional Radon transform.Level set method (LSM):  a numerical technique for tracking interfaces and shapes==== Interpolation and extrapolation ====Birkhoff interpolation: an extension of polynomial interpolationCubic interpolationHermite interpolationLagrange interpolation: interpolation using Lagrange polynomialsLinear interpolation: a method of curve fitting using linear polynomialsMonotone cubic interpolation: a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.Multivariate interpolationBicubic interpolation, a generalization of cubic interpolation to two dimensionsBilinear interpolation: an extension of linear interpolation for interpolating functions of two variables on a regular gridLanczos resampling ("Lanzosh"): a multivariate interpolation method used to compute new values for any digitally sampled dataNearest-neighbor interpolationTricubic interpolation, a generalization of cubic interpolation to three dimensionsPareto interpolation: a method of estimating the median and other properties of a population that follows a Pareto distribution.Polynomial interpolationNeville's algorithmSpline interpolation: Reduces error with Runge's phenomenon.De Boor algorithm: B-splinesDe Casteljau's algorithm: Bézier curvesTrigonometric interpolation==== Linear algebra ====Eigenvalue algorithmsArnoldi iterationInverse iterationJacobi methodLanczos iterationPower iterationQR algorithmRayleigh quotient iterationGram–Schmidt process: orthogonalizes a set of vectorsMatrix multiplication algorithmsCannon's algorithm: a distributed algorithm for matrix multiplication especially suitable for computers laid out in an N × N meshCoppersmith–Winograd algorithm: square matrix multiplicationFreivalds' algorithm: a randomized algorithm used to verify matrix multiplicationStrassen algorithm: faster matrix multiplicationSolving systems of linear equationsBiconjugate gradient method: solves systems of linear equationsConjugate gradient: an algorithm for the numerical solution of particular systems of linear equationsGaussian eliminationGauss–Jordan elimination: solves systems of linear equationsGauss–Seidel method: solves systems of linear equations iterativelyLevinson recursion: solves equation involving a Toeplitz matrixStone's method: also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equationsSuccessive over-relaxation (SOR): method used to speed up convergence of the Gauss–Seidel methodTridiagonal matrix algorithm (Thomas algorithm): solves systems of tridiagonal equationsSparse matrix algorithmsCuthill–McKee algorithm: reduce the bandwidth of a symmetric sparse matrixMinimum degree algorithm: permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decompositionSymbolic Cholesky decomposition: Efficient way of storing sparse matrix==== Monte Carlo ====Gibbs sampling: generate a sequence of samples from the joint probability distribution of two or more random variablesHybrid Monte Carlo: generate a sequence of samples using Hamiltonian weighted Markov chain Monte Carlo, from a probability distribution which is difficult to sample directly.Metropolis–Hastings algorithm: used to generate a sequence of samples from the probability distribution of one or more variablesWang and Landau algorithm: an extension of Metropolis–Hastings algorithm sampling==== Numerical integration ====MISER algorithm: Monte Carlo simulation, numerical integration==== Root finding ====Bisection methodFalse position method: approximates roots of a functionNewton's method: finds zeros of functions with calculusHalley's method: uses first and second derivativesSecant method: 2-point, 1-sidedFalse position method and Illinois method: 2-point, bracketingRidder's method: 3-point, exponential scalingMuller's method: 3-point, quadratic interpolation=== Optimization algorithms ===Alpha–beta pruning: search to reduce number of nodes in minimax algorithmBranch and boundBruss algorithm: see odds algorithmChain matrix multiplicationCombinatorial optimization: optimization problems where the set of feasible solutions is discreteGreedy randomized adaptive search procedure (GRASP): successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local searchHungarian method: a combinatorial optimization algorithm which solves the assignment problem in polynomial timeConstraint satisfactionGeneral algorithms for the constraint satisfactionAC-3 algorithmDifference map algorithmMin conflicts algorithmChaff algorithm: an algorithm for solving instances of the boolean satisfiability problemDavis–Putnam algorithm: check the validity of a first-order logic formulaDavis–Putnam–Logemann–Loveland algorithm (DPLL): an algorithm for deciding the satisfiability of propositional logic formula in conjunctive normal form, i.e. for solving the CNF-SAT problemExact cover problemAlgorithm X: a nondeterministic algorithmDancing Links: an efficient implementation of Algorithm XCross-entropy method: a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance samplingDifferential evolutionDynamic Programming: problems exhibiting the properties of overlapping subproblems and optimal substructureEllipsoid method: is an algorithm for solving convex optimization problemsEvolutionary computation: optimization inspired by biological mechanisms of evolutionEvolution strategyGene expression programmingGenetic algorithmsFitness proportionate selection - also known as roulette-wheel selectionStochastic universal samplingTruncation selectionTournament selectionMemetic algorithmSwarm intelligenceAnt colony optimizationBees algorithm: a search algorithm which mimics the food foraging behavior of swarms of honey beesParticle swarmgolden section search: an algorithm for finding the maximum of a real functionGradient descentHarmony search (HS): a metaheuristic algorithm mimicking the improvisation process of musiciansInterior point methodLinear programmingBenson's algorithm: an algorithm for solving linear vector optimization problemsDantzig–Wolfe decomposition: an algorithm for solving linear programming problems with special structureDelayed column generationInteger linear programming: solve linear programming problems where some or all the unknowns are restricted to integer valuesBranch and cutCutting-plane methodKarmarkar's algorithm: The first reasonably efficient algorithm that solves the linear programming problem in polynomial time.Simplex algorithm: An algorithm for solving linear programming problemsLine searchLocal search: a metaheuristic for solving computationally hard optimization problemsRandom-restart hill climbingTabu searchMinimax used in game programmingNearest neighbor search (NNS): find closest points in a metric spaceBest Bin First: find an approximate solution to the Nearest neighbor search problem in very-high-dimensional spacesNewton's method in optimizationNonlinear optimizationBFGS method: A nonlinear optimization algorithmGauss–Newton algorithm: An algorithm for solving nonlinear least squares problems.Levenberg–Marquardt algorithm: An algorithm for solving nonlinear least squares problems.Nelder–Mead method (downhill simplex method): A nonlinear optimization algorithmOdds algorithm (Bruss algorithm) : Finds the optimal strategy to predict a last specific event in a random sequence eventSimulated annealingStochastic tunnelingSubset sum algorithm== Computational science ===== Astronomy ===Doomsday algorithm: day of the weekZeller's congruence is an algorithm to calculate the day of the week for any Julian or Gregorian calendar datevarious Easter algorithms are used to calculate the day of Easter=== Bioinformatics ===Basic Local Alignment Search Tool also known as BLAST: an algorithm for comparing primary biological sequence informationKabsch algorithm: calculate the optimal alignment of two sets of points in order to compute the root mean squared deviation between two protein structures.Velvet: a set of algorithms manipulating de Bruijn graphs for genomic sequence assemblySorting by signed reversals: an algorithm for understanding genomic evolution.Maximum parsimony (phylogenetics): an algorithm for finding the simplest phylogenetic tree to explain a given character matrix.UPGMA: a distance-based phylogenetic tree construction algorithm.=== Geoscience ===Vincenty's formulae: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoidGeohash: a public domain algorithm that encodes a decimal latitude/longitude pair as a hash string=== Linguistics ===Lesk algorithm: word sense disambiguationStemming algorithm: a method of reducing words to their stem, base, or root formSukhotin's algorithm: a statistical classification algorithm for classifying characters in a text as vowels or consonants=== Medicine ===ESC algorithm for the diagnosis of heart failureManning Criteria for irritable bowel syndromePulmonary embolism diagnostic algorithmsTexas Medication Algorithm Project=== Physics ===Constraint algorithm: a class of algorithms for satisfying constraints for bodies that obey Newton's equations of motionDemon algorithm:  a Monte Carlo method for efficiently sampling members of a microcanonical ensemble with a given energyFeatherstone's algorithm: compute the effects of forces applied to a structure of joints and linksGround state approximationVariational methodRitz methodN-body problemsBarnes–Hut simulation: Solves the n-body problem in an approximate way that has the order O(n log n) instead of O(n2) as in a direct-sum simulation.Fast multipole method (FMM): speeds up the calculation of long-ranged forcesRainflow-counting algorithm: Reduces a complex stress history to a count of elementary stress-reversals for use in fatigue analysisSweep and prune: a broad phase algorithm used during collision detection to limit the number of pairs of solids that need to be checked for collisionVEGAS algorithm: a method for reducing error in Monte Carlo simulations=== Statistics ===Algorithms for calculating variance: avoiding instability and numerical overflowApproximate counting algorithm: Allows counting large number of events in a small registerBayesian statisticsNested sampling algorithm: a computational approach to the problem of comparing models in Bayesian statisticsClustering AlgorithmsAverage-linkage clustering: a simple agglomerative clustering algorithmCanopy clustering algorithm:  an unsupervised pre-clustering algorithm related to the K-means algorithmComplete-linkage clustering: a simple agglomerative clustering algorithmDBSCAN: a density based clustering algorithmExpectation-maximization algorithmFuzzy clustering: a class of clustering algorithms where each point has a degree of belonging to clustersFuzzy c-meansFLAME clustering (Fuzzy clustering by Local Approximation of MEmberships): define clusters in the dense parts of a dataset and perform cluster assignment solely based on the neighborhood relationships among objectsKHOPCA clustering algorithm: a local clustering algorithm, which produces hierarchical multi-hop clusters in static and mobile environments.k-means clustering: cluster objects based on attributes into partitionsk-means++: a variation of this, using modified random seedsk-medoids: similar to k-means, but chooses datapoints or medoids as centersLinde–Buzo–Gray algorithm: a vector quantization algorithm to derive a good codebookLloyd's algorithm (Voronoi iteration or relaxation): group data points into a given number of categories, a popular algorithm for k-means clusteringOPTICS: a density based clustering algorithm with a visual evaluation methodSingle-linkage clustering: a simple agglomerative clustering algorithmSUBCLU: a subspace clustering algorithmWard's method : an agglomerative clustering algorithm, extended to more general Lance–Williams algorithmsWACA clustering algorithm: a local clustering algorithm with potentially multi-hop structures; for dynamic networksEstimation TheoryExpectation-maximization algorithm A class of related algorithms for finding maximum likelihood estimates of parameters in probabilistic modelsOrdered subset expectation maximization (OSEM): used in medical imaging for positron emission tomography, single photon emission computed tomography and X-ray computed tomography.Odds algorithm (Bruss algorithm) Optimal online search for distinguished value in sequential random inputKalman filter: estimate the state of a linear dynamic system from a series of noisy measurementsFalse nearest neighbor algorithm (FNN) estimates fractal dimensionHidden Markov modelBaum–Welch algorithm: compute maximum likelihood estimates and posterior mode estimates for the parameters of a hidden markov modelForward-backward algorithm a dynamic programming algorithm for computing the probability of a particular observation sequenceViterbi algorithm: find the most likely sequence of hidden states in a hidden markov modelPartial least squares regression:  finds a linear model describing some predicted variables in terms of other observable variablesQueuing theoryBuzen's algorithm: an algorithm for calculating the normalization constant G(K) in the Gordon–Newell theoremRANSAC (an abbreviation for "RANdom SAmple Consensus"): an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliersScoring algorithm: is a form of Newton's method used to solve maximum likelihood equations numericallyYamartino method: calculate an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming dataZiggurat algorithm: generate random numbers from a non-uniform distribution== Computer science ===== Computer architecture ===Tomasulo algorithm: allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially=== Computer graphics ===ClippingLine clippingCohen–SutherlandCyrus–BeckFast-clippingLiang–BarskyNicholl–Lee–NichollPolygon clippingSutherland–HodgmanVattiWeiler–AthertonContour lines and IsosurfacesMarching cubes: extract a polygonal mesh of an isosurface from a three-dimensional scalar field (sometimes called voxels)Marching squares: generate contour lines for a two-dimensional scalar fieldMarching tetrahedrons: an alternative to Marching cubesDiscrete Green's Theorem: is an algorithm for computing double integral over a generalized rectangular domain in constant time.  It is a natural extension to the summed area table algorithmFlood fill: fills a connected region of a multi-dimensional array with a specified symbolGlobal illumination algorithms: Considers direct illumination and reflection from other objects.Ambient occlusionBeam tracingCone tracingImage-based lightingMetropolis light transportPath tracingPhoton mappingRadiosityRay tracingHidden surface removal or Visual surface determinationNewell's algorithm: eliminate polygon cycles in the depth sorting required in hidden surface removalPainter's algorithm: detects visible parts of a 3-dimensional sceneryScanline rendering: constructs an image by moving an imaginary line over the imageWarnock algorithmLine Drawing: graphical algorithm for approximating a line segment on discrete graphical media.Bresenham's line algorithm: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses decision variables)DDA line algorithm: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses floating-point math)Xiaolin Wu's line algorithm: algorithm for line antialiasing.Midpoint circle algorithm: an algorithm used to determine the points needed for drawing a circleRamer–Douglas–Peucker algorithm: Given a 'curve' composed of line segments to find a curve not too dissimilar but that has fewer pointsShadingGouraud shading: an algorithm to simulate the differing effects of light and colour across the surface of an object in 3D computer graphicsPhong shading: an algorithm to interpolate surface normal-vectors for surface shading in 3D computer graphicsSlerp (spherical linear interpolation): quaternion interpolation for the purpose of animating 3D rotationSummed area table (also known as an integral image): an algorithm for computing the sum of values in a rectangular subset of a grid in constant time=== Cryptography ===Asymmetric (public key) encryption:ElGamalElliptic curve cryptographyMAE1NTRUEncryptRSADigital signatures (asymmetric authentication):DSA, and its variants:ECDSA and Deterministic ECDSAEdDSA (Ed25519)RSACryptographic hash functions (see also the section on message authentication codes):BLAKEMD5 – Note that there is now a method of generating collisions for MD5RIPEMD-160SHA-1 – Note that there is now a method of generating collisions for SHA-1SHA-2 (SHA-224, SHA-256, SHA-384, SHA-512)SHA-3 (SHA3-224, SHA3-256, SHA3-384, SHA3-512, SHAKE128, SHAKE256)Tiger (TTH), usually used in Tiger tree hashesWHIRLPOOLCryptographically secure pseudo-random number generatorsBlum Blum Shub - based on the hardness of factorizationFortuna, intended as an improvement on Yarrow algorithmLinear-feedback shift register (note: many LFSR-based algorithms are weak or have been broken)Yarrow algorithmKey exchangeDiffie–Hellman key exchangeElliptic-curve Diffie-Hellman (ECDH)Key derivation functions, often used for password hashing and key stretchingbcryptPBKDF2scryptArgon2Message authentication codes (symmetric authentication algorithms, which take a key as a parameter):HMAC: keyed-hash message authenticationPoly1305SipHashSecret sharing, Secret Splitting, Key Splitting, M of N algorithmsBlakey's SchemeShamir's SchemeSymmetric (secret key) encryption:Advanced Encryption Standard (AES), winner of NIST competition, also known as RijndaelBlowfishTwofishThreefishData Encryption Standard (DES), sometimes DE Algorithm, winner of NBS selection competition, replaced by AES for most purposesIDEARC4 (cipher)Tiny Encryption Algorithm (TEA)Salsa20, and its updated variant ChaCha20Post-quantum cryptographyProof-of-work algorithms=== Digital logic ===Boolean minimizationQuine–McCluskey algorithm: Also called as Q-M algorithm, programmable method for simplifying the boolean equations.Petrick's method: Another algorithm for boolean simplification.Espresso heuristic logic minimizer: Fast algorithm for boolean function minimization.=== Machine learning and statistical classification ===ALOPEX: a correlation-based machine-learning algorithmAssociation rule learning: discover interesting relations between variables, used in data miningApriori algorithmEclat algorithmFP-growth algorithmOne-attribute ruleZero-attribute ruleBoosting (meta-algorithm): Use many weak learners to boost effectivenessAdaBoost: adaptive boostingBrownBoost:a boosting algorithm that may be robust to noisy datasetsLogitBoost: logistic regression boostingLPBoost: linear programming boostingBootstrap aggregating (bagging): technique to improve stability and classification accuracyComputer VisionGrabcut based on Graph cutsDecision TreesC4.5 algorithm: an extension to ID3ID3 algorithm (Iterative Dichotomiser 3):  Use heuristic to generate small decision treesClustering: Class of unsupervised learning algorithms for grouping and bucketing related input vector.k-nearest neighbors (k-NN): a method for classifying objects based on closest training examples in the feature spaceLinde–Buzo–Gray algorithm: a vector quantization algorithm used to derive a good codebookLocality-sensitive hashing (LSH): a method of performing probabilistic dimension reduction of high-dimensional dataNeural NetworkBackpropagation: A supervised learning method which requires a teacher that knows, or can calculate, the desired output for any given inputHopfield net:  a Recurrent neural network in which all connections are symmetricPerceptron: the simplest kind of feedforward neural network: a linear classifier.Pulse-coupled neural networks (PCNN): Neural models proposed by modeling a cat's visual cortex and developed for high-performance biomimetic image processing.Radial basis function network: an artificial neural network that uses radial basis functions as activation functionsSelf-organizing map: an unsupervised network that produces a low-dimensional representation of the input space of the training samplesRandom forest: classify using many decision treesReinforcement Learning:Q-learning:  learn an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafterState-Action-Reward-State-Action (SARSA): learn a Markov decision process policyTemporal difference learningRelevance Vector Machine (RVM): similar to SVM, but provides probabilistic classificationSupervised Learning: Learning by examples (labelled data-set split into training-set and test-set)Support Vector Machines (SVM): a set of methods which divide multidimensional data by finding a dividing hyperplane with the maximum margin between the two setsStructured SVM: allows training of a classifier for general structured output labels.Winnow algorithm: related to the perceptron, but uses a multiplicative weight-update scheme=== Programming language theory ===C3 linearization: an algorithm used primarily to obtain a consistent linearization of a multiple inheritance hierarchy in object-oriented programmingChaitin's algorithm: a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metricHindley–Milner type inference algorithmRete algorithm: an efficient pattern matching algorithm for implementing production rule systemsSethi-Ullman algorithm: generate optimal code for arithmetic expressions==== Parsing ====CYK algorithm: An O(n3) algorithm for parsing context-free grammars in Chomsky normal formEarley parser: Another O(n3) algorithm for parsing any context-free grammarGLR parser:An algorithm for parsing any context-free grammar by Masaru Tomita. It is tuned for deterministic grammars, on which it performs almost linear time and O(n3) in worst case.Inside-outside algorithm: An O(n3) algorithm for re-estimating production probabilities in probabilistic context-free grammarsLL parser: A relatively simple linear time parsing algorithm for a limited class of context-free grammarsLR parser: A more complex linear time parsing algorithm for a larger class of context-free grammars.  Variants:Canonical LR parserLALR (Look-ahead LR) parserOperator-precedence parserSLR (Simple LR) parserSimple precedence parserPackrat parser: A linear time parsing algorithm supporting some context-free grammars and parsing expression grammarsRecursive descent parser: A top-down parser suitable for LL(k) grammarsShunting yard algorithm: convert an infix-notation math expression to postfixPratt parserLexical analysis=== Quantum algorithms ===Deutsch–Jozsa algorithm: criterion of balance for Boolean functionGrover's algorithm: provides quadratic speedup for many search problemsShor's algorithm: provides exponential speedup (relative to currently known non-quantum algorithms) for factoring a numberSimon's algorithm: provides a provably exponential speedup (relative to any non-quantum algorithm) for a black-box problem=== Theory of computation and automata ===Hopcroft's algorithm, Moore's algorithm, and Brzozowski's algorithm: algorithms for minimizing the number of states in a deterministic finite automatonPowerset construction: Algorithm to convert nondeterministic automaton to deterministic automaton.Tarski–Kuratowski algorithm: a non-deterministic algorithm which provides an upper bound for the complexity of formulas in the arithmetical hierarchy and analytical hierarchy== Information theory and signal processing ===== Coding theory ======= Error detection and correction ====BCH CodesBerlekamp–Massey algorithmPeterson–Gorenstein–Zierler algorithmReed–Solomon error correctionBCJR algorithm: decoding of error correcting codes defined on trellises (principally convolutional codes)Forward error correctionGray codeHamming codesHamming(7,4):  a Hamming code that encodes 4 bits of data into 7 bits by adding 3 parity bitsHamming distance: sum number of positions which are differentHamming weight (population count): find the number of 1 bits in a binary wordRedundancy checksAdler-32Cyclic redundancy checkDamm algorithmFletcher's checksumLongitudinal redundancy check (LRC)Luhn algorithm: a method of validating identification numbersLuhn mod N algorithm: extension of Luhn to non-numeric charactersParity: simple/fast error detection techniqueVerhoeff algorithm==== Lossless compression algorithms ====Burrows–Wheeler transform: preprocessing useful for improving lossless compressionContext tree weightingDelta encoding: aid to compression of data in which sequential data occurs frequentlyDynamic Markov compression: Compression using predictive arithmetic codingDictionary codersByte pair encoding (BPE)DEFLATELempel–ZivLZ77 and LZ78Lempel–Ziv Jeff Bonwick (LZJB)Lempel–Ziv–Markov chain algorithm (LZMA)Lempel–Ziv–Oberhumer (LZO): speed orientedLempel–Ziv–Stac (LZS)Lempel–Ziv–Storer–Szymanski (LZSS)Lempel–Ziv–Welch (LZW)LZWL: syllable-based variantLZXLempel–Ziv Ross Williams (LZRW)Entropy encoding: coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbolsArithmetic coding: advanced entropy codingRange encoding: same as arithmetic coding, but looked at in a slightly different wayHuffman coding: simple lossless compression taking advantage of relative character frequenciesAdaptive Huffman coding: adaptive coding technique based on Huffman codingPackage-merge algorithm: Optimizes Huffman coding subject to a length restriction on code stringsShannon–Fano codingShannon–Fano–Elias coding: precursor to arithmetic encodingEntropy coding with known entropy characteristicsGolomb coding: form of entropy coding that is optimal for alphabets following geometric distributionsRice coding: form of entropy coding that is optimal for alphabets following geometric distributionsTruncated binary encodingUnary coding: code that represents a number n with n ones followed by a zeroUniversal codes: encodes positive integers into binary code wordsElias delta, gamma, and omega codingExponential-Golomb codingFibonacci codingLevenshtein codingFast Efficient & Lossless Image Compression System (FELICS): a lossless image compression algorithmIncremental encoding: delta encoding applied to sequences of stringsPrediction by partial matching (PPM): an adaptive statistical data compression technique based on context modeling and predictionRun-length encoding: lossless data compression taking advantage of strings of repeated charactersSEQUITUR algorithm: lossless compression by incremental grammar inference on a string==== Lossy compression algorithms ====3Dc: a lossy data compression algorithm for normal mapsAudio and Speech compressionA-law algorithm: standard companding algorithmCode-excited linear prediction (CELP): low bit-rate speech compressionLinear predictive coding (LPC): lossy compression by representing the spectral envelope of a digital signal of speech in compressed formMu-law algorithm: standard analog signal compression or companding algorithmWarped Linear Predictive Coding (WLPC)Image compressionBlock Truncation Coding (BTC): a type of lossy image compression technique for greyscale imagesEmbedded Zerotree Wavelet (EZW)Fast Cosine Transform algorithms (FCT algorithms): compute Discrete Cosine Transform (DCT) efficientlyFractal compression: method used to compress images using fractalsSet Partitioning in Hierarchical Trees (SPIHT)Wavelet compression: form of data compression well suited for image compression (sometimes also video compression and audio compression)Transform coding: type of data compression for "natural" data like audio signals or photographic imagesVideo compressionVector quantization: technique often used in lossy data compression=== Digital signal processing ===Adaptive-additive algorithm (AA algorithm): find the spatial frequency phase of an observed wave sourceDiscrete Fourier transform: determines the frequencies contained in a (segment of a) signalBluestein's FFT algorithmBruun's FFT algorithmCooley–Tukey FFT algorithmFast Fourier transformPrime-factor FFT algorithmRader's FFT algorithmFast folding algorithm: an efficient algorithm for the detection of approximately periodic events within time series dataGerchberg–Saxton algorithm: Phase retrieval algorithm for optical planesGoertzel algorithm: identify a particular frequency component in a signal.  Can be used for DTMF digit decoding.Karplus-Strong string synthesis: physical modelling synthesis to simulate the sound of a hammered or plucked string or some types of percussion==== Image processing ====Contrast EnhancementHistogram equalization: use histogram to improve image contrastAdaptive histogram equalization: histogram equalization which adapts to local changes in contrastConnected-component labeling: find and label disjoint regionsDithering and half-toningError diffusionFloyd–Steinberg ditheringOrdered ditheringRiemersma ditheringElser difference-map algorithm: a search algorithm for general constraint satisfaction problems.  Originally used for X-Ray diffraction microscopyFeature detectionCanny edge detector: detect a wide range of edges in imagesGeneralised Hough transformHough transformMarr–Hildreth algorithm: an early edge detection algorithmSIFT (Scale-invariant feature transform): is an algorithm to detect and describe local features in images.SURF (Speeded Up Robust Features): is a robust local feature detector, first presented by Herbert Bay et al. in 2006, that can be used in computer vision tasks like object recognition or 3D reconstruction. It is partly inspired by the SIFT descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.Richardson–Lucy deconvolution: image de-blurring algorithmBlind deconvolution: image de-blurring algorithm when point spread function is unknown.Median filteringSeam carving: content-aware image resizing algorithmSegmentation: partition a digital image into two or more regionsGrowCut algorithm: an interactive segmentation algorithmRandom walker algorithmRegion growingWatershed transformation: a class of algorithms based on the watershed analogy== Software engineering ==Cache algorithmsCHS conversion: converting between disk addressing systemsDouble dabble: Convert binary numbers to BCDHash Function: convert a large, possibly variable-sized amount of data into a small datum, usually a single integer that may serve as an index into an arrayFowler–Noll–Vo hash function: fast with low collision ratePearson hashing: computes 8 bit value only, optimized for 8 bit computersZobrist hashing: used in the implementation of transposition tablesUnicode Collation AlgorithmXor swap algorithm: swaps the values of two variables without using a buffer== Database algorithms ==Algorithms for Recovery and Isolation Exploiting Semantics (ARIES): transaction recoveryJoin algorithmsBlock nested loopHash joinNested loop joinSort-Merge Join== Distributed systems algorithms ==Bully algorithm:  a method for dynamically selecting a coordinatorByzantine fault tolerance: good fault tolerance.Clock synchronizationBerkeley algorithmCristian's algorithmIntersection algorithmMarzullo's algorithmDetection of Process TerminationDijkstra-Scholten algorithmHuang's algorithmLamport ordering: a partial ordering of events based on the happened-before relationMutual exclusionLamport's Distributed Mutual Exclusion AlgorithmNaimi-Trehel's log(n) AlgorithmMaekawa's AlgorithmRaymond's AlgorithmRicart-Agrawala AlgorithmPaxos algorithm: a family of protocols for solving consensus in a network of unreliable processorsSnapshot algorithm: record a consistent global state for an asynchronous systemChandy-Lamport algorithmVector clocks: generate a partial ordering of events in a distributed system and detect causality violations=== Memory allocation and deallocation algorithms ===Buddy memory allocation: Algorithm to allocate memory such that fragmentation is less.Garbage collectorsCheney's algorithm: An improvement on the Semi-space collectorGenerational garbage collector: Fast garbage collectors that segregate memory by ageMark-compact algorithm:  a combination of the mark-sweep algorithm  and Cheney's copying algorithmMark and sweepSemi-space collector: An early copying collectorReference counting== Networking ==Karn's algorithm: addresses the problem of getting accurate estimates of the round-trip time for messages when using TCPLuleå algorithm: a technique for storing and searching internet routing tables efficientlyNetwork congestionExponential backoffNagle's algorithm: improve the efficiency of TCP/IP networks by coalescing packetsTruncated binary exponential backoff== Operating systems algorithms ==Banker's algorithm: Algorithm used for deadlock avoidance.Page replacement algorithms: Selecting the victim page under low memory conditions.Adaptive replacement cache: better performance than LRUClock with Adaptive Replacement (CAR): is a page replacement algorithm that has performance comparable to Adaptive replacement cache=== Process synchronization ===Dekker's algorithmLamport's Bakery algorithmPeterson's algorithm=== Scheduling ===Earliest deadline first schedulingFair-share schedulingLeast slack time schedulingList schedulingMulti level feedback queueRate-monotonic schedulingRound-robin schedulingShortest job nextShortest remaining timeTop-nodes algorithm: resource calendar management=== I/O scheduling ======= Disk scheduling ====Elevator algorithm: Disk scheduling algorithm that works like an elevator.Shortest seek first: Disk scheduling algorithm to reduce seek time.== See also ==List of data structuresList of machine learning algorithmsList of pathfinding algorithmsList of algorithm general topicsList of terms relating to algorithms and data structuresHeuristic== References ==
	In abstract algebra and formal logic, the distributive property of binary operations generalizes the distributive law from Boolean algebra and elementary algebra. In propositional logic, distribution refers to two valid rules of replacement. The rules allow one to reformulate conjunctions and disjunctions within logical proofs.For example, in arithmetic:2 ⋅ (1 + 3) = (2 ⋅ 1) + (2 ⋅ 3), but 2 / (1 + 3) ≠ (2 / 1) + (2 / 3).In the left-hand  side of the first equation, the 2 multiplies the sum of 1 and 3; on the right-hand side, it multiplies the 1 and the 3 individually, with the products added afterwards.Because these give the same final answer (8),  multiplication by 2 is said to distribute over addition of 1 and 3.Since one could have put any real numbers in place of 2, 1, and 3 above, and still have obtained a true equation, multiplication of real numbers distributes over addition of real numbers.== Definition ==Given a set S and two binary operators ∗ and + on S, the operation:∗ is left-distributive over + if, given any elements x, y and z of S,                    x        ∗        (        y        +        z        )        =        (        x        ∗        y        )        +        (        x        ∗        z        )        ,              {\displaystyle x*(y+z)=(x*y)+(x*z),}  ∗ is right-distributive over + if, given any elements x, y, and z of S,                    (        y        +        z        )        ∗        x        =        (        y        ∗        x        )        +        (        z        ∗        x        )        ,              {\displaystyle (y+z)*x=(y*x)+(z*x),}   and∗ is distributive over + if it is left- and right-distributive.Notice that when ∗ is commutative, the three conditions above are logically equivalent.== Meaning ==The operators used for examples in this section are the binary operations of addition (                    +              {\displaystyle +}  ) and multiplication (                    ⋅              {\displaystyle \cdot }  ) of numbers.There is a distinction between left-distributivity and right-distributivity:                    a        ⋅                  (                      b            ±            c                    )                =        a        ⋅        b        ±        a        ⋅        c              {\displaystyle a\cdot \left(b\pm c\right)=a\cdot b\pm a\cdot c}    (left-distributive)                    (        a        ±        b        )        ⋅        c        =        a        ⋅        c        ±        b        ⋅        c              {\displaystyle (a\pm b)\cdot c=a\cdot c\pm b\cdot c}    (right-distributive)In either case, the distributive property can be described in words as:To multiply a sum (or difference) by a factor, each summand (or minuend and subtrahend) is multiplied by this factor and the resulting products are added (or subtracted).If the operation outside the parentheses (in this case, the multiplication) is commutative, then left-distributivity implies right-distributivity  and vice versa.One example of an operation that is "only" right-distributive is division, which is not commutative:                    (        a        ±        b        )        ÷        c        =        a        ÷        c        ±        b        ÷        c              {\displaystyle (a\pm b)\div c=a\div c\pm b\div c}  In this case, left-distributivity does not apply:                    a        ÷        (        b        ±        c        )        ≠        a        ÷        b        ±        a        ÷        c              {\displaystyle a\div (b\pm c)\neq a\div b\pm a\div c}  The distributive laws are among the axioms for rings (like the ring of integers) and fields (like the field of rational numbers). Here multiplication is distributive over addition, but addition is not distributive over multiplication. Examples of structures in which two operations are mutually related to each other by the distributive law (e. g., they distribute over each other) are Boolean algebras such as the algebra of sets or the switching algebra.Multiplying sums can be put into words as follows: When a sum is multiplied by a sum, multiply each summand of a sum with each summand of the other sums (keeping track of signs), and then adding up all of the resulting products.== Examples ===== Real numbers ===In the following examples, the use of the distributive law on the set of real numbers                               R                      {\displaystyle \mathbb {R} }   is illustrated. When multiplication is mentioned in elementary mathematics, it usually refers to this kind of multiplication. From the point of view of algebra, the real numbers form a field, which ensures the validity of the distributive law.First example (mental and written multiplication)During mental arithmetic, distributivity is often used unconsciously:                    6        ⋅        16        =        6        ⋅        (        10        +        6        )        =        6        ⋅        10        +        6        ⋅        6        =        60        +        36        =        96              {\displaystyle 6\cdot 16=6\cdot (10+6)=6\cdot 10+6\cdot 6=60+36=96}  Thus, to calculate 6 ⋅ 16 in one's head, one first multiplies 6 ⋅ 10 and 6 ⋅ 6 and add the intermediate results. Written multiplication is also based on the distributive law.Second example (with variables)                    3                  a                      2                          b        ⋅        (        4        a        −        5        b        )        =        3                  a                      2                          b        ⋅        4        a        −        3                  a                      2                          b        ⋅        5        b        =        12                  a                      3                          b        −        15                  a                      2                                    b                      2                                {\displaystyle 3a^{2}b\cdot (4a-5b)=3a^{2}b\cdot 4a-3a^{2}b\cdot 5b=12a^{3}b-15a^{2}b^{2}}  Third example (with two sums)                                                                        (                a                +                b                )                ⋅                (                a                −                b                )                                                            =                a                ⋅                (                a                −                b                )                +                b                ⋅                (                a                −                b                )                =                                  a                                      2                                                  −                a                b                +                b                a                −                                  b                                      2                                                  =                                  a                                      2                                                  −                                  b                                      2                                                                                                                                    =                (                a                +                b                )                ⋅                a                −                (                a                +                b                )                ⋅                b                =                                  a                                      2                                                  +                b                a                −                a                b                −                                  b                                      2                                                  =                                  a                                      2                                                  −                                  b                                      2                                                                                            {\displaystyle {\begin{aligned}(a+b)\cdot (a-b)&=a\cdot (a-b)+b\cdot (a-b)=a^{2}-ab+ba-b^{2}=a^{2}-b^{2}\\&=(a+b)\cdot a-(a+b)\cdot b=a^{2}+ba-ab-b^{2}=a^{2}-b^{2}\end{aligned}}}  Here the distributive law was applied twice, and it does not matter which bracket is first multiplied out.Fourth ExampleHere the distributive law is applied the other way around compared to the previous examples. Consider                    12                  a                      3                                    b                      2                          −        30                  a                      4                          b        c        +        18                  a                      2                                    b                      3                                    c                      2                                  .              {\displaystyle 12a^{3}b^{2}-30a^{4}bc+18a^{2}b^{3}c^{2}\,.}  Since the factor                     6                  a                      2                          b              {\displaystyle 6a^{2}b}   occurs in all summands, it can be factored out. That is, due to the distributive law one obtains                    12                  a                      3                                    b                      2                          −        30                  a                      4                          b        c        +        18                  a                      2                                    b                      3                                    c                      2                          =        6                  a                      2                          b        (        2        a        b        −        5                  a                      2                          c        +        3                  b                      2                                    c                      2                          )                .              {\displaystyle 12a^{3}b^{2}-30a^{4}bc+18a^{2}b^{3}c^{2}=6a^{2}b(2ab-5a^{2}c+3b^{2}c^{2})\,.}  === Matrices ===The distributive law is valid for matrix multiplication. More precisely,                    (        A        +        B        )        ⋅        C        =        A        ⋅        C        +        B        ⋅        C              {\displaystyle (A+B)\cdot C=A\cdot C+B\cdot C}  for all                      l        ×        m              {\displaystyle l\times m}  -matrices                     A        ,        B              {\displaystyle A,B}   and                     m        ×        n              {\displaystyle m\times n}  -matrices                     C              {\displaystyle C}  , as well as                    A        ⋅        (        B        +        C        )        =        A        ⋅        B        +        A        ⋅        C              {\displaystyle A\cdot (B+C)=A\cdot B+A\cdot C}  for all                      l        ×        m              {\displaystyle l\times m}  -matrices                     A              {\displaystyle A}   and                     m        ×        n              {\displaystyle m\times n}  -matrices                     B        ,        C              {\displaystyle B,C}  .  Because the commutative property does not hold for matrix multiplication, the second law does not follow from the first law. In this case, they are two different laws.=== Other examples ===Multiplication of ordinal numbers, in contrast, is only left-distributive, not right-distributive.The cross product is left- and right-distributive over vector addition, though not commutative.The union of sets is distributive over intersection, and intersection is distributive over union.Logical disjunction ("or") is distributive over logical conjunction ("and"), and vice versa.For real numbers (and for any totally ordered set), the maximum operation is distributive over the minimum operation, and vice versa: max(a, min(b, c)) = min(max(a, b), max(a, c)) and min(a, max(b, c)) = max(min(a, b), min(a, c)).For integers, the greatest common divisor is distributive over the least common multiple, and vice versa: gcd(a, lcm(b, c)) = lcm(gcd(a, b), gcd(a, c)) and lcm(a, gcd(b, c)) = gcd(lcm(a, b), lcm(a, c)).For real numbers, addition distributes over the maximum operation, and also over the minimum operation: a + max(b, c) = max(a + b, a + c) and a + min(b, c) = min(a + b, a + c).For binomial multiplication, distribution is sometimes referred to as the FOIL Method (First terms ac, Outer ad, Inner bc, and Last bd) such as: (a + b) · (c + d) =  ac + ad + bc + bd.Polynomial multiplication is similar to that for binomials: (a + b) · (c + d + e) =  ac + ad + ae + bc + bd + be.Complex number multiplication is distributive:                     u        (        v        +        w        )        =        u        v        +        u        w        ,        (        u        +        v        )        w        =        u        w        +        v        w              {\displaystyle u(v+w)=uv+uw,(u+v)w=uw+vw}  == Propositional logic ===== Rule of replacement ===In standard truth-functional propositional logic, distribution in logical proofs uses two valid rules of replacement to expand individual occurrences of certain logical connectives, within some formula, into separate applications of those connectives across subformulas of the given formula. The rules are                    (        P        ∧        (        Q        ∨        R        )        )        ⇔        (        (        P        ∧        Q        )        ∨        (        P        ∧        R        )        )              {\displaystyle (P\land (Q\lor R))\Leftrightarrow ((P\land Q)\lor (P\land R))}  and                     (        P        ∨        (        Q        ∧        R        )        )        ⇔        (        (        P        ∨        Q        )        ∧        (        P        ∨        R        )        )              {\displaystyle (P\lor (Q\land R))\Leftrightarrow ((P\lor Q)\land (P\lor R))}  where "                    ⇔              {\displaystyle \Leftrightarrow }  ", also written ≡, is a metalogical symbol representing "can be replaced in a proof with" or "is logically equivalent to".=== Truth functional connectives ===Distributivity is a property of some logical connectives of truth-functional propositional logic. The following logical equivalences demonstrate that distributivity is a property of particular connectives. The following are truth-functional tautologies.Distribution of conjunction over conjunction                    (        P        ∧        (        Q        ∧        R        )        )        ↔        (        (        P        ∧        Q        )        ∧        (        P        ∧        R        )        )              {\displaystyle (P\land (Q\land R))\leftrightarrow ((P\land Q)\land (P\land R))}  Distribution of conjunction over disjunction                    (        P        ∧        (        Q        ∨        R        )        )        ↔        (        (        P        ∧        Q        )        ∨        (        P        ∧        R        )        )              {\displaystyle (P\land (Q\lor R))\leftrightarrow ((P\land Q)\lor (P\land R))}  Distribution of disjunction over conjunction                    (        P        ∨        (        Q        ∧        R        )        )        ↔        (        (        P        ∨        Q        )        ∧        (        P        ∨        R        )        )              {\displaystyle (P\lor (Q\land R))\leftrightarrow ((P\lor Q)\land (P\lor R))}  Distribution of disjunction over disjunction                    (        P        ∨        (        Q        ∨        R        )        )        ↔        (        (        P        ∨        Q        )        ∨        (        P        ∨        R        )        )              {\displaystyle (P\lor (Q\lor R))\leftrightarrow ((P\lor Q)\lor (P\lor R))}  Distribution of implication                    (        P        →        (        Q        →        R        )        )        ↔        (        (        P        →        Q        )        →        (        P        →        R        )        )              {\displaystyle (P\to (Q\to R))\leftrightarrow ((P\to Q)\to (P\to R))}  Distribution of implication over equivalence                    (        P        →        (        Q        ↔        R        )        )        ↔        (        (        P        →        Q        )        ↔        (        P        →        R        )        )              {\displaystyle (P\to (Q\leftrightarrow R))\leftrightarrow ((P\to Q)\leftrightarrow (P\to R))}  Distribution of disjunction over equivalence                    (        P        ∨        (        Q        ↔        R        )        )        ↔        (        (        P        ∨        Q        )        ↔        (        P        ∨        R        )        )              {\displaystyle (P\lor (Q\leftrightarrow R))\leftrightarrow ((P\lor Q)\leftrightarrow (P\lor R))}  Double distribution                                                                        (                (                P                ∧                Q                )                ∨                (                R                ∧                S                )                )                                                            ↔                (                (                (                P                ∨                R                )                ∧                (                P                ∨                S                )                )                ∧                (                (                Q                ∨                R                )                ∧                (                Q                ∨                S                )                )                )                                                                    (                (                P                ∨                Q                )                ∧                (                R                ∨                S                )                )                                                            ↔                (                (                (                P                ∧                R                )                ∨                (                P                ∧                S                )                )                ∨                (                (                Q                ∧                R                )                ∨                (                Q                ∧                S                )                )                )                                                          {\displaystyle {\begin{aligned}((P\land Q)\lor (R\land S))&\leftrightarrow (((P\lor R)\land (P\lor S))\land ((Q\lor R)\land (Q\lor S)))\\((P\lor Q)\land (R\lor S))&\leftrightarrow (((P\land R)\lor (P\land S))\lor ((Q\land R)\lor (Q\land S)))\end{aligned}}}  == Distributivity and rounding ==In practice, the distributive property of multiplication (and division) over addition may appear to be compromised or lost because of the limitations of arithmetic precision.  For example, the identity ⅓ + ⅓ + ⅓ = (1 + 1 + 1) / 3 appears to fail if the addition is conducted in decimal arithmetic; however, if many significant digits are used, the calculation will result in a closer approximation to the correct results.  For example, if the arithmetical calculation takes the form: 0.33333 + 0.33333 + 0.33333 = 0.99999 ≠ 1, this result is a closer approximation than if fewer significant digits had been used.  Even when fractional numbers can be represented exactly in arithmetical form, errors will be introduced if those arithmetical values are rounded or truncated.  For example, buying two books, each priced at £14.99 before a tax of 17.5%, in two separate transactions will actually save £0.01, over buying them together: £14.99 × 1.175 = £17.61 to the nearest £0.01, giving a total expenditure of £35.22, but £29.98 × 1.175 = £35.23.  Methods such as banker's rounding may help in some cases, as may increasing the precision used, but ultimately some calculation errors are inevitable.== Distributivity in rings ==Distributivity is most commonly found in rings and distributive lattices.A ring has two binary operations, + and * (commonly), and one of the requirements of a ring is that ∗ must distribute over +.Most kinds of numbers (example 1) and matrices (example 4) form rings.A lattice is another kind of algebraic structure with two binary operations, ∧ and ∨.If either of these operations (say ∧) distributes over the other (∨), then ∨ must also distribute over ∧, and the lattice is called distributive. See also the article on distributivity (order theory).Examples 4 and 5 are Boolean algebras, which can be interpreted either as a special kind of ring (a Boolean ring) or a special kind of distributive lattice (a Boolean lattice). Each interpretation is responsible for different distributive laws in the Boolean algebra. Examples 6 and 7 are distributive lattices which are not Boolean algebras.Failure of one of the two distributive laws brings about near-rings and near-fields instead of rings and division rings respectively. The operations are usually configured to have the near-ring or near-field distributive on the right but not on the left.Rings and distributive lattices are both special kinds of rigs, certain generalizations of rings.Those numbers in example 1 that don't form rings at least form rigs.Near-rigs are a further generalization of rigs that are left-distributive but not right-distributive; example 2 is a near-rig.== Generalizations of distributivity ==In several mathematical areas, generalized distributivity laws are considered. This may involve the weakening of the above conditions or the extension to infinitary operations. Especially in order theory one finds numerous important variants of distributivity, some of which include infinitary operations, such as the infinite distributive law; others being defined in the presence of only one binary operation, such as the according definitions and their relations are given in the article distributivity (order theory). This also includes the notion of a completely distributive lattice.In the presence of an ordering relation, one can also weaken the above equalities by replacing = by either ≤ or ≥. Naturally, this will lead to meaningful concepts only in some situations. An application of this principle is the notion of sub-distributivity as explained in the article on interval arithmetic.In category theory, if (S, μ, η) and (S′, μ′, η′) are monads on a category C, a distributive law S.S′ → S′.S is a natural transformation λ : S.S′ → S′.S such that (S′, λ) is a lax map of monads S → S and (S, λ) is a colax map of monads S′ → S′. This is exactly the data needed to define a monad structure on S′.S: the multiplication map is S′μ.μ′S2.S′λS and the unit map is η′S.η. See: distributive law between monads.A generalized distributive law has also been proposed in the area of information theory.=== Notions of antidistributivity ===The ubiquitous identity that relates inverses to the binary operation in any group, namely (xy)−1 = y−1x−1, which is taken as an axiom in the more general context of a semigroup with involution, has sometimes been called an antidistributive property (of inversion as a unary operation).In the context of a near-ring, which removes the commutativity of the additively written group and assumes only one-sided distributivity, one can speak of (two-sided) distributive elements but also of antidistributive elements. The latter reverse the order of (the non-commutative) addition; assuming a left-nearring (i.e. one which all elements distribute when multiplied on the left), then an antidistributive element a reverses the order of addition when multiplied to the right: (x + y)a = ya + xa.In the study of propositional logic and Boolean algebra, the term antidistributive law is sometimes used to denote the interchange between conjunction and disjunction when implication factors over them:(a ∨ b) ⇒ c  ≡  (a ⇒ c) ∧ (b ⇒ c)(a ∧ b) ⇒ c  ≡  (a ⇒ c) ∨ (b ⇒ c)These two tautologies are a direct consequence of the duality in De Morgan's laws.== Notes ==== External links ==A demonstration of the Distributive Law for integer arithmetic (from cut-the-knot)
	AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. It is most effective in cases when there is inband noise present. In those cases AVT is better at filtering data then, band-pass filter or any digital filtering based on variation of.Conventional filtering is useful when signal/data has different frequency than noise and signal/data is separated/filtered by frequency discrimination of noise. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Those filters are created using passive and active components and sometimes are implemented using software algorithms based on FFT.AVT filtering is implemented in software and its inner working is based on statistical analysis of raw data.When signal frequency/(useful data distribution frequency) coincides with noise frequency/(noisy data distribution frequency) we have inband noise. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable and where AVT excels. To achieve filtering in such conditions there are several methods/algorithms available which are briefly described below.== Averaging Algorithm ==Collect n samples of dataCalculate average value of collected dataPresent/record result as actual data== Median Algorithm ==Collect n samples of dataSort the data in ascending or descending order. Note that order does not matterSelect the data that happen to be in n/2 position and present/record it as final result representing data sample== AVT Algorithm ==AVT algorithm stands for Antonyan Vardan Transform and its implementation explained below.Collect n samples of dataCalculate the standard deviation and average valueDrop any data that is greater or less than average ± one standard deviationCalculate average value of remaining dataPresent/record result as actual value representing data sampleThis algorithm is based on amplitude discrimination and can easily reject any noise that is not like actual signal, otherwise statistically different then 1 standard deviation of the signal. Note that this type of filtering can be used in situations where the actual environmental noise is not known in advance.== Filtering algorithms comparison ==Using a system that has signal value of 1 and has noise added at 0.1% and 1% levels will simplify quantification of algorithm performance. The R script is used to create pseudo random noise added to signal and analyze the results of filtering using several algorithms. Please refer to "Reduce Inband Noise with the AVT Algorithm"  article for details.This graphs show that AVT algorithm provides best results compared with Median and Averaging algorithms while using data sample size of 32, 64 and 128 values. Note that this graph was created by analyzing random data array of 10000 values. Sample of this data is graphically represented below.== AVT algorithm variations ===== Cascaded AVT ===In some situations better results can be obtained by cascading several stages of AVT filtering. This will produce singular constant value which can be used for equipment that has known stable characteristics like thermometers, thermistors and other slow acting sensors.=== Reverse AVT ===Collect n samples of dataCalculate the standard deviation and average valueDrop any data that is within one standard deviation ± average bandCalculate average value of remaining dataPresent/record result as actual dataThis is useful for detecting minute signals that are close to background noise level.== Possible applications and uses ==Used in planet detection to filter out raw data from Kepler (spacecraft)Filter out noise from sound sources where all other filtering methods (Low-pass filter, High-pass filter, Band-pass filter, Digital filter) fail.Preprocess scientific data for data analysis (Smoothness) before plotting see (Plot (graphics))Used in SETI (Search for extraterrestrial intelligence) for detecting/distinguishing extraterrestrial signals from cosmic background== References ==
	A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare.  Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty. A medical prescription is also a type of medical algorithm.== Scope ==Medical algorithms are part of a broader field which is usually fit under the aims of medical informatics and medical decision-making. Medical decisions occur in several areas of medical activity including medical test selection, diagnosis, therapy and prognosis, and automatic control of medical equipment.In relation to logic-based and artificial neural network-based clinical decision support systems, which are also computer applications used in the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.== Examples ==A wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple calculations to complex outcome predictions.  Most clinicians use only a small subset routinely.Examples of medical algorithms are:Calculators, e.g. an on-line or stand-alone calculator for body mass index (BMI) when stature and body weight are given;Flowcharts and drakon-charts, e.g. a binary decision tree for deciding what is the etiology of chest painLook-up tables, e.g. for looking up food energy and nutritional contents of foodstuffsNomograms, e.g. a moving circular slide to calculate body surface area or drug dosages.A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the Texas Medication Algorithm Project or otherwise working on their production.A grammar—the Arden syntax—exists for describing algorithms in terms of medical logic modules. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.== Purpose ==The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of medical care. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm automation intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example critical care scoring systems.Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to evidence-based guidelines, and be a resource for education and research.Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as protocols and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.== Cautions ==In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.Computations obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and physician judgment.== See also ==Artificial intelligence in healthcareMedical guidelineOdds algorithm== Further reading ==Johnson, Kathy A.; Svirbely, John R.; Sriram, M.G.; Smith, Jack W.; Kantor, Gareth; Rodriguez, Jorge Raul (November 2002). "Automated Medical Algorithms:  Issues for Medical Errors". Journal of the American Medical Informatics Association. 9 (6 Suppl 1): s56–s57. doi:10.1197/jamia.M1228. PMC 419420.
	In mathematics and computer science, an algorithm ( (listen)) is a sequence of instructions, typically to solve a class of problems or perform a computation. Algorithms are unambiguous specifications for performing calculation, data processing, automated reasoning, and other tasks.As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, was used by ancient Babylonian mathematicians circa 2500 BC and Egyptian mathematicians circa 1550 BC. Greek mathematicians later used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as Al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.The word algorithm itself is derived from the 9th-century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized Algoritmi. A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.== Etymology ==The word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to algorismus. Al-Khwārizmī (Arabic: الخوارزمي‎,  Persian: خوارزمی‎, c. 780–850) was a Persian  mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan.About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the "decimal number system". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.which translates as:Algorism is the art by which at present we use those Indian figures, which number two times five.The poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.== Informal definition ==An informal definition could be "a set of rules that precisely defines a sequence of operations", which would include all computer programs, including programs that do not perform numeric calculations, and (for example) any prescribed bureaucratic procedure.Generally, a program is only an algorithm if it stops eventually.A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:No human being can write fast enough, or long enough, or small enough† ( †"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n – two arbitrary "input variables" m and n that produce an output y. But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):Precise instructions (in language understood by "the computer") for a fast, efficient, "good" process that specifies the "moves" of "the computer" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and "effectively" produce, in a "reasonable" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.== Formalization ==Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000): Minsky: "But we will also maintain, with Turing ... that any procedure which could "naturally" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute". Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage [1987], an algorithm is a computational process defined by a Turing machine".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case. This is because of a major theorem of Computability Theory known as the Halting Problem.Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.For some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by flow of control.So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.=== Expressing algorithms ===Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).Representations of algorithms can be classed into three accepted levels of Turing machine description:1 High-level description"...prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head."2 Implementation description"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function."3 Formal descriptionMost detailed, "lowest level", gives the Turing machine's "state table".For an example of the simple algorithm "Add m+n" described in all three levels, see Algorithm#Examples.== Design ==Algorithm design refers to a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.One of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.Typical steps in the development of algorithms:Problem definitionDevelopment of a modelSpecification of the algorithmDesigning an algorithmChecking the correctness of the algorithmAnalysis of algorithmImplementation of algorithmProgram testingDocumentation preparation== Implementation ==Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.== Computer algorithms ==In computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended "target" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology."Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:Knuth: " ... we want good algorithms in some loosely defined aesthetic sense. One criterion ... is the length of time taken to perform the algorithm .... Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc"Chaitin: " ... a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does"Chaitin prefaces his definition with: "I'll show you can't prove that a program is 'elegant'"—such a proof would solve the Halting problem (ibid).Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.Computers (and computors), models of computation: A computer (or human "computor") is a restricted type of machine, a "discrete deterministic mechanical device" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's "abacus" model in his "Very Simple Bases for Computability". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write "code" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction " Z ← 0 "; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.== Examples ===== Algorithm example ===One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:High-level description:If there are no numbers in the set then there is no highest number.Assume the first number in the set is the largest number in the set.For each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.When there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:=== Euclid's algorithm ===Euclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII ("Elementary Number Theory") of his Elements. Euclid poses the problem thus: "Given two numbers not prime to one another, to find their greatest common measure". He defines "A number [to be] a multitude composed of units": a counting number, a positive integer not including zero. To "measure" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the "modulus", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.==== Computer language for Euclid's algorithm ====Only a few instruction types are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.A location is symbolized by upper case letter(s), e.g. S, A, etc.The varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.==== An inelegant program for Euclid's algorithm ====The following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:INPUT:1 [Into two locations L and S put the numbers l and s that represent the two lengths]:  INPUT L, S2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:  R ← LE0: [Ensure r ≥ s.]3 [Ensure the smaller of the two numbers is in S and the larger in R]:  IF R > S THEN    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:    GOTO step 6  ELSE    swap the contents of R and S.4   L ← R (this first step is redundant, but is useful for later discussion).5   R ← S6   S ← LE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.7 IF S > R THEN    done measuring so    GOTO 10  ELSE    measure again,8   R ← R − S9   [Remainder-loop]:    GOTO 7.E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.10 IF R = 0 THEN     done so     GOTO step 15   ELSE     CONTINUE TO step 11,E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.11  L ← R12  R ← S13  S ← L14  [Repeat the measuring process]:    GOTO 7OUTPUT:15 [Done. S contains the greatest common divisor]:   PRINT SDONE:16 HALT, END, STOP.==== An elegant program for Euclid's algorithm ====The following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by "Inelegant"; worse, "Inelegant" requires more types of instructions. The flowchart of "Elegant" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.How "Elegant" works: In place of an outer "Euclid loop", "Elegant" shifts back and forth between two "co-loops", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the "sense" of the subtraction reverses.The following version can be used with Object Oriented languages:=== Testing the Euclid algorithms ===Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.But "exceptional cases" must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.=== Measuring and improving the Euclid algorithms ===Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.Can the algorithms be improved?: Once the programmer judges a program "fit" and "effective"—that is, it computes the function intended by its author—then the question becomes, can it be improved?The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.== Algorithmic analysis ==It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.=== Formal versus empirical ===The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.=== Execution efficiency ===To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.== Classification ==There are various ways to classify algorithms, each with its own merits.=== By implementation ===One way to classify algorithms is by implementation means.RecursionA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.LogicalAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.Serial, parallel or distributedAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms and are called inherently serial problems.Deterministic or non-deterministicDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.Exact or approximateWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.Quantum algorithmThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.=== By design paradigm ===Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:Brute-force or exhaustive searchThis is the naive method of trying every possible solution to see which is best.Divide and conquerA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.Search and enumerationMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.Randomized algorithmSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexityThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.Back trackingIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.=== Optimization problems ===For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:Linear programmingWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.Dynamic programmingWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.The greedy methodA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.The heuristic methodIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.=== By field of study ===Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.=== By complexity ===Algorithms can be classified by the amount of time they need to complete compared to their input size:Constant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.Linear time: if the time is proportional to the input size. E.g. the traverse of a list.Logarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.Polynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.Exponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.== Continuous algorithms ==The adjective "continuous" when applied to the word "algorithm" can mean:An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; orAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.== Legal issues ==Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).== History: Development of the notion of "algorithm" ===== Ancient Near East ===The earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).=== Discrete and distinguishable symbols ===Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.=== Manipulation of symbols as "place holders" for numbers: algebra ===Muhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms "algorism" and "algorithm" are derived from the name al-Khwārizmī, while the term "algebra" is derived from the book Al-jabr. In Europe, the word "algorithm" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.=== Cryptographic algorithms ===The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.=== Mechanical contrivances with discrete states ===The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.Logical machines 1870 – Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc] ...". With this machine he could analyze a "syllogism or any other simple logical argument".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and "telephone switching technologies" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as "dots and dashes" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".Davis (2000) observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):It was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned."=== Mathematics during the 19th century up to the mid-20th century ===Symbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".But Heijenoort gives Frege (1879) this kudos: Frege's is "perhaps the most important single work ever written in logic. ... in which we see a " 'formula language', that is a lingua characterica, a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine"—in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S.C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".=== Emil Post (1936) and Alan Turing (1936–37, 1939) ===Emil Post (1936) described the actions of a "computer" (human being) as follows:"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be"a two-way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke."One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes..."A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]". See more at Post–Turing machineAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.Turing—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers."Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite..."The behavior of the computer at any moment is determined by the symbols which he is observing, and his "state of mind" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite..."Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided."Turing's reduction yields the following:"The simple operations must therefore include:"(a) Changes of the symbol on one of the observed squares"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares."It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:"(A) A possible change (a) of symbol together with a possible change of state of mind."(B) A possible change (b) of observed squares, together with a possible change of state of mind""We may now construct a machine to do the work of this computer."A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:"A function is said to be "effectively calculable" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability † with effective calculability ... ."† We shall use the expression "computable function" to mean a function calculable by a machine, and we let "effectively calculable" refer to the intuitive idea without particular identification with any one of these definitions".=== J.B. Rosser (1939) and S.C. Kleene (1943) ===J. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one." (Rosser 1939:225–226)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.Stephen C. Kleene defined as his now-famous "Thesis I" known as the Church–Turing thesis. But he did this in the following context (boldface in original):"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, "yes" or "no," to the question, "is the predicate value true?"" (Kleene 1943:273)=== History after 1950 ===A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.== See also ==== Notes ==== Bibliography ==== Further reading ==== External links ==Hazewinkel, Michiel, ed. (2001) [1994], "Algorithm", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4Algorithms at CurlieWeisstein, Eric W. "Algorithm". MathWorld.Dictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony BrookCollected Algorithms of the ACM – Association for Computing MachineryThe Stanford GraphBase – Stanford University
	A hyphenation algorithm is a set of rules, especially one codified for implementation in a computer program, that decides at which points a word can be broken over two lines with a hyphen. For example, a hyphenation algorithm might decide that impeachment can be broken as impeach-ment or im-peachment but not impe-achment.One of the reasons for the complexity of the rules of word-breaking is that different "dialects" of English tend to differ on hyphenation: American English tends to work on sound, but British English tends to look to the origins of the word and then to sound. There are also a large number of exceptions, which further complicates matters.Some rules of thumb can be found in the Major Keary's: "On Hyphenation – Anarchy of Pedantry."  Among the algorithmic approaches to hyphenation, the one implemented in the TeX typesetting system is widely used. It is thoroughly documented in the first two volumes of Computers and Typesetting and in Francis Mark Liang's dissertation. The aim of Liang's work was to get the algorithm as accurate as he practically could and to keep any exception dictionary small.In TeX's original hyphenation patterns for American English, the exception list contains only 14 words.== In TeX ==Ports of the TeX hyphenation algorithm are available as libraries for several programming languages, including Haskell, JavaScript, Perl, PostScript, Python, Ruby, C#, and TeX can be made to show hyphens in the log by the command \showhyphens.In LaTeX, hyphenation correction can be added by users by using:\hyphenation{words}The \hyphenation command declares allowed hyphenation points in which words is a list of words, separated by spaces, in which each hyphenation point is indicated by a - character. For example,\hyphenation{fortran er-go-no-mic}declares that in the current job "fortran" should not be hyphenated and that if "ergonomic" must be hyphenated, it will be at one of the indicated points.However, there are several limits. For example, the stock \hyphenation command accepts only ASCII letters by default and so it cannot be used to correct hyphenation for words with non-ASCII characters (like ä, é, ç), which are very common in almost all languages except English. Simple workarounds exist, however.== References =="TeX hyphenation patterns". TeX Users Group (TUG)."TeX-Hyphen". Comprehensive Perl Archive Network. Retrieved October 18, 2005."text-hyphen". RubyForge. Retrieved October 18, 2005."Knuth-Liang hyphenation for the PostScript language". anastigmatix.net. Retrieved October 6, 2005."TeXHyphenator-J: TeX Hyphenator in Java". Retrieved September 14, 2006."Hyphenation in Python, using Frank Liang's algorithm". Retrieved July 10, 2007."Hyphenator.js-Hyphenation in JavaScript, using Frank Liang's algorithm". Retrieved July 28, 2015."Tex::Hyphen - Perl implementation of TeX82 Hyphenation rules"."phpSyllable - PHP implementation of Frank Liang's algorithm"."Hyphenator - JavaScript implementation of Frank Liang's algorithm"."NHyphenator - C# version of Knuth-Liang hyphenation algorithm".== External links ==Test TeX/OpenOffice hyphenation algorithm onlineText & Information Processing (TIP), On-line Hyphenator (hyphenates Spanish)
	In mathematics, the Sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit.It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, 2. The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.The earliest known reference to the sieve (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous) is in Nicomachus of Gerasa's Introduction to Arithmetic, which describes it and attributes it to Eratosthenes of Cyrene, a Greek mathematician.One of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.== Overview ==A prime number is a natural number that has exactly two distinct natural number divisors: 1 and itself.To find all the prime numbers less than or equal to a given integer n by Eratosthenes' method:Create a list of consecutive integers from 2 through n: (2, 3, 4, ..., n).Initially, let p equal 2, the smallest prime number.Enumerate the multiples of p by counting in increments of p from 2p to n, and mark them in the list (these will be 2p, 3p, 4p, ...; the p itself should not be marked).Find the first number greater than p in the list that is not marked. If there was no such number, stop. Otherwise, let p now equal this new number (which is the next prime), and repeat from step 3.When the algorithm terminates, the numbers remaining not marked in the list are all the primes below n.The main idea here is that every value given to p will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).As a refinement, it is sufficient to mark the numbers in step 3 starting from p2, as all the smaller multiples of p will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when p2 is greater than n.Another refinement is to initially list odd numbers only, (3, 5, ..., n), and count in increments of 2p from p2 in step 3, thus marking only odd multiples of p. This actually appears in the original algorithm.  This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of p are generated that are coprime with those small primes, in the first place.=== Example ===To find all the prime numbers less than or equal to 30, proceed as follows.First, generate a list of integers from 2 to 30: 2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30The first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list): 2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30The next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list): 2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30The next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5): 2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30The next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30: 2  3     5     7           11    13          17    19          23                29== Algorithm and variants ===== Pseudocode ===The sieve of Eratosthenes can be expressed in pseudocode, as follows: Input: an integer n > 1.  Let A be an array of Boolean values, indexed by integers 2 to n, initially all set to true.  for i = 2, 3, 4, ..., not exceeding √n:   if A[i] is true:     for j = i2, i2+i, i2+2i, i2+3i, ..., not exceeding n:       A[j] := false.  Output: all i such that A[i] is true.This algorithm produces all primes not greater than n. It includes a common optimization, which is to start enumerating the multiples of each prime i from i2. The time complexity of this algorithm is O(n log log n), provided the array update is an O(1) operation, as is usually the case.=== Segmented sieve ===As Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements. For large n, the range of primes may not fit in memory; worse, even for moderate n, its cache use is highly suboptimal. The algorithm walks through the entire array A, exhibiting almost no locality of reference.A solution to these problems is offered by segmented sieves, where only portions of the range are sieved at a time. These have been known since the 1970s, and work as follows:Divide the range 2 through n into segments of some size Δ ≤ √n.Find the primes in the first (i.e. the lowest) segment, using the regular sieve.For each of the following segments, in increasing order, with m being the segment's topmost value, find the primes in it as follows:Set up a Boolean array of size Δ, andEliminate from it the multiples of each prime p ≤ √m found so far, by calculating the lowest multiple of p between m - Δ and m, and enumerating its multiples in steps of p as usual, marking the corresponding positions in the array as non-prime.If Δ is chosen to be √n, the space complexity of the algorithm is O(√n), while the time complexity is the same as that of the regular sieve.For ranges with upper limit n so large that the sieving primes below √n as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the sieve of Sorenson can be used instead.=== Incremental sieve ===An incremental formulation of the sieve generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime p are generated directly by counting up from the square of the prime in increments of p (or 2p for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the dataflow paradigm as  primes = [2, 3, ...] \ [[p², p²+p, ...] for p in primes],using list comprehension notation with \ denoting set subtraction of arithmetic progressions of numbers.Primes can also be produced by iteratively sieving out the composites through divisibility testing by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical complexity than that of the sieve of Eratosthenes in generating ranges of primes.When testing each prime, the optimal trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes "for free", between the composites. The widely known 1975 functional sieve code by David Turner is often presented as an example of the sieve of Eratosthenes but is actually a sub-optimal trial division sieve.== Computational analysis ==The work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or                    n                  ∑                      p            ≤            n                                                1            p                          ,              {\displaystyle n\sum _{p\leq n}{\frac {1}{p}},}  where n is the sieving range in this and all further analysis.By rearranging Mertens' second theorem, this is equal to n ( log log n + M ) as n approaches infinity, where M is the Meissel–Mertens constant of about 0.2614972128476427837554268386086958590516...The optimization of starting at the square of each prime and only culling for primes less than the square root changes the "n" in the above expression to √n (or n1/2) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first x primes is 1/2x2 log x and the prime number theorem says that x is approximately x/log x, then the sum of primes to n is n2/2 log n, and therefore the sum of base primes to √n is 1/log n expressed as a factor of n. The extra offset of two per base prime is 2π(√n), where π is the prime-counting function in this case, or 4√n/log n; expressing this as a factor of n as are the other terms, this is 4/√n log n. Combining all of this, the expression for the number of optimized operations without wheel factorization is                    log        ⁡        log        ⁡        n        −                              1                          log              ⁡              n                                                (                      1            −                                          4                                  n                                                              )                +        M        −        log        ⁡        2.              {\displaystyle \log \log n-{\frac {1}{\log n}}\left(1-{\frac {4}{\sqrt {n}}}\right)+M-\log 2.}  For the wheel factorization cases, there is a further offset of the operations not done of                              ∑                      p            ≤            x                                                1            p                                {\displaystyle \sum _{p\leq x}{\frac {1}{p}}}  where x is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is                              ∏                      p            ≤            x                          p              {\displaystyle \prod _{p\leq x}p}  and it can easily be determined that this wheel factor is                              ∏                      p            ≤            x                                                              p              −              1                        p                                {\displaystyle \prod _{p\leq x}{\frac {p-1}{p}}}  as p − 1/p is the fraction of remaining candidates for the highest wheel prime, x, and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.Combining all of the above analysis, the total number of operations for a sieving range up to n including wheel factorization for primes up to x is approximately                              ∏                      p            ≤            x                                                              p              −              1                        p                                    (                      log            ⁡            log            ⁡            n            −                                          1                                  log                  ⁡                  n                                                                    (                              1                −                                                      4                                          n                                                                                  )                        +            M            −            log            ⁡            2            −                          ∑                              p                ≤                x                                                                    1                p                                              )                      {\displaystyle \prod _{p\leq x}{\frac {p-1}{p}}\left(\log \log n-{\frac {1}{\log n}}\left(1-{\frac {4}{\sqrt {n}}}\right)+M-\log 2-\sum _{p\leq x}{\frac {1}{p}}\right)}  .To show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):The above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (105) and above.== Algorithmic complexity ==The sieve of Eratosthenes is a popular way to benchmark computer performance. As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the time complexity of calculating all primes below n in the random access machine model is O(n log log n) operations, a direct consequence of the fact that the prime harmonic series asymptotically approaches log log n. It has an exponential time complexity with regard to input size, though, which makes it a pseudo-polynomial algorithm. The basic algorithm requires O(n) of memory.The bit complexity of the algorithm is O(n (log n) (log log n)) bit operations with a memory requirement of O(n).The normally implemented page segmented version has the same operational complexity of O(n log log n) as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size O(√n/log n).A special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses O(n) operations and O(√nlog log n/log n) bits of memory.To show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the log log n estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):The above shows that the log log n estimate is not very accurate even for maximum practical ranges of about 1016. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing log log n term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.One should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared sieve of Atkin if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 1013 at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.Using big O notation is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve has an O(n) performance, but its basic implementation requires either a "one large array" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about O(n/log n) bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using O(√n/log n) bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has O(n) performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.== Euler's sieve ==Euler's proof of the zeta product formula contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once. The same sieve was rediscovered and observed to take linear time by Gries & Misra (1978). It, too, starts with a list of numbers from 2 to n in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:   [2] (3) 5  7  9  11  13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79  ... [3]    (5) 7     11  13    17 19    23 25    29 31    35 37    41 43    47 49    53 55    59 61    65 67    71 73    77 79  ... [4]       (7)    11  13    17 19    23       29 31       37    41 43    47 49    53       59 61       67    71 73    77 79  ... [5]             (11) 13    17 19    23       29 31       37    41 43    47       53       59 61       67    71 73       79  ... [...]Here the example is shown starting from odds, after the first step of the algorithm. Thus, on the kth step all the remaining multiples of the kth prime are removed from the list, which will thereafter contain only numbers coprime with the first k primes (cf. wheel factorization), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.Thus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime. In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.Note that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is 3 × 3 = 9, 3 × 5 = 15, 3 × 7 = 21, 3 × 9 = 27, ..., 3 × 15 = 45, ..., so care must be taken dealing with this.== See also ==Sieve of SundaramSieve of AtkinSieve theory== References ==== External links ==primesieve - Very fast highly optimized C/C++ segmented Sieve of EratosthenesEratosthenes, sieve of at Encyclopaedia of MathematicsInteractive JavaScript PageSieve of Eratosthenes by George Beck, Wolfram Demonstrations Project.Sieve of Eratosthenes in HaskellSieve of Eratosthenes algorithm illustrated and explained. Java and C++ implementations.A related sieve written in x86 assembly languageFast optimized highly parallel  CUDA segmented Sieve of Eratosthenes in CSieveOfEratosthenesInManyProgrammingLanguages c2 wiki pageThe Art of Prime Sieving Sieve of Eratosthenes in C from 1998 with nice features and algorithmic tricks explained.
	Algorism is the technique of performing basic arithmetic by writing numbers in place value form and applying a set of memorized rules and facts to the digits. One who practices algorism is known as an algorist. This positional notation system largely superseded earlier calculation systems that used a different set of symbols for each numerical magnitude, such as Roman numerals, and in some cases required a device such as an abacus.== Etymology ==The word algorism comes from the name Al-Khwārizmī (c. 780–850), a Persian mathematician, astronomer, geographer and scholar in the House of Wisdom in Baghdad, whose name means "the native of Khwarezm", a city that was part of the Greater Iran during his era and now is in modern-day Uzbekistan He wrote a treatise in Arabic language in the 9th century, which was translated into Latin in the 12th century under the title Algoritmi de numero Indorum. This title means "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through his other book, the Algebra. In late medieval Latin, algorismus, the corruption of his name, simply meant the "decimal number system" that is still the meaning of modern English algorism. During the 17th century, the French form for the word –but not its meaning– was changed to algorithm, following the model of the word logarithm, this form alluding to the ancient Greek arithmos = number. English adopted the French very soon afterwards, but it wasn't until the late 19th century that "algorithm" took on the meaning that it has in modern English. In English, it was first used about 1230 and then by Chaucer in 1391. Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.which translates as:This present art, in which we use those twice five Indian figures, is called algorismus.The word algorithm also derives from algorism, a generalization of the meaning to any set of rules specifying a computational procedure. Occasionally algorism is also used in this generalized meaning, especially in older texts.== History ==Starting with the integer arithmetic developed in India using base 10 notation, Al-Khwārizmī along with other mathematicians in medieval Islam, both Iranian and Arabic, documented new arithmetic methods and made many other contributions to decimal arithmetic (see the articles linked below). These included the concept of the decimal fractions as an extension of the notation, which in turn led to the notion of the decimal point.  This system was popularized in Europe by Leonardo of Pisa, now known as Fibonacci.== See also ==Algorithmic artPositional notationHindu–Arabic numeral systemHistory of the Hindu–Arabic numeral system== References ==
	The following timeline outlines the development of algorithms (mainly "mathematical recipes") since their inception.== Medieval Period ==Before – writing about "recipes" (on cooking, rituals, agriculture and other sorts of themes like willa&Mayan)c. 1700–2000 BC – Egyptians develop earliest known algorithms for multiplying two numbersc. 1600 BC – Babylonians develop earliest known algorithms for factorization and finding square rootsc. 300 BC – Euclid's algorithmc. 200 BC – the Sieve of Eratosthenes263 AD – Gaussian elimination described by Liu Hui628 – Chakravala method described by Brahmaguptac. 820 – Al-Khawarizmi described algorithms for solving linear equations and quadratic equations in his Algebra; the word algorithm comes from his name825 – Al-Khawarizmi described the algorism, algorithms for using the Hindu-Arabic numeral system, in his treatise On the Calculation with Hindu Numerals, which was translated into Latin as Algoritmi de numero Indorum, where "Algoritmi", the translator's rendition of the author's name gave rise to the word algorithm (Latin algorithmus) with a meaning "calculation method"c. 850 – cryptanalysis and frequency analysis algorithms developed by Al-Kindi (Alkindus) in A Manuscript on Deciphering Cryptographic Messages, which contains algorithms on breaking encryptions and ciphersc. 1025 – Ibn al-Haytham (Alhazen), was the first mathematician to derive the formula for the sum of the fourth powers, and in turn, he develops an algorithm for determining the general formula for the sum of any integral powers, which was fundamental to the development of integral calculusc. 1400 – Ahmad al-Qalqashandi gives a list of ciphers in his Subh al-a'sha which include both substitution and transposition, and for the first time, a cipher with multiple substitutions for each plaintext letter; he also gives an exposition on and worked example of cryptanalysis, including the use of tables of letter frequencies and sets of letters which can not occur together in one word== Before 1940 ==1540 – Lodovico Ferrari discovered a method to find the roots of a quartic polynomial1545 – Gerolamo Cardano published Cardano's method for finding the roots of a cubic polynomial1614 – John Napier develops method for performing calculations using logarithms1671 – Newton–Raphson method developed by Isaac Newton1690 – Newton–Raphson method independently developed by Joseph Raphson1706 – John Machin develops a quickly converging inverse-tangent series for π and computes π to 100 decimal places1789 – Jurij Vega improves Machin's formula and computes π to 140 decimal places,1805 – FFT-like algorithm known by Carl Friedrich Gauss1842 – Ada Lovelace writes the first algorithm for a computing engine1903 – A Fast Fourier Transform algorithm presented by Carle David Tolmé Runge1926 – Borůvka's algorithm1926 – Primary decomposition algorithm presented by Grete Hermann1934 – Delaunay triangulation developed by Boris Delaunay1936 – Turing machine, an abstract machine developed by Alan Turing, with others developed the modern notion of algorithm.== 1940s ==1942 –  A Fast Fourier Transform algorithm developed by G.C. Danielson and Cornelius Lanczos1945 – Merge sort developed by John von Neumann1947 – Simplex algorithm developed by George Dantzig== 1950s ==1952 – Huffman coding developed by David A. Huffman1953 – Simulated annealing introduced by Nicholas Metropolis1954 – Radix sort computer algorithm developed by Harold H. Seward1956 – Kruskal's algorithm developed by Joseph Kruskal1957 – Prim's algorithm developed by Robert Prim1957 – Bellman–Ford algorithm developed by Richard E. Bellman and L. R. Ford, Jr.1959 – Dijkstra's algorithm developed by Edsger Dijkstra1959 – Shell sort developed by Donald L. Shell1959 – De Casteljau's algorithm developed by Paul de Casteljau1959 – QR factorization algorithm developed independently by John G.F. Francis and Vera Kublanovskaya== 1960s ==1960 – Karatsuba multiplication1962 – AVL trees1962 – Quicksort developed by C. A. R. Hoare1962 – Ford–Fulkerson algorithm developed by L. R. Ford, Jr. and D. R. Fulkerson1962 – Bresenham's line algorithm developed by Jack E. Bresenham1962 – Gale–Shapley 'stable-marriage' algorithm developed by David Gale and Lloyd Shapley1964 – Heapsort developed by J. W. J. Williams1964 – multigrid methods first proposed by R. P. Fedorenko1965 – Cooley–Tukey algorithm rediscovered by James Cooley and John Tukey1965 – Levenshtein distance developed by Vladimir Levenshtein1965 – Cocke–Younger–Kasami (CYK) algorithm independently developed by Tadao Kasami1965 – Buchberger's algorithm for computing Gröbner bases developed by Bruno Buchberger1966 – Dantzig algorithm for shortest path in a graph with negative edges1967 – Viterbi algorithm proposed by Andrew Viterbi1967 – Cocke–Younger–Kasami (CYK) algorithm independently developed by Daniel H. Younger1968 – A* graph search algorithm described by Peter Hart, Nils Nilsson, and Bertram Raphael1968 – Risch algorithm for indefinite integration developed by Robert Henry Risch1969 – Strassen algorithm for matrix multiplication developed by Volker Strassen== 1970s ==1970 – Dinic's algorithm for computing maximum flow in a flow network by Yefim (Chaim) A. Dinitz1970 – Knuth–Bendix completion algorithm developed by Donald Knuth and Peter B. Bendix1970 – BFGS method of the quasi-Newton class1972 – Graham scan developed by Ronald Graham1972 – Red–black trees and B-trees discovered1973 – RSA encryption algorithm discovered by Clifford Cocks1973 – Jarvis march algorithm developed by R. A. Jarvis1973 – Hopcroft–Karp algorithm developed by John Hopcroft and Richard Karp1974 – Pollard's p − 1 algorithm developed by John Pollard1975 – Genetic algorithms popularized by John Holland1975 – Pollard's rho algorithm developed by John Pollard1975 – Aho–Corasick string matching algorithm developed by Alfred V. Aho and Margaret J. Corasick1975 – Cylindrical algebraic decomposition developed by George E. Collins1976 – Salamin–Brent algorithm independently discovered by Eugene Salamin and Richard Brent1976 – Knuth–Morris–Pratt algorithm developed by Donald Knuth and Vaughan Pratt and independently by J. H. Morris1977 – Boyer–Moore string search algorithm for searching the occurrence of a string into another string.1977 – RSA encryption algorithm rediscovered by Ron Rivest, Adi Shamir, and Len Adleman1977 – LZ77 algorithm developed by Abraham Lempel and Jacob Ziv1977 – multigrid methods developed independently by Achi Brandt and Wolfgang Hackbusch1978 – LZ78 algorithm developed from LZ77 by Abraham Lempel and Jacob Ziv1978 – Bruun's algorithm proposed for powers of two by Georg Bruun1979 – Khachiyan's ellipsoid method developed by Leonid Khachiyan1979 – ID3 decision tree algorithm developed by Ross Quinlan== 1980s ==1980 – Brent's Algorithm for cycle detection Richard P. Brendt1981 – Quadratic sieve developed by Carl Pomerance1983 – Simulated annealing developed by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi1983 – Classification and regression tree (CART) algorithm developed by Leo Breiman, et al.1984 – LZW algorithm developed from LZ78 by Terry Welch1984 – Karmarkar's interior-point algorithm developed by Narendra Karmarkar1984 - ACORN_PRNG discovered by Roy Wikramaratna and used privately1985 – Simulated annealing independently developed by V. Cerny1985 - Car–Parrinello molecular dynamics developed by Roberto Car and Michele Parrinello1985 – Splay trees discovered by Sleator and Tarjan1986 – Blum Blum Shub proposed by L. Blum, M. Blum, and M. Shub1986 – Push relabel maximum flow algorithm by Andrew Goldberg and Robert Tarjan1987 – Fast multipole method developed by Leslie Greengard and Vladimir Rokhlin1988 – Special number field sieve developed by John Pollard1989 - ACORN_PRNG published by Roy Wikramaratna== 1990s ==1990 – General number field sieve developed from SNFS by Carl Pomerance, Joe Buhler, Hendrik Lenstra, and Leonard Adleman1991 – Wait-free synchronization developed by Maurice Herlihy1992 – Deutsch–Jozsa algorithm proposed by D. Deutsch and Richard Jozsa1992 – C4.5 algorithm, a descendant of ID3 decision tree algorithm, was developed by Ross Quinlan1993 – Apriori algorithm developed by Rakesh Agrawal and Ramakrishnan Srikant1993 – Karger's algorithm to compute the minimum cut of a connected graph by David Karger1994 – Shor's algorithm developed by Peter Shor1994 – Burrows–Wheeler transform developed by Michael Burrows and David Wheeler1994 – Bootstrap aggregating (bagging) developed by Leo Breiman1995 – AdaBoost algorithm, the first practical boosting algorithm, was introduced by Yoav Freund and Robert Schapire1995 – soft-margin support vector machine algorithm was published by Vladimir Vapnik and Corinna Cortes. It adds a soft-margin idea to the 1992 algorithm by Boser, Nguyon, Vapnik, and is the algorithm that people usually refer to when saying SVM1995 – Ukkonen's algorithm for construction of suffix trees1996 – Bruun's algorithm generalized to arbitrary even composite sizes by H. Murakami1996 – Grover's algorithm developed by Lov K. Grover1996 – RIPEMD-160 developed by Hans Dobbertin, Antoon Bosselaers, and Bart Preneel1997 – Mersenne Twister a pseudo random number generator developed by Makoto Matsumoto and Tajuki Nishimura1998 – PageRank algorithm was published by Larry Page1998 – rsync algorithm developed by Andrew Tridgell1999 – gradient boosting algorithm developed by Jerome H. Friedman1999 – Yarrow algorithm designed by Bruce Schneier, John Kelsey, and Niels Ferguson== 2000s ==2000 – Hyperlink-induced topic search a hyperlink analysis algorithm developed by Jon Kleinberg2001 – Lempel–Ziv–Markov chain algorithm for compression developed by Igor Pavlov2001 – Viola–Jones algorithm for real-time face detection was developed by Paul Viola and Michael Jones.2002 – AKS primality test developed by Manindra Agrawal, Neeraj Kayal and Nitin Saxena2002 – Girvan–Newman algorithm to detect communities in complex systems== References ==
	Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution and enables more efficient use of multiple execution units. It was developed by Robert Tomasulo at IBM in 1967 and was first implemented in the IBM System/360 Model 91’s floating point unit.The major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.Robert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.== Implementation concepts ==The following are the concepts necessary to the implementation of Tomasulo's Algorithm:=== Common data bus ===The Common Data Bus (CDB) connects reservation stations directly to functional units. According to Tomasulo it "preserves precedence while encouraging concurrency". This has two important effects:Functional units can access the result of any operation without involving a floating-point-register, allowing multiple units waiting on a result to proceed without waiting to resolve contention for access to register file read ports.Hazard Detection and control execution are distributed. The reservation stations control when an instruction can execute, rather than a single dedicated hazard unit.=== Instruction order ===Instructions are issued sequentially so that the effects of a sequence of instructions, such as exceptions raised by these instructions, occur in the same order as they would on an in-order processor, regardless of the fact that they are being executed out-of-order (i.e. non-sequentially). === Register renaming ===Tomasulo's Algorithm uses register renaming to correctly perform out-of-order execution. All general-purpose and reservation station registers hold either a real value or a placeholder value. If a real value is unavailable to a destination register during the issue stage, a placeholder value is initially used. The placeholder value is a tag indicating which reservation station will produce the real value. When the unit finishes and broadcasts the result on the CDB, the placeholder will be replaced with the real value.Each functional unit has a single reservation station. Reservation stations hold information needed to execute a single instruction, including the operation and the operands. The functional unit begins processing when it is free and when all source operands needed for an instruction are real.=== Exceptions ===Practically speaking, there may be exceptions for which not enough status information about an exception is available, in which case the processor may raise a special exception, called an "imprecise" exception. Imprecise exceptions cannot occur in in-order implementations, as processor state is changed only in program order (see RISC Pipeline Exceptions).Programs that experience "precise" exceptions, where the specific instruction that took the exception can be determined, can restart or re-execute at the point of the exception. However, those that experience "imprecise" exceptions generally cannot restart or re-execute, as the system cannot determine the specific instruction that took the exception.== Instruction lifecycle ==The three stages listed below are the stages through which each instruction passes from the time it is issued to the time its execution is complete.=== Register legend ===Op - represents the operation being performed on operandsQj, Qk - the reservation station that will produce the relevant source operand (0 indicates the value is in Vj, Vk)Vj, Vk - the value of the source operandsA - used to hold the memory address information for a load or storeBusy - 1 if occupied, 0 if not occupiedQi - (Only register unit) the reservation station whose result should be stored in this register (if blank or 0, no values are destined for this register)=== Stage 1: issue ===In the issue stage, instructions are issued for execution if all operands and reservation stations are ready or else they are stalled. Registers are renamed in this step, eliminating WAR and WAW hazards.Retrieve the next instruction from the head of the instruction queue. If the instruction operands are currently in the registers, thenIf a matching functional unit is available, issue the instruction.Else, as there is no available functional unit, stall the instruction until a station or buffer is free.Otherwise, we can assume the operands are not in the registers, and so use virtual values. The functional unit must calculate the real value to keep track of the functional units that produce the operand.=== Stage 2: execute ===In the execute stage, the instruction operations are carried out. Instructions are delayed in this step until all of their operands are available, eliminating RAW hazards. Program correctness is maintained through effective address calculation to prevent hazards through memory.If one or more of the operands is not yet available then: wait for operand to become available on the CDB.When all operands are available, then: if the instruction is a load or storeCompute the effective address when the base register is available, and place it in the load/store bufferIf the instruction is a load then: execute as soon as the memory unit is availableElse, if the instruction is a store then: wait for the value to be stored before sending it to the memory unitElse, the instruction is an arithmetic logic unit (ALU) operation then: execute the instruction at the corresponding functional unit=== Stage 3: write result ===In the write Result stage, ALU operations results are written back to registers and store operations are written back to memory.If the instruction was an ALU operationIf the result is available, then: write it on the CDB and from there into the registers and any reservation stations waiting for this resultElse, if the instruction was a store then: write the data to memory during this step== Algorithm improvements ==The concepts of reservation stations, register renaming, and the common data bus in Tomasulo's algorithm presents significant advancements in the design of high-performance computers.Reservation stations take on the responsibility of waiting for operands in the presence of data dependencies and other inconsistencies such as varying storage access time and circuit speeds, thus freeing up the functional units. This improvement overcomes long floating point delays and memory accesses. In particular the algorithm is more tolerant of cache misses. Additionally, programmers are freed from implementing optimized code. This is a result of the common data bus and reservation station working together to preserve dependencies as well as encouraging concurrency.By tracking operands for instructions in the reservation stations and register renaming in hardware the algorithm minimizes read-after-write (RAW) and eliminates write-after-write (WAW) and Write-after-Read (WAR) computer architecture hazards. This improves performance by reducing wasted time that would otherwise be required for stalls.An equally important improvement in the algorithm is the design is not limited to a specific pipeline structure. This improvement allows the algorithm to be more widely adopted by multiple-issue processors. Additionally, the algorithm is easily extended to enable branch speculation. == Applications and legacy ==Tomasulo's algorithm, outside of IBM, was unused for several years after its implementation in the System/360 Model 91 architecture. However, it saw a vast increase in usage during the 1990s for 3 reasons:Once caches became commonplace, the Tomasulo algorithm's ability to maintain concurrency during unpredictable load times caused by cache misses became valuable in processors.Dynamic scheduling and the branch speculation that the algorithm enables helped performance as processors issued more and more instructions.Proliferation of mass-market software meant that programmers would not want to compile for a specific pipeline structure. The algorithm can function with any pipeline architecture and thus software requires few architecture-specific modifications. Many modern processors implement dynamic scheduling schemes that are derivative of Tomasulo’s original algorithm, including popular Intel x86-64 chips.== See also ==Re-order buffer (ROB)Instruction-level parallelism (ILP)== References ==== Further reading ==Savard, John J. G. (2018) [2014]. "Pipelined and Out-of-Order Execution". quadibloc. Archived from the original on 2018-07-03. Retrieved 2018-07-16.== External links ==Dynamic Scheduling - Tomasulo's AlgorithmHASE Java applet simulation of the Tomasulo's Algorithm
	In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.The idea is inspired by the use of partial evaluation in optimising program translation. Many core operations in theorem provers exhibit the following pattern.Suppose that we need to execute some algorithm                                           a            l            g                          (        A        ,        B        )              {\displaystyle {\mathit {alg}}(A,B)}   in a situation where a value of                     A              {\displaystyle A}   is fixed for potentially many different values of                     B              {\displaystyle B}  . In order to do this efficiently, we can try to find a specialization of                                           a            l            g                                {\displaystyle {\mathit {alg}}}   for every fixed                     A              {\displaystyle A}  , i.e., such an algorithm                                                         a              l              g                                            A                                {\displaystyle {\mathit {alg}}_{A}}  , that executing                                                         a              l              g                                            A                          (        B        )              {\displaystyle {\mathit {alg}}_{A}(B)}   is equivalent to executing                                           a            l            g                          (        A        ,        B        )              {\displaystyle {\mathit {alg}}(A,B)}  .The specialized algorithm may be more efficient than the generic one, since it can exploit some particular properties of the fixed value                     A              {\displaystyle A}  . Typically,                                                         a              l              g                                            A                          (        B        )              {\displaystyle {\mathit {alg}}_{A}(B)}   can avoid some operations that                                           a            l            g                          (        A        ,        B        )              {\displaystyle {\mathit {alg}}(A,B)}   would have to perform, if they are known to be redundant for this particular parameter                     A              {\displaystyle A}  . In particular, we can often identify some tests that are true or false for                     A              {\displaystyle A}  , unroll loops and recursion, etc.== Difference from partial evaluation ==The key difference between run-time specialization and partial evaluation is that the values of                     A              {\displaystyle A}   on which                                           a            l            g                                {\displaystyle {\mathit {alg}}}   is specialised are not known statically, so the specialization takes place at run-time.There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of                                           a            l            g                                {\displaystyle {\mathit {alg}}}  . We only have to imagine                                           a            l            g                                {\displaystyle {\mathit {alg}}}   when we program the specialization procedure.All we need is a concrete representation of the specialized version                                                         a              l              g                                            A                                {\displaystyle {\mathit {alg}}_{A}}  . This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm                                           a            l            g                                {\displaystyle {\mathit {alg}}}  . An important advantage of doing so is that we can use some powerful ad hoc tricks exploiting peculiarities of                                           a            l            g                                {\displaystyle {\mathit {alg}}}   and the representation of                     A              {\displaystyle A}   and                     B              {\displaystyle B}  , which are beyond the reach of any universal specialization methods.== Specialization with compilation ==The specialized algorithm has to be represented in a form that can be interpreted.In many situations, usually when                                                         a              l              g                                            A                          (        B        )              {\displaystyle {\mathit {alg}}_{A}(B)}   is to be computed on many values                     B              {\displaystyle B}   in a row, we can write                                                         a              l              g                                            A                                {\displaystyle {\mathit {alg}}_{A}}   as a code of a special abstract machine, and we often say that                     A              {\displaystyle A}   is compiled.  Then the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine.Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional parameters of the instruction, for example a pointer to anotherinstruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree.Interpretation is done by fetching instructions in some order, identifying their typeand executing the actions associated with this type. In C or C++ we can use a switch statement to associate some actions with different instruction tags. Modern compilers usually compile a switch statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value                     i              {\displaystyle i}   in the                     i              {\displaystyle i}  -th cell of a special array. One can exploit thisby taking values for instruction tags from a small interval of integers.== Data-and-algorithm specialization ==There are situations when many instances of                     A              {\displaystyle A}   are intended for long-term storage and the calls of                                           a            l            g                          (        A        ,        B        )              {\displaystyle {\mathit {alg}}(A,B)}   occur with different                     B              {\displaystyle B}   in an unpredictable order.For example, we may have to check                                           a            l            g                          (                  A                      1                          ,                  B                      1                          )              {\displaystyle {\mathit {alg}}(A_{1},B_{1})}   first, then                                           a            l            g                          (                  A                      2                          ,                  B                      2                          )              {\displaystyle {\mathit {alg}}(A_{2},B_{2})}  , then                                           a            l            g                          (                  A                      1                          ,                  B                      3                          )              {\displaystyle {\mathit {alg}}(A_{1},B_{3})}  , and so on.In such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage.  However, we can sometimes find a compact specialized representation                               A                      ′                                {\displaystyle A^{\prime }}  for every                     A              {\displaystyle A}  , that can be stored with, or instead of,                     A              {\displaystyle A}  . We also define a variant                                                         a              l              g                                            ′                                {\displaystyle {\mathit {alg}}^{\prime }}   that works on this representation and any call to                                           a            l            g                          (        A        ,        B        )              {\displaystyle {\mathit {alg}}(A,B)}   is replaced by                                                         a              l              g                                            ′                          (                  A                      ′                          ,        B        )              {\displaystyle {\mathit {alg}}^{\prime }(A^{\prime },B)}  , intended to do the same job faster.== See also ==Psyco, a specializing run-time compiler for Pythonmulti-stage programming== References ==A. Voronkov, "The Anatomy of Vampire: Implementing Bottom-Up Procedures with Code Trees", Journal of Automated Reasoning, 15(2), 1995 (original idea)== Further reading ==A. Riazanov and A. Voronkov, "Efficient Checking of Term Ordering Constraints", Proc. IJCAR 2004, Lecture Notes in Artificial Intelligence 3097, 2004 (compact but self-contained illustration of the method)A. Riazanov and A. Voronkov, Efficient Instance Retrieval with Standard and Relational Path Indexing, Information and Computation, 199(1-2), 2005 (contains another illustration of the method)A. Riazanov, "Implementing an Efficient Theorem Prover", PhD thesis, The University of Manchester, 2003 (contains the most comprehensive description of the method and many examples)
	There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.Mazes containing no loops are known as "simply connected", or "perfect" mazes, and are equivalent to a tree in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.== Random mouse algorithm ==This is a trivial method that can be implemented by a very unintelligent robot or perhaps a mouse. It is simply to proceed following the current passage until a junction is reached, and then to make a random decision about the next direction to follow. Although such a method would always eventually find the right solution, this algorithm can be extremely slow.== Wall follower ==The wall follower, the best-known rule for traversing mazes, is also known as either the left-hand rule or the right-hand rule. If the maze is simply connected, that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the solver is guaranteed not to get lost and will reach a different exit if there is one; otherwise, the algorithm will return to the entrance having traversed every corridor next to that connected section of walls at least once.Another perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle. Then wall following reduces to walking around a circle from start to finish. To further this idea, notice that by grouping together connected components of the maze walls, the boundaries between these are precisely the solutions, even if there is more than one solution (see figures on the right).If the maze is not simply-connected (i.e. if the start or endpoints are in the center of the structure surrounded by passage loops, or the pathways cross over and under each other and such parts of the solution path are surrounded by passage loops), this method will not reach the goal.Another concern is that care should be taken to begin wall-following at the entrance to the maze. If the maze is not simply-connected and one begins wall-following at an arbitrary point inside the maze, one could find themselves trapped along a separate wall that loops around on itself and containing no entrances or exits.  Should it be the case that wall-following begins late, attempt to mark the position in which wall-following began. Because wall-following will always lead you back to where you started, if you come across your starting point a second time, you can conclude the maze is not simply-connected, and you should switch to an alternative wall not yet followed. See the Pledge Algorithm, below, for an alternative methodology.Wall-following can be done in 3D or higher-dimensional mazes if its higher-dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze "up" passages can be assumed to lead Northwest, and "down" passages can be assumed to lead southeast, then standard wall following rules can apply. However, unlike in 2D, this requires that the current orientation is known, to determine which direction is the first on the left or right.== Pledge algorithm ==Disjoint mazes can be solved with the wall follower method, so long as the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after Jon Pledge of Exeter) can solve this problem.The Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward, which will be preferential. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted (clockwise turn is positive, counter-clockwise turn is negative). When the solver is facing the original preferential direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.The hand is removed from the wall only when both "sum of turns made" and "current heading" are at zero. This allows the algorithm to avoid traps shaped like an upper case letter "G". Assuming the algorithm turns left at the first wall, one gets turned around a full 360 degrees by the walls. An algorithm that only keeps track of "current heading" leads into an infinite loop as it leaves the lower rightmost wall heading left and runs into the curved section on the left hand side again. The Pledge algorithm does not leave the rightmost wall due to the "sum of turns made" not being zero at that point (note 360 degrees is not equal to 0 degrees). It follows the wall all the way around, finally leaving it heading left outside and just underneath the letter shape.This algorithm allows a person with a compass to find their way from any point inside to an outer exit of any finite two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.== Trémaux's algorithm ==Trémaux's algorithm, invented by Charles Pierre Trémaux, is an efficient method to find the way out of a maze that requires drawing lines on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages, but it is not guaranteed to find the shortest route.A path from a junction is either unvisited, marked once or marked twice. The algorithm works according to the following rules:Mark each path once, when you follow it. The marks need to be visible at both ends of the path. Therefore, if they are being made as physical marks, rather than stored as part of a computer algorithm, the same mark should be made at both ends of the path.Never enter a path which has two marks on it.If you arrive at a junction that has no marks (except possibly for the one on the path by which you entered), choose an arbitrary unmarked path, follow it, and mark it.Otherwise:If the path you came in on has only one mark, turn around and return along that path, marking it again. In particular this case should occur whenever you reach a dead end.If not, choose arbitrarily one of the remaining paths with the fewest marks (zero if possible, else one), follow that path, and mark it.When you finally reach the solution, paths marked exactly once will indicate a way back to the start. If there is no exit, this method will take you back to the start where all paths are marked twice.In this case each path is walked down exactly twice, once in each direction. The resulting walk is called a bidirectional double-tracing.Essentially, this algorithm, which was discovered in the 19th century, has been used about a hundred years later as depth-first search.== Dead-end filling ==Dead-end filling is an algorithm for solving mazes that fills all dead ends, leaving only the correct ways unfilled. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze since this method looks at the entire maze at once. The method is to 1) find all of the dead-ends in the maze, and then 2) "fill in" the path from each dead-end until the first junction is met. Note that some passages won't become parts of dead end passages until other dead ends are removed first. A video of dead-end filling in action can be seen here: [1][2].Dead-end filling cannot accidentally "cut off" the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop "too soon" since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. [3]== Recursive algorithm ==If given an omniscient view of the maze, a simple recursive algorithm can tell one how to get to the end. The algorithm will be given a starting X and Y value. If the X and Y values are not on a wall, the method will call itself with all adjacent X and Y values, making sure that it did not already use those X and Y values before. If the X and Y values are those of the end location, it will save all the previous instances of the method as the correct path. Here is a sample code in Java:== Maze-routing algorithm ==The maze-routing algorithm  is a low overhead method to find the way between any two locations of the maze. The algorithm is initially proposed for chip multiprocessors (CMPs) domain and guarantees to work for any grid-based maze. In addition to finding paths between two location of the grid (maze), the algorithm can detect when there is no path between the source and destination. Also, the algorithm is to be used by an inside traveler with no prior knowledge of the maze with fixed memory complexity regardless of the maze size; requiring 4 variables in total for finding the path and detecting the unreachable locations. Nevertheless, the algorithm is not to find the shortest path.Maze-routing algorithm uses the notion of Manhattan distance (MD) and relies on the property of grids that the MD increments/decrements exactly by 1 when moving from one location to any 4 neighboring locations. Here is the pseudocode without the capability to detect unreachable locations.== Shortest path algorithm ==When a maze has multiple solutions, the solver may want to find the shortest path from start to finish. There are several algorithms to find shortest paths, most of them coming from graph theory. One such algorithm finds the shortest path by implementing a breadth-first search, while another, the A* algorithm, uses a heuristic technique. The breadth-first search algorithm uses a queue to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path. The breadth-first search in its simplest form has its limitations, like finding the shortest path in weighted graphs.== See also ==MazesMaze generation algorithm== References ==== External links ==Think Labyrinth: Maze algorithms (details on these and other maze solving algorithms)MazeBlog: Solving mazes using image analysisVideo: Maze solving simulationSimon Ayrinhac, Electric current solves mazes, © 2014 IOP Publishing Ltd.
	The Boyer–Moore majority vote algorithm is an algorithm for finding the majority of a sequence of elements using linear time and constant space. It is named after Robert S. Boyer and J Strother Moore, who published it in 1981, and is a prototypical example of a streaming algorithm.In its simplest form, the algorithm finds a majority element, if there is one: that is, an element that occurs repeatedly for more than half of the elements of the input.A version of the algorithm that makes a second pass through the data can be used to verify that the element found in the first pass really is a majority.However, if there is no majority, the algorithm will not detect that fact, and will still output one of the elements.For instance, if an element repeats for exactly half of the input elements, it is not a majority. When there is no majority,the output element can be arbitrary; it is not guaranteed to be the element that occurs most often (the  mode of the sequence).It is not possible for a streaming algorithm to find the most frequent element in less than linear space, for sequences whose number of repetitions can be small.== Description ==The algorithm maintains in its local variables a sequence element and a counter, with the counter initially zero.It then processes the elements of the sequence, one at a time.When processing an element x, if the counter is zero, the algorithm stores x as its remembered sequence element and sets the counter to one.Otherwise, it compares x to the stored element and either increments the counter (if they are equal) or decrements the counter (otherwise).At the end of this process, if the sequence has a majority, it will be the element stored by the algorithm.This can be expressed in pseudocode as the following steps:Initialize an element m and a counter i with i = 0For each element x of the input sequence:If i = 0, then assign m = x and i = 1else if m = x, then assign i = i + 1else assign i = i − 1Return mEven when the input sequence has no majority, the algorithm will report one of the sequence elements as its result.However, it is possible to perform a second pass over the same input sequence in order to count the number of times the reported element occurs and determine whether it is actually a majority.This second pass is needed, as it is not possible for a sublinear-space algorithm to determine whether there exists a majority element in a single pass through the input.== Analysis ==The amount of memory that the algorithm needs is the space for one element and one counter. In the random access model of computing usually used for the analysis of algorithms, each of these values can be stored in a machine word and the total space needed is O(1). If an array index is needed to keep track of the algorithm's position in the input sequence, it doesn't change the overall constant space bound.The algorithm's bit complexity (the space it would need, for instance, on a Turing machine) is higher, the sum of the binary logarithms of the input length and the size of the universe from which the elements are drawn. Both the random access model and bit complexity analyses only count the working storage of the algorithm, and not the storage for the input sequence itself.Similarly, on a random access machine the algorithm takes time O(n) (linear time) on an input sequence of n items, because it performs only a constant number of operations per input item. The algorithm can also be implemented on a Turing machine in time linear in the input length (n times the number of bits per input item).== Correctness ==If there is a majority element, the algorithm will always find it. For, supposing that the majority element is m, let c be a number defined at any step of the algorithm to be either the counter, if the stored element is m, or the negation of the counter otherwise. Then at each step in which the algorithm encounters a value equal to m, the value of c will increase by one, and at each step at which it encounters a different value, the value of c may either increase or decrease by one. If m truly is the majority, there will be more increases than decreases, and c will be positive at the end of the algorithm. But this can be true only when the final stored element is m, the majority element.== See also ==Element distinctness problem, the problem of testing whether a collection of elements has any repeated elementsMajority function, the majority of a collection of Boolean valuesMajority problem (cellular automaton), the problem of finding a majority element in the cellular automaton computational model== References ==
	HAKMEM, alternatively known as AI Memo 239, is a February 1972 "memo" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, "a bizarre and eclectic potpourri of technical trivia".Contributors included about two dozen members and associates of the AI Lab. The title of the report is short for "hacks memo", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).== History ==HAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.HAKMEM contains original work in some fields, notably continued fractions.== Introduction ==Compiled with the hope that a record of the random things people do around here can save some duplication of effort -- except for fun.Here is some little known data which may be of interest to computer hackers. The items and examples are so sketchy that to decipher them may require more sincerity and curiosity than a non-hacker can muster. Doubtless, little of this is new, but nowadays it's hard to tell. So we must be content to give you an insight, or save you some cycles, and to welcome further contributions of items, new or used.== See also ==Hacker's DelightAI Memo== References ==== External links ==Its official record at the DSpace@MIT's AI Memos collectionSchroeppel, Richard C.; Orman, Hilarie K. (1972-02-29), "compilation", HAKMEM, by Beeler, Michael; Gosper, Ralph William; Schroeppel, Richard C.,  Baker, Henry (ed.),  (report) (retyped & converted (April 1995) ed.), Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, MIT AI Memo 239, retrieved 2016-01-02HAKMEM facsimile (PDF) (searchable version)
	In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. "Distinct" means that the variables are stored at different, non-overlapping, memory addresses as the algorithm would set a single aliased value to zero; the actual values of the variables do not have to be different.== The algorithm ==Conventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:The algorithm typically corresponds to three machine-code instructions. Since XOR is a commutative operation, X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:In the above System/370 assembly code sample, R1 and R2 are distinct registers, and each XR operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and xor places the result of the operation in the first register.However, the algorithm fails if x and y use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be "swapped with itself". Note that this is not the same as if x and y have the same values.  The trouble only comes when x and y use the same storage location, in which case their values must already be equal. That is, if x and y use the same storage location, then the line:sets x to zero (because x = y so X XOR Y is zero) and sets y to zero (since it uses the same storage location), causing x and y to lose their original values.== Proof of correctness ==The binary operation XOR over bit strings of length                     N              {\displaystyle N}   exhibits the following properties (where                     ⊕              {\displaystyle \oplus }   denotes XOR):L1. Commutativity:                     A        ⊕        B        =        B        ⊕        A              {\displaystyle A\oplus B=B\oplus A}  L2. Associativity:                     (        A        ⊕        B        )        ⊕        C        =        A        ⊕        (        B        ⊕        C        )              {\displaystyle (A\oplus B)\oplus C=A\oplus (B\oplus C)}  L3. Identity exists: there is a bit string, 0, (of length N) such that                     A        ⊕        0        =        A              {\displaystyle A\oplus 0=A}   for any                     A              {\displaystyle A}  L4. Each element is its own inverse: for each                     A              {\displaystyle A}  ,                     A        ⊕        A        =        0              {\displaystyle A\oplus A=0}  .Suppose that we have two distinct registers R1 and R2 as in the table below, with initial values A and B respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.=== Linear algebra interpretation ===As XOR can be interpreted as binary addition and a pair of bits can be interpreted as a vector in a two-dimensional vector space over the field with two elements, the steps in the algorithm can be interpreted as multiplication by 2×2 matrices over the field with two elements. For simplicity, assume initially that x and y are each single bits, not bit vectors.For example, the step:which also has the implicit:corresponds to the matrix                               (                                                                                          1                                                        1                                                                                        0                                                        1                                                                                )                      {\displaystyle \left({\begin{smallmatrix}1&1\\0&1\end{smallmatrix}}\right)}   as                                          (                                                            1                                                  1                                                                              0                                                  1                                                      )                                                (                                                            x                                                                              y                                                      )                          =                              (                                                            x                  +                  y                                                                              y                                                      )                          .              {\displaystyle {\begin{pmatrix}1&1\\0&1\end{pmatrix}}{\begin{pmatrix}x\\y\end{pmatrix}}={\begin{pmatrix}x+y\\y\end{pmatrix}}.}  The sequence of operations is then expressed as:                                          (                                                            1                                                  1                                                                              0                                                  1                                                      )                                                (                                                            1                                                  0                                                                              1                                                  1                                                      )                                                (                                                            1                                                  1                                                                              0                                                  1                                                      )                          =                              (                                                            0                                                  1                                                                              1                                                  0                                                      )                                {\displaystyle {\begin{pmatrix}1&1\\0&1\end{pmatrix}}{\begin{pmatrix}1&0\\1&1\end{pmatrix}}{\begin{pmatrix}1&1\\0&1\end{pmatrix}}={\begin{pmatrix}0&1\\1&0\end{pmatrix}}}  (working with binary values, so                     1        +        1        =        0              {\displaystyle 1+1=0}  ), which expresses the elementary matrix of switching two rows (or columns) in terms of the transvections (shears) of adding one element to the other.To generalize to where X and Y are not single bits, but instead bit vectors of length n, these 2×2 matrices are replaced by 2n×2n block matrices such as                               (                                                                                                                I                                              n                                                                                                                        I                                              n                                                                                                                                  0                                                                              I                                              n                                                                                                                          )                .              {\displaystyle \left({\begin{smallmatrix}I_{n}&I_{n}\\0&I_{n}\end{smallmatrix}}\right).}  Note that these matrices are operating on values, not on variables (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.== Code example ==A C function that implements the XOR swap algorithm:Note that the code does not swap the integers passed immediately, but first checks if their addresses are distinct. This is because, if the addresses are equal, the algorithm will fold to a triple *x ^= *x resulting in zero.The XOR swap algorithm can also be defined with a macro:== Reasons for use in practice ==In most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:on a processor where the instruction set encoding permits the XOR swap to be encoded in a smaller number of bytesin a region with high register pressure, it may allow the register allocator to avoid spilling a registerin microcontrollers where available RAM is very limited.in cryptographic applications which need constant time functions to prevent time-based side-channel attacksBecause these situations are rare, most optimizing compilers do not generate XOR swap code.== Reasons for avoidance in practice ==Most modern compilers can optimize away the temporary variable in the three-way swap, in which case it will use the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. In addition to that, the XOR swap is completely opaque to anyone unfamiliar with the technique.On modern CPU architectures, the XOR technique can be slower than using a temporary variable to do swapping. At least on recent x86 CPUs, both by AMD and Intel, moving between registers regularly incurs zero latency. (This is called MOV-elimination.) Even if there is not any architectural register available to use, the XCHG instruction will be at least as fast as the three XORs taken together. Another reason is that modern CPUs strive to execute instructions in parallel via instruction pipelines. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of instruction-level parallelism.A historical reason was that it used to be patented (US4197590).  Even then, this was only for computer graphics.=== Aliasing ===The XOR swap is also complicated in practice by aliasing. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.Similar problems occur with call by name, as in Jensen's Device, where swapping i and A[i] via a temporary variable yields incorrect results due to the arguments being related: swapping via temp = i; i = A[i]; A[i] = temp changes the value for i in the second statement, which then results in the incorrect i value for A[i] in the third statement.== Variations ==The underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:Unlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as modular arithmetic or bignums to guarantee that the computation of X + Y cannot cause an error due to integer overflow. Therefore, it is seen even more rarely in practice than the XOR swap.Note, however, that the implementation of AddSwap above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and  subtraction of unsigned integers follow the rules of modular arithmetic, i. e. are done in the cyclic group                               Z                          /                          2                      s                                    Z                      {\displaystyle \mathbb {Z} /2^{s}\mathbb {Z} }   where                     s              {\displaystyle s}   is the number of bits of unsigned int. Indeed, the correctness of the algorithm follows from the fact that the formulas                     (        x        +        y        )        −        y        =        x              {\displaystyle (x+y)-y=x}   and                     (        x        +        y        )        −        (        (        x        +        y        )        −        y        )        =        y              {\displaystyle (x+y)-((x+y)-y)=y}   hold in any abelian group. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group                     (                  Z                          /                2                  Z                          )                      s                                {\displaystyle (\mathbb {Z} /2\mathbb {Z} )^{s}}   (which is the direct sum of s copies of                               Z                          /                2                  Z                      {\displaystyle \mathbb {Z} /2\mathbb {Z} }  ).Please note that the above doesn't hold when dealing with the signed int type (the default for int). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).== See also ==Symmetric differenceXOR linked listFeistel cipher (the XOR swap algorithm is a degenerate form of a Feistel cypher)== Notes ==== References ==
	In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus "sequential" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used."Sequential algorithm" may also refer specifically to an algorithm for decoding a convolutional code.== See also ==Online algorithmStreaming algorithm== References ==
	Chinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.Chinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment.  The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.== Algorithm ==The algorithm works in the following way in an undirected unweighted graph:All nodes are assigned to a random class. The number of initial classes equals the number of nodes.Then all of the network nodes are selected one by one in a random order. Every node moves to the class which the given node connects with the most links. In the case of equality the cluster is randomly chosen from the equally linked classes.Step two repeats itself until a predetermined number of iteration or until the process converges. In the end the emerging classes represent the clusters of the network.The predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.== Strengths and Weaknesses ==The main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the small world property.On the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears. For this reason for small graphs other clustering methods are recommended.== Applications ==Chinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of Natural Language Processing problems. On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for Gephi which is an open source program designed for network analysis.== References ==== External links ==Implementation in Python
	A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components."Hybrid algorithm" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.== Examples ==In computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of divide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing. This is useful for efficiency when the algorithm usually encounters the base case many times, as in many tree algorithms, but is otherwise considered poor style, particularly in academia, due to the added complexity.Another example of hybrid algorithms for performance reasons are introsort and introselect, which combine one algorithm for fast average performance, falling back on another algorithm to ensure (asymptotically) optimal worst-case performance. Introsort begins with a quicksort, but switches to a heap sort if quicksort is not progressing well; analogously introselect begins with quickselect, but switches to median of medians if quickselect is not progressing well.Centralized distributed algorithms can often be considered as hybrid algorithms, consisting of an individual algorithm (run on each distributed processor), and a combining algorithm (run on a centralized distributor) – these correspond respectively to running the entire algorithm on one processor, or running the entire computation on the distributor, combining trivial results (a one-element data set from each processor). A basic example of these algorithms are distribution sorts, particularly used for external sorting, which divide the data into separate subsets, sort the subsets, and then combine the subsets into totally sorted data; examples include bucket sort and flashsort.However, in general distributed algorithms need not be hybrid algorithms, as individual algorithms or combining or communication algorithms may be solving different problems. For example, in models such as MapReduce, the Map and Reduce step solve different problems, and are combined to solve a different, third problem.== See also ==Hybrid algorithm (constraint satisfaction)Hybrid genetic algorithmHybrid input output (HIO) algorithm for phase retrieval
	The Algorithm Auction is the world’s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.Five lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid’s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.Two other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.Exhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktivist Richard Jones.Sebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.== Effects ==Following the auction, the Museum of Modern Art held a salon titled The Way of the Algorithm highlighting algorithms as "a ubiquitous and indispensable component of our lives."== References ==
	In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network. External memory algorithms are analyzed in the external memory model.== Model ==External memory algorithms are analyzed in an idealized model of computation called the external memory model (or I/O model, or disk access model). The external memory model is an abstract machine similar to the RAM machine model, but with a cache in addition to main memory. The model captures the fact that read and write operations are much faster in a cache than in main memory, and that reading long contiguous blocks is faster than reading randomly using a disk read-and-write head. The running time of an algorithm in the external memory model is defined by the number of reads and writes to memory required. The model was introduced by Alok Aggarwal and Jeffrey Vitter in 1988. The external memory model is related to the cache-oblivious model, but algorithms in the external memory model may know both the block size and the cache size. For this reason, the model is sometimes referred to as the cache-aware model.The model consists of a processor with an internal memory or cache of size M, connected to an unbounded external memory. Both the internal and external memory are divided into blocks of size B. One input/output or memory transfer operation consists of moving a block of B contiguous elements from external to internal memory, and the running time of an algorithm is determined by the number of these input/output operations.== Algorithms ==Algorithms in the external memory model take advantage of the fact that retrieving one object from external memory retrieves an entire block of size                     B              {\displaystyle B}  . This property is sometimes referred to as locality.Searching for an element among                     N              {\displaystyle N}   objects is possible in the external memory model using a B-tree with branching factor                     B              {\displaystyle B}  . Using a B-tree, searching, insertion, and deletion can be achieved in                     O        (                  log                      B                          ⁡        N        )              {\displaystyle O(\log _{B}N)}   time (in Big O notation). Information theoretically, this is the minimum running time possible for these operations, so using a B-tree is asymptotically optimal.External sorting is sorting in an external memory setting. External sorting can be done via distribution sort, which is similar to quicksort, or via a                                                         M              B                                            {\displaystyle {\tfrac {M}{B}}}  -way merge sort. Both variants achieve the asymptotically optimal runtime of                     O        (                                            N              B                                                log                                                    M                B                                                    ⁡                                            N              B                                      )              {\displaystyle O({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}})}   to sort N objects. This bound also applies to the Fast Fourier Transform in the external memory model.The permutation problem is to rearrange                     N              {\displaystyle N}   elements into a specific permutation. This can either be done either by sorting, which requires the above sorting runtime, or inserting each element in order and ignoring the benefit of locality. Thus, permutation can be done in                     O        (        min        (        N        ,                                            N              B                                                log                                                    M                B                                                    ⁡                                            N              B                                      )        )              {\displaystyle O(\min(N,{\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}))}   time.== Applications ==The external memory model captures the memory hierarchy, which is not modeled in other common models used in analyzing data structures, such as the random access machine, and is useful for proving lower bounds for data structures. The model is also useful for analyzing algorithms that work on datasets too big to fit in internal memory.A typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.This methodology extends beyond general purpose CPUs and also includes GPU computing as well as classical digital signal processing. In general-purpose computing on graphics processing units (GPGPU), powerful graphics cards (GPUs) with little memory (compared with the more familiar system memory, which is most often referred to simply as RAM) are utilized with relatively slow CPU to GPU memory transfer (when compared with computation bandwidth).== History ==An early use of the term "out-of-core" as an adjective is in 1962 in reference to devices that are other than the core memory of an IBM 360. An early use of the term "out-of-core" with respect to algorithms appears in 1971.== See also ==External sortingOnline algorithmStreaming algorithmCache-oblivious algorithmParallel external memoryExternal memory graph traversal== References ==== External links ==Out of Core SVD and QROut of core graphicsScalapack design
	The Manhattan address algorithm refers to the formulas used to estimate the closest east–west cross street for building numbers on north–south avenues in the New York City borough of Manhattan.== Algorithm ==To find the approximate number of the closest cross street, divide the building number by a divisor (generally 20) and add (or subtract) the "magic number" from the table below:== See also ==List of numbered streets in ManhattanNumbered street== References ==
	Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.== References ==
	In computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.This divide-and-conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).Understanding and designing divide-and-conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These divide-and-conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.== Divide and conquer == The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. For example, to sort a given list of n natural numbers, split it into two lists of about n/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the merge sort algorithm.The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding).  These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops.  Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm".  Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems. The name decrease and conquer has been proposed instead for the single-subproblem class.An important application of divide and conquer is in optimization, where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.== Early historical examples ==Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into single subproblems, and indeed can be solved iteratively.Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC. Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.Another notable example is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two n-digit numbers in                     O        (                  n                                    log                              2                                      ⁡            3                          )              {\displaystyle O(n^{\log _{2}3})}   operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that                     Ω        (                  n                      2                          )              {\displaystyle \Omega (n^{2})}   operations would be required for that task.As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered. This is related to a radix sort, described for punch-card sorting machines as early as 1929.== Advantages ===== Solving difficult problems ===Divide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height n to moving a tower of height n − 1.=== Algorithm efficiency ===The divide-and-conquer paradigm often helps in the discovery of efficient algorithms.  It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size n, and (b) there is a bounded number p of subproblems of size ~ n/p at each stage, then the cost of the divide-and-conquer algorithm will be O(n logpn).=== Parallelism ===Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.=== Memory access ===Divide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called cache-oblivious, because it does not contain the cache size as an explicit parameter.Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be optimal cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is blocking, as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.The same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.=== Roundoff control ===In computations with rounded arithmetic, e.g. with floating-point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add N numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.  While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.== Implementation issues ===== Recursion ===Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.=== Explicit stack ===Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue.  This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.=== Stack size ===In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow.  D&C algorithms that are time-efficient often have relatively small recursion depth.  For example, the quicksort algorithm can be implemented so that it never requires more than                               log                      2                          ⁡        n              {\displaystyle \log _{2}n}   nested recursive calls to  sort                     n              {\displaystyle n}   items.Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it.  Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure.  Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.=== Choosing the base cases ===In any recursive algorithm, there is considerable freedom in the choice of the base cases, the small subproblems that are solved directly in order to terminate the recursion.Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve.  For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small.  Note that, if the empty list were the only base case, sorting a list with n entries would entail maximally n quicksort calls that would do nothing but return immediately.  Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation).  For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.  Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.=== Sharing repeated subproblems ===For some problems, the branched recursion may end up evaluating the same sub-problem many times over.  In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization.  Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.== See also ==Akra–Bazzi methodDecomposable aggregation functionFork–join modelMaster theorem (analysis of algorithms)Mathematical inductionMapReduceHeuristic (computer science)== References ==
	Maze generation algorithms are automated methods for the creation of mazes.== Graph theory based methods ==A maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a connected graph with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph in which it is challenging to find a route between two particular nodes.If the subgraph is not connected, then there are regions of the graph that are wasted because they do not contribute to the search space.  If the graph contains loops, then there may be multiple paths between the chosen nodes.  Because of this, maze generation is often approached as generating a random spanning tree.  Loops, which can confound naive maze solvers, may be introduced by adding random edges to the result during the course of the algorithm.The animation shows the maze generation steps for a graph that is not on a rectangular grid.First, the computer creates a random planar graph Gshown in blue, and its dual Fshown in yellow. Second, computer traverses F using a chosenalgorithm, such as a depth-first search, coloring the path red.During the traversal, whenever a red edge crosses over a blue edge,the blue edge is removed.Finally, when all vertices of F have been visited, F is erasedand two edges from G, one for the entrance and one for the exit, are removed.=== Depth-first search ===This algorithm is a randomized version of the depth-first search algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the wall between the two cells and marks the new cell as visited, and adds it to the stack to facilitate backtracking. The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. We can be sure every cell is visited.As given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the beginning.Mazes generated with a depth-first search have a low branching factor and contain many long corridors, because the algorithm explores as far as possible along each branch before backtracking.=== Recursive backtracker ===The depth-first search algorithm of maze generation is frequently implemented using backtracking:Make the initial cell the current cell and mark it as visitedWhile there are unvisited cellsIf the current cell has any neighbours which have not been visitedChoose randomly one of the unvisited neighboursPush the current cell to the stack if it has more than one unvisited neighborRemove the wall between the current cell and the chosen cellMake the chosen cell the current cell and mark it as visitedElse if stack is not emptyPop a cell from the stack while the stack is not empty and the popped cell has no unvisited neighborsMake it the current cell=== Randomized Kruskal's algorithm ===This algorithm is a randomized version of Kruskal's algorithm.Create a list of all walls, and create a set for each cell, each containing just that one cell.For each wall, in some random order:If the cells divided by this wall belong to distinct sets:Remove the current wall.Join the sets of the formerly divided cells.There are several data structures that can be used to model the sets of cells.  An efficient implementation using a disjoint-set data structure can perform each union and find operation on two sets in nearly constant amortized time (specifically,                     O        (        α        (        V        )        )              {\displaystyle O(\alpha (V))}   time;                     α        (        x        )        <        5              {\displaystyle \alpha (x)<5}   for any plausible value of                     x              {\displaystyle x}  ), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.It matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.Because the effect of this algorithm is to produce a minimal spanning tree from a graph with equally weighted edges, it tends to produce regular patterns which are fairly easy to solve.=== Randomized Prim's algorithm ===This algorithm is a randomized version of Prim's algorithm.Start with a grid full of walls.Pick a cell, mark it as part of the maze. Add the walls of the cell to the wall list.While there are walls in the list:Pick a random wall from the list. If only one of the two cells that the wall divides is visited, then:Make the wall a passage and mark the unvisited cell as part of the maze.Add the neighboring walls of the cell to the wall list.Remove the wall from the list.It will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.Note that simply running classical Prim's on a graph with random edge weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms.  Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.==== Modified version ====Although the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells.  If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random.  This will tend to branch slightly more than the edge-based version above.=== Wilson's algorithm ===All the above algorithms have biases of various sorts: depth-first search is biased toward long corridors, while Kruskal's/Prim's algorithms are biased toward many short dead ends. Wilson's algorithm, on the other hand, generates an unbiased sample from the uniform distribution over all mazes, using loop-erased random walks.We begin the algorithm by initializing the maze with one cell chosen arbitrarily. Then we start at a new cell chosen arbitrarily, and perform a random walk until we reach a cell already in the maze—however, if at any point the random walk reaches its own path, forming a loop, we erase the loop from the path before proceeding. When the path reaches the maze, we add it to the maze. Then we perform another loop-erased random walk from another arbitrary starting cell, repeating until all cells have been filled.This procedure remains unbiased no matter which method we use to arbitrarily choose starting cells. So we could always choose the first unfilled cell in (say) left-to-right, top-to-bottom order for simplicity.== Recursive division method ==Mazes can be created with recursive division, an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contains a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.For example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.== Simple algorithms ==Other algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. Eller's algorithm prevents loops by storing which cells in the current line are connected through cells in the previous lines, and never removes walls between any two cells already connected. The Sidewinder algorithm starts with an open passage along the entire the top row, and subsequent rows consist of shorter horizontal passages with one connection to the passage above. The Sidewinder algorithm is trivial to solve from the bottom up because it has no upward dead ends. Given a starting width, both algorithm create perfect mazes of unlimited height.Most maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a binary tree, with the upper left corner its root. As with Sidewinder, the binary tree maze has no dead ends in the directions of bias. A related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages.  (The manual for the Commodore 64 presents a BASIC program using this algorithm, but using PETSCII diagonal line graphic characters instead for a smoother graphic appearance.)== Cellular automaton algorithms ==Certain types of cellular automata can be used to generate mazes. Two well-known such cellular automata, Maze and Mazectric, have rulestrings B3/S12345 and B3/S1234. In the former, this means that cells survive from one generation to the next if they have at least one and at most five neighbours. In the latter, this means that cells survive if they have one to four neighbours. If a cell has exactly three neighbours, it is born. It is similar to Conway's Game of Life in that patterns that do not have a living cell adjacent to 1, 4, or 5 other living cells in any generation will behave identically to it. However, for large patterns, it behaves very differently from Life.For a random starting pattern, these maze-generating cellular automata will evolve into complex mazes with well-defined walls outlining corridors. Mazecetric, which has the rule B3/S1234 has a tendency to generate longer and straighter corridors compared with Maze, with the rule B3/S12345. Since these cellular automaton rules are deterministic, each maze generated is uniquely determined by its random starting pattern. This is a significant drawback since the mazes tend to be relatively predictable.Like some of the graph-theory based methods described above, these cellular automata typically generate mazes from a single starting pattern; hence it will usually be relatively easy to find the way to the starting cell, but harder to find the way anywhere else.== Python code example ==Example implementation of a variant of Prim's algorithm in Python/NumPy. Prim's algorithm above starts with a grid full of walls and grows a single component of pathable tiles. In this example, we start with an open grid and grow multiple components of walls.This algorithm works by creating n (density) islands of length p (complexity). An island is created by choosing a random starting point with odd coordinates, then a random direction is chosen. If the cell two steps in the direction is free, then a wall is added at both one step and two steps in this direction. The process is iterated for n steps for this island. p islands are created. n and p are expressed as float to adapt them to the size of the maze. With a low complexity, islands are very small and the maze is easy to solve. With low density, the maze has more "big empty rooms".== C code example ==The code below is an example of depth-first search maze generator in C. == See also ==Maze solving algorithmSelf-avoiding walk== References ==== External links ==Think Labyrinth: Maze algorithms (details on these and other maze generation algorithms)Jamis Buck: HTML 5 Presentation with Demos of Maze generation AlgorithmsMaze generation visualizationJava implementation of Prim's algorithmImplementations of DFS maze creation algorithm in multiple languages at Rosetta CodeArmin Reichert: 34 maze algorithms in Java 8, with demo applicationCADforum: Maze generation algorithm in VisualLISPCoding Challenge #10.1: Maze Generator with p5.js - Part 1: Maze generation algorithm in JavaScript with p5Maze Generator by Charles Bond, COMPUTE! Magazine, December 1981
	Each clue in a Jumble word puzzle is a word that has been “jumbled” by permuting the letters of each word to make an anagram. A dictionary of such anagrams may be used to solve puzzles or verify that a jumbled word is unique when creating puzzles.== Computerized solution ==Algorithms have been designed to solve Jumbles, using a dictionary. Common algorithms work by printing all words that can be formed from a set of letters. The solver then chooses the right word.First algorithm:BeginInput: J, all the jumbled letters that form an unknown W word(s)Sort the letters of J in alphabetical order, preserving duplicatesLook up sorted letters in a hash table, initialised with a dictionary, that maps a sorted set of letters to unscrambled wordsPrint the set of words, which is WEndSecond algorithm:BeginInput: J, all the jumbled letters that form an unknown W word(s)Frame a word list Y with all permutations of JFor each word in Y check if the word is existing in the dictionaryIf a match is found then collect it in word list WPrint the words in WEndAlgorithm to find the permutations of J:BeginInitialize a string with first character of J denoted by J(1)Add the second character of J denoted by J(2) on either side of J(1) to get two stringsJ(1)J(2)J(2)J(1)Add the third character of J denoted by J(3) on either side and in between the above 2 strings to get 6 stringsJ(1)J(2)J(3)J(1)J(3)J(2)J(3)J(1)J(2)J(2)J(1)J(3)J(2)J(3)J(1)J(3)J(2)J(1)In the same way add J(4) to each of the above strings in either sides and between two characters to get 24 stringsContinue this until all the characters are completedThough the algorithm looks complex it is easy to program.Douglas Hofstadter developed a program called Jumbo that tries to solve Jumble problems as a human mind would.The program doesn't rely on a dictionary and doesn't try to find real English words, but rather words that could be English, exploiting a database of plausibilities for various combinations of letters.Letters are combined non-deterministically, following a strategy inspired by chemical reactions and free associations.
	The British Museum algorithm is a general approach to finding a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.Newell, Shaw, and Simon called this procedure the British Museum algorithm "... since it seemed to them as sensible as placing monkeys in front of typewriters in order to reproduce all the books in the British Museum."== See also ==BogosortBranch and boundBreadth-first searchBrute-force search== Sources ==Original text by  This article incorporates public domain material from the NIST document: Black, Paul E. "British Museum technique". Dictionary of Algorithms and Data Structures..== References ==
	Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or "activation" and then iteratively propagating or "spreading" that activation out to other nodes linked to the source nodes.  Most often these "weights" are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.Spreading activation models are used in cognitive psychology to model the fan out effect.Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.== Cognitive psychology ==As it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. For memory psychology, Spreading activation model means people organize their knowledge of the world based on their personal experience, which is saying those personal experiences form the network of ideas that is the person's knowledge of the world.When a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word "doctor" when it is preceded by "nurse" than when it is preceded by an unrelated word like "carrot". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.As another example, if the original concept is "red" and the concept "vehicles" is primed, they are much more likely to say "fire engine" instead of something unrelated to vehicles, such as "cherries". If instead "fruits" was primed, they would likely name "cherries" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.== Algorithm ==A directed graph is populated by Nodes[ 1...N ]  each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0].  A Link[ i, j ] connects source node[ i ] with target node[ j ].  Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].Parameters:Firing threshold F, a real number in the range [0.0 ... 1.0]Decay factor D, a real number in the range [0.0 ... 1.0]Steps:Initialize the graph setting all activation values A [ i ] to zero.   Set one or more origin nodes to an initial activation value greater than the firing threshold F.  A typical initial value is 1.0.For each unfired node [ i ] in the graph having an activation value A [ i ] greater than the node firing threshold F:For each Link [ i, j ] connecting the source node [ i ] with target node [ j ], adjust A [ j ] = A [ j ] + (A [ i ] * W [ i, j ] * D) where D is the decay factor.If a target node receives an adjustment to its activation value so that it would exceed 1.0, then set its new activation value to 1.0.  Likewise maintain 0.0 as a lower bound on the target node's activation value should it receive an adjustment to below 0.0.Once a node has fired it may not fire again, although variations of the basic algorithm permit repeated firings and loops through the graph.Nodes receiving a new activation value that exceeds the firing threshold F are marked for firing on the next spreading activation cycle.If activation originates from more than one node, a variation of the algorithm permits marker passing to distinguish the paths by which activation is spread over the graphThe procedure terminates when either there are no more nodes to fire or in the case of marker passing from multiple origins, when a node is reached from more than one path. Variations of the algorithm that permit repeated node firings and activation loops in the graph, terminate after a steady activation state, with respect to some delta, is reached, or when a maximum number of iterations is exceeded.== Examples ==== See also ==Connectionism== Notes ==== References ==Nils J. Nilsson. "Artificial Intelligence: A New Synthesis". Morgan Kaufmann Publishers, Inc., San Francisco, California, 1998, pages 121-122Rodriguez, M.A., " Grammar-Based Random Walkers in Semantic Networks", Knowledge-Based Systems, 21(7), 727-739, doi:10.1016/j.knosys.2008.03.030, 2008.Karalyn Patterson, Peter J. Nestor & Timothy T. Rogers "Where do you know what you know? The representation of semantic knowledge in the human brain", Nature Reviews Neuroscience 8, 976-987 (December 2007)
	The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.== Structure ==KiSAO consists of three main branches:simulation algorithmsimulation algorithm characteristicsimulation algorithm parameterThe elements of each algorithm branch are linked to characteristic and parameter branches using has characteristic and has parameter relationships accordingly. The algorithm branch itself is hierarchically structured using relationships which denote that the descendant algorithms were derived from, or specify, more general ancestors.== See also ==COMBINESED-MLMIRIAMSBOTEDDY== References ==
	In computational algebra, the Cantor–Zassenhaus algorithm is a method for factoring polynomials over finite fields (also called Galois fields).The algorithm consists mainly of exponentiation and polynomial GCD computations. It was invented by David G. Cantor and Hans Zassenhaus in 1981.It is arguably the dominant algorithm for solving the problem, having replaced the earlier Berlekamp's algorithm of 1967. It is currently implemented in many computer algebra systems.== Overview ===== Background ===The Cantor–Zassenhaus algorithm takes as input a squarefree polynomial                     f        (        x        )              {\displaystyle f(x)}   (i.e. one with no repeated factors) of degree n with coefficients in a finite field                                           F                                q                                {\displaystyle \mathbb {F} _{q}}   whose irreducible polynomial factors are all of equal degree (algorithms exist for efficiently factoring arbitrary polynomials into a product of polynomials satisfying these conditions, for instance,                     f        (        x        )                  /                G        c        d        (        f        (        x        )        ,                  f          ′                (        x        )        )              {\displaystyle f(x)/Gcd(f(x),f'(x))}   is a squarefree polynomial with the same factors as                     f        (        x        )              {\displaystyle f(x)}  , so that the Cantor–Zassenhaus algorithm can be used to factor arbitrary polynomials).  It gives as output a polynomial                     g        (        x        )              {\displaystyle g(x)}   with coefficients in the same field such that                     g        (        x        )              {\displaystyle g(x)}   divides                     f        (        x        )              {\displaystyle f(x)}  .  The algorithm may then be applied recursively to these and subsequent divisors, until we find the decomposition of                     f        (        x        )              {\displaystyle f(x)}   into powers of irreducible polynomials (recalling that the ring of polynomials over any field is a unique factorisation domain).All possible factors of                     f        (        x        )              {\displaystyle f(x)}   are contained within the factor ring                    R        =                                                                              F                                                  q                                            [              x              ]                                      ⟨              f              (              x              )              ⟩                                            {\displaystyle R={\frac {\mathbb {F} _{q}[x]}{\langle f(x)\rangle }}}  .  If we suppose that                     f        (        x        )              {\displaystyle f(x)}   has irreducible factors                               p                      1                          (        x        )        ,                  p                      2                          (        x        )        ,        …        ,                  p                      s                          (        x        )              {\displaystyle p_{1}(x),p_{2}(x),\ldots ,p_{s}(x)}  , all of degree d, then this factor ring is isomorphic to the direct product of factor rings                     S        =                  ∏                      i            =            1                                s                                                                                                F                                                  q                                            [              x              ]                                      ⟨                              p                                  i                                            (              x              )              ⟩                                            {\displaystyle S=\prod _{i=1}^{s}{\frac {\mathbb {F} _{q}[x]}{\langle p_{i}(x)\rangle }}}  .  The isomorphism from R to S, say                     ϕ              {\displaystyle \phi }  , maps a polynomial                     g        (        x        )        ∈        R              {\displaystyle g(x)\in R}   to the s-tuple of its reductions modulo each of the                               p                      i                          (        x        )              {\displaystyle p_{i}(x)}  , i.e. if:                                                                        g                (                x                )                                                                                            ≡                                  g                                      1                                                  (                x                )                                                    (                  mod                                                        p                                          1                                                        (                  x                  )                  )                                ,                                                                    g                (                x                )                                                                                            ≡                                  g                                      2                                                  (                x                )                                                    (                  mod                                                        p                                          2                                                        (                  x                  )                  )                                ,                                                                                                                                                    ⋮                                                                    g                (                x                )                                                                                            ≡                                  g                                      s                                                  (                x                )                                                    (                  mod                                                        p                                          s                                                        (                  x                  )                  )                                ,                                                          {\displaystyle {\begin{aligned}g(x)&{}\equiv g_{1}(x){\pmod {p_{1}(x)}},\\g(x)&{}\equiv g_{2}(x){\pmod {p_{2}(x)}},\\&{}\ \ \vdots \\g(x)&{}\equiv g_{s}(x){\pmod {p_{s}(x)}},\end{aligned}}}  then                     ϕ        (        g        (        x        )        +        ⟨        f        (        x        )        ⟩        )        =        (                  g                      1                          (        x        )        +        ⟨                  p                      1                          (        x        )        ⟩        ,        …        ,                  g                      s                          (        x        )        +        ⟨                  p                      s                          (        x        )        ⟩        )              {\displaystyle \phi (g(x)+\langle f(x)\rangle )=(g_{1}(x)+\langle p_{1}(x)\rangle ,\ldots ,g_{s}(x)+\langle p_{s}(x)\rangle )}  .  It is important to note the following at this point, as it shall be of critical importance later in the algorithm:  Since the                               p                      i                          (        x        )              {\displaystyle p_{i}(x)}   are each irreducible, each of the factor rings in this direct sum is in fact a field.  These fields each have degree                               q                      d                                {\displaystyle q^{d}}  .=== Core result ===The core result underlying the Cantor–Zassenhaus algorithm is the following:  If                     a        (        x        )        ∈        R              {\displaystyle a(x)\in R}   is a polynomial satisfying:                    a        (        x        )        ≠        0        ,        ±        1              {\displaystyle a(x)\neq 0,\pm 1}                                a                      i                          (        x        )        ∈        {        0        ,        −        1        ,        1        }                   for                 i        =        1        ,        2        ,        …        ,        s        ,              {\displaystyle a_{i}(x)\in \{0,-1,1\}{\text{ for }}i=1,2,\ldots ,s,}  where                               a                      i                          (        x        )              {\displaystyle a_{i}(x)}   is the reduction of                     a        (        x        )              {\displaystyle a(x)}   modulo                               p                      i                          (        x        )              {\displaystyle p_{i}(x)}   as before, and if any two of the following three sets is non-empty:                    A        =        {        i                  |                          a                      i                          (        x        )        =        0        }        ,              {\displaystyle A=\{i|a_{i}(x)=0\},}                      B        =        {        i                  |                          a                      i                          (        x        )        =        −        1        }        ,              {\displaystyle B=\{i|a_{i}(x)=-1\},}                      C        =        {        i                  |                          a                      i                          (        x        )        =        1        }        ,              {\displaystyle C=\{i|a_{i}(x)=1\},}  then there exist the following non-trivial factors of                     f        (        x        )              {\displaystyle f(x)}  :                    gcd        (        f        (        x        )        ,        a        (        x        )        )        =                  ∏                      i            ∈            A                                    p                      i                          (        x        )        ,              {\displaystyle \gcd(f(x),a(x))=\prod _{i\in A}p_{i}(x),}                      gcd        (        f        (        x        )        ,        a        (        x        )        +        1        )        =                  ∏                      i            ∈            B                                    p                      i                          (        x        )        ,              {\displaystyle \gcd(f(x),a(x)+1)=\prod _{i\in B}p_{i}(x),}                      gcd        (        f        (        x        )        ,        a        (        x        )        −        1        )        =                  ∏                      i            ∈            C                                    p                      i                          (        x        )        .              {\displaystyle \gcd(f(x),a(x)-1)=\prod _{i\in C}p_{i}(x).}  === Algorithm ===The Cantor–Zassenhaus algorithm computes polynomials of the same type as                     a        (        x        )              {\displaystyle a(x)}   above using the isomorphism discussed in the Background section.  It proceeds as follows, in the case where the field                                           F                                q                                {\displaystyle \mathbb {F} _{q}}   is of odd-characteristic (the process can be generalised to characteristic 2 fields in a fairly straightforward way).  Select a random polynomial                     b        (        x        )        ∈        R              {\displaystyle b(x)\in R}   such that                     b        (        x        )        ≠        0        ,        ±        1              {\displaystyle b(x)\neq 0,\pm 1}  .  Set                     m        =        (                  q                      d                          −        1        )                  /                2              {\displaystyle m=(q^{d}-1)/2}   and compute                     b        (        x                  )                      m                                {\displaystyle b(x)^{m}}  .  Since                     ϕ              {\displaystyle \phi }   is an isomorphism, we have (using our now-established notation):                    ϕ        (        b        (        x                  )                      m                          )        =        (                  b                      1                                m                          (        x        )        +        ⟨                  p                      1                          (        x        )        ⟩        ,        …        ,                  b                      s                                m                          (        x        )        +        ⟨                  p                      s                          (        x        )        ⟩        )        .              {\displaystyle \phi (b(x)^{m})=(b_{1}^{m}(x)+\langle p_{1}(x)\rangle ,\ldots ,b_{s}^{m}(x)+\langle p_{s}(x)\rangle ).}  Now, each                               b                      i                          (        x        )        +        ⟨                  p                      i                          (        x        )        ⟩              {\displaystyle b_{i}(x)+\langle p_{i}(x)\rangle }   is an element of a field of order                               q                      d                                {\displaystyle q^{d}}  , as noted earlier.  The multiplicative subgroup of this field has order                               q                      d                          −        1              {\displaystyle q^{d}-1}   and so, unless                               b                      i                          (        x        )        =        0              {\displaystyle b_{i}(x)=0}  , we have                               b                      i                          (        x                  )                                    q                              d                                      −            1                          =        1              {\displaystyle b_{i}(x)^{q^{d}-1}=1}   for each i and hence                               b                      i                          (        x                  )                      m                          =        ±        1              {\displaystyle b_{i}(x)^{m}=\pm 1}   for each i.  If                               b                      i                          (        x        )        =        0              {\displaystyle b_{i}(x)=0}  , then of course                               b                      i                          (        x                  )                      m                          =        0              {\displaystyle b_{i}(x)^{m}=0}  .  Hence                     b        (        x                  )                      m                                {\displaystyle b(x)^{m}}   is a polynomial of the same type as                     a        (        x        )              {\displaystyle a(x)}   above.  Further, since                     b        (        x        )        ≠        0        ,        ±        1              {\displaystyle b(x)\neq 0,\pm 1}  , at least two of the sets                     A        ,        B              {\displaystyle A,B}   and C are non-empty and by computing the above GCDs we may obtain non-trivial factors.  Since the ring of polynomials over a field is a Euclidean domain, we may compute these GCDs using the Euclidean algorithm.== Applications ==One important application of the Cantor–Zassenhaus algorithm is in computing discrete logarithms over finite fields of prime-power order.  Computing discrete logarithms is an important problem in public key cryptography.  For a field of prime-power order, the fastest known method is the index calculus method, which involves the factorisation of field elements.  If we represent the prime-power order field in the usual way – that is, as polynomials over the prime order base field, reduced modulo an irreducible polynomial of appropriate degree – then this is simply polynomial factorisation, as provided by the Cantor–Zassenhaus algorithm.== Implementation in computer algebra systems ==The Cantor–Zassenhaus algorithm is implemented in the PARI/GP computer algebra system as the factorcantor() function.== See also ==Polynomial factorizationFactorization of polynomials over finite fields== References ==Cantor, David G.; Zassenhaus, Hans (April 1981), "A new algorithm for factoring polynomials over finite fields", Mathematics of Computation, 36 (154): 587–592, doi:10.1090/S0025-5718-1981-0606517-5, JSTOR 2007663, MR 0606517http://blog.fkraiem.org/2013/12/01/polynomial-factorisation-over-finite-fields-part-3-final-splitting-cantor-zassenhaus-in-odd-characteristic/
	A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.Among HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method that performs in almost linear time with respect to the size of the source, making it practically useful to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.== Introduction ==One and the same thing can be used for many purposes.  A chair might be used to support a sitting person but also as a ladder to stand on while changing a light bulb or as a clothes valet. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.Different uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a rope though it might materially be the same thing. The same in everyday language, where a leash indicates a use different to a line.In computer science, this practice of naming things by its intended use is put to an extreme called typing and the names or expressions called types:Contrary to the richly varying raw materials in the physical world, computing has only one raw material, bits, and much like letters in a book, sequences of them are used to build up and express everything in a computer's memory.Programmers not only have many words indicating the different uses for bytes, but rather a plethora of formal type languages, each having their own grammar.There are branches of research in computer science and mathematics dedicated to developing and enhancing such languages. Their theoretical foundations have type systems and type theory as their subjects.Types are not only used in specifications and documentations, but are also an integral part of programming languages. There, they are mechanised to strongly support the programmers' tasks.Beside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a crate that could only be used as a support or a container at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.In programming, these uses are expressed as functions or procedures which serve the role of verbs in natural language. As an example for typing verbs, an English dictionary might define gift as "to give someone something", indicating that the object must be a person and the indirect object a physical thing. In programming, "someone" and "something" would be called types. Using a physical thing in the place of "someone" would be indicated as a programming error by a type checker.Beside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence "Mary gifts John a bilber" the types could be used to conclude that a "bilber" is likely a physical thing. This activity and conclusion is called type inference. As the story unfolds, more and more information about the unknown "bilber" may be gained, and eventually enough details become known to form a complete image of that kind of thing.The type inference method designed by Hindley and Milner does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types).  As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.=== History of type inference ===Historically, type inference to this extent was developed for a particular group of programming languages, called functional languages. These started in 1958 with Lisp, a programming language based on the lambda calculus and that compares well with modern scripting languages like Python or Lua. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.Data in Lisp is dynamically typed and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.Thus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:Polymorphism. Some kinds of data are very generic. In particular, Lisp is a "list programming language" where lists are data whose type can be a "list of something", e.g. a list of numbers.  Functions for such generic data types are often themselves generic. For instance, counting the number of items on a list is independent of the type of its items. However, a generic function that adds another item to a given list needs type checking to ensure that the list will remain consistent with respect to the type of its items. For example, that only numbers may be added to a list of numbers.  Types for such "generic" data and functions are called polymorphic, meaning that they can be used for more than one type. The polymorphic function for adding an item can be used for a list of numbers as well as for a list of words or even a list of anything. More precisely, this kind of polymorphism is called parametric polymorphism, where "something" is the parameter in "list of something".  More formally, "list of something" may be written List T with T being the type parameter. The type of a function that adds a new item to a list is forall T . T -> List T -> List T, meaning that for any and all types T the adding function needs an item of type T and a list-of-Ts, to produce a resulting (new) list-of-Ts.  Thus, the first challenge was to design a type system that properly expressed parametric polymorphism.Type inference. Unfortunately, polymorphic functions requiring type checking must be continuously informed of the types. The above function would need the type T as an additional first parameter, resulting in program text so cluttered with type information that it becomes unreadable. Additionally, when keying in a program, a programmer would spend a significant portion of time keying in types.As an example, polymorphically constructing the list "(1 2)" of two numbers would mean writing:This example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.To handle this issue, effective methods for type inference were the subject of research, and Hindley–Milner's method was one of them. Their method was first used in ML (1973) and is also used in an extended form in Haskell (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.The following text gives an impression of the resulting programming style for the quicksort procedure in Haskell:Though all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.Over the years, other programming languages added their own version of parametric types. C++ templates were introduced in 1998 and Java introduced generics in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in imperative languages too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance "auto" in C++11 (2014). Typically, the stronger type inference methods developed for functional programming cannot easily be integrated in the imperative languages, as their type systems' features are in part incompatible. However, through the support of additional techniques, it is actually possible to provide Haskell- and ML-style type inference even for a language like C which was designed decades ago, without any consideration for such a mechanism.== Features of the Hindley–Milner method ==Before presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.=== Type-checking vs. type-inference ===In a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.In this setting, the following questions are of particular interest:E : T? In this case, both an expression E and a type T are given. Now, is E really a T? This scenario is known as type-checking.E : _? Here, only the expression is known. So, what type is E? If there is a way to derive a type for E, then we have accomplished type inference._ : T? The other way round. Given only a type, is there any expression for it or does the type have no values? Is there any example of a T? And in light of the Curry–Howard isomorphism, is there a proof for T?For the simply typed lambda calculus, all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.=== Monomorphism vs. polymorphism ===In the simply typed lambda calculus, types                     T              {\displaystyle T}   are either atomic type constants or function types of form                     T        →        T              {\displaystyle T\rightarrow T}  .  Such types are monomorphic. Typical examples are the types used in arithmetic values: 3       : Number add 3 4 : Number add     : Number -> Number -> NumberContrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity functionid                     ≡        λ              {\displaystyle \equiv \lambda }   x . xwhich simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.While polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of type schemes in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.: cons : forall a . a -> List a -> List a nil  : forall a . List a. id   : forall a . a -> aPolymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic instances are:id'  : String -> Stringnil' : List NumberMore generally, types are polymorphic when they contain type variables, while types without them are monomorphic.Contrary to the type systems used for example in Pascal (1970) or C (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like C++ (1985), focused on different types of polymorphism, namely subtyping in connection with object-oriented programming and overloading. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.=== Let-polymorphism ===When extending the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in: (λ id .  ... (id 3) ... (id "text") ... ) (λ x . x)Unfortunately, type inference in polymorphic lambda calculus is not decidable. Instead, HM provides a let-polymorphism of the form let id = λ x . x  in ... (id 3) ... (id "text") ...restricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.== Overview ==The remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:The HM type system is defined. This is done by describing a deduction system that makes precise what expressions have what type, if any.From there, it works towards an implementation of the type inference method. After introducing a syntax driven variant of the above deductive system, it sketches an efficient implementation (algorithm J), appealing mostly to the reader's metalogical intuition.Because it remains open whether algorithm J indeed realises the initial deduction system, a less efficient implementation (algorithm W), is introduced and its use in a proof is hinted.Finally, further topics related to the algorithm are discussed.The same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.== The Hindley–Milner type system ==The type system can be formally described by syntax rules that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the surface grammar, but rather the depth grammar, and leaves some syntactical details open. This form of presentation is usual. Building on this, type rules are used to define how expressions and types are related. As before, the form used is a bit liberal.=== Syntax ===The expressions to be typed are exactly those of the lambda calculus extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.Types are syntactically split into two groups, monotypes and polytypes.==== Monotypes ====Monotypes always designate a particular type. Monotypes                     τ              {\displaystyle \tau }   are syntactically represented as terms.Examples of monotypes include type constants like                                           i            n            t                                {\displaystyle {\mathtt {int}}}   or                                           s            t            r            i            n            g                                {\displaystyle {\mathtt {string}}}  , and parametric types like                                           M            a            p                         (            S            e            t                         s            t            r            i            n            g            )                         i            n            t                                {\displaystyle {\mathtt {Map\ (Set\ string)\ int}}}  .   The latter types are examples of applications of type functions, for example, from the set                    {                              M            a                          p                              2                                      ,                         S            e                          t                              1                                      ,                         s            t            r            i            n                          g                              0                                      ,                         i            n                          t                              0                                                    ,                           →                      2                          }              {\displaystyle \{{\mathtt {Map^{2},\ Set^{1},\ string^{0},\ int^{0}}},\ \rightarrow ^{2}\}}  , where the superscript indicates the number of type parameters.  The complete set of type functions                     C              {\displaystyle C}   is arbitrary in HM, except that it must contain at least                               →                      2                                {\displaystyle \rightarrow ^{2}}  , the type of functions.  It is often written in infix notation for convenience.  For example, a function mapping integers to strings has type                                           i            n            t                          →                              s            t            r            i            n            g                                {\displaystyle {\mathtt {int}}\rightarrow {\mathtt {string}}}  . Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.Type variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.Two monotypes are equal if they have identical terms.==== Polytypes ====Polytypes (or type schemes) are types containing variables bound by one or more for-all quantifiers, e.g.                     ∀        α        .        α        →        α              {\displaystyle \forall \alpha .\alpha \rightarrow \alpha }  .A function with polytype                     ∀        α        .        α        →        α              {\displaystyle \forall \alpha .\alpha \rightarrow \alpha }   can map any value of the same type to itself,and the identity function is a value for this type.As another example,                     ∀        α        .        (                              S            e            t                                   α        )        →                              i            n            t                                {\displaystyle \forall \alpha .({\mathtt {Set}}\ \alpha )\rightarrow {\mathtt {int}}}   is the type of a function mapping all finite sets to integers. A function which returns the cardinality of a set would be a value of this type.Quantifiers can only appear top level. For instance, a type                     ∀        α        .        α        →        ∀        α        .        α              {\displaystyle \forall \alpha .\alpha \rightarrow \forall \alpha .\alpha }   is excluded by the syntax of types. Also monotypes are included in the polytypes, thus a type has the general form                     ∀                  α                      1                          …        ∀                  α                      n                          .        τ        ,        n        ≥        0              {\displaystyle \forall \alpha _{1}\dots \forall \alpha _{n}.\tau ,n\geq 0}  , where                     τ              {\displaystyle \tau }   is a monotype.Equality of polytypes is up to reordering the quantification and renaming the quantified variables (                    α              {\displaystyle \alpha }  -conversion). Further, quantified variables not occurring in the monotype can be dropped.==== Context and typing ====To meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs                     x        :        σ              {\displaystyle x:\sigma }  , called assignments, assumptions or bindings, each pair stating that value variable                               x                      i                                {\displaystyle x_{i}}  has type                               σ                      i                                {\displaystyle \sigma _{i}}  . All three parts combined give a typing judgment of the form                     Γ                 ⊢                 e        :        σ              {\displaystyle \Gamma \ \vdash \ e:\sigma }  , stating that under assumptions                     Γ              {\displaystyle \Gamma }  , the expression                     e              {\displaystyle e}   has type                     σ              {\displaystyle \sigma }  .==== Free type variables ====In a type                     ∀                  α                      1                          …        ∀                  α                      n                          .        τ              {\displaystyle \forall \alpha _{1}\dots \forall \alpha _{n}.\tau }  , the symbol                     ∀              {\displaystyle \forall }   is the quantifier binding the type variables                               α                      i                                {\displaystyle \alpha _{i}}   in the monotype                     τ              {\displaystyle \tau }  . The variables                               α                      i                                {\displaystyle \alpha _{i}}   are called quantified and any occurrence of a quantified type variable in                     τ              {\displaystyle \tau }   is called bound and all unbound type variables in                     τ              {\displaystyle \tau }   are called free. Additionally to the quantification                     ∀              {\displaystyle \forall }   in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the                     ⊢              {\displaystyle \vdash }  . Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.The presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in Prolog. Likely in Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type a -> a means                     ∀        α        .        α        →        α              {\displaystyle \forall \alpha .\alpha \rightarrow \alpha }   here.=== Type order ===Polymorphism means that one and the same expression can have (perhapsinfinitely) many types. But in this type system, these types are not completelyunrelated, but rather orchestrated by the parametric polymorphism.As an example, the identity                     λ        x        .        x              {\displaystyle \lambda x.x}   can have                     ∀        α        .        α        →        α              {\displaystyle \forall \alpha .\alpha \rightarrow \alpha }   as its type as well as                                          string                          →                              string                                {\displaystyle {\texttt {string}}\rightarrow {\texttt {string}}}   or                                           int                          →                              int                                {\displaystyle {\texttt {int}}\rightarrow {\texttt {int}}}   and many others, but not                                           int                          →                              string                                {\displaystyle {\texttt {int}}\rightarrow {\texttt {string}}}  . The most general type for this function is                    ∀        α        .        α        →        α              {\displaystyle \forall \alpha .\alpha \rightarrow \alpha }  , while theothers are more specific and can be derived from the general one by consistentlyreplacing another type for the type parameter, i.e. the quantifiedvariable                     α              {\displaystyle \alpha }  .  The counter-example fails because thereplacement is not consistent.The consistent replacement can be made formal by applying a substitution                     S        =                  {                                                 a                              i                                      ↦                          τ                              i                                      ,                         …                                 }                      {\displaystyle S=\left\{\ a_{i}\mapsto \tau _{i},\ \dots \ \right\}}   to the term of a type                     τ              {\displaystyle \tau }  , written                     S        τ              {\displaystyle S\tau }  . As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.Formally, in HM, a type                     σ              {\displaystyle \sigma }   is more general than                               σ          ′                      {\displaystyle \sigma '}  , formally                     σ        ⊑                  σ          ′                      {\displaystyle \sigma \sqsubseteq \sigma '}   if some quantified variable in                     σ              {\displaystyle \sigma }   isconsistently substituted such that one gains                               σ          ′                      {\displaystyle \sigma '}   as shown in the side bar.This order is part of the type definition of the type system.While substituting a monomorphic (ground) type for a quantified variable isstraight forward, substituting a polytype has some pitfalls caused by thepresence of free variables. Most particularly, unbound variables must not bereplaced. They are treated as constants here. Additionally, quantifications can only occur top-level. Substituting a parametric type,one has to lift its quantors. The table on the right makes the rule precise.Alternatively, consider an equivalent notation for the polytypes withoutquantors in which quantified variables are represented by a different set ofsymbols. In such a notation, the specialization reduces to plain consistentreplacement of such variables.The relation                     ⊑              {\displaystyle \sqsubseteq }   is a partial orderand                      ∀        α        .        α              {\displaystyle \forall \alpha .\alpha }   is its smallest element.==== Principal type ====While specialization of a type scheme is one use of the order, it plays acrucial second role in the type system. Type inference with polymorphismfaces the challenge of summarizing all possible types an expression may have.The order guarantees that such a summary exists as the most general typeof the expression.==== Substitution in typings ====The type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:                    Γ        ⊢        e        :        σ                ⟹                S        Γ        ⊢        e        :        S        σ              {\displaystyle \Gamma \vdash e:\sigma \quad \Longrightarrow \quad S\Gamma \vdash e:S\sigma }  Contrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.Free type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free typevariables on the right hand side of                     ⊢              {\displaystyle \vdash }   that prohibits their substitution in the specialisation rule is againthat a replacement has to be consistent and would need to include the whole typing.=== Deductive system ===The syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.A proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the                     [                              N            a            m            e                          ]              {\displaystyle [{\mathtt {Name}}]}   of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.==== Typing rules ====The side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:The first four rules                     [                              V            a            r                          ]              {\displaystyle [{\mathtt {Var}}]}   (variable or function access),                     [                              A            p            p                          ]              {\displaystyle [{\mathtt {App}}]}   (application, i.e. function call with one parameter),                     [                              A            b            s                          ]              {\displaystyle [{\mathtt {Abs}}]}   (abstraction, i.e. function declaration) and                     [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}   (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.The second group is formed by the  remaining two rules                     [                              I            n            s            t                          ]              {\displaystyle [{\mathtt {Inst}}]}   and                     [                              G            e            n                          ]              {\displaystyle [{\mathtt {Gen}}]}  .They handle specialization and generalization of types. While the rule                     [                              I            n            s            t                          ]              {\displaystyle [{\mathtt {Inst}}]}   should be clear from the section on specialization above,                     [                              G            e            n                          ]              {\displaystyle [{\mathtt {Gen}}]}   complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.The following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.Example: A proof for                     Γ                  ⊢                      D                          i        d        (        n        )        :        i        n        t              {\displaystyle \Gamma \vdash _{D}id(n):int}   where                     Γ        =        i        d        :        ∀        α        .        α        →        α        ,                 n        :        i        n        t              {\displaystyle \Gamma =id:\forall \alpha .\alpha \rightarrow \alpha ,\ n:int}  ,could be written                                                                        1                :                                            Γ                                  ⊢                                      D                                                  i                d                :                ∀                α                .                α                →                α                                            [                                                      V                    a                    r                                                  ]                                            (                i                d                :                ∀                α                .                α                →                α                ∈                Γ                )                                                                    2                :                                            Γ                                  ⊢                                      D                                                  i                d                :                i                n                t                →                i                n                t                                            [                                                      I                    n                    s                    t                                                  ]                                            (                1                )                ,                                 (                ∀                α                .                α                →                α                ⊑                i                n                t                →                i                n                t                )                                                                    3                :                                            Γ                                  ⊢                                      D                                                  n                :                i                n                t                                            [                                                      V                    a                    r                                                  ]                                            (                n                :                i                n                t                ∈                Γ                )                                                                    4                :                                            Γ                                  ⊢                                      D                                                  i                d                (                n                )                :                i                n                t                                            [                                                      A                    p                    p                                                  ]                                            (                2                )                ,                                 (                3                )                                                          {\displaystyle {\begin{array}{llll}1:&\Gamma \vdash _{D}id:\forall \alpha .\alpha \rightarrow \alpha &[{\mathtt {Var}}]&(id:\forall \alpha .\alpha \rightarrow \alpha \in \Gamma )\\2:&\Gamma \vdash _{D}id:int\rightarrow int&[{\mathtt {Inst}}]&(1),\ (\forall \alpha .\alpha \rightarrow \alpha \sqsubseteq int\rightarrow int)\\3:&\Gamma \vdash _{D}n:int&[{\mathtt {Var}}]&(n:int\in \Gamma )\\4:&\Gamma \vdash _{D}id(n):int&[{\mathtt {App}}]&(2),\ (3)\\\end{array}}}  Example: To demonstrate generalization,                              ⊢                      D                                                         let                                  i        d        =        λ        x        .        x                                       in                                   i        d                :                ∀        α        .        α        →        α              {\displaystyle \vdash _{D}\ {\textbf {let}}\,id=\lambda x.x\ {\textbf {in}}\ id\,:\,\forall \alpha .\alpha \rightarrow \alpha }  is shown below:                                                                        1                :                                            x                :                α                                  ⊢                                      D                                                  x                :                α                                            [                                                      V                    a                    r                                                  ]                                            (                x                :                α                ∈                                  {                                      x                    :                    α                                    }                                )                                                                    2                :                                                              ⊢                                      D                                                  λ                x                .                x                :                α                →                α                                            [                                                      A                    b                    s                                                  ]                                            (                1                )                                                                    3                :                                                              ⊢                                      D                                                  λ                x                .                x                :                ∀                α                .                α                →                α                                            [                                                      G                    e                    n                                                  ]                                            (                2                )                ,                                 (                α                ∉                f                r                e                e                (                ϵ                )                )                                                                    4                :                                            i                d                :                ∀                α                .                α                →                α                                  ⊢                                      D                                                  i                d                :                ∀                α                .                α                →                α                                            [                                                      V                    a                    r                                                  ]                                            (                i                d                :                ∀                α                .                α                →                α                ∈                                  {                                      i                    d                    :                    ∀                    α                    .                    α                    →                    α                                    }                                )                                                                    5                :                                                              ⊢                                      D                                                                                        let                                                                  i                d                =                λ                x                .                x                                                                       in                                                                   i                d                                :                                ∀                α                .                α                →                α                                            [                                                      L                    e                    t                                                  ]                                            (                3                )                ,                                 (                4                )                                                          {\displaystyle {\begin{array}{llll}1:&x:\alpha \vdash _{D}x:\alpha &[{\mathtt {Var}}]&(x:\alpha \in \left\{x:\alpha \right\})\\2:&\vdash _{D}\lambda x.x:\alpha \rightarrow \alpha &[{\mathtt {Abs}}]&(1)\\3:&\vdash _{D}\lambda x.x:\forall \alpha .\alpha \rightarrow \alpha &[{\mathtt {Gen}}]&(2),\ (\alpha \not \in free(\epsilon ))\\4:&id:\forall \alpha .\alpha \rightarrow \alpha \vdash _{D}id:\forall \alpha .\alpha \rightarrow \alpha &[{\mathtt {Var}}]&(id:\forall \alpha .\alpha \rightarrow \alpha \in \left\{id:\forall \alpha .\alpha \rightarrow \alpha \right\})\\5:&\vdash _{D}{\textbf {let}}\,id=\lambda x.x\ {\textbf {in}}\ id\,:\,\forall \alpha .\alpha \rightarrow \alpha &[{\mathtt {Let}}]&(3),\ (4)\\\end{array}}}  === Let-polymorphism ===Not visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules                     [                              A            b            s                          ]              {\displaystyle [{\mathtt {Abs}}]}   and                     [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}  . Remember that                     σ              {\displaystyle \sigma }   and                     τ              {\displaystyle \tau }   denote poly- and monotypes respectively.In rule                     [                              A            b            s                          ]              {\displaystyle [{\mathtt {Abs}}]}  , the value variable of the parameter of the function                     λ        x        .        e              {\displaystyle \lambda x.e}   is added to the context with a monomorphic type through the premise                     Γ        ,                 x        :        τ                  ⊢                      D                          e        :                  τ          ′                      {\displaystyle \Gamma ,\ x:\tau \vdash _{D}e:\tau '}  , while in the rule                      [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}  , the variable enters the environment in polymorphic form                     Γ        ,                 x        :        σ                  ⊢                      D                                    e                      1                          :        τ              {\displaystyle \Gamma ,\ x:\sigma \vdash _{D}e_{1}:\tau }  . Though in both cases the presence of                     x              {\displaystyle x}   in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter                     x              {\displaystyle x}   in a                     λ              {\displaystyle \lambda }  -expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.As a consequence of this regulation,                     λ        f        .        (        f                                      true                          ,        f                                      0                          )              {\displaystyle \lambda f.(f\,{\textrm {true}},f\,{\textrm {0}})}   cannot be typed,since the parameter                     f              {\displaystyle f}   is in a monomorphic position, while                                           let                                   f        =        λ        x        .        x                                      in                                  (        f                                      true                          ,        f                                      0                          )              {\displaystyle {\textbf {let}}\ f=\lambda x.x\,{\textbf {in}}\,(f\,{\textrm {true}},f\,{\textrm {0}})}   has type                     (        b        o        o        l        ,        i        n        t        )              {\displaystyle (bool,int)}  , because                     f              {\displaystyle f}   has been introduced in a let-expression and is treated polymorphic therefore.=== Generalization rule ===The generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise                     Γ        ⊢        e        :        σ              {\displaystyle \Gamma \vdash e:\sigma }   is simply moved to the right hand side of                               ⊢                      D                                {\displaystyle \vdash _{D}}   in the conclusion. This is possible, since                     α              {\displaystyle \alpha }   does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.== An inference algorithm ==Now that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.Alternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof areformed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.=== Degrees of freedom choosing the rules ===Isolating the points in a proof, where no decision is possible at all,the first group of rules centered around the syntax leaves no choice sinceto each syntactical rule corresponds a unique typing rule, which determinesa part of the proof, while between the conclusion and the premises of thesefixed parts chains of                     [                              I            n            s            t                          ]              {\displaystyle [{\mathtt {Inst}}]}   and                     [                              G            e            n                          ]              {\displaystyle [{\mathtt {Gen}}]}  could occur. Such a chain could also exist between the conclusion of theproof and the rule for topmost expression. All proofs must havethe so sketched shape.Because the only choice in a proof with respect of rule selection are the                    [                              I            n            s            t                          ]              {\displaystyle [{\mathtt {Inst}}]}   and                     [                              G            e            n                          ]              {\displaystyle [{\mathtt {Gen}}]}   chains, theform of the proof suggests the question whether it can be made more precise,where these chains might be needed. This is in fact possible and leads to avariant of the rules system with no such rules.=== Syntax-directed rule system ===A contemporary treatment of HM uses a purely syntax-directed rule system due toClementas an intermediate step. In this system, the specialization is located directly after the original                     [                              V            a            r                          ]              {\displaystyle [{\mathtt {Var}}]}   ruleand merged into it, while the generalization becomes part of the                     [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}   rule. There the generalization isalso determined to always produce the most general type by introducing the function                                                         Γ              ¯                                      (        τ        )              {\displaystyle {\bar {\Gamma }}(\tau )}  , which quantifiesall monotype variables not bound in                     Γ              {\displaystyle \Gamma }  .Formally, to validate, that this new rule system                               ⊢                      S                                {\displaystyle \vdash _{S}}   is equivalent to the original                               ⊢                      D                                {\displaystyle \vdash _{D}}  , one hasto show that                     Γ                  ⊢                      D                                   e        :        σ        ⇔        Γ                  ⊢                      S                                   e        :        σ              {\displaystyle \Gamma \vdash _{D}\ e:\sigma \Leftrightarrow \Gamma \vdash _{S}\ e:\sigma }  , which falls apart into two sub-proofs:                    Γ                  ⊢                      D                                   e        :        σ        ⇐        Γ                  ⊢                      S                                   e        :        σ              {\displaystyle \Gamma \vdash _{D}\ e:\sigma \Leftarrow \Gamma \vdash _{S}\ e:\sigma }   (Consistency)                    Γ                  ⊢                      D                                   e        :        σ        ⇒        Γ                  ⊢                      S                                   e        :        σ              {\displaystyle \Gamma \vdash _{D}\ e:\sigma \Rightarrow \Gamma \vdash _{S}\ e:\sigma }   (Completeness)While consistency can be seen by decomposing the rules                     [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}   and                     [                              V            a            r                          ]              {\displaystyle [{\mathtt {Var}}]}  of                               ⊢                      S                                {\displaystyle \vdash _{S}}   into proofs in                               ⊢                      D                                {\displaystyle \vdash _{D}}  , it is likely visible that                               ⊢                      S                                {\displaystyle \vdash _{S}}   is incomplete, asone cannot show                     λ                 x        .        x        :        ∀        α        .        α        →        α              {\displaystyle \lambda \ x.x:\forall \alpha .\alpha \rightarrow \alpha }   in                               ⊢                      S                                {\displaystyle \vdash _{S}}  , for instance, but only                    λ                 x        .        x        :        α        →        α              {\displaystyle \lambda \ x.x:\alpha \rightarrow \alpha }  .  An only slightly weaker version of completeness is provable though, namely                    Γ                  ⊢                      D                                   e        :        σ        ⇒        Γ                  ⊢                      S                                   e        :        τ        ∧                                            Γ              ¯                                      (        τ        )        ⊑        σ              {\displaystyle \Gamma \vdash _{D}\ e:\sigma \Rightarrow \Gamma \vdash _{S}\ e:\tau \wedge {\bar {\Gamma }}(\tau )\sqsubseteq \sigma }  implying, one can derive the principal type for an expression in                               ⊢                      S                                {\displaystyle \vdash _{S}}   allowing us to generalize the proof in the end.Comparing                               ⊢                      D                                {\displaystyle \vdash _{D}}   and                               ⊢                      S                                {\displaystyle \vdash _{S}}  , now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as trees). Thus the expression fully determines the shape of the proof. In                               ⊢                      D                                {\displaystyle \vdash _{D}}   the shape would likely be determined with respect to all rules except                     [                              I            n            s            t                          ]              {\displaystyle [{\mathtt {Inst}}]}   and                     [                              G            e            n                          ]              {\displaystyle [{\mathtt {Gen}}]}  , which allow building arbitrarily long branches (chains) between the other nodes.=== Degrees of freedom instantiating the rules ===Now that the shape of the proof is known, one is already close to formulating a type inference algorithm.Because any proof for a given expression must have the same shape, one can assume the monotypes in theproof's judgements to be undetermined and consider how to determine them.Here, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of                               ⊢                      S                                {\displaystyle \vdash _{S}}   suggests:                    [        A        b        s        ]              {\displaystyle [Abs]}  : The critical choice is                     τ              {\displaystyle \tau }  . At this point, nothing is known about                     τ              {\displaystyle \tau }  , so one can only assume the most general type, which is                     ∀        α        .        α              {\displaystyle \forall \alpha .\alpha }  . The plan is to specialize the type if it should become necessary. Unfortunately, a polytype is not permitted in this place, so some                     α              {\displaystyle \alpha }   has to do for the moment. To avoid unwanted captures, a type variable not yet in the proof is a safe choice. Additionally, one has to keep in mind that this monotype is not yet fixed, but might be further refined.                    [        V        a        r        ]              {\displaystyle [Var]}  : The choice is how to refine                     σ              {\displaystyle \sigma }  . Because any choice of a type                     τ              {\displaystyle \tau }   here depends on the usage of the variable, which is not locally known, the safest bet is the most general one. Using the same method as above one can instantiate all quantified variables in                     σ              {\displaystyle \sigma }   with fresh monotype variables, again keeping them open to further refinement.                    [        L        e        t        ]              {\displaystyle [Let]}  : The rule does not leave any choice. Done.                    [        A        p        p        ]              {\displaystyle [App]}  : Only the application rule might force a refinement to the variables "opened" so far.The first premise forces the outcome of the inference to be of the form                     τ        →                  τ          ′                      {\displaystyle \tau \rightarrow \tau '}  .If it is, then fine. One can later pick its                               τ          ′                      {\displaystyle \tau '}   for the result.If not, it might be an open variable. Then this can be refined to the required form with two new variables as before.Otherwise, the type checking fails because the first premise inferred a type which is not and cannot be made into a function type.The second premise requires that the inferred type is equal to                     τ              {\displaystyle \tau }   of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to "make two terms equal" by substitution, Robinson's Unification in combination with the so-called Union-Find algorithm.To briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into equivalence classes by means of a                                           u            n            i            o            n                                {\displaystyle {\mathtt {union}}}  procedure and to pick a representative for each such class using a                                           f            i            n            d                                {\displaystyle {\mathtt {find}}}   procedure. Emphasizing the word procedure in the sense of side effect, we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a                                           u            n            i            o            n                          (        a        ,        b        )              {\displaystyle {\mathtt {union}}(a,b)}   is determined such that, if both                     a              {\displaystyle a}   and                     b              {\displaystyle b}   are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:unify(ta,tb):  ta = find(ta)  tb = find(tb)  if both ta,tb are terms of the form D p1..pn with identical D,n then    unify(ta[i],tb[i]) for each corresponding ith parameter  else  if at least one of ta,tb is a type variable then    union(ta,tb)  else    error 'types do not match'Now having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner P. 370 ff. as algorithm J.=== Algorithm J ===The presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with                               ⊢                      S                                {\displaystyle \vdash _{S}}   while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters                     Γ        ,        e              {\displaystyle \Gamma ,e}   yielding                     τ              {\displaystyle \tau }   in the conclusion where the execution of the premises proceeds from left to right.The procedure                     i        n        s        t        (        σ        )              {\displaystyle inst(\sigma )}   specializes the polytype                     σ              {\displaystyle \sigma }   by copying the term and replacing the bound type variables consistently by new monotype variables. '                    n        e        w        v        a        r              {\displaystyle newvar}  ' produces a new monotype variable. Likely,                                                         Γ              ¯                                      (        τ        )              {\displaystyle {\bar {\Gamma }}(\tau )}   has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result                     τ              {\displaystyle \tau }   has to be generalized to                                                         Γ              ¯                                      (        τ        )              {\displaystyle {\bar {\Gamma }}(\tau )}   in the end, to gain the most general type for a given expression.Because the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.Efficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of                                                         Γ              ¯                                      (        τ        )              {\displaystyle {\bar {\Gamma }}(\tau )}   and enable an occurs check to prevent the building of recursive types during                     u        n        i        o        n        (        α        ,        τ        )              {\displaystyle union(\alpha ,\tau )}  .An example of such a case is                     λ                 x        .        (        x                 x        )              {\displaystyle \lambda \ x.(x\ x)}  , for which no type can be derived using HM.  Practically, types are only small terms and do not build up expanding structures.  Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.== Proving the algorithm ==In the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation.  While this leads to an efficient algorithm J, it isnot clear whether the algorithm properly reflects the deduction systems D or Swhich serve as a semantic base line.The most critical point in the above argumentation is the refinement of monotypevariables bound by the context. For instance, the algorithm boldly changes thecontext while inferring e.g.                     λ        f        .        (        f                 1        )              {\displaystyle \lambda f.(f\ 1)}  ,because the monotype variable added to the context for the parameter                     f              {\displaystyle f}   later needs to be refinedto                     i        n        t        →        β              {\displaystyle int\rightarrow \beta }   when handling application.The problem is that the deduction rules do not allow such a refinement.Arguing that the refined type could have been added earlier instead of themonotype variable is an expedient at best.The key to reaching a formally satisfying argument is to properly includethe context within the refinement. Formally,typing is compatible with substitution of free type variables.                    Γ                  ⊢                      S                          e        :        τ                ⟹                S        Γ                  ⊢                      S                          e        :        S        τ              {\displaystyle \Gamma \vdash _{S}e:\tau \quad \Longrightarrow \quad S\Gamma \vdash _{S}e:S\tau }  To refine the free variables thus means to refine the whole typing.=== Algorithm W ===From there, a proof of algorithm J leads to algorithm W, which only makes theside effects imposed by the procedure                                           union                                {\displaystyle {\textit {union}}}   explicit byexpressing its serial composition by means of the substitutions                              S                      i                                {\displaystyle S_{i}}  . The presentation of algorithm W in the sidebar still makes use of side effectsin the operations set in italic, but these are now limited to generatingfresh symbols. The form of judgement is                     Γ        ⊢        e        :        τ        ,        S              {\displaystyle \Gamma \vdash e:\tau ,S}  ,denoting a function with a context and expression as parameter producing a monotype together witha substitution.                                           mgu                                {\displaystyle {\textsf {mgu}}}   is a side-effect free versionof                                           union                                {\displaystyle {\textit {union}}}   producing a substitution which is the most general unifier.While algorithm W is normally considered to be the HM algorithm and isoften directly presented after the rule system in literature, its purpose isdescribed by Milner on P. 369 as follows:As it stands, W is hardly an efficient algorithm; substitutions are applied too often. It was formulated to aid the proof of soundness. We now present a simpler algorithm J which simulates W in a precise sense.While he considered W more complicated and less efficient, he presented it in his publication before J. It has its merits when side effects are unavailable or unwanted.By the way, W is also needed to prove completeness, which is factored by him into the soundness proof.=== Proof obligations ===Before formulating the proof obligations, a deviation between the rules systemsD and S and the algorithms presented needs to be emphasized.While the development above sort of misused the monotypes as "open" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be "kept in mind" as such. This promise is not fulfilled by the algorithm.Having a context                     1        :        i        n        t        ,                 f        :        α              {\displaystyle 1:int,\ f:\alpha }  , the expression                     f                 1              {\displaystyle f\ 1}  cannot be typed in either                               ⊢                      D                                {\displaystyle \vdash _{D}}   or                               ⊢                      S                                {\displaystyle \vdash _{S}}  , but the algorithms come up withthe type                     β              {\displaystyle \beta }  , where W additionally delivers the substitution                               {                      α            ↦            i            n            t            →            β                    }                      {\displaystyle \left\{\alpha \mapsto int\rightarrow \beta \right\}}  ,meaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proofvariables and monotype variables.The authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.While more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,they were not needed for the intended application where none of the items in a preexisting context have freevariables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.The remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be madefor contexts with                     f        r        e        e        (        Γ        )        =        ∅              {\displaystyle free(\Gamma )=\emptyset }   as a side condition.                                                                                          (Correctness)                                                            Γ                                  |                                                  −                                      W                                                  e                :                τ                ,                S                                                            ⟹                                Γ                                  ⊢                                      S                                                  e                :                τ                                                                                      (Completeness)                                                            Γ                                  |                                                  −                                      S                                                  e                :                τ                                                            ⟹                                Γ                                  ⊢                                      W                                                  e                :                                  τ                  ′                                ,                S                                                                  forall                                                 τ                                                   where                                                                                       ∅                    ¯                                                  (                                  τ                  ′                                )                ⊑                τ                                                          {\displaystyle {\begin{array}{lll}{\text{(Correctness)}}&\Gamma |-_{W}e:\tau ,S&\quad \Longrightarrow \quad \Gamma \vdash _{S}e:\tau \\{\text{(Completeness)}}&\Gamma |-_{S}e:\tau &\quad \Longrightarrow \quad \Gamma \vdash _{W}e:\tau ',S\quad \quad {\text{forall}}\ \tau \ {\text{where}}\ {\overline {\emptyset }}(\tau ')\sqsubseteq \tau \end{array}}}  The side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.To properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution                     S              {\displaystyle S}   through                               ⊢                      S                                {\displaystyle \vdash _{S}}   and                               ⊢                      W                                {\displaystyle \vdash _{W}}  . From there, the proofs are by induction over the expression.Another proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.== Extensions ===== Recursive definitions ===To make programming practical recursive functions are needed.A central property of the lambda calculus is that recursive definitionsare not directly available, but can instead be expressed with a fixed point combinator.But unfortunately, the fixpoint combinator cannot be formulated in a typed versionof the lambda calculus without having a disastrous effect on the system as outlinedbelow.==== Typing rule ====The original paper shows recursion can be realized by a combinator                                          f            i            x                          :        ∀        α        .        (        α        →        α        )        →        α              {\displaystyle {\mathit {fix}}:\forall \alpha .(\alpha \rightarrow \alpha )\rightarrow \alpha }  . A possible recursive definition could thus be formulated as                                          r            e            c                                   v        =                  e                      1                                                         i            n                                             e                      2                                   ::=                              l            e            t                                   v        =                              f            i            x                          (        λ        v        .                  e                      1                          )                                       i            n                                             e                      2                                {\displaystyle {\mathtt {rec}}\ v=e_{1}\ {\mathtt {in}}\ e_{2}\ ::={\mathtt {let}}\ v={\mathit {fix}}(\lambda v.e_{1})\ {\mathtt {in}}\ e_{2}}  .Alternatively an extension of the expression syntax and an extra typing rule is possible:                                                                        Γ                ,                                  Γ                  ′                                ⊢                                  e                                      1                                                  :                                  τ                                      1                                                                  …                                Γ                ,                                  Γ                  ′                                ⊢                                  e                                      n                                                  :                                  τ                                      n                                                                  Γ                ,                                  Γ                  ″                                ⊢                e                :                τ                                            Γ                                 ⊢                                                                       r                    e                    c                                                                                     v                                      1                                                  =                                  e                                      1                                                                                                         a                    n                    d                                                                   …                                                                       a                    n                    d                                                                                     v                                      n                                                  =                                  e                                      n                                                                                                         i                    n                                                                   e                :                τ                                                        [                                    R              e              c                                ]                      {\displaystyle \displaystyle {\frac {\Gamma ,\Gamma '\vdash e_{1}:\tau _{1}\quad \dots \quad \Gamma ,\Gamma '\vdash e_{n}:\tau _{n}\quad \Gamma ,\Gamma ''\vdash e:\tau }{\Gamma \ \vdash \ {\mathtt {rec}}\ v_{1}=e_{1}\ {\mathtt {and}}\ \dots \ {\mathtt {and}}\ v_{n}=e_{n}\ {\mathtt {in}}\ e:\tau }}\quad [{\mathtt {Rec}}]}  where                              Γ          ′                =                  v                      1                          :                  τ                      1                          ,                 …        ,                           v                      n                          :                  τ                      n                                {\displaystyle \Gamma '=v_{1}:\tau _{1},\ \dots ,\ v_{n}:\tau _{n}}                                Γ          ″                =                  v                      1                          :                                            Γ              ¯                                      (                           τ                      1                                   )        ,                 …        ,                           v                      n                          :                                            Γ              ¯                                      (                           τ                      n                                   )              {\displaystyle \Gamma ''=v_{1}:{\bar {\Gamma }}(\ \tau _{1}\ ),\ \dots ,\ v_{n}:{\bar {\Gamma }}(\ \tau _{n}\ )}  basically merging                     [                              A            b            s                          ]              {\displaystyle [{\mathtt {Abs}}]}   and                     [                              L            e            t                          ]              {\displaystyle [{\mathtt {Let}}]}   while including the recursively definedvariables in monotype positions where they occur to the left of the                                           i            n                                {\displaystyle {\mathtt {in}}}   but as polytypes to the right of it. Thisformulation perhaps best summarizes the essence of let-polymorphism.==== Consequences ====While the above is straightforward it does come at a price.Type theory connects lambda calculus computation and logic.The easy modification above has effects on both:The strong normalisation property is spoiled.The logic collapses.Programs in simply typed lambda calculus are guaranteed to always terminate. Moreover, theyare even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separatingterminating from non-terminating programs is most difficult, and especially in lambda calculus,which is so expressive that it can formulate recursion with just a few symbols. Thus the initialinability of HM to provide recursive functions was not an omission, but a feature. Addingrecursion enables normal programming but the guarantee is not longer valid.Another reading of the typing is given by the Curry–Howard isomorphism. Herethe types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from thisperspective, assuming the variables to have logical value:                    (        α        →        α        )        →        α                ⇔                                      true                          →        α                ⇔                α              {\displaystyle (\alpha \rightarrow \alpha )\rightarrow \alpha \quad \Leftrightarrow \quad {\textsf {true}}\rightarrow \alpha \quad \Leftrightarrow \quad \alpha }  But this is invalid.Adding an invalid axiom will break the logic in the sense thatevery formula can then be shown to be true in it, e.g.                               true                ⇔                  false                      {\displaystyle {\text{true}}\Leftrightarrow {\text{false}}}  .Thus the ability to distinguish even two simple things is no longer given. Everything is the same andcollapses into 42. The fixpointcombinator that came in so handy above also plays a role in Curry's paradox.Logic aside, does this matter for typing programs? It does. Since one is now able to formulatenon-terminating functions, one can make a function that would return whatever one wants but never really returns:                              fix                                   id                :        ∀        a        .        a              {\displaystyle {\text{fix}}\ {\text{id}}:\forall a.a}  .In practical programming such a function can come in handy when breaking out of a computation,like with exit(1) in C, while silencing the type checker inthe current branch by returning essentially nothing but with a suitable type.Less desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like                               fix                                   id                      {\displaystyle {\text{fix}}\ {\text{id}}}  . The function would return a value of this type, but it cannot because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be claimed to be checked, but the program can still be typed wrong. Only if all functions are terminating does                     α              {\displaystyle \alpha }   in the logic above have a true value, and the assertions of the type checker become strong again.=== Overloading ===Overloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,<,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like int or real. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.While ad-hoc overloading has been avoided in functional programming for the computation costs both in type checking and inference, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. "Instances" in this systematic are not objects (i.e. on value level), but rather types.The quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:Herein, the type a is not only polymorphic, but also restricted to be an instance of some type class Ord, that provides the order predicates < and >= used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.Because the "classes" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a lattice.=== Meta types ===Parametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into "type functions" as in the "parametric" type constants, leads to the question how to more properly type types themselves. A meta type, the "type of types" would be useful to create an even more expressive type system.Though this would be a straight forward extension, unfortunately, only unification is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.Additionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.Research in second order lambda calculus, one step upwards, showed, that type inference is undecidable in this generality.Parts of one extra level has been introduced into Haskell named kind, where it is used helping to type monads. Kinds are left implicit, working behind the scenes in the inner mechanics of the extended type system.=== Subtyping ===Attempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose. It is not difficult to set up a type system with subtyping enabling object-oriented style, as e.g. Cardelli shows in his system                               F                      <:                                {\displaystyle F_{<:}}  .The type equivalence can be broken up into a subtyping relation "<:".Extra type rules define this relation e.g. for the functions.A suiting record type is then added whose values represent the objects.Such objects would be immutable in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.The subtyping rule for the record types is:                                                                        n                ≤                m                                                                  τ                                      1                                                  <:                                  τ                                      1                                    ′                                                …                                                  τ                                      n                                                  <:                                  τ                                      n                                    ′                                                            Γ                                 ⊢                                                   R                  e                  c                  o                  r                  d                                                                   v                                      1                                                  :                                  τ                                      1                                                                   …                                                   v                                      n                                                  :                                  τ                                      n                                                                                     e                  n                  d                                <:                                  R                  e                  c                  o                  r                  d                                                                   v                                      1                                                  :                                  τ                                      1                                    ′                                                 …                                                   v                                      m                                                  :                                  τ                                      m                                    ′                                                                   e                  n                  d                                                                        [                                    R              e              c              o              r              d                                ]                      {\displaystyle \displaystyle {\frac {n\leq m\quad \quad \tau _{1}<:\tau _{1}'\quad \dots \quad \tau _{n}<:\tau _{n}'}{\Gamma \ \vdash \ \mathbf {Record} \ v_{1}:\tau _{1}\ \dots \ v_{n}:\tau _{n}\ \mathbf {end} <:\mathbf {Record} \ v_{1}:\tau _{1}'\ \dots \ v_{m}:\tau _{m}'\ \mathbf {end} }}\quad [{\mathtt {Record}}]}  Syntatically, record expressions would have form                              r          e          c          o          r          d                                   v                      1                          =                  e                      1                          ,                 …                           e          n          d                      {\displaystyle \mathbf {record} \ v_{1}=e_{1},\ \dots \ \mathbf {end} }  and have a type rule leading to the above type.Such record values could then be used the same way as objects in object-oriented programming.== Notes ==== References ==== External links ==A literate Haskell implementation of Algorithm W along with its source code on GitHub.A simple implementation of Hindley-Milner algorithm in Python.
	In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.Randomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.== Uses ==Randomizing functions are used to turn algorithms that have good expected performance for random inputs, into algorithms that have the same performance for any input.For example, consider a sorting algorithm like quicksort, which has small expected running time when the input items are presented in random order, but is very slow when they are presented in certain unfavorable orders. A randomizing function from the integers 1 to n to the integers 1 to n can be used to rearrange the n input items in "random" order, before calling that algorithm.  This modified (randomized) algorithm will have small expected running time, whatever the input order.=== Randomness ===In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed.  The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time).  With such a "pseudo-randomization" function, one could in principle construct a sequence of calls such that the function would always yield a "bad" case for the underlying deterministic algorithm.  For that sequence of calls, the average cost would be closer to the worst-case cost, rather than the average cost for random inputs.In practice, however, the main concern is that some "bad" cases for the deterministic algorithm may occur in practice much more often than it would be predicted by chance.  For example, in a naive variant of quicksort, the worst case is when the input items are already sorted — which is a very common occurrence in many applications.  For such algorithms, even a fixed pseudo-random permutation may be good enough.  Even though the resulting "pseudo-randomized" algorithm would still have as many "bad" cases as the original, they will be certain peculiar orders that would be quite unlikely to arise in real applications.  So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external "random" data such as the program's startup time.=== Uniformity ===The uniformity requirements for a randomizing function are usually much weaker than those of hash functions and pseudo-random generators.  The minimum requirement is that it maps any input of the deterministic algorithm into a "good" input with a sufficiently high probability.  (However, analysis is usually simpler if the randomizing function implements each possible mapping with uniform probability.)== References ==
	Algorithmic logic is a calculus of programs which allows the expression of semantic properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see Mirkowska & Salwicki (1987), Banachowski et al. (1977).The following diagram helps to locate algorithmic logic among other logics.                                      [                                                                                          P                    r                    o                    p                    o                    s                    i                    t                    i                    o                    n                    a                    l                                         l                    o                    g                    i                    c                                                                                                o                  r                                                                                                  S                    e                    n                    t                    e                    n                    t                    i                    a                    l                                         c                    a                    l                    c                    u                    l                    u                    s                                                                                ]                ⊂                  [                                                                                          P                    r                    e                    d                    i                    c                    a                    t                    e                                         c                    a                    l                    c                    u                    l                    u                    s                                                                                                o                  r                                                                                                  F                    i                    r                    s                    t                                         o                    r                    d                    e                    r                                         l                    o                    g                    i                    c                                                                                ]                ⊂                  [                                                                                          C                    a                    l                    c                    u                    l                    u                    s                                         o                    f                                         p                    r                    o                    g                    r                    a                    m                    s                                                                                                o                  r                                                                                                                        Algorithmic logic                                                                                                    ]                      {\displaystyle \qquad \left[{\begin{array}{l}\mathrm {Propositional\ logic} \\or\\\mathrm {Sentential\ calculus} \end{array}}\right]\subset \left[{\begin{array}{l}\mathrm {Predicate\ calculus} \\or\\\mathrm {First\ order\ logic} \end{array}}\right]\subset \left[{\begin{array}{l}\mathrm {Calculus\ of\ programs} \\or\\{\mbox{Algorithmic logic}}\end{array}}\right]}  The formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: Terms - i.e. expressions denoting operations on elements of data structures, formulas - i.e. expressions denoting the relations among elements of data structures,  programs - i.e. algorithms - these expressions describe the computations.For semantics of terms and formulas consult pages on first order logic and Tarski's semantic. The meaning of a program                     K              {\displaystyle K}   is the set of possible computations of the program.Algorithmic logic is one of many logics of programs.Another logic of programs is dynamic logic, see dynamic logic, Harel, Kozen & Tiuryn (2000).== Bibliography ==[Mirkowska & Salwicki] |Mirkowska, Grażyna; Salwicki, Andrzej (1987). Algorithmic Logic. Warszawa  & Boston: PWN & D. Reidel Publ. p. 372. ISBN 8301068590.[Banachowski et al.]  |Banachowski, Lech; Kreczmar, Antoni; Mirkowska, Grażyna; Rasiowa, Helena; Salwicki, Andrzej (1977). An introduction to Algorithmic Logic - Metamathematical Investigations of Theory of Programs. Banach Center Publications. 2. Warszawa: PWN. pp. 7–99.Harel, David; Kozen, Dexter; Tiuryn, Jerzy (2000). Dynamic Logic. Cambridge Massachusetts: MIT Press. p. 459.
	The Métis () are members of ethnic groups native to Canada and parts of the United States that trace their descent to both indigenous North Americans and European settlers. Originally the term applied to French-speaking mixed-race families, especially in the Red River area of what became Manitoba, Canada; in the late 19th century in Canada, those of mixed English descent were classified separately as Mixed Bloods.Since the late 20th century, the Métis in Canada have been recognized as a distinct aboriginal people under the Constitution Act of 1982; they number 587,545 as of 2016. Smaller communities self-identifying as Métis exist in the U.S.== Etymology ==The word derives from the French adjective métis, also spelled metice, referring to a hybrid, or someone of mixed ancestry. In the 16th century, French colonists used the term métis as a noun for people of mixed European and indigenous American parentage in New France (Quebec) and La Louisiane in North America; at the time, it applied generally to French-speaking people who were of partial ethnic French descent. It later came to be used for people of mixed European and Indigenous backgrounds in other French colonies, including Guadeloupe in the Caribbean; Senegal in West Africa; Algeria in North Africa; and the former French Indochina in Southeast Asia. The Métis in Canada married within their own group, and over time, created a distinct culture and ethnicity of their own.== Métis people in Canada ===== History of Métis in Canada ===The Métis Nation is considered to be rooted in what is known as the "Métis Homeland," an area ranging from northwestern Ontario and moving westward across the prairies. In this area, fur trappers married indigenous  Cree and Saulteaux women. A distinct ethnicity developed, as mixed-race descendants married within this group and remained involved with fur trapping and trading. They also began to farm in the Red River of the North area.People of "mixed ancestry," although not of the Métis Nation, have a distinct history of their own. These unions began in the east, extending from the Atlantic coast to the Great Lakes. The fur trade and colonial development drew French voyageurs and coureurs des bois to the west, along with the later Hudson's Bay Company employees. Wintering partners of the fur trading companies typically took a wikt:country wife for their months away from the eastern cities.After the fall of New France to the British 1763, many mixed-race populations continued to establish themselves, often specializing in the fur trade and related hunting. Some served as interpreters, as they often were fluent in both indigenous and European languages. English and Scottish traders married indigenous women, often the daughters of high-ranking chiefs, forming an elite mixed society. As the eighteenth century ended, the fur trade moved westward into the Plains.The Métis Nation promoted their distinct and unique Indigenous identity in 1812, when Cuthbert Grant led a battle in the Pemmican War, flying the Métis flag. Many treaties throughout Canada were being negotiated in the nineteenth century, including in Ontario with the Robinson–Huron treaty. In 1870 the Métis at Red River, led by Louis Riel, resisted Canada's efforts to take over this area from the Hudson's Bay Company. They negotiated entry into Canada as the province of Manitoba, with promises to protect their land and rights. In 1885, the Métis resisted Canadian colonialism with the North-West Rebellion. The Métis were defeated and Riel was hanged as a traitor to Canada, but his role in history is controversial.Métis in the Métis Nation homeland received scrip under Section 32 of the Manitoba Act,and in the Northwest Territory via a series of Scrip Commissions that accompanied the negotiators of the numbered treaties in the Northwest Territory. Because scrip could only be applied to surveyed land, many Metis could not use it to purchase the unsurveyed land on which they were already living. They often moved onto the road allowances, where no roads had yet been built. The term "Road Allowance people" was used to describe these dispossessed Metis. Racism toward Métis peoples in the west was prevalent during a large part of the late nineteenth and twentieth centuries.In 1982, Métis were included as a distinct indigenous people in the Canadian constitution. They are defined as an ethnic group with their own culture, distinct from First Nations and Inuit peoples. Métis peoples have formed a variety of political organizations to promote their interests, and lobby the federal government through their primary national political association, the Métis National Council (MNC).In 2003, the Canadian Supreme Court ruled in R v Powley that a family of Métis people in Ontario had the right to hunt moose as part of their Métis aboriginal rights. This case was funded by the Métis Nation of Ontario (MNO), a provincial affiliate of the MNC. The case established that the Métis had a history in Ontario, which was long debated by many people. The case also established the "Powley test", which helps to define who is Métis, and therefore eligible to rights as an aboriginal person. On April 14, 2016, the Supreme Court in Daniels v. Canada (Indian Affairs and Northern Development) reached a landmark decision, ruling that Métis and non-status Indians are "Indians" for the purpose of s 91(24) of the Constitution Act, 1867.== See also ==ColouredsMétis in the United StatesMultiracialMestizoAnglo-MétisBasterHalf-casteKahnawake surnamesLittle Shell Band of Chippewa IndiansNunatuKavut people== References ==== Bibliography ==Barkwell, Lawrence. "Métis Rights and Land Claims in Canada" Accessed September 1, 2019,  annotated BibliographyBarkwell, Lawrence J.; Dorion, Leah; Hourie, Audreen (2006). "Métis legacy Michif culture, heritage, and folkways". Métis legacy series. 2. Saskatoon: Gabriel Dumont Institute. ISBN 0-920915-80-9.Barkwell, Lawrence J.; Dorion, Leah; Prefontaine, Darren (2001). Métis Legacy: A Historiography and Annotated Bibliography. Winnipeg: Pemmican Publications Inc. and Saskatoon: Gabriel Dumont Institute. ISBN 1-894717-03-1.== External links ==The Rupertsland Institute (Alberta) – A service dedicated to the research and development, education, and training and employment of Metis individuals. It is affiliated with the Metis Nations of Alberta. Along with providing financial aid, the Rupertsland Institute helps Metis individuals acquire essential skills for employment.
	An adaptive algorithm is an algorithm that changes its behavior at the time it is run, based on information available and on a priori defined reward mechanism (or criterion). Such information could be the story of recently received data, information on the available computational resources, or other run-time acquired (or a priori known) information related to the environment in which it operates.Among the most used adaptive algorithms is the Widrow-Hoff’s least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. In adaptive filtering the LMS is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square  of the error signal (difference between the desired and the actual signal).For example, stable partition, using no additional memory is O(n lg n) but given O(n) memory, it can be O(n) in time. As implemented by the C++ Standard Library, stable_partition is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory.  Another example is adaptive sort, whose behavior changes upon the presortedness of its input.An example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.In machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.In data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.In signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called "adaptive" because the window length (the size of an audio "chunk") can change according to the nature of the sound being compressed, to try to achieve the best-sounding compression strategy.== See also ==Adaptation (computer science)Adaptive filterAdaptive grammarAdaptive optimization== References ==
	Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of                     k              {\displaystyle k}   items from a list                     S              {\displaystyle S}   containing                     n              {\displaystyle n}   items, where                     n              {\displaystyle n}   is either a very large or unknown number. Typically,                     n              {\displaystyle n}   is too large to fit the whole list into main memory.== Example ==Suppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items n, then the solution is easy: select 10 distinct indices i between 1 and n with equal probability, and keep the i-th elements. The problem is that we do not always know the exact n in advance. A possible solution is the following:Keep the first ten items in memory.When the i-th item arrives (for                     i        >        10              {\displaystyle i>10}  ):with probability                     10                  /                i              {\displaystyle 10/i}  , keep the new item (discard an old one, selecting which to replace at random, each with chance 1/10)with probability                     1        −        10                  /                i              {\displaystyle 1-10/i}  , keep the old items (ignore the new one)Thus,when there are 10 items or fewer, each is kept with probability 1;when there are 11 items, each of them is kept with probability 10/11; for the old items, that is (1)(1/11 + (10/11)(9/10)) = 1/11 + 9/11 = 10/11In other words, the 10 old items are kept either if the new one is not selected 1/11 or the new one is selected to replace one of the other 9 items 10/11 × 9/10, and since "or" is represented with addition, we derive (1/11 + (10/11)(9/10)).when there are 12 items, the twelfth item is kept with probability 10/12, and each of the previous 11 items are also kept with probability (10/11)(2/12 + (10/12)(9/10)) = (10/11)(11/12) = 10/12;by induction, it is easy to prove that when there are n items, each item is kept with probability                     10                  /                n              {\displaystyle 10/n}  .== Algorithm R ==The most common example was labelled Algorithm R by Jeffrey Vitter in his paper on the subject. This simple O(n) algorithm as described in the Dictionary of Algorithms and Data Structures consists of the following steps (assuming k < n and using one-based array indexing):The algorithm creates a "reservoir" array of size                     k              {\displaystyle k}   and populates it with the first                     k              {\displaystyle k}   items of                     S              {\displaystyle S}  . It then iterates through the remaining elements of                     S              {\displaystyle S}   until                     S              {\displaystyle S}   is exhausted. At the                     i              {\displaystyle i}  th element of                     S              {\displaystyle S}  , the algorithm generates a random number                     j              {\displaystyle j}   between 1 and                     i              {\displaystyle i}  . If                     j              {\displaystyle j}   is less than or equal to                     k              {\displaystyle k}  , the                     j              {\displaystyle j}  th element of the reservoir array is replaced with the                     i              {\displaystyle i}  th element of                     S              {\displaystyle S}  . In effect, for all                     i              {\displaystyle i}  , the                     i              {\displaystyle i}  th element of                     S              {\displaystyle S}   is chosen to be included in the reservoir with probability                     k                  /                i              {\displaystyle k/i}  . Similarly, at each iteration the                     j              {\displaystyle j}  th element of the reservoir array is chosen to be replaced with probability                     1                  /                k        ×        k                  /                i        =        1                  /                i              {\displaystyle 1/k\times k/i=1/i}  . It can be shown that when the algorithm has finished executing, each item in                     S              {\displaystyle S}   has equal probability (i.e.,                     k                  /                length        ⁡        (        S        )              {\displaystyle k/\operatorname {length} (S)}  ) of being chosen for the reservoir.To see this, consider the following proof by induction. First, the base case: when                     i        =        k              {\displaystyle i=k}  , each of the first                     k              {\displaystyle k}   elements of                     S              {\displaystyle S}   is in the reservoir with probability                     k                  /                i        =        k                  /                k        =        1              {\displaystyle k/i=k/k=1}  .Second, the induction step.  We may assume that before round                     i              {\displaystyle i}  , each of the first                     i        −        1              {\displaystyle i-1}   elements of                     S              {\displaystyle S}   is in the reservoir with probability                     k                  /                (        i        −        1        )              {\displaystyle k/(i-1)}  .  The probability of any particular element of the reservoir being replaced during round                     i              {\displaystyle i}   is                     1                  /                i              {\displaystyle 1/i}  , so the probability of it not being replaced is                     (        i        −        1        )                  /                i              {\displaystyle (i-1)/i}  .  Thus, the odds of any of the first                     i        −        1              {\displaystyle i-1}   elements remaining in the reservoir after round                     i              {\displaystyle i}   is                     k                  /                (        i        −        1        )        ×        (        i        −        1        )                  /                i        =        k                  /                i              {\displaystyle k/(i-1)\times (i-1)/i=k/i}  , as desired.Finally, during round                     i              {\displaystyle i}  , the                     i              {\displaystyle i}  th element is added to the reservoir with probability                     k                  /                i              {\displaystyle k/i}  .  Thus, after round                     i              {\displaystyle i}  , each of the first                     i              {\displaystyle i}   elements of                     S              {\displaystyle S}   is in the reservoir with probability                     k                  /                i              {\displaystyle k/i}  .∎== Reservoir with random sort ==A simple reservoir-based algorithm can be designed using random sort and implemented using priority queue data structure. This algorithm assigns random number as keys to each item and maintain k items with minimum value for keys. In essence, this is equivalent to assigning a random number to each item as key, sorting items using these keys and taking top k items. The worse case run time of the algorithm is                     O        (        n        log        ⁡        k        )              {\displaystyle O(n\log k)}   while the best case runtime is                     O        (        n        )              {\displaystyle O(n)}  . Even though the worse case runtime is not as good as Algorithm R, this algorithm can easily be extended to weighted sampling. Note that both algorithms can operate on streams of unspecified lengths.== Weighted random sampling using Reservoir ==In many applications sampling is required to be according to the weights that are assigned to each items available in set. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. There are two ways to interpret weights assigned to each item in the set:Let the weight of each item be                               w                      i                                {\displaystyle w_{i}}   and sum of all weights be W. We can convert weight to probability of item getting selected in sample as                               P                      i                          =                  w                      i                                    /                W              {\displaystyle P_{i}=w_{i}/W}  .Let the weight of two items i and j be                               w                      i                                {\displaystyle w_{i}}   and                               w                      j                                {\displaystyle w_{j}}  . Let the probability of item i getting selected in sample be                               p                      i                                {\displaystyle p_{i}}  , then we give                               p                      j                          =        min        (        1        ,                  p                      i                                                              w                              j                                                    w                              i                                                    )              {\displaystyle p_{j}=\min(1,p_{i}{\frac {w_{j}}{w_{i}}})}  .=== Algorithm A-Res ===The following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:This algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the line how we generate the key using random number generator. The algorithm is equivalent to assigning each item a key                               r                      1                          /                                      w                              i                                                          {\displaystyle r^{1/w_{i}}}   where r is the random number and then sort items using these keys and finally select top k items for the sample.=== Algorithm A-Chao ===Following algorithm was given by M. T. Chao uses interpretation 2:For each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.== Distributed Reservoir sampling ==In many applications, amount of data from which a small sample is needed is too large and it is desirable to distribute sampling tasks among many machines in parallel to speed up the process. A simple approach that is often used, although less performant, is to assign a random number as key to each item and then perform a distributed sort and finally obtain a sample of desired size from top k items. If weighted sample is desired then key is computed using                               r                      1                          /                                      w                              i                                                          {\displaystyle r^{1/w_{i}}}   where r is the random number and                               w                      i                                {\displaystyle w_{i}}   is the weight of an item. The inefficiency in this approach obviously arises from required distributed sort on very large amount of data.Another more efficient approach for distributed weighted random sampling is as follows:Distribute data among m machines.Each machine does its own weighted sampling using key                               r                      1                          /                                      w                              i                                                          {\displaystyle r^{1/w_{i}}}   as described in previous section and produces a sample of size ≤ k items.Collects all m samples of size ≤ k. We should have total items                               n          ′                ≤        m        k              {\displaystyle n'\leq mk}  .Now sample k items from n' items from step 3 using key that was already computed in Step 2. This means instead of re-generating key using random number generator in sampling algorithm, we use the key we already had assigned in step 2.The Step 4 uses keys from Step 2 because we might have unbalanced data distribution on machines. For example, lets say                     k        =        1              {\displaystyle k=1}  , machine m1 only gets 1 item with weight 10 while machine m2 gets 2 items each with weight 100. Intuitively probability for items from m1 getting in final sample is 10/210. In Step 3, we will get 1 item from m1 as well as m2. If we recalculate keys in step 4 then the probability that item from m1 will be in final sample is 10/110 instead of required 10/210. Now observe that weighted reservoir sampling algorithm from previous section decreases max key value in priority queue as it processes more items. Therefore, items sampled from machine with larger chunk will have lower key values and thus higher chance of getting selected.== Relation to Fisher-Yates shuffle ==Suppose one wanted to draw k random cards from a deck of cards.A natural approach would be to shuffle the deck and then take the top k cards.In the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the Fisher-Yates shuffle:  To initialize an array a of n elements to a randomly shuffled copy of S, both 0-based:    a[0] ← S[0]    for i from 1 to n − 1 do        r ← random (0 .. i)        a[i] ← a[r]        a[r] ← S[i]Note that although the rest of the cards are shuffled, only the first k are important in the present context.Therefore, the array a need only track the cards in the first k positions while performing the shuffle, reducing the amount of memory needed.Truncating a to length k, the algorithm is modified accordingly:  To initialize an array a to k random elements of S (which is of length n), both 0-based:    a[0] ← S[0]    for i from 1 to k − 1 do        r ← random (0 .. i)        a[i] ← a[r]        a[r] ← S[i]     for i from k to n − 1 do        r ← random (0 .. i)        if (r < k) then a[r] ← S[i]Since the order of the first k cards is immaterial, the first loop can be removed and a can be initialized to be the first k items of S.This yields Algorithm R.== Fast approximation ==A fast approximation to reservoir sampling.Uses a good-quality approximation to the sampling-gap distribution to skip over the gaps; i.e. consecutive runs of data thatare not sampled.== Example implementation ==The following is a simple implementation of the algorithm in Python that samples the set of English Wikipedia page titles:== Statistical properties ==Probabilities of selection of the reservoir methods are discussed in Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to                     k                  /                n              {\displaystyle k/n}   (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).== Limitations ==Reservoir sampling makes the assumption that the desired sample fits into main memory, often implying that                     k              {\displaystyle k}   is a constant independent of                     n              {\displaystyle n}  . In applications where we would like to select a large subset of the input list (say a third, i.e.                     k        =        n                  /                3              {\displaystyle k=n/3}  ), other methods need to be adopted. Distributed implementations for this problem have been proposed.== See also ==Moving average== References ==
	In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them holographic because "their effect can be viewed as that of producing interference patterns among the solution fragments". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.Holographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory.  Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.Holographic algorithms have some similarities with quantum computation, but are completely classical.== Holant problems ==Holographic algorithms exist in the context of Holant problems, which generalize counting constraint satisfaction problems (#CSP). A #CSP instance is a hypergraph G=(V,E) called the constraint graph. Each hyperedge represents a variable and each vertex                     v              {\displaystyle v}   is assigned a constraint                               f                      v                          .              {\displaystyle f_{v}.}   A vertex is connected to an hyperedge if the constraint on the vertex involves the variable on the hyperedge. The counting problem is to compute                              ∑                      σ            :            E            →            {            0            ,            1            }                                    ∏                      v            ∈            V                                    f                      v                          (        σ                              |                                E            (            v            )                          )        ,                                                                                                  (        1        )              {\displaystyle \sum _{\sigma :E\to \{0,1\}}\prod _{v\in V}f_{v}(\sigma |_{E(v)}),~~~~~~~~~~(1)}  which is a sum over all variable assignments, the product of every constraint, where the inputs to the constrain                               f                      v                                {\displaystyle f_{v}}   are the variables on the incident hyperedges of                     v              {\displaystyle v}  .A Holant problem is like a #CSP except the input must be a graph, not a hypergraph. Restricting the class of input graphs in this way is indeed a generalization. Given a #CSP instance, replace each hyperedge e of size s with a vertex v of degree s with edges incident to the vertices contained in e. The constraint on v is the equality function of arity s.  This identifies all of the variables on the edges incident to v, which is the same effect as the single variable on the hyperedge e.In the context of Holant problems, the expression in (1) is called the Holant after a related exponential sum introduced by Valiant.== Holographic reduction ==A standard technique in complexity theory is a many-one reduction, where an instance of one problem is reduced to an instance of another (hopefully simpler) problem.However, holographic reductions between two computational problems preserve the sum of solutions without necessarily preserving correspondences between solutions.  For instance, the total number of solutions in both sets can be preserved, even though individual problems do not have matching solutions. The sum can also be weighted, rather than simply counting the number of solutions, using linear basis vectors.=== General example ===It is convenient to consider holographic reductions on bipartite graphs. A general graph can always be transformed it into a bipartite graph while preserving the Holant value. This is done by replacing each edge in the graph by a path of length 2, which is also known as the 2-stretch of the graph. To keep the same Holant value, each new vertex is assigned the binary equality constraint.Consider a bipartite graph G=(U,V,E) where the constraint assigned to every vertex                     u        ∈        U              {\displaystyle u\in U}   is                               f                      u                                {\displaystyle f_{u}}   and the constraint assigned to every vertex                     v        ∈        V              {\displaystyle v\in V}   is                               f                      v                                {\displaystyle f_{v}}  . Denote this counting problem by                               Holant                (        G        ,                  f                      u                          ,                  f                      v                          )        .              {\displaystyle {\text{Holant}}(G,f_{u},f_{v}).}   If the vertices in U are viewed as one large vertex of degree |E|, then the constraint of this vertex is the tensor product of                               f                      u                                {\displaystyle f_{u}}   with itself |U| times, which is denoted by                               f                      u                                ⊗                          |                        U                          |                                      .              {\displaystyle f_{u}^{\otimes |U|}.}   Likewise, if the vertices in V are viewed as one large vertex of degree |E|, then the constraint of this vertex is                               f                      v                                ⊗                          |                        V                          |                                      .              {\displaystyle f_{v}^{\otimes |V|}.}   Let the constraint                               f                      u                                {\displaystyle f_{u}}   be represented by its weighted truth table as a row vector and the constraint                               f                      v                                {\displaystyle f_{v}}   be represented by its weighted truth table as a column vector. Then the Holant of this constraint graph is simply                               f                      u                                ⊗                          |                        U                          |                                                f                      v                                ⊗                          |                        V                          |                                      .              {\displaystyle f_{u}^{\otimes |U|}f_{v}^{\otimes |V|}.}  Now for any complex 2-by-2 invertible matrix T (the columns of which are the linear basis vectors mentioned above), there is a holographic reduction between                               Holant                (        G        ,                  f                      u                          ,                  f                      v                          )              {\displaystyle {\text{Holant}}(G,f_{u},f_{v})}   and                               Holant                (        G        ,                  f                      u                                    T                      ⊗            (            deg            ⁡            u            )                          ,        (                  T                      −            1                                    )                      ⊗            (            deg            ⁡            v            )                                    f                      v                          )        .              {\displaystyle {\text{Holant}}(G,f_{u}T^{\otimes (\deg u)},(T^{-1})^{\otimes (\deg v)}f_{v}).}   To see this, insert the identity matrix                               T                      ⊗                          |                        E                          |                                      (                  T                      −            1                                    )                      ⊗                          |                        E                          |                                            {\displaystyle T^{\otimes |E|}(T^{-1})^{\otimes |E|}}   in between                               f                      u                                ⊗                          |                        U                          |                                                f                      v                                ⊗                          |                        V                          |                                            {\displaystyle f_{u}^{\otimes |U|}f_{v}^{\otimes |V|}}   to get                              f                      u                                ⊗                          |                        U                          |                                                f                      v                                ⊗                          |                        V                          |                                            {\displaystyle f_{u}^{\otimes |U|}f_{v}^{\otimes |V|}}                      =                  f                      u                                ⊗                          |                        U                          |                                                T                      ⊗                          |                        E                          |                                      (                  T                      −            1                                    )                      ⊗                          |                        E                          |                                                f                      v                                ⊗                          |                        V                          |                                            {\displaystyle =f_{u}^{\otimes |U|}T^{\otimes |E|}(T^{-1})^{\otimes |E|}f_{v}^{\otimes |V|}}                      =                              (                                          f                                  u                                                            T                                  ⊗                  (                  deg                  ⁡                  u                  )                                                      )                                ⊗                          |                        U                          |                                                            (                                          f                                  v                                            (                              T                                  −                  1                                                            )                                  ⊗                  (                  deg                  ⁡                  v                  )                                                      )                                ⊗                          |                        V                          |                                      .              {\displaystyle =\left(f_{u}T^{\otimes (\deg u)}\right)^{\otimes |U|}\left(f_{v}(T^{-1})^{\otimes (\deg v)}\right)^{\otimes |V|}.}  Thus,                               Holant                (        G        ,                  f                      u                          ,                  f                      v                          )              {\displaystyle {\text{Holant}}(G,f_{u},f_{v})}   and                               Holant                (        G        ,                  f                      u                                    T                      ⊗            (            deg            ⁡            u            )                          ,        (                  T                      −            1                                    )                      ⊗            (            deg            ⁡            v            )                                    f                      v                          )              {\displaystyle {\text{Holant}}(G,f_{u}T^{\otimes (\deg u)},(T^{-1})^{\otimes (\deg v)}f_{v})}   have exactly the same Holant value for every constraint graph. They essentially define the same counting problem.=== Specific examples ======= Vertex covers and independent sets ====Let G be a graph. There is a 1-to-1 correspondence between the vertex covers of G and the independent sets of G. For any set S of vertices of G, S is a vertex cover in G if and only if the complement of S is an independent set in G. Thus, the number of vertex covers in G is exactly the same as the number of independent sets in G.The equivalence of these two counting problems can also be proved using a holographic reduction. For simplicity, let G be a 3-regular graph. The 2-stretch of G gives a bipartite graph H=(U,V,E), where U corresponds to the edges in G and V corresponds to the vertices in G. The Holant problem that naturally corresponds to counting the number of vertex covers in G is                               Holant                (        H        ,                              OR                                2                          ,                              EQUAL                                3                          )        .              {\displaystyle {\text{Holant}}(H,{\text{OR}}_{2},{\text{EQUAL}}_{3}).}   The truth table of OR2 as a row vector is (0,1,1,1). The truth table of EQUAL3 as a column vector is                     (        1        ,        0        ,        0        ,        0        ,        0        ,        0        ,        0        ,        1                  )                      T                          =                                            [                                                                    1                                                                                        0                                                              ]                                            ⊗            3                          +                                            [                                                                    0                                                                                        1                                                              ]                                            ⊗            3                                {\displaystyle (1,0,0,0,0,0,0,1)^{T}={\begin{bmatrix}1\\0\end{bmatrix}}^{\otimes 3}+{\begin{bmatrix}0\\1\end{bmatrix}}^{\otimes 3}}  . Then under a holographic transformation by                                           [                                                            0                                                  1                                                                              1                                                  0                                                      ]                          ,              {\displaystyle {\begin{bmatrix}0&1\\1&0\end{bmatrix}},}                                            OR                                2                                ⊗                          |                        U                          |                                                            EQUAL                                3                                ⊗                          |                        V                          |                                            {\displaystyle {\text{OR}}_{2}^{\otimes |U|}{\text{EQUAL}}_{3}^{\otimes |V|}}                      =        (        0        ,        1        ,        1        ,        1                  )                      ⊗                          |                        U                          |                                                            (                                                                                [                                                                                            1                                                                                                                      0                                                                                      ]                                                                    ⊗                  3                                            +                                                                    [                                                                                            0                                                                                                                      1                                                                                      ]                                                                    ⊗                  3                                                      )                                ⊗                          |                        V                          |                                            {\displaystyle =(0,1,1,1)^{\otimes |U|}\left({\begin{bmatrix}1\\0\end{bmatrix}}^{\otimes 3}+{\begin{bmatrix}0\\1\end{bmatrix}}^{\otimes 3}\right)^{\otimes |V|}}                      =        (        0        ,        1        ,        1        ,        1                  )                      ⊗                          |                        U                          |                                                                          [                                                                    0                                                        1                                                                                        1                                                        0                                                              ]                                            ⊗                          |                        E                          |                                                                          [                                                                    0                                                        1                                                                                        1                                                        0                                                              ]                                            ⊗                          |                        E                          |                                                            (                                                                                [                                                                                            1                                                                                                                      0                                                                                      ]                                                                    ⊗                  3                                            +                                                                    [                                                                                            0                                                                                                                      1                                                                                      ]                                                                    ⊗                  3                                                      )                                ⊗                          |                        V                          |                                            {\displaystyle =(0,1,1,1)^{\otimes |U|}{\begin{bmatrix}0&1\\1&0\end{bmatrix}}^{\otimes |E|}{\begin{bmatrix}0&1\\1&0\end{bmatrix}}^{\otimes |E|}\left({\begin{bmatrix}1\\0\end{bmatrix}}^{\otimes 3}+{\begin{bmatrix}0\\1\end{bmatrix}}^{\otimes 3}\right)^{\otimes |V|}}                      =                              (                          (              0              ,              1              ,              1              ,              1              )                                                                    [                                                                                            0                                                                          1                                                                                                                      1                                                                          0                                                                                      ]                                                                    ⊗                  2                                                      )                                ⊗                          |                        U                          |                                                            (                                                            (                                                                                    [                                                                                                            0                                                                                      1                                                                                                                                          1                                                                                      0                                                                                                      ]                                                                                                            [                                                                                                            1                                                                                                                                          0                                                                                                      ]                                                                              )                                                  ⊗                  3                                            +                                                (                                                                                    [                                                                                                            0                                                                                      1                                                                                                                                          1                                                                                      0                                                                                                      ]                                                                                                            [                                                                                                            0                                                                                                                                          1                                                                                                      ]                                                                              )                                                  ⊗                  3                                                      )                                ⊗                          |                        V                          |                                            {\displaystyle =\left((0,1,1,1){\begin{bmatrix}0&1\\1&0\end{bmatrix}}^{\otimes 2}\right)^{\otimes |U|}\left(\left({\begin{bmatrix}0&1\\1&0\end{bmatrix}}{\begin{bmatrix}1\\0\end{bmatrix}}\right)^{\otimes 3}+\left({\begin{bmatrix}0&1\\1&0\end{bmatrix}}{\begin{bmatrix}0\\1\end{bmatrix}}\right)^{\otimes 3}\right)^{\otimes |V|}}                      =        (        1        ,        1        ,        1        ,        0                  )                      ⊗                          |                        U                          |                                                            (                                                                                [                                                                                            0                                                                                                                      1                                                                                      ]                                                                    ⊗                  3                                            +                                                                    [                                                                                            1                                                                                                                      0                                                                                      ]                                                                    ⊗                  3                                                      )                                ⊗                          |                        V                          |                                            {\displaystyle =(1,1,1,0)^{\otimes |U|}\left({\begin{bmatrix}0\\1\end{bmatrix}}^{\otimes 3}+{\begin{bmatrix}1\\0\end{bmatrix}}^{\otimes 3}\right)^{\otimes |V|}}                      =                              NAND                                2                                ⊗                          |                        U                          |                                                            EQUAL                                3                                ⊗                          |                        V                          |                                      ,              {\displaystyle ={\text{NAND}}_{2}^{\otimes |U|}{\text{EQUAL}}_{3}^{\otimes |V|},}  which is                               Holant                (        H        ,                              NAND                                2                          ,                              EQUAL                                3                          )        ,              {\displaystyle {\text{Holant}}(H,{\text{NAND}}_{2},{\text{EQUAL}}_{3}),}   the Holant problem that naturally corresponds to counting the number of independent sets in G.== History ==As with any type of reduction, a holographic reduction does not, by itself, yield a polynomial time algorithm. In order to get a polynomial time algorithm, the problem being reduced to must also have a polynomial time algorithm. Valiant's original application of holographic algorithms used a holographic reduction to a problem where every constraint is realizable by matchgates, which he had just proved is tractable by a further reduction to counting the number of perfect matchings in a planar graph. The latter problem is tractable by the FKT algorithm, which dates to the 1960s.Soon after, Valiant found holographic algorithms with reductions to matchgates for #7Pl-Rtw-Mon-3CNF and #7Pl-3/2Bip-VC. These problems may appear somewhat contrived, especially with respect to the modulus. Both problems were already known to be #P-hard when ignoring the modulus and Valiant supplied proofs of #P-hardness modulo 2, which also used holographic reductions. Valiant found these two problems by a computer search that looked for problems with holographic reductions to matchgates. He called their algorithms accidental algorithms, saying "when applying the term accidental to an algorithm we intend to point out that the algorithm arises from satisfying an apparently onerous set of constraints." The "onerous" set of constraints in question are polynomial equations that, if satisfied, imply the existence of a holographic reduction to matchgate realizable constraints.After several years of developing (what is known as) matchgate signature theory, Jin-Yi Cai and Pinyan Lu were able to explain the existence of Valiant's two accidental algorithms.  These two problems are just special cases of two much larger families of problems: #2k-1Pl-Rtw-Mon-kCNF and #2k-1Pl-k/2Bip-VC for any positive integer k. The modulus 7 is just the third Mersenne number and Cai and Lu showed that these types of problems with parameter k can be solved in polynomial time exactly when the modulus is the kth Mersenne number by using holographic reductions to matchgates and the Chinese remainder theorem.Around the same time, Jin-Yi Cai, Pinyan Lu and Mingji Xia gave the first holographic algorithm that did not reduce to a problem that is tractable by matchgates. Instead, they reduced to a problem that is tractable by Fibonacci gates, which are symmetric constraints whose truth tables satisfy a recurrence relation similar to one that defines the Fibonacci numbers. They also used holographic reductions to prove that certain counting problems are #P-hard. Since then, holographic reductions have been used extensively as ingredients in both polynomial time algorithms and proofs of #P-hardness.== References ==
	Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan                               t                      s                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{s}\in [0,\infty ]}   and elapsed time                               t                      e                          ∈        [        0        ,        ∞        )              {\displaystyle t_{e}\in [0,\infty )}   by introducing two other time variables, last event time,                               t                      l                          ∈        [        0        ,        ∞        )              {\displaystyle t_{l}\in [0,\infty )}  , and next event time                               t                      n                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{n}\in [0,\infty ]}   with the following relations:                                       t                      e                          =        t        −                  t                      l                                {\displaystyle \,t_{e}=t-t_{l}}  and                                      t                      s                          =                  t                      n                          −                  t                      l                                {\displaystyle \,t_{s}=t_{n}-t_{l}}  where                     t        ∈        [        0        ,        ∞        )              {\displaystyle t\in [0,\infty )}   denotes the current time. And the remaining time,                                       t                      r                          =                  t                      s                          −                  t                      e                                {\displaystyle \,t_{r}=t_{s}-t_{e}}   is equivalently computed as                                      t                      r                          =                  t                      n                          −        t              {\displaystyle \,t_{r}=t_{n}-t}  , apparently                               t                      r                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{r}\in [0,\infty ]}  .Since the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.== Common parts ==Regardless of two different views of total states, algorithms for initialization and internal transition cases are commonly defined as below.DEVS-simulator  variables:    parent // parent coordinator                                  t                      l                                {\displaystyle t_{l}}       // time of last event                                  t                      n                                {\displaystyle t_{n}}       // time of next event                        A        =        (        X        ,        Y        ,        S        ,        t        a        ,                  δ                      e            x            t                          ,                  δ                      i            n            t                          ,        λ        )              {\displaystyle A=(X,Y,S,ta,\delta _{ext},\delta _{int},\lambda )}  // the associated Atomic DEVS model   when receive init-message(Time                     t              {\displaystyle t}  )                                   t                      l                          ←        t        ;              {\displaystyle t_{l}\leftarrow t;}                                     t                      n                          ←                  t                      l                          +        t        a        (        s        )        ;              {\displaystyle t_{n}\leftarrow t_{l}+ta(s);}    when receive star-message(Time                     t              {\displaystyle t}  )     if                     t        ≠                  t                      n                                {\displaystyle t\neq t_{n}}   then        error: bad synchronization;                         y        ←        λ        (        s        )        ;              {\displaystyle y\leftarrow \lambda (s);}       send y-message(                    y        ,        t              {\displaystyle y,t}  ) to parent;                         s        ←                  δ                      i            n            t                          (        s        )              {\displaystyle s\leftarrow \delta _{int}(s)}                                     t                      l                          ←        t        ;              {\displaystyle t_{l}\leftarrow t;}                                     t                      n                          ←                  t                      l                          +        t        a        (        s        )        ;              {\displaystyle t_{n}\leftarrow t_{l}+ta(s);}  == View 1: total states = states * elapsed times ==As addressed in Behavior of Atomic DEVS, when DEVS receives an input event, right calling                               δ                      e            x            t                                {\displaystyle \delta _{ext}}  , the last event time,                              t                      l                                {\displaystyle t_{l}}   is set by the current time,                    t              {\displaystyle t}  , thus the elapsed time                              t                      e                                {\displaystyle t_{e}}   becomes zero because                               t                      e                          =        t        −                  t                      l                                {\displaystyle t_{e}=t-t_{l}}  .  when receive x-message(                    x        ∈        X              {\displaystyle x\in X}  , Time                     t              {\displaystyle t}  )     if                     (                  t                      l                          ≤        t              {\displaystyle (t_{l}\leq t}   and                     t        ≤                  t                      n                          )              {\displaystyle t\leq t_{n})}   == false then        error: bad synchronization;                         s        ←                  δ                      e            x            t                          (        s        ,        t        −                  t                      l                          ,        x        )              {\displaystyle s\leftarrow \delta _{ext}(s,t-t_{l},x)}                                     t                      l                          ←        t        ;              {\displaystyle t_{l}\leftarrow t;}                                     t                      n                          ←                  t                      l                          +        t        a        (        s        )        ;              {\displaystyle t_{n}\leftarrow t_{l}+ta(s);}  == View 2: total states = states * lifespans * elapsed times ==Notice that as addressed in Behavior of Atomic DEVS, depending on the value of                     b              {\displaystyle b}   return by                               δ                      e            x            t                                {\displaystyle \delta _{ext}}  , last event time,                              t                      l                                {\displaystyle t_{l}}  , and next event time,                              t                      n                                {\displaystyle t_{n}}  ,consequently, elapsed time,                               t                      e                                {\displaystyle t_{e}}  , and lifespan                              t                      n                                {\displaystyle t_{n}}  , are updated (if                     b        =        1              {\displaystyle b=1}  ) or preserved (if                     b        =        0              {\displaystyle b=0}  ).  when receive x-message(                    x        ∈        X              {\displaystyle x\in X}  , Time                     t              {\displaystyle t}  )     if                     (                  t                      l                          ≤        t              {\displaystyle (t_{l}\leq t}   and                     t        ≤                  t                      n                          )              {\displaystyle t\leq t_{n})}   == false then        error: bad synchronization;                         (        s        ,        b        )        ←                  δ                      e            x            t                          (        s        ,        t        −                  t                      l                          ,        x        )              {\displaystyle (s,b)\leftarrow \delta _{ext}(s,t-t_{l},x)}       if                     b        =        1              {\displaystyle b=1}   then                                       t                      l                          ←        t        ;              {\displaystyle t_{l}\leftarrow t;}                                        t                      n                          ←                  t                      l                          +        t        a        (        s        )        ;              {\displaystyle t_{n}\leftarrow t_{l}+ta(s);}  == See also ==Atomic DEVSBehavior of atomic DEVSSimulation algorithms for coupled DEVS== References ==[Zeigler84] Bernard Zeigler (1984). Multifacetted Modeling and Discrete Event Simulation. Academic Press, London; Orlando. ISBN 978-0-12-778450-2.[ZKP00] Bernard Zeigler; Tag Gon Kim; Herbert Praehofer (2000). Theory of Modeling and Simulation (second ed.). Academic Press, New York. ISBN 978-0-12-778455-7.
	Given a coupled DEVS model, simulation algorithms are methods to generate the model's legal behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.)  [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan                               t                      s                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{s}\in [0,\infty ]}   and elapsed time                               t                      e                          ∈        [        0        ,        ∞        )              {\displaystyle t_{e}\in [0,\infty )}   by introducing two other time variables, last event time,                               t                      l                          ∈        [        0        ,        ∞        )              {\displaystyle t_{l}\in [0,\infty )}  , and next event time                               t                      n                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{n}\in [0,\infty ]}   with the following relations:                                       t                      e                          =        t        −                  t                      l                                {\displaystyle \,t_{e}=t-t_{l}}  and                                      t                      s                          =                  t                      n                          −                  t                      l                                {\displaystyle \,t_{s}=t_{n}-t_{l}}  where                     t        ∈        [        0        ,        ∞        )              {\displaystyle t\in [0,\infty )}   denotes the current time. And the remaining time,                                       t                      r                          =                  t                      s                          −                  t                      e                                {\displaystyle \,t_{r}=t_{s}-t_{e}}   is equivalently computed as                                      t                      r                          =                  t                      n                          −        t              {\displaystyle \,t_{r}=t_{n}-t}  , apparently                               t                      r                          ∈        [        0        ,        ∞        ]              {\displaystyle t_{r}\in [0,\infty ]}  .Based on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.== Algorithms ==DEVS-coordinator  Variables:     parent // parent coordinator                                   t                      l                                {\displaystyle t_{l}}  : // time of last event                                   t                      n                                {\displaystyle t_{n}}  : // time of next event                         N        =        (        X        ,        Y        ,        D        ,        {                  M                      i                          }        ,                  C                      x            x                          ,                  C                      y            x                          ,                  C                      y            y                          ,        S        e        l        e        c        t        )              {\displaystyle N=(X,Y,D,\{M_{i}\},C_{xx},C_{yx},C_{yy},Select)}  // the associated Coupled DEVS model  when receive init-message(Time t)     for each                     i        ∈        D              {\displaystyle i\in D}   do        send init-message(t) to child                     i              {\displaystyle i}                                     t                      l                          ←        max        {                  t                      l            i                          :        i        ∈        D        }              {\displaystyle t_{l}\leftarrow \max\{t_{li}:i\in D\}}  ;                                   t                      n                          ←        min        {                  t                      n            i                          :        i        ∈        D        }              {\displaystyle t_{n}\leftarrow \min\{t_{ni}:i\in D\}}  ;  when receive star-message(Time t)     if                     t        ≠                  t                      n                                {\displaystyle t\neq t_{n}}   then        error: bad synchronization;                                   i                      ∗                          ←        S        e        l        e        c        t        (        {        i        ∈        D        :                  t                      n            i                          =                  t                      n                          }        )        ;              {\displaystyle i^{*}\leftarrow Select(\{i\in D:t_{ni}=t_{n}\});}       send star-message(t)to                               i                      ∗                                {\displaystyle i^{*}}                                     t                      l                          ←        max        {                  t                      l            i                          :        i        ∈        D        }              {\displaystyle t_{l}\leftarrow \max\{t_{li}:i\in D\}}  ;                                   t                      n                          ←        min        {                  t                      n            i                          :        i        ∈        D        }              {\displaystyle t_{n}\leftarrow \min\{t_{ni}:i\in D\}}  ;  when receive x-message(                    x        ∈        X              {\displaystyle x\in X}  , Time t)     if                     (                  t                      l                          ≤        t              {\displaystyle (t_{l}\leq t}   and                     t        ≤                  t                      n                          )              {\displaystyle t\leq t_{n})}   == false then        error: bad synchronization;     for each                     (        x        ,                  x                      i                          )        ∈                  C                      x            x                                {\displaystyle (x,x_{i})\in C_{xx}}   do        send x-message(                              x                      i                                {\displaystyle x_{i}}  ,t) to child                     i              {\displaystyle i}                                     t                      l                          ←        max        {                  t                      l            i                          :        i        ∈        D        }              {\displaystyle t_{l}\leftarrow \max\{t_{li}:i\in D\}}  ;                                   t                      n                          ←        min        {                  t                      n            i                          :        i        ∈        D        }              {\displaystyle t_{n}\leftarrow \min\{t_{ni}:i\in D\}}  ;  when receive y-message(                              y                      i                          ∈                  Y                      i                                {\displaystyle y_{i}\in Y_{i}}  , Time t)     for each                     (                  y                      i                          ,                  x                      i                          )        ∈                  C                      y            x                                {\displaystyle (y_{i},x_{i})\in C_{yx}}   do        send x-message(                              x                      i                                {\displaystyle x_{i}}  ,t) to child                     i              {\displaystyle i}       if                               C                      y            y                          (                  y                      i                          )        ≠        ϕ              {\displaystyle C_{yy}(y_{i})\neq \phi }   then        send y-message(                              C                      y            y                          (                  y                      i                          )              {\displaystyle C_{yy}(y_{i})}  , t) to parent;                                   t                      l                          ←        max        {                  t                      l            i                          :        i        ∈        D        }              {\displaystyle t_{l}\leftarrow \max\{t_{li}:i\in D\}}  ;                                   t                      n                          ←        min        {                  t                      n            i                          :        i        ∈        D        }              {\displaystyle t_{n}\leftarrow \min\{t_{ni}:i\in D\}}  ;== See also ==Coupled DEVSBehavior of Coupled DEVSSimulation Algorithms for Atomic DEVS== References ==[Zeigler84] Bernard Zeigler (1984). Multifacetted Modeling and Discrete Event Simulation. Academic Press, London; Orlando. ISBN 978-0-12-778450-2.[ZKP00] Bernard Zeigler; Tag Gon Kim; Herbert Praehofer (2000). Theory of Modeling and Simulation (second ed.). Academic Press, New York. ISBN 978-0-12-778455-7.
	In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus "sequential" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used."Sequential algorithm" may also refer specifically to an algorithm for decoding a convolutional code.== See also ==Online algorithmStreaming algorithm== References ==
	In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given nondeterministic finite automaton (NFA) into a regular expression. Together with other conversion algorithms, it establishes the equivalence of several description formats for regular languages. Alternative presentations of the same method include the "elimination method" attributed to Brzozowski and McCluskey, the algorithm of McNaughton and Yamada, and the use of Arden's lemma.== Algorithm description ==According to Gross and Yellen (2004), the algorithm can be traced back to Kleene (1956). A presentation of the algorithm in the case of deterministic finite automata (DFAs) is given in Hopcroft and Ullman (1979). The presentation of the algorithm for NFAs below follows Gross and Yellen (2004).Given a nondeterministic finite automaton M = (Q, Σ, δ, q0, F), with Q = { q0,...,qn } its set of states, the algorithm computes the sets Rkij of all strings that take M from state qi to qj without going through any state numbered higher than k.Here, "going through a state" means entering and leaving it, so both i and j may be higher than k, but no intermediate state may.Each set Rkij is represented by a regular expression; the algorithm computes them step by step for k = -1, 0, ..., n. Since there is no state numbered higher than n, the regular expression Rn0j represents the set of all strings that take M from its start state q0 to qj. If F = { q1,...,qf } is the set of accept states, the regular expression Rn01 | ... | Rn0f represents the language accepted by M.The initial regular expressions, for k = -1, are computed as follows for i≠j:R−1ij = a1 | ... | am       where qj ∈ δ(qi,a1), ..., qj ∈ δ(qi,am)and as follows for i=j:R−1ii = a1 | ... | am | ε       where qi ∈ δ(qi,a1), ..., qi ∈ δ(qi,am)In other words, R−1ij mentions all letters that label a transition from i to j, and we also include ε in the case where i=j.After that, in each step the expressions Rkij are computed from the previous ones byRkij = Rk-1ik (Rk-1kk)* Rk-1kj | Rk-1ijAnother way to understand the operation of the algorithm is as an "elimination method", where the states from 0 to n are successively removed: when state k is removed, the regular expression Rk-1ij, which describes the words that label a path from state i>k to state j>k, is rewritten into Rkij so as to take into account the possibility of going via the "eliminated" state k.By induction on k, it can be shown that the length of each expression Rkij is at most 4k+1(6s+7) - 4/3 symbols, where s denotes the number of characters in Σ.Therefore, the length of the regular expression representing the language accepted by M is at most 4n+1(6s+7)f - f - 3/3 symbols, where f denotes the number of final states.This exponential blowup is inevitable, because there exist families of DFAs for which any equivalent regular expression must be of exponential size.In practice, the size of the regular expression obtained by running the algorithm can be very different depending on the order in which the states are considered by the procedure, i.e., the order in which they are numbered from 0 to n.== Example ==The automaton shown in the picture can be described as M = (Q, Σ, δ, q0, F) withthe set of states Q = { q0, q1, q2 },the input alphabet Σ = { a, b },the transition function δ with δ(q0,a)=q0,   δ(q0,b)=q1,   δ(q1,a)=q2,   δ(q1,b)=q1,   δ(q2,a)=q1, and δ(q2,b)=q1,the start state q0, andset of accept states F = { q1 }.Kleene's algorithm computes the initial regular expressions asAfter that, the Rkij are computed from the Rk-1ij step by step for k = 0, 1, 2.Kleene algebra equalities are used to simplify the regular expressions as much as possible.Step 0Step 1Step 2Since q0 is the start state and q1 is the only accept state, the regular expression R201 denotes the set of all strings accepted by the automaton.== See also ==Floyd-Warshall algorithm — an algorithm on weighted graphs that can be implemented by Kleene's algorithm using a particular Kleene algebraStar height problem — what is the minimum stars' nesting depth of all regular expressions corresponding to a given DFA?Generalized star height problem — if a complement operator is allowed additionally in regular expressions, can the stars' nesting depth of Kleene's algorithm's output be limited to a fixed bound?Thompson's construction algorithm — transforms a regular expression to a finite automaton== References ==
	In robotics and motion planning, kinodynamic planning is a class of problems for which velocity,  acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles.  The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a  provably polynomial-time ε-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control  ("fastest path") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral  (3D) obstacles, subject to  state  bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics. More recently, many  practical heuristic algorithms  based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However,  none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and  none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).== References ==
	Rendezvous or highest random weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to.Rendezvous hashing is more general than consistent hashing, which becomes a special case (for                     k        =        1              {\displaystyle k=1}  ) of rendezvous hashing.== History ==Rendezvous hashing was invented  by David Thaler and Chinya Ravishankar at the University of Michigan in 1996. Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE) to identify multicast rendezvous points in a distributed fashion. It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing. Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching, router design, secure key establishment, and sharding and distributed databases.== Overview ===== Algorithm ===Rendezvous hashing solves the distributed hash table problem: How can a set of clients, given an object O, agree on where in a set of n sites (servers, say) to place O? Each client is to select a site independently, but all clients must end up picking the same site. This is non-trivial if we add a minimal disruption constraint, and require that only objects mapping to a removed site may be reassigned to other sites.The basic idea is to give each site Sj a score (a weight) for each object Oi, and assign the object to the highest scoring site. All clients first agree on a hash function h(). For object Oi, the site Sj is defined to have weight wi,j = h(Oi, Sj). HRW assigns Oi to the site Sm whose weight wi,m is the largest. Since h() is agreed upon, each client can independently compute the weights wi,1, wi,2, ..., wi,n and pick the largest. If the goal is distributed k-agreement, the clients can independently pick the sites with the k largest hash values.If a site S is added or removed, only the objects mapping to S are remapped to different sites, satisfying the minimal disruption constraint above. The HRW assignment can be computed independently by any client, since it depends only on the identifiers for the set of sites S1, S2, ..., Sn and the object being assigned.HRW easily accommodates different capacities among sites. If site Sk has twice the capacity of the other sites, we simply represent Sk twice in the list, say, as Sk,1 and Sk,2. Clearly, twice as many objects will now map to Sk as to the other sites.=== Properties ===It might first appear sufficient to treat the n sites as buckets in a hash table and hash the object name O into this table. However, if any of the sites fails or is unreachable, the hash table size changes, requiring all objects to be remapped. This massive disruption makes such direct hashing unworkable. Under rendezvous hashing, however, clients handle site failures by picking the site that yields the next largest weight. Remapping is required only for objects currently mapped to the failed site, and disruption is minimal.Rendezvous hashing has the following properties:Low overhead: The hash function used is efficient, so overhead at the clients is very low.Load balancing: Since the hash function is randomizing, each of the n sites is equally likely to receive the object O. Loads are uniform across the sites.Site capacity: Sites with different capacities can be represented in the site list with multiplicity in proportion to capacity. A site with twice the capacity of the other sites will be represented twice in the list, while every other site is represented once.High hit rate: Since all clients agree on placing an object O into the same site SO , each fetch or placement of O into SO yields the maximum utility in terms of hit rate. The object O will always be found unless it is evicted by some replacement algorithm at SO.Minimal disruption: When a site fails, only the objects mapped to that site need to be remapped. Disruption is at the minimal possible level, as proved in.Distributed k-agreement: Clients can reach distributed agreement on k sites simply by selecting the top k sites in the ordering.=== Comparison with consistent hashing ===Consistent hashing operates by mapping sites uniformly and randomly to points on a unit circle called tokens. Objects are also mapped to the unit circle and placed in the site whose token is the first encountered traveling clockwise from the object's location. When a site is removed, the objects it owns are transferred to the site owning the next token encountered moving clockwise. Provided each site is mapped to a large number (100-200, say) of tokens this will reassign objects in a relatively uniform fashion among the remaining sites.If sites are mapped to points on the circle randomly by hashing 200 variants of the site ID, say, the assignment of any object requires storing or recalculating 200 hash values for each site. However, the tokens associated with a given site can be precomputed and stored in a sorted list, requiring only a single application of the hash function to the object, and a binary search to compute the assignment. Even with many tokens per site, however, the basic version of consistent hashing may not balance objects uniformly over sites, since when a site is removed each object assigned to it is distributed only over as many other sites as the site has tokens (say 100-200).Variants of consistent hashing (such as Amazon's Dynamo) that use more complex logic to distribute tokens on the unit circle offer better load balancing than basic consistent hashing, reduce the overhead of adding new sites, and reduce metadata overhead and offer other benefits.In contrast, rendezvous hashing (HRW) is much simpler conceptually and in practice. It also distributes objects uniformly over all sites, given a uniform hash function. Unlike consistent hashing, HRW requires no precomputing or storage of tokens. An object Oi is placed into one of n sites S1, ..., Sn by computing the n hash values h(Oi,Sj) and picking the site Sk that yields the highest hash value. If a new site Sn+1 is added, new object placements or requests will compute n+1 hash values, and pick the largest of these. If an object already in the system at Sk maps to this new site Sn+1, it will be fetched afresh and cached at Sn+1. All clients will henceforth obtain it from this site, and the old cached copy at Sk will ultimately be replaced by the local cache management algorithm. If Sk is taken offline, its objects will be remapped uniformly to the remaining n-1 sites.Variants of the HRW algorithm, such as the use of a skeleton (see below), can reduce the                     O        (        n        )              {\displaystyle O(n)}   time for object location to                     O        (        l        o        g        (        n        )        )              {\displaystyle O(log(n))}  , at the cost of less global uniformity of placement. When n is not too large, however, the                     O        (        n        )              {\displaystyle O(n)}   placement cost of basic HRW is not likely to be a problem. HRW completely avoids all the overhead and complexity associated with correctly handling multiple tokens for each site and associated metadata.Rendezvous hashing also has the great advantage that it provides simple solutions to other important problems, such as distributed k-agreement.==== Reducing consistent hashing ====Consistent hashing can be reduced to an instance of HRW by an appropriate choice of a two-place hash function. From the site identifier                     S              {\displaystyle S}   the simplest version of consistent hashing computes a list of token positions, e.g.,                               t                      i                          =                  h                      c                          (        S                                                          ^                                      i        )              {\displaystyle t_{i}=h_{c}(S{\hat {}}i)}   where                               h                      c                                {\displaystyle h_{c}}   hashes values to locations on the unit circle. Define the two place hash function                     h        (        S        ,        O        )              {\displaystyle h(S,O)}   to be                                           1                                          min                                  i                                                            h                                  c                                            (              O              )              −                              t                                  i                                                                          {\displaystyle {\frac {1}{\min _{i}h_{c}(O)-t_{i}}}}   where                               h                      c                          (        O        )        −                  t                      i                                {\displaystyle h_{c}(O)-t_{i}}   denotes the distance along the unit circle from                               h                      c                          (        O        )              {\displaystyle h_{c}(O)}   to                               t                      i                                {\displaystyle t_{i}}   (since                               h                      c                          (        O        )        −                  t                      i                                {\displaystyle h_{c}(O)-t_{i}}   has some minimal non-zero value there is no problem translating this value to a unique integer in some bounded range). This will duplicate exactly the assignment produced by consistent hashing.It is not possible, however, to reduce HRW to consistent hashing (assuming the number of tokens per site is bounded), since HRW potentially reassigns the objects from a removed site to an unbounded number of other sites.== Weighted variations ==In the standard implementation of rendezvous hashing, every node receives a statically equal proportion of the keys. This behavior, however, is undesirable when the nodes have different capacities for processing or holding their assigned keys. For example, if one of the nodes had twice the storage capacity as the others, it would be beneficial if the algorithm could take this into account such that this more powerful node would receive twice the number of keys as each of the others.A straightforward mechanism to handle this case is to assign two virtual locations to this node, so that if either of that larger node's virtual locations has the highest hash, that node receives the key. But this strategy does not work when the relative weights are not integer multiples. For example, if one node had 42% more storage capacity, it would require adding many virtual nodes in different proportions, leading to greatly reduced performance. Several modifications to rendezvous hashing have been proposed to overcome this limitation.=== Cache Array Routing Protocol ===The Cache Array Routing Protocol (CARP) is a 1998 IETF draft that describes a method for computing load factors which can be multiplied by each node's hash score to yield an arbitrary level of precision for weighting nodes differently. However, one disadvantage of this approach is that when any node's weight is changed, or when any node is added or removed, all the load factors must be re-computed and relatively scaled. When the load factors change relative to one another, it triggers movement of keys between nodes whose weight was not changed, but whose load factor did change relative to other nodes in the system. This results in excess movement of keys.=== Controlled replication ===Controlled replication under scalable hashing or CRUSH is an extension to RUSH that improves upon rendezvous hashing by constructing a tree where a pseudo-random function (hash) is used to navigate down the tree to find which node is ultimately responsible for a given key. It permits perfect stability for adding nodes however it is not perfectly stable when removing or re-weighting nodes, with the excess movement of keys being proportional to the height of the tree.The CRUSH algorithm is used by the ceph data storage system to map data objects to the nodes responsible for storing them.=== Skeleton-based variant ===When n is extremely large, a skeleton-based variant can improve running time. This approach creates a virtual hierarchical structure (called a "skeleton"), and achieves                     O        (        l        o        g        n        )              {\displaystyle O(logn)}   running time by applying HRW at each level while descending the hierarchy. The idea is to first choose some constant m and organize the n sites into c = ceiling(n/m) clusters C1, = {S1, S2, ... ,Sm}, C2 = {Sm+1, Sm+2, ... ,S2m}, ... Next, build a virtual hierarchy by choosing a constant f and imagining these c clusters placed at the leaves of a tree T of virtual nodes, each with fanout f.In the accompanying diagram, the cluster size is m = 4, and the skeleton fanout is                     f        =        3              {\displaystyle f=3}  . Assuming 108 sites (real nodes) for convenience, we get a three-tier virtual hierarchy. Since f = 3, each virtual node has a natural numbering in octal. Thus, the 27 virtual nodes at the lowest tier would be numbered 000, 001, 002, ..., 221, 222, in octal. (We can, of course, vary the fanout at each level. In that case, each node will be identified with the corresponding mixed-radix number.)Instead of applying HRW to all 108 real nodes, we can first apply HRW to the 27 lowest-tier virtual nodes, selecting one. We then apply HRW to the four real nodes in its cluster, and choose the winning site. We only need                     27        +        4        =        31              {\displaystyle 27+4=31}   hashes, rather than 108. If we apply this method starting one level higher in the hierarchy, we would need                     9        +        3        +        4        =        16              {\displaystyle 9+3+4=16}   hashes to get to the winning site. The figure shows how, if we proceed starting from the root of the skeleton, we may successively choose the virtual nodes (2)3, (20)3, and (200)3, and finally end up with site 74.We can start at any level in the virtual hierarchy, not just at the root. Starting lower in the hierarchy requires more hashes, but may improve load distribution in the case of failures. Also, the virtual hierarchy need not be stored, but can be created on demand, since the virtual nodes names are simply prefixes of base-f (or mixed-radix) representations. We can easily create appropriately sorted strings from the digits, as required. In the example, we would be working with the strings 0, 1, 2 (at tier 1), 20, 21, 22 (at tier 2), and 200, 201, 202 (at tier 3). Clearly, T has height                     h        =        O        (        l        o        g        c        )        =        O        (        l        o        g        n        )              {\displaystyle h=O(logc)=O(logn)}  , since m and f are both constants. The work done at each level is O (1), since f is a constant.For any given object O, it is clear that the method chooses each cluster, and hence each of the n sites, with equal probability. If the site finally selected is unavailable, we can select a different site within the same cluster, in the usual manner. Alternatively, we could go up one or more tiers in the skeleton and select an alternate from among the sibling virtual nodes at that tier, and once again descend the hierarchy to the real nodes, as above.The value of m can be chosen based on factors like the anticipated failure rate and the degree of desired load balancing. A higher value of m leads to less load skew in the event of failure at the cost of higher search overhead.The choice                     m        =        m              {\displaystyle m=m}   is equivalent to non-hierarchical rendezvous hashing. In practice, the hash function                     h        (        )              {\displaystyle h()}   is very cheap, so                     m        =        n              {\displaystyle m=n}   can work quite well unless n is very high.=== Other variants ===In 2005, Christian Schindelhauer and Gunnar Schomaker described a logarithmic method for re-weighting hash scores in a way that does not require relative scaling of load factors when a node's weight changes or when nodes are added or removed. This enabled the dual benefits of perfect precision when weighting nodes, along with perfect stability, as only a minimum number of keys needed to be remapped to new nodes.A similar logarithm-based hashing strategy is used to assign data to storage nodes in Cleversafe's data storage system, now IBM Cloud Object Storage.== Implementation ==Implementation is straightforward once a hash function                     h        (        )              {\displaystyle h()}   is chosen (the original work on the HRW method makes a hash function recommendation). Each client only needs to compute a hash value for each of the n sites, and then pick the largest. This algorithm runs in                     O        (        n        )              {\displaystyle O(n)}   time. If the hash function is efficient, the                     O        (        n        )              {\displaystyle O(n)}   running time is not a problem unless n is very large.=== Weighted rendezvous hash ===Python code implementing a weighted rendezvous hash:Example outputs of WRH:== References ==
	The Chandy–Misra–Haas algorithm resource model checks for deadlock in a distributed system. It was developed by K. Mani Chandy, Jayadev Misra and Laura M Haas.== Locally dependent ==Consider the n processes P1, P2, P3, P4, P5,, ... ,Pn which are performed in a single system(controller). P1 is locally dependent on Pn, if P1 depends on P2, P2 on P3, so on and Pn−1 on Pn. That is, if                               P                      1                          →                  P                      2                          →                  P                      3                          →        …        →                  P                      n                                {\displaystyle P_{1}\rightarrow P_{2}\rightarrow P_{3}\rightarrow \ldots \rightarrow P_{n}}  , then                               P                      1                                {\displaystyle P_{1}}   is locally dependent on                               P                      n                                {\displaystyle P_{n}}  . If P1 is said to be locally dependent to itself if it is locally dependent on Pn and Pn depends on P1: i.e. if                               P                      1                          →                  P                      2                          →                  P                      3                          →        …        →                  P                      n                          →                  P                      1                                {\displaystyle P_{1}\rightarrow P_{2}\rightarrow P_{3}\rightarrow \ldots \rightarrow P_{n}\rightarrow P_{1}}  , then                               P                      1                                {\displaystyle P_{1}}   is locally dependent on itself.== Description ==The algorithm uses a message called probe(i,j,k) to transfer a message from controller of process Pj to controller of process Pk. It specifies a message started by process Pi to find whether a deadlock has occurred or not.  Every process Pj maintains a boolean array dependent which contains the information about the processes that depend on it. Initially the values of each array are all "false".=== Controller sending a probe ===Before sending, the probe checks whether Pj is locally dependent on itself.  If so, a deadlock occurs. Otherwise it checks whether Pj, and Pk are in different controllers, are locally dependent and Pj is waiting for the resource that is locked by Pk. Once all the conditions are satisfied it sends the probe.=== Controller receiving a probe ===On the receiving side, the controller checks whether Pk is performing a task. If so, it neglects the probe. Otherwise, it checks the responses given Pk to Pj and dependentk(i) is false. Once it is verified, it assigns true to dependentk(i). Then it checks whether k is equal to i. If both are equal, a deadlock occurs, otherwise it sends the probe to next dependent process.== Algorithm ==In pseudocode, the algorithm works as follows:=== Controller sending a probe ===if Pj is locally dependent on itself     then declare deadlockelse for all Pj,Pk  such that     (i) Pi is locally dependent on Pj,     (ii) Pj is waiting for 'Pk and     (iii) Pj, Pk are on different controllers.send probe(i, j, k). to home site of Pk=== Controller receiving a probe ===if     (i)Pk is idle / blocked     (ii) dependentk(i) = false, and     (iii) Pk has not replied to all requests of to Pjthen begin     "dependents""k"(i) = true;     if k == i     then declare that Pi  is deadlocked     else for all Pa,Pb  such that            (i) Pk is locally dependent on Pa,            (ii) Pa is waiting for 'Pb and            (iii) Pa, Pb are on different controllers.     send probe(i, a, b). to home site of Pbend== Example ==P1 initiates deadlock detection. C1 sends the probe saying P2 depends on P3. Once the message is received by C2, it checks whether P3 is idle. P3 is idle because it is locally dependent on P4 and updates dependent3(2) to True.As above, C2  sends probe to C3 and C3 sends probe to C1. At C1, P1 is idle so it update dependent1(1) to True. Therefore, deadlock can be declared.== Complexity ==Consider that there are "m" controllers and "p" process to perform, to declare whether a deadlock has occurred or not, the worst case for controllers and processes must be visited. Therefore, the solution is O(m+p). The time complexity is O(n).== References ==
	KHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (                    k              {\textstyle k}  -hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA  operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.KHOPCA's clustering process explicitly supports joining and leaving of nodes, which makes KHOPCA suitable for highly dynamic networks. However, it has been demonstrated that KHOPCA also performs in static networks.Besides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, networked swarming, and real-time data clustering and analysis.== Algorithm description ==KHOPCA (                    k              {\textstyle k}  -hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable                     k              {\textstyle k}  -hops. A set of local rules describes the state transition between nodes. A node's weight is determined only depending on the current state of its neighbors in communication range. Each node of the network is continuously involved in this process. As result,                     k              {\textstyle k}  -hop clusters are formed and maintained in static as well as dynamic networks.KHOPCA does not require any predetermined initial configuration. Therefore, a node can potentially choose any weight (between                     M        I        N              {\textstyle MIN}   and                     M        A        X              {\textstyle MAX}  ). However, the choice of the initial configuration does influence the convergence time.=== Initialization ===The prerequisites in the start configuration for the application of the rules are the following.                              N                      {\displaystyle \mathrm {N} }   is the network with nodes and links, whereby each node has a weight                     w              {\displaystyle w}  .Each node                     n              {\textstyle n}   in                               N                      {\displaystyle \mathrm {N} }   node stores the same positive values                     M        I        N              {\textstyle MIN}   and                     M        A        X              {\textstyle MAX}  , with                     M        I        N        <        M        A        X              {\textstyle MIN<MAX}  .A node                     n              {\textstyle n}   with weight                               w                      n                          =        M        A        X              {\textstyle w_{n}=MAX}   is called cluster center.                    k              {\textstyle k}   is                     M        A        X              {\textstyle MAX}   -                     M        I        N              {\textstyle MIN}   and represents the maximum size a cluster can have from the most outer node to the cluster center. The cluster diameter is therefore                     k        ⋅        2        −        1              {\textstyle k\cdot 2-1}  .                              N                (        n        )              {\displaystyle \mathrm {N} (n)}   returns the direct neighbors of node                     n              {\textstyle n}  .                    W        (                  N                )              {\textstyle W(\mathrm {N} )}   is the set of weights of all nodes of                               N                      {\displaystyle \mathrm {N} }  .The following rules describe the state transition for a node                     n              {\textstyle n}   with weight                               w                      n                                {\textstyle w_{n}}  . These rules have to be executed on each node in the order described here.=== Rule 1 ===The first rule has the function of constructing an order within the cluster. This happens through a node                     n              {\textstyle n}   detects the direct neighbor with the highest weight                     w              {\textstyle w}  , which is higher than the node's own weight                               w                      n                                {\textstyle w_{n}}  . If such a direct neighbor is detected, the node                     n              {\textstyle n}   changes its own weight to be the weight of the highest weight within the neighborhood subtracted by 1. Applied iteratively, this process creates a top-to-down hierarchical cluster structure.=== Rule 2 ===The second rule deals with the situation where nodes in a neighborhood are on the minimum weight level. This situation can happen if, for instance, the initial configuration assigns the minimum weight to all nodes. If there is a neighborhood with all nodes having the minimum weight level, the node                     n              {\textstyle n}   declares itself as cluster center. Even if coincidently all nodes declare themselves as cluster centers, the conflict situation will be resolved by one of the other rules.=== Rule 3 ===The third rule describes situations where nodes with leveraged weight values, which are not cluster centers, attract surrounding nodes with lower weights. This behavior can lead to fragmented clusters without a cluster center. In order to avoid fragmented clusters, the node with higher weight value is supposed to successively decrease its own weight with the objective to correct the fragmentation by allowing the other nodes to reconfigure according to the rules. === Rule 4 ===The fourth rule resolves the situation where two cluster centers connect in 1-hop neighborhood and need to decide which cluster center should continue its role as cluster center. Given any specific criterion (e.g., device ID, battery power), one cluster center remains while the other cluster center is hierarchized in 1-hop neighborhood to that new cluster center. The choice of the specific criterion to resolve the decision-making depends on the used application scenario and on the available information. == Examples ===== 1-D ===An exemplary sequence of state transitions applying the described four rules is illustrated below.=== 2-D ===KHOPCA acting in a dynamic 2-D simulation. The geometry is based on a geometric random graph; all existing links are drawn in this network.=== 3-D ===KHOPCA also works in a dynamic 3-D environment. The cluster connections are illustrated with bold lines.== Guarantees ==It has been demonstrated that KHOPCA terminates after a finite number of state transitions in static networks.== References ==
	Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. It can be used to find the roots of a forest of rooted trees, and can also be applied to parallelize many other graph algorithms including connected components, minimum spanning trees, and biconnected components.== List ranking ==One of the simpler tasks that can be solved by a pointer jumping algorithm is the list ranking problem. This problem is defined as follows: given a linked list of N nodes, find the distance (measured in the number of nodes) of each node to the end of the list. The distance d(n) is defined as follows, for nodes n that point to their successor by a pointer called next:If n.next is nil, then d(n) = 0.For any other node, d(n) = d(n.next) + 1.This problem can easily be solved in linear time on a sequential machine, but a parallel algorithm can do better: given n processors, the problem can be solved in logarithmic time, O(log N), by the following pointer jumping algorithm:The pointer jumping occurs in the last line of the algorithm, where each node's next pointer is reset to skip the node's direct successor. It is assumed, as in common in the PRAM model of computation, that memory access are performed in lock-step, so that each n.next.next memory fetch is performed before each n.next memory store; otherwise, processors may clobber each other's data, producing inconsistencies.Analyzing the algorithm yields a logarithmic running time. The initialization loop takes constant time, because each of the N processors performs a constant amount of work, all in parallel. The inner loop of the main loop also takes constant time, as does (by assumption) the termination check for the loop, so the running time is determined by how often this inner loop is executed. Since the pointer jumping in each iteration splits the list into two parts, one consisting of the "odd" elements and one of the "even" elements, the length of the list pointed to by each processor's n is halved in each iteration, which can be done at most  O(log N) time before each list has a length of at most one.== Root finding ==Following a path in a graph is an inherently serial operation, but pointer jumping reduces the total amount of work by following all paths simultaneously and sharing results among dependent operations. Pointer jumping iterates and finds a successor — a vertex closer to the tree root — each time. By following successors computed for other vertices, the traversal down each path can be doubled every iteration, which means that the tree roots can be found in logarithmic time.Pointer doubling operates on an array successor with an entry for every vertex in the graph. Each successor[i] is initialized with the parent index of vertex i if that vertex is not a root or to i itself if that vertex is a root. At each iteration, each successor is updated to its successor's successor. The root is found when the successor's successor points to itself.The following pseudocode demonstrates the algorithm.Input: An array parent representing a forest of trees. parent[i] is the parent of vertex i or itself for a rootOutput: An array containing the root ancestor for every vertexfor i ← 1 to length(parent) do in parallel    successor[i] ← parent[i]while true    for i ← 1 to length(successor) do in parallel       successor_next[i] ← successor[successor[i]]   if successor_next = successor then       break    for i ← 1 to length(successor) do in parallel       successor[i] ← successor_next[i]return successorThe following image provides an example of using pointer jumping on a small forest. On each iteration the successor points to the vertex following one more successor. After two iterations, every vertex points to its root node.== References ==
	The lossy count algorithm is an algorithm to identify elements in a data stream whose frequency count exceed a user-given threshold. The frequency computed by this algorithm is not always accurate, but has an error threshold that can be specified by the user. The run time space required by the algorithm is inversely proportional to the specified error threshold, hence larger the error, the smaller the footprint. It was created by eminent computer scientists Rajeev Motwani and Gurmeet Singh Manku. This algorithm finds huge application in computations where data takes the form of a continuous data stream instead of a finite data set, for e.g. network traffic measurements, web server logs, clickstreams.== References ==
	In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris. It identifies the steepest-edge approximately in its search for the optimal solution.== References ==
	The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.The DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.This very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such asMinimum and maximum stretch durationMinimum and maximum break durationMinimum and maximum work durationMinimum and maximum total durationMaximum extra work durationMaximum number of vehicle changesMinimum driving duration of a particular vehicleOperations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece.In both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.== References ==
	AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.Every learning algorithm tends to suit some problem types better than others, and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples.== Overview ==Problems in machine learning often suffer from the curse of dimensionality — each sample may consist of a huge number of potential features (for instance, there can be 162,336 Haar features, as used by the Viola–Jones object detection framework, in a 24×24 pixel image window), and evaluating every feature can reduce not only the speed of classifier training and execution, but in fact reduce predictive power. Unlike neural networks and SVMs, the AdaBoost training process selects only those features known to improve the predictive power of the model, reducing dimensionality and potentially improving execution time as irrelevant features need not be computed.=== Training ===AdaBoost refers to a particular method of training a boosted classifier. A boost classifier is a classifier in the form                              F                      T                          (        x        )        =                  ∑                      t            =            1                                T                                    f                      t                          (        x        )                              {\displaystyle F_{T}(x)=\sum _{t=1}^{T}f_{t}(x)\,\!}  where each                               f                      t                                {\displaystyle f_{t}}   is a weak learner that takes an object                     x              {\displaystyle x}   as input and returns a value indicating the class of the object. For example, in the two-class problem, the sign of the weak learner output identifies the predicted object class and the absolute value gives the confidence in that classification. Similarly, the                     T              {\displaystyle T}  th classifier is positive if the sample is in a positive class and negative otherwise.Each weak learner produces an output hypothesis,                     h        (                  x                      i                          )              {\displaystyle h(x_{i})}  , for each sample in the training set. At each iteration                     t              {\displaystyle t}  , a weak learner is selected and assigned a coefficient                               α                      t                                {\displaystyle \alpha _{t}}   such that the sum training error                               E                      t                                {\displaystyle E_{t}}   of the resulting                     t              {\displaystyle t}  -stage boost classifier is minimized.                              E                      t                          =                  ∑                      i                          E        [                  F                      t            −            1                          (                  x                      i                          )        +                  α                      t                          h        (                  x                      i                          )        ]              {\displaystyle E_{t}=\sum _{i}E[F_{t-1}(x_{i})+\alpha _{t}h(x_{i})]}  Here                               F                      t            −            1                          (        x        )              {\displaystyle F_{t-1}(x)}   is the boosted classifier that has been built up to the previous stage of training,                     E        (        F        )              {\displaystyle E(F)}   is some error function and                               f                      t                          (        x        )        =                  α                      t                          h        (        x        )              {\displaystyle f_{t}(x)=\alpha _{t}h(x)}   is the weak learner that is being considered for addition to the final classifier.=== Weighting ===At each iteration of the training process, a weight                               w                      i            ,            t                                {\displaystyle w_{i,t}}   is assigned to each sample in the training set equal to the current error                     E        (                  F                      t            −            1                          (                  x                      i                          )        )              {\displaystyle E(F_{t-1}(x_{i}))}   on that sample. These weights can be used to inform the training of the weak learner, for instance, decision trees can be grown that favor splitting sets of samples with high weights.== Derivation ==This derivation follows Rojas (2009):Suppose we have a data set                     {        (                  x                      1                          ,                  y                      1                          )        ,        …        ,        (                  x                      N                          ,                  y                      N                          )        }              {\displaystyle \{(x_{1},y_{1}),\ldots ,(x_{N},y_{N})\}}   where each item                               x                      i                                {\displaystyle x_{i}}   has an associated class                               y                      i                          ∈        {        −        1        ,        1        }              {\displaystyle y_{i}\in \{-1,1\}}  , and a set of weak classifiers                     {                  k                      1                          ,        …        ,                  k                      L                          }              {\displaystyle \{k_{1},\ldots ,k_{L}\}}   each of which outputs a classification                               k                      j                          (                  x                      i                          )        ∈        {        −        1        ,        1        }              {\displaystyle k_{j}(x_{i})\in \{-1,1\}}   for each item. After the                     (        m        −        1        )              {\displaystyle (m-1)}  -th iteration our boosted classifier is a linear combination of the weak classifiers of the form:                              C                      (            m            −            1            )                          (                  x                      i                          )        =                  α                      1                                    k                      1                          (                  x                      i                          )        +        ⋯        +                  α                      m            −            1                                    k                      m            −            1                          (                  x                      i                          )              {\displaystyle C_{(m-1)}(x_{i})=\alpha _{1}k_{1}(x_{i})+\cdots +\alpha _{m-1}k_{m-1}(x_{i})}  At the                     m              {\displaystyle m}  -th iteration we want to extend this to a better boosted classifier by adding another weak classifier                               k                      m                                {\displaystyle k_{m}}  , with another weight                               α                      m                                {\displaystyle \alpha _{m}}  :                              C                      m                          (                  x                      i                          )        =                  C                      (            m            −            1            )                          (                  x                      i                          )        +                  α                      m                                    k                      m                          (                  x                      i                          )              {\displaystyle C_{m}(x_{i})=C_{(m-1)}(x_{i})+\alpha _{m}k_{m}(x_{i})}  So it remains to determine which weak classifier is the best choice for                               k                      m                                {\displaystyle k_{m}}  , and what its weight                               α                      m                                {\displaystyle \alpha _{m}}   should be. We define the total error                     E              {\displaystyle E}   of                               C                      m                                {\displaystyle C_{m}}   as the sum of its exponential loss on each data point, given as follows:                    E        =                  ∑                      i            =            1                                N                                    e                      −                          y                              i                                                    C                              m                                      (                          x                              i                                      )                          =                  ∑                      i            =            1                                N                                    e                      −                          y                              i                                                    C                              (                m                −                1                )                                      (                          x                              i                                      )                                    e                      −                          y                              i                                                    α                              m                                                    k                              m                                      (                          x                              i                                      )                                {\displaystyle E=\sum _{i=1}^{N}e^{-y_{i}C_{m}(x_{i})}=\sum _{i=1}^{N}e^{-y_{i}C_{(m-1)}(x_{i})}e^{-y_{i}\alpha _{m}k_{m}(x_{i})}}  Letting                               w                      i                                (            1            )                          =        1              {\displaystyle w_{i}^{(1)}=1}   and                               w                      i                                (            m            )                          =                  e                      −                          y                              i                                                    C                              m                −                1                                      (                          x                              i                                      )                                {\displaystyle w_{i}^{(m)}=e^{-y_{i}C_{m-1}(x_{i})}}   for                     m        >        1              {\displaystyle m>1}  , we have:                    E        =                  ∑                      i            =            1                                N                                    w                      i                                (            m            )                                    e                      −                          y                              i                                                    α                              m                                                    k                              m                                      (                          x                              i                                      )                                {\displaystyle E=\sum _{i=1}^{N}w_{i}^{(m)}e^{-y_{i}\alpha _{m}k_{m}(x_{i})}}  We can split this summation between those data points that are correctly classified by                               k                      m                                {\displaystyle k_{m}}   (so                               y                      i                                    k                      m                          (                  x                      i                          )        =        1              {\displaystyle y_{i}k_{m}(x_{i})=1}  ) and those that are misclassified (so                               y                      i                                    k                      m                          (                  x                      i                          )        =        −        1              {\displaystyle y_{i}k_{m}(x_{i})=-1}  ):                    E        =                  ∑                                    y                              i                                      =                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                    e                      −                          α                              m                                                    +                  ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                    e                                    α                              m                                                          {\displaystyle E=\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}e^{-\alpha _{m}}+\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}e^{\alpha _{m}}}                      =                  ∑                      i            =            1                                N                                    w                      i                                (            m            )                                    e                      −                          α                              m                                                    +                  ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                          (                  e                                    α                              m                                                    −                  e                      −                          α                              m                                                    )              {\displaystyle =\sum _{i=1}^{N}w_{i}^{(m)}e^{-\alpha _{m}}+\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}(e^{\alpha _{m}}-e^{-\alpha _{m}})}  Since the only part of the right-hand side of this equation that depends on                               k                      m                                {\displaystyle k_{m}}   is                               ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                {\displaystyle \sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}}  , we see that the                               k                      m                                {\displaystyle k_{m}}   that minimizes                     E              {\displaystyle E}   is the one that minimizes                               ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                {\displaystyle \sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}}   [assuming that                               α                      m                          >        0              {\displaystyle \alpha _{m}>0}  ], i.e. the weak classifier with the lowest weighted error (with weights                               w                      i                                (            m            )                          =                  e                      −                          y                              i                                                    C                              m                −                1                                      (                          x                              i                                      )                                {\displaystyle w_{i}^{(m)}=e^{-y_{i}C_{m-1}(x_{i})}}  ).To determine the desired weight                               α                      m                                {\displaystyle \alpha _{m}}   that minimizes                     E              {\displaystyle E}   with the                               k                      m                                {\displaystyle k_{m}}   that we just determined, we differentiate:                                                        d              E                                      d                              α                                  m                                                                    =                                            d              (                              ∑                                                      y                                          i                                                        =                                      k                                          m                                                        (                                      x                                          i                                                        )                                                            w                                  i                                                  (                  m                  )                                                            e                                  −                                      α                                          m                                                                                  +                              ∑                                                      y                                          i                                                        ≠                                      k                                          m                                                        (                                      x                                          i                                                        )                                                            w                                  i                                                  (                  m                  )                                                            e                                                      α                                          m                                                                                  )                                      d                              α                                  m                                                                          {\displaystyle {\frac {dE}{d\alpha _{m}}}={\frac {d(\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}e^{-\alpha _{m}}+\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}e^{\alpha _{m}})}{d\alpha _{m}}}}  Setting this to zero and solving for                               α                      m                                {\displaystyle \alpha _{m}}   yields:                              α                      m                          =                              1            2                          ln        ⁡                  (                                                                      ∑                                                            y                                              i                                                              =                                          k                                              m                                                              (                                          x                                              i                                                              )                                                                    w                                      i                                                        (                    m                    )                                                                                                ∑                                                            y                                              i                                                              ≠                                          k                                              m                                                              (                                          x                                              i                                                              )                                                                    w                                      i                                                        (                    m                    )                                                                                )                      {\displaystyle \alpha _{m}={\frac {1}{2}}\ln \left({\frac {\sum _{y_{i}=k_{m}(x_{i})}w_{i}^{(m)}}{\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}}}\right)}  We calculate the weighted error rate of the weak classifier to be                               ϵ                      m                          =                  ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                    /                          ∑                      i            =            1                                N                                    w                      i                                (            m            )                                {\displaystyle \epsilon _{m}=\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}/\sum _{i=1}^{N}w_{i}^{(m)}}  , so it follows that:                              α                      m                          =                              1            2                          ln        ⁡                  (                                                    1                −                                  ϵ                                      m                                                                              ϵ                                  m                                                              )                      {\displaystyle \alpha _{m}={\frac {1}{2}}\ln \left({\frac {1-\epsilon _{m}}{\epsilon _{m}}}\right)}  which is the negative logit function multiplied by 0.5.Thus we have derived the AdaBoost algorithm: At each iteration, choose the classifier                               k                      m                                {\displaystyle k_{m}}  , which minimizes the total weighted error                               ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                {\displaystyle \sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}}  , use this to calculate the error rate                               ϵ                      m                          =                  ∑                                    y                              i                                      ≠                          k                              m                                      (                          x                              i                                      )                                    w                      i                                (            m            )                                    /                          ∑                      i            =            1                                N                                    w                      i                                (            m            )                                {\displaystyle \epsilon _{m}=\sum _{y_{i}\neq k_{m}(x_{i})}w_{i}^{(m)}/\sum _{i=1}^{N}w_{i}^{(m)}}  , use this to calculate the weight                               α                      m                          =                              1            2                          ln        ⁡                  (                                                    1                −                                  ϵ                                      m                                                                              ϵ                                  m                                                              )                      {\displaystyle \alpha _{m}={\frac {1}{2}}\ln \left({\frac {1-\epsilon _{m}}{\epsilon _{m}}}\right)}  , and finally use this to improve the boosted classifier                               C                      m            −            1                                {\displaystyle C_{m-1}}   to                               C                      m                          =                  C                      (            m            −            1            )                          +                  α                      m                                    k                      m                                {\displaystyle C_{m}=C_{(m-1)}+\alpha _{m}k_{m}}  .== Statistical understanding of boosting ==Boosting is a form of linear regression in which the features of each sample                               x                      i                                {\displaystyle x_{i}}   are the outputs of some weak learner                     h              {\displaystyle h}   applied to                               x                      i                                {\displaystyle x_{i}}  .While regression tries to fit                     F        (        x        )              {\displaystyle F(x)}   to                     y        (        x        )              {\displaystyle y(x)}   as precisely as possible without loss of generalization, typically using least square error                     E        (        f        )        =        (        y        (        x        )        −        f        (        x        )                  )                      2                                {\displaystyle E(f)=(y(x)-f(x))^{2}}  , the AdaBoost error function                     E        (        f        )        =                  e                      −            y            (            x            )            f            (            x            )                                {\displaystyle E(f)=e^{-y(x)f(x)}}   takes into account the fact that only the sign of the final result is used, thus                               |                F        (        x        )                  |                      {\displaystyle |F(x)|}   can be far larger than 1 without increasing error. However, the exponential increase in the error for sample                               x                      i                                {\displaystyle x_{i}}   as                     −        y        (                  x                      i                          )        f        (                  x                      i                          )              {\displaystyle -y(x_{i})f(x_{i})}   increases results in excessive weight being assigned to outliers.One feature of the choice of exponential error function is that the error of the final additive model is the product of the error of each stage, that is,                               e                                    ∑                              i                                      −                          y                              i                                      f            (                          x                              i                                      )                          =                  ∏                      i                                    e                      −                          y                              i                                      f            (                          x                              i                                      )                                {\displaystyle e^{\sum _{i}-y_{i}f(x_{i})}=\prod _{i}e^{-y_{i}f(x_{i})}}  . Thus it can be seen that the weight update in the AdaBoost algorithm is equivalent to recalculating the error on                               F                      t                          (        x        )              {\displaystyle F_{t}(x)}   after each stage.There is a lot of flexibility allowed in the choice of loss function. As long as the loss function is monotonic and continuously differentiable, the classifier is always driven toward purer solutions. Zhang (2004) provides a loss function based on least squares, a modified Huber loss function:                    ϕ        (        y        ,        f        (        x        )        )        =                              {                                                            −                  4                  y                  f                  (                  x                  )                                                                                            if                                                         y                  f                  (                  x                  )                  <                  −                  1                  ,                                                                              (                  y                  f                  (                  x                  )                  −                  1                                      )                                          2                                                                                                                                  if                                                         −                  1                  ≤                  y                  f                  (                  x                  )                  ≤                  1                  ,                                                                              0                                                                                            if                                                         y                  f                  (                  x                  )                  >                  1                                                                                      {\displaystyle \phi (y,f(x))={\begin{cases}-4yf(x)&{\mbox{if }}yf(x)<-1,\\(yf(x)-1)^{2}&{\mbox{if }}-1\leq yf(x)\leq 1,\\0&{\mbox{if }}yf(x)>1\end{cases}}}  This function is more well-behaved than LogitBoost for                     f        (        x        )              {\displaystyle f(x)}   close to 1 or -1, does not penalise ‘overconfident’ predictions (                    y        f        (        x        )        >        1              {\displaystyle yf(x)>1}  ), unlike unmodified least squares, and only penalises samples misclassified with confidence greater than 1 linearly, as opposed to quadratically or exponentially, and is thus less susceptible to the effects of outliers.== Boosting as gradient descent ==Boosting can be seen as minimization of a convex loss function over a convex set of functions. Specifically, the loss being minimized by AdaBoost is the exponential loss                               ∑                      i                          ϕ        (        i        ,        y        ,        f        )        =                  ∑                      i                                    e                      −                          y                              i                                      f            (                          x                              i                                      )                                {\displaystyle \sum _{i}\phi (i,y,f)=\sum _{i}e^{-y_{i}f(x_{i})}}  , whereas LogitBoost performs logistic regression, minimizing                               ∑                      i                          ϕ        (        i        ,        y        ,        f        )        =                  ∑                      i                          ln        ⁡                  (                      1            +                          e                              −                                  y                                      i                                                  f                (                                  x                                      i                                                  )                                              )                      {\displaystyle \sum _{i}\phi (i,y,f)=\sum _{i}\ln \left(1+e^{-y_{i}f(x_{i})}\right)}  .In the gradient descent analogy, the output of the classifier for each training point is considered a point                               (                                    F                              t                                      (                          x                              1                                      )            ,            …            ,                          F                              t                                      (                          x                              n                                      )                    )                      {\displaystyle \left(F_{t}(x_{1}),\dots ,F_{t}(x_{n})\right)}   in n-dimensional space, where each axis corresponds to a training sample, each weak learner                     h        (        x        )              {\displaystyle h(x)}   corresponds to a vector of fixed orientation and length, and the goal is to reach the target point                     (                  y                      1                          ,        …        ,                  y                      n                          )              {\displaystyle (y_{1},\dots ,y_{n})}   (or any region where the value of loss function                               E                      T                          (                  x                      1                          ,        …        ,                  x                      n                          )              {\displaystyle E_{T}(x_{1},\dots ,x_{n})}   is less than the value at that point), in the least number of steps. Thus AdaBoost algorithms perform either Cauchy (find                     h        (        x        )              {\displaystyle h(x)}   with the steepest gradient, choose                     α              {\displaystyle \alpha }   to minimize test error) or Newton (choose some target point, find                     α        h        (        x        )              {\displaystyle \alpha h(x)}   that brings                               F                      t                                {\displaystyle F_{t}}   closest to that point) optimization of training error.== Example algorithm (Discrete AdaBoost) ==With:Samples                               x                      1                          …                  x                      n                                {\displaystyle x_{1}\dots x_{n}}  Desired outputs                               y                      1                          …                  y                      n                          ,        y        ∈        {        −        1        ,        1        }              {\displaystyle y_{1}\dots y_{n},y\in \{-1,1\}}  Initial weights                               w                      1            ,            1                          …                  w                      n            ,            1                                {\displaystyle w_{1,1}\dots w_{n,1}}   set to                                           1            n                                {\displaystyle {\frac {1}{n}}}  Error function                     E        (        f        (        x        )        ,        y        ,        i        )        =                  e                      −                          y                              i                                      f            (                          x                              i                                      )                                {\displaystyle E(f(x),y,i)=e^{-y_{i}f(x_{i})}}  Weak learners                     h        :        x        →        {        −        1        ,        1        }              {\displaystyle h\colon x\rightarrow \{-1,1\}}  For                     t              {\displaystyle t}   in                     1        …        T              {\displaystyle 1\dots T}  :Choose                               h                      t                          (        x        )              {\displaystyle h_{t}(x)}  :Find weak learner                               h                      t                          (        x        )              {\displaystyle h_{t}(x)}   that minimizes                               ϵ                      t                                {\displaystyle \epsilon _{t}}  , the weighted sum error for misclassified points                               ϵ                      t                          =                  ∑                                                                                          h                                          t                                                        (                                      x                                          i                                                        )                  ≠                                      y                                          i                                                                                        i                  =                  1                                                                          n                                    w                      i            ,            t                                {\displaystyle \epsilon _{t}=\sum _{\stackrel {i=1}{h_{t}(x_{i})\neq y_{i}}}^{n}w_{i,t}}  Choose                               α                      t                          =                              1            2                          ln        ⁡                  (                                                    1                −                                  ϵ                                      t                                                                              ϵ                                  t                                                              )                      {\displaystyle \alpha _{t}={\frac {1}{2}}\ln \left({\frac {1-\epsilon _{t}}{\epsilon _{t}}}\right)}  Add to ensemble:                              F                      t                          (        x        )        =                  F                      t            −            1                          (        x        )        +                  α                      t                                    h                      t                          (        x        )              {\displaystyle F_{t}(x)=F_{t-1}(x)+\alpha _{t}h_{t}(x)}  Update weights:                              w                      i            ,            t            +            1                          =                  w                      i            ,            t                                    e                      −                          y                              i                                                    α                              t                                                    h                              t                                      (                          x                              i                                      )                                {\displaystyle w_{i,t+1}=w_{i,t}e^{-y_{i}\alpha _{t}h_{t}(x_{i})}}   for all iRenormalize                               w                      i            ,            t            +            1                                {\displaystyle w_{i,t+1}}   such that                               ∑                      i                                    w                      i            ,            t            +            1                          =        1              {\displaystyle \sum _{i}w_{i,t+1}=1}  (Note: It can be shown that                                                                         ∑                                                      h                                          t                      +                      1                                                        (                                      x                                          i                                                        )                  =                                      y                                          i                                                                                                  w                                  i                  ,                  t                  +                  1                                                                                    ∑                                                      h                                          t                      +                      1                                                        (                                      x                                          i                                                        )                  ≠                                      y                                          i                                                                                                  w                                  i                  ,                  t                  +                  1                                                                    =                                                            ∑                                                      h                                          t                                                        (                                      x                                          i                                                        )                  =                                      y                                          i                                                                                                  w                                  i                  ,                  t                                                                                    ∑                                                      h                                          t                                                        (                                      x                                          i                                                        )                  ≠                                      y                                          i                                                                                                  w                                  i                  ,                  t                                                                          {\displaystyle {\frac {\sum _{h_{t+1}(x_{i})=y_{i}}w_{i,t+1}}{\sum _{h_{t+1}(x_{i})\neq y_{i}}w_{i,t+1}}}={\frac {\sum _{h_{t}(x_{i})=y_{i}}w_{i,t}}{\sum _{h_{t}(x_{i})\neq y_{i}}w_{i,t}}}}   at every step, which can simplify the calculation of the new weights.)=== Choosing αt ===                              α                      t                                {\displaystyle \alpha _{t}}   is chosen as it can be analytically shown to be the minimizer of the exponential error function for Discrete AdaBoost.Minimize:                              ∑                      i                                    w                      i                                    e                      −                          y                              i                                                    h                              i                                                    α                              t                                                          {\displaystyle \sum _{i}w_{i}e^{-y_{i}h_{i}\alpha _{t}}}  Using the convexity of the exponential function, and assuming that                     ∀        i        ,                  h                      i                          ∈        [        −        1        ,        1        ]              {\displaystyle \forall i,h_{i}\in [-1,1]}   we have:                                                                                          ∑                                      i                                                                    w                                      i                                                                    e                                      −                                          y                                              i                                                                                    h                                              i                                                                                    α                                              t                                                                                                                                        ≤                                  ∑                                      i                                                                    (                                                                                    1                        −                                                  y                                                      i                                                                                                    h                                                      i                                                                                              2                                                        )                                                  w                                      i                                                                    e                                                            α                                              t                                                                                            +                                  ∑                                      i                                                                    (                                                                                    1                        +                                                  y                                                      i                                                                                                    h                                                      i                                                                                              2                                                        )                                                  w                                      i                                                                    e                                      −                                          α                                              t                                                                                                                                                                              =                                  (                                                                                    ϵ                                                  t                                                                    2                                                        )                                                  e                                                            α                                              t                                                                                            +                                  (                                                                                    1                        −                                                  ϵ                                                      t                                                                                              2                                                        )                                                  e                                      −                                          α                                              t                                                                                                                                      {\displaystyle {\begin{aligned}\sum _{i}w_{i}e^{-y_{i}h_{i}\alpha _{t}}&\leq \sum _{i}\left({\frac {1-y_{i}h_{i}}{2}}\right)w_{i}e^{\alpha _{t}}+\sum _{i}\left({\frac {1+y_{i}h_{i}}{2}}\right)w_{i}e^{-\alpha _{t}}\\&=\left({\frac {\epsilon _{t}}{2}}\right)e^{\alpha _{t}}+\left({\frac {1-\epsilon _{t}}{2}}\right)e^{-\alpha _{t}}\end{aligned}}}  We then differentiate that expression with respect to                               α                      t                                {\displaystyle \alpha _{t}}   and set it to zero to find the minimum of the upper bound:                                                                                          (                                                                                    ϵ                                                  t                                                                    2                                                        )                                                  e                                                            α                                              t                                                                                            −                                  (                                                                                    1                        −                                                  ϵ                                                      t                                                                                              2                                                        )                                                  e                                      −                                          α                                              t                                                                                                                                        =                0                                                                                      α                                      t                                                                                              =                                                      1                    2                                                  ln                ⁡                                  (                                                                                    1                        −                                                  ϵ                                                      t                                                                                                                      ϵ                                                  t                                                                                                      )                                                                          {\displaystyle {\begin{aligned}\left({\frac {\epsilon _{t}}{2}}\right)e^{\alpha _{t}}-\left({\frac {1-\epsilon _{t}}{2}}\right)e^{-\alpha _{t}}&=0\\\alpha _{t}&={\frac {1}{2}}\ln \left({\frac {1-\epsilon _{t}}{\epsilon _{t}}}\right)\end{aligned}}}  Note that this only applies when                               h                      i                          ∈        {        −        1        ,        1        }              {\displaystyle h_{i}\in \{-1,1\}}  , though it can be a good starting guess in other cases, such as when the weak learner is biased (                    h        (        x        )        ∈        {        a        ,        b        }        ,        a        ≠        −        b              {\displaystyle h(x)\in \{a,b\},a\neq -b}  ), has multiple leaves (                    h        (        x        )        ∈        {        a        ,        b        ,        …        ,        n        }              {\displaystyle h(x)\in \{a,b,\dots ,n\}}  ) or is some other function                     h        (        x        )        ∈                  R                      {\displaystyle h(x)\in \mathbb {R} }  . In such cases the choice of weak learner and coefficient can be condensed to a single step in which                               f                      t                          =                  α                      t                                    h                      t                          (        x        )              {\displaystyle f_{t}=\alpha _{t}h_{t}(x)}   is chosen from all possible                     α        ,        h              {\displaystyle \alpha ,h}   as the minimizer of                               ∑                      i                                    w                      i            ,            t                                    e                      −                          y                              i                                                    f                              t                                      (                          x                              i                                      )                                {\displaystyle \sum _{i}w_{i,t}e^{-y_{i}f_{t}(x_{i})}}   by some numerical searching routine.== Variants ===== Real AdaBoost ===The output of decision trees is a class probability estimate                     p        (        x        )        =        P        (        y        =        1                  |                x        )              {\displaystyle p(x)=P(y=1|x)}  , the probability that                     x              {\displaystyle x}   is in the positive class. Friedman, Hastie and Tibshirani derive an analytical minimizer for                               e                      −            y                          (                                                F                                      t                    −                    1                                                  (                x                )                +                                  f                                      t                                                  (                p                (                x                )                )                            )                                            {\displaystyle e^{-y\left(F_{t-1}(x)+f_{t}(p(x))\right)}}   for some fixed                     p        (        x        )              {\displaystyle p(x)}   (typically chosen using weighted least squares error):                              f                      t                          (        x        )        =                              1            2                          ln        ⁡                  (                                    x                              1                −                x                                              )                      {\displaystyle f_{t}(x)={\frac {1}{2}}\ln \left({\frac {x}{1-x}}\right)}  .Thus, rather than multiplying the output of the entire tree by some fixed value, each leaf node is changed to output half the logit transform of its previous value.=== LogitBoost ===LogitBoost represents an application of established logistic regression techniques to the AdaBoost method. Rather than minimizing error with respect to y, weak learners are chosen to minimize the (weighted least-squares) error of                               f                      t                          (        x        )              {\displaystyle f_{t}(x)}   with respect to                              z                      t                          =                                                            y                                  ∗                                            −                              p                                  t                                            (              x              )                                      2                              p                                  t                                            (              x              )              (              1              −                              p                                  t                                            (              x              )              )                                      ,              {\displaystyle z_{t}={\frac {y^{*}-p_{t}(x)}{2p_{t}(x)(1-p_{t}(x))}},}  where                              p                      t                          (        x        )        =                                            e                                                F                                      t                    −                    1                                                  (                x                )                                                                    e                                                      F                                          t                      −                      1                                                        (                  x                  )                                            +                              e                                  −                                      F                                          t                      −                      1                                                        (                  x                  )                                                                    ,              {\displaystyle p_{t}(x)={\frac {e^{F_{t-1}(x)}}{e^{F_{t-1}(x)}+e^{-F_{t-1}(x)}}},}                                w                      t                          =                  p                      t                          (        x        )        (        1        −                  p                      t                          (        x        )        )              {\displaystyle w_{t}=p_{t}(x)(1-p_{t}(x))}                                y                      ∗                          =                                            y              +              1                        2                          .              {\displaystyle y^{*}={\frac {y+1}{2}}.}  That is                               z                      t                                {\displaystyle z_{t}}   is the Newton–Raphson approximation of the minimizer of the log-likelihood error at stage                     t              {\displaystyle t}  , and the weak learner                               f                      t                                {\displaystyle f_{t}}   is chosen as the learner that best approximates                               z                      t                                {\displaystyle z_{t}}   by weighted least squares.As p approaches either 1 or 0, the value of                               p                      t                          (                  x                      i                          )        (        1        −                  p                      t                          (                  x                      i                          )        )              {\displaystyle p_{t}(x_{i})(1-p_{t}(x_{i}))}   becomes very small and the z term, which is large for misclassified samples, can become numerically unstable, due to machine precision rounding errors. This can be overcome by enforcing some limit on the absolute value of z and the minimum value of w=== Gentle AdaBoost ===While previous boosting algorithms choose                               f                      t                                {\displaystyle f_{t}}   greedily, minimizing the overall test error as much as possible at each step, GentleBoost features a bounded step size.                               f                      t                                {\displaystyle f_{t}}   is chosen to minimize                               ∑                      i                                    w                      t            ,            i                          (                  y                      i                          −                  f                      t                          (                  x                      i                          )                  )                      2                                {\displaystyle \sum _{i}w_{t,i}(y_{i}-f_{t}(x_{i}))^{2}}  , and no further coefficient is applied. Thus, in the case where a weak learner exhibits perfect classification performance, GentleBoost chooses                               f                      t                          (        x        )        =                  α                      t                                    h                      t                          (        x        )              {\displaystyle f_{t}(x)=\alpha _{t}h_{t}(x)}   exactly equal to                     y              {\displaystyle y}  , while steepest descent algorithms try to set                               α                      t                          =        ∞              {\displaystyle \alpha _{t}=\infty }  . Empirical observations about the good performance of GentleBoost appear to back up Schapire and Singer's remark that allowing excessively large values of                     α              {\displaystyle \alpha }   can lead to poor generalization performance.=== Early Termination ===A technique for speeding up processing of boosted classifiers, early termination refers to only testing each potential object with as many layers of the final classifier necessary to meet some confidence threshold, speeding up computation for cases where the class of the object can easily be determined. One such scheme is the object detection framework introduced by Viola and Jones: in an application with significantly more negative samples than positive, a cascade of separate boost classifiers is trained, the output of each stage biased such that some acceptably small fraction of positive samples is mislabeled as negative, and all samples marked as negative after each stage are discarded. If 50% of negative samples are filtered out by each stage, only a very small number of objects would pass through the entire classifier, reducing computation effort. This method has since been generalized, with a formula provided for choosing optimal thresholds at each stage to achieve some desired false positive and false negative rate.In the field of statistics, where AdaBoost is more commonly applied to problems of moderate dimensionality, early stopping is used as a strategy to reduce overfitting. A validation set of samples is separated from the training set, performance of the classifier on the samples used for training is compared to performance on the validation samples, and training is terminated if performance on the validation sample is seen to decrease even as performance on the training set continues to improve.=== Totally corrective algorithms ===For steepest descent versions of AdaBoost, where                               α                      t                                {\displaystyle \alpha _{t}}   is chosen at each layer t to minimize test error, the next layer added is said to be maximally independent of layer t: it is unlikely to choose a weak learner t+1 that is similar to learner t. However, there remains the possibility that t+1 produces similar information to some other earlier layer. Totally corrective algorithms, such as LPBoost, optimize the value of every coefficient after each step, such that new layers added are always maximally independent of every previous layer. This can be accomplished by backfitting, linear programming or some other method.=== Pruning ===Pruning is the process of removing poorly performing weak classifiers to improve memory and execution-time cost of the boosted classifier. The simplest methods, which can be particularly effective in conjunction with totally corrective training, are weight- or margin-trimming: when the coefficient, or the contribution to the total test error, of some weak classifier falls below a certain threshold, that classifier is dropped. Margineantu & Dietterich suggest an alternative criterion for trimming: weak classifiers should be selected such that the diversity of the ensemble is maximized. If two weak learners produce very similar outputs, efficiency can be improved by removing one of them and increasing the coefficient of the remaining weak learner.== See also ==Bootstrap aggregatingCoBoostingBrownBoostGradient boostingMultiplicative weight update method § AdaBoost algorithm== References ==
	Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of antiobjects, objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.Collaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.== References ==
	Stefan Savage (born 1969) is an American computer science researcher, currently a Professor in the Systems and Networking Group at the University of California, San Diego. There, he holds the Irwin and Joan Jacobs Chair in Information and Computer Science.  Savage is widely cited in computer security, particularly in the areas of email spam, network worms and malware propagation, distributed denial of service (DDOS) mitigation and traceback, automotive hacking and wireless security. He received his undergraduate degree at Carnegie Mellon and his Ph.D. from the University of Washington.== Career ==In 1999, Savage's research team published TCP Congestion Control with a Misbehaving Receiver, which uncovered protocol flaws in the TCP protocol that carries most Internet traffic. By exploiting these flaws, Savage proposed means for attackers to evade congestion control, allowing attackers to monopolize crowded network connections that would otherwise be shared by multiple users. This was the first paper to address congestion control evasion as a vulnerability, rather than as a theoretical design implication. That same year, Savage published "Sting", a paper and software tool that presented a mechanism to abuse quirks in the TCP protocol to allow a single party to infer bidirectional packet loss, a valuable contribution to traffic measurement.In 2000, Savage's team published Practical Network Support for IP Traceback, which proposed a simple stochastic extension to internet routers that would enable them to trace floods of traffic back to their origin. IP traceback is a major open networking research question, with significant implications towards DDOS mitigation: if IP traffic can be traced, Internet Service Providers can track down and halt DDOS floods. Savage later co-founded Asta Networks, which offered a product that addressed these problems.In 2001, Savage, with colleagues at UCSD and CAIDA, published Inferring Internet Denial-of-Service Activity, which introduced the idea of the network telescope and provided major empirical results regarding DDOS attacks. Follow-on work has provided insight into the spread of network worms, including Code Red II and SQL Slammer.In 2003, John Bellardo and Savage published 802.11 Denial-of-Service Attacks: Real Vulnerabilities and Practical Solutions, which introduced practical attacks on 802.11 wireless protocol flaws that would allow attackers to force legitimate clients off wireless networks. The paper is also a notable example of applied reverse engineering in an academic setting; Bellardo and Savage reverse engineered the Intersil wireless chipset, finding an undocumented diagnostic mode that allowed them to directly inject malicious wireless packets onto a network.In 2004, Savage and George Varghese led a research team that published Automated Worm Fingerprinting, which introduced a novel hashing technique that allowed network operators to monitor network traffic and uncover data patterns that were "propagating", spreading across the network at an unusual rate. Propagating traffic is a strong indicator for network worm outbreaks, a key unsolved problem in network security. Varghese later co-founded Netsift to capitalize on this research; Cisco purchased Netsift in 2005.In 2005, Ishwar Ramani and Stefan Savage developed Syncscan algorithm that cuts the time needed to switch between Wi-Fi access points.In 2010 he was named a Fellow of the Association for Computing Machinery.In 2013, Savage received the ACM SIGOPS Mark Weiser Award.In 2015, he received the ACM Prize in Computing for "innovative research in network security, privacy, and reliability that has taught us to view attacks and attackers as elements of an integrated technological, societal, and economic system."In 2017, he was named a MacArthur Foundation Fellow (the "genius grant") for his body of work.== External links ==Stefan Savage's home page at UCSD== References ==
	Within computer science and operations research,many combinatorial optimization problems are computationally intractable to solve exactly (to optimality).Many such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.Randomized rounding(Raghavan & Tompson 1987)is a widely used approach for designing and analyzing such approximation algorithms.  The basic idea is to use the probabilistic methodto convert an optimal solution of a relaxationof the problem into an approximately optimal solution to the original problem.== Overview ==The basic approach has three steps:Formulate the problem to be solved as an integer linear program (ILP).Compute an optimal fractional solution                     x              {\displaystyle x}   to the linear programming relaxation (LP) of the ILP.Round the fractional solution                     x              {\displaystyle x}   of the LP to an integer solution                               x          ′                      {\displaystyle x'}   of the ILP.(Although the approach is most commonly applied with linear programs,other kinds of relaxations are sometimes used.For example, see Goeman's and Williamson's semi-definite programming-basedMax-Cut approximation algorithm.)The challenge in the first step is to choose a suitable integer linear program.Familiarity with linear programming is required, in particular, familiarity withhow to model problems using linear programs and integer linear programs.But, for many problems, there is a natural integer linear program that works well,such as in the Set Cover example below.  (The integer linear program should have a smallintegrality gap;indeed randomized rounding is often used to prove bounds on integrality gaps.)In the second step, the optimal fractional solution can typically  be computedin polynomial timeusing any standard linear programming algorithm.In the third step, the fractional solution must be converted into an integer solution(and thus a solution to the original problem).This is called rounding the fractional solution.The resulting integer solution should (provably) have costnot much larger than the cost of the fractional solution.This will ensure that the cost of the integer solutionis not much larger than the cost of the optimal integer solution.The main technique used to do the third step (rounding) is to use randomization,and then to use probabilistic arguments to bound the increase in cost due to the rounding(following the probabilistic method from combinatorics).There, probabilistic arguments are used to show the existence of discrete structures withdesired properties.  In this context, one uses such arguments to show the following:Given any fractional solution                     x              {\displaystyle x}   of the LP, with positive probability the randomized rounding process produces an integer solution                               x          ′                      {\displaystyle x'}   that approximates                     x              {\displaystyle x}   according to some desired criterion.Finally, to make the third step computationally efficient,one either shows that                               x          ′                      {\displaystyle x'}   approximates                     x              {\displaystyle x}  with high probability (so that the step can remain randomized)or one derandomizes the rounding step,typically using the method of conditional probabilities.The latter method converts the randomized rounding processinto an efficient deterministic process that is guaranteedto reach a good outcome.== Comparison to other applications of the probabilistic method ==The randomized rounding step differs from most applications of the probabilistic method in two respects:The computational complexity of the rounding step is important.  It should be implementable by a fast (e.g. polynomial time) algorithm.The probability distribution underlying the random experiment is a function of the solution                     x              {\displaystyle x}   of a relaxation of the problem instance.  This fact is crucial to proving the performance guarantee of the approximation algorithm --- that is, that for any problem instance, the algorithm returns a solution that approximates the optimal solution for that specific instance.  In comparison, applications of the probabilistic method in combinatorics typically show the existence of structures whose features depend on other parameters of the input.  For example, consider Turán's theorem, which can be stated as "any graph with                     n              {\displaystyle n}   vertices of average degree                     d              {\displaystyle d}   must have an independent set of size at least                     n                  /                (        d        +        1        )              {\displaystyle n/(d+1)}  .   (See  this for a probabilistic proof of Turán's theorem.) While there are graphs for which this bound is tight, there are also graphs which have independent sets much larger than                     n                  /                (        d        +        1        )              {\displaystyle n/(d+1)}  .  Thus, the size of the independent set shown to exist by Turán's theorem in a graph may, in general, be much smaller than the maximum independent set for that graph.== Set cover example ==The following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.Fix any instance                     ⟨        c        ,                              S                          ⟩              {\displaystyle \langle c,{\mathcal {S}}\rangle }   of set cover over a universe                                           U                                {\displaystyle {\mathcal {U}}}  .For step 1, let IP be the standard integer linear program for set cover for this instance.For step 2, let LP be the linear programming relaxation of IP,and compute an optimal solution                               x                      ∗                                {\displaystyle x^{*}}   to LPusing any standard linear programming algorithm.(This takes time polynomial in the input size.)(The feasible solutions to LP are the vectors                     x              {\displaystyle x}  that assign each set                     s        ∈                              S                                {\displaystyle s\in {\mathcal {S}}}  a non-negative weight                               x                      s                                {\displaystyle x_{s}}  ,such that, for each element                     e        ∈                              U                                {\displaystyle e\in {\mathcal {U}}}  ,                              x          ′                      {\displaystyle x'}   covers                     e              {\displaystyle e}  -- the total weight assigned to the sets containing                     e              {\displaystyle e}  is at least 1, that is,                              ∑                      s            ∋            e                                    x                      s                          ≥        1.              {\displaystyle \sum _{s\ni e}x_{s}\geq 1.}  The optimal solution                               x                      ∗                                {\displaystyle x^{*}}  is a feasible solution whose cost                              ∑                      s            ∈                                          S                                                    c        (        S        )                  x                      s                                ∗                                {\displaystyle \sum _{s\in {\mathcal {S}}}c(S)x_{s}^{*}}  is as small as possible.)Note that any set cover                                           C                                {\displaystyle {\mathcal {C}}}   for                                           S                                {\displaystyle {\mathcal {S}}}  gives a feasible solution                     x              {\displaystyle x}  (where                               x                      s                          =        1              {\displaystyle x_{s}=1}   for                     s        ∈                              C                                {\displaystyle s\in {\mathcal {C}}}  ,                              x                      s                          =        0              {\displaystyle x_{s}=0}   otherwise).The cost of this                                           C                                {\displaystyle {\mathcal {C}}}   equals the cost of                     x              {\displaystyle x}  , that is,                              ∑                      s            ∈                                          C                                                    c        (        s        )        =                  ∑                      s            ∈                                          S                                                    c        (        s        )                  x                      s                          .              {\displaystyle \sum _{s\in {\mathcal {C}}}c(s)=\sum _{s\in {\mathcal {S}}}c(s)x_{s}.}  In other words, the linear program LP is a relaxationof the given set-cover problem.Since                               x                      ∗                                {\displaystyle x^{*}}   has minimum cost among feasible solutions to the LP,the cost of                               x                      ∗                                {\displaystyle x^{*}}   is a lower bound on the cost of the optimal set cover.=== Step 3: The randomized rounding step ===Here is a description of the third step—the rounding step,which must convert the minimum-cost fractional set cover                               x                      ∗                                {\displaystyle x^{*}}  into a feasible integer solution                               x          ′                      {\displaystyle x'}   (corresponding to a true set cover).The rounding step should produce an                               x          ′                      {\displaystyle x'}   that, with positive probability,has cost  within a small factor of the cost of                               x                      ∗                                {\displaystyle x^{*}}  .Then (since the cost of                               x                      ∗                                {\displaystyle x^{*}}   is a lower bound on the cost of the optimal set cover),the cost of                               x          ′                      {\displaystyle x'}   will be within a small factor of the optimal cost.As a starting point, consider the most natural rounding scheme:For each set                     s        ∈                              S                                {\displaystyle s\in {\mathcal {S}}}   in turn, take                               x                      s                    ′                =        1              {\displaystyle x'_{s}=1}   with probability                     min        (        1        ,                  x                      s                                ∗                          )              {\displaystyle \min(1,x_{s}^{*})}  , otherwise take                               x                      s                    ′                =        0              {\displaystyle x'_{s}=0}  .With this rounding scheme,the expected cost of the chosen sets is at most                               ∑                      s                          c        (        s        )                  x                      s                                ∗                                {\displaystyle \sum _{s}c(s)x_{s}^{*}}  ,the cost of the fractional cover.This is good.  Unfortunately the coverage is not good.When the variables                               x                      s                                ∗                                {\displaystyle x_{s}^{*}}   are small,the probability that an element                     e              {\displaystyle e}   is not covered is about                              ∏                      s            ∋            e                          1        −                  x                      s                                ∗                          ≈                  ∏                      s            ∋            e                          exp        ⁡        (        −                  x                      s                                ∗                          )        =        exp        ⁡                              (                          −                  ∑                      s            ∋            e                                    x                      s                                ∗                                                )                          ≈        exp        ⁡        (        −        1        )        .              {\displaystyle \prod _{s\ni e}1-x_{s}^{*}\approx \prod _{s\ni e}\exp(-x_{s}^{*})=\exp {\Big (}-\sum _{s\ni e}x_{s}^{*}{\Big )}\approx \exp(-1).}  So only a constant fraction of the elements will be covered in expectation.To make                               x          ′                      {\displaystyle x'}   cover every element with high probability,the standard rounding schemefirst scales up the rounding probabilitiesby an appropriate factor                     λ        >        1              {\displaystyle \lambda >1}  .Here is the standard rounding scheme:Fix a parameter                     λ        ≥        1              {\displaystyle \lambda \geq 1}  .  For each set                     s        ∈                              S                                {\displaystyle s\in {\mathcal {S}}}   in turn,take                               x                      s                    ′                =        1              {\displaystyle x'_{s}=1}   with probability                     min        (        λ                  x                      s                                ∗                          ,        1        )              {\displaystyle \min(\lambda x_{s}^{*},1)}  , otherwise take                               x                      s                    ′                =        0              {\displaystyle x'_{s}=0}  .Scaling the probabilities up by                     λ              {\displaystyle \lambda }  increases the expected cost by                     λ              {\displaystyle \lambda }  ,but makes coverage of all elements likely.The idea is to choose                     λ              {\displaystyle \lambda }   as smallas possible so that all elements are provablycovered with non-zero probability.Here is a detailed analysis.==== Lemma (approximation guarantee for rounding scheme) ====Fix                     λ        =        ln        ⁡        (        2                  |                                      U                                    |                )              {\displaystyle \lambda =\ln(2|{\mathcal {U}}|)}  .  With positive probability, the rounding scheme returns a set cover                               x          ′                      {\displaystyle x'}   of cost at most                     2        ln        ⁡        (        2                  |                                      U                                    |                )        c        ⋅                  x                      ∗                                {\displaystyle 2\ln(2|{\mathcal {U}}|)c\cdot x^{*}}   (and thus of cost                     O        (        log        ⁡                  |                                      U                                    |                )              {\displaystyle O(\log |{\mathcal {U}}|)}   times the cost of the optimal set cover).(Note: with care the                      O        (        log        ⁡                  |                                      U                                    |                )              {\displaystyle O(\log |{\mathcal {U}}|)}  can be reduced to                     ln        ⁡        (                  |                                      U                                    |                )        +        O        (        log        ⁡        log        ⁡                  |                                      U                                    |                )              {\displaystyle \ln(|{\mathcal {U}}|)+O(\log \log |{\mathcal {U}}|)}  .)==== Proof ====The output                               x          ′                      {\displaystyle x'}   of the random rounding scheme has the desired propertiesas long as none of the following "bad" events occur:the cost                     c        ⋅                  x          ′                      {\displaystyle c\cdot x'}   of                               x          ′                      {\displaystyle x'}   exceeds                     2        λ        c        ⋅                  x                      ∗                                {\displaystyle 2\lambda c\cdot x^{*}}  , orfor some element                     e              {\displaystyle e}  ,                               x          ′                      {\displaystyle x'}   fails to cover                     e              {\displaystyle e}  .The expectation of each                                x                      s                    ′                      {\displaystyle x'_{s}}   is at most                     λ                  x                      s                                ∗                                {\displaystyle \lambda x_{s}^{*}}  .By linearity of expectation,the expectation of                      c        ⋅                  x          ′                      {\displaystyle c\cdot x'}  is at most                               ∑                      s                          c        (        s        )        λ                  x                      s                                ∗                          =        λ        c        ⋅                  x                      ∗                                {\displaystyle \sum _{s}c(s)\lambda x_{s}^{*}=\lambda c\cdot x^{*}}  .Thus, by Markov's inequality, the probability of the first bad eventabove is at most                     1                  /                2              {\displaystyle 1/2}  .For the remaining bad events (one for each element                     e              {\displaystyle e}  ), note that,since                               ∑                      s            ∋            e                                    x                      s                                ∗                          ≥        1              {\displaystyle \sum _{s\ni e}x_{s}^{*}\geq 1}   for any given element                     e              {\displaystyle e}  ,the probability that                     e              {\displaystyle e}   is not covered is                                                                                          ∏                                      s                    ∋                    e                                                                                        (                                                  1                −                min                (                λ                                  x                                      s                                                        ∗                                                  ,                1                )                                                      )                                                                                              <                                  ∏                                      s                    ∋                    e                                                  exp                ⁡                (                                  −                                λ                                  x                                      s                                                        ∗                                                  )                =                exp                ⁡                                                      (                                                                    −                                λ                                  ∑                                      s                    ∋                    e                                                                    x                                      s                                                        ∗                                                                                        )                                                                                                                                    ≤                exp                ⁡                (                                  −                                λ                )                =                1                                  /                                (                2                                  |                                                                      U                                                                    |                                )                .                                                          {\displaystyle {\begin{aligned}\prod _{s\ni e}{\big (}1-\min(\lambda x_{s}^{*},1){\big )}&<\prod _{s\ni e}\exp({-}\lambda x_{s}^{*})=\exp {\Big (}{-}\lambda \sum _{s\ni e}x_{s}^{*}{\Big )}\\&\leq \exp({-}\lambda )=1/(2|{\mathcal {U}}|).\end{aligned}}}  (This uses the inequality                     1        +        z        ≤                  e                      z                                {\displaystyle 1+z\leq e^{z}}  ,which is strict for                     z        ≠        0              {\displaystyle z\neq 0}  .)Thus, for each of the                               |                                      U                                    |                      {\displaystyle |{\mathcal {U}}|}   elements,the probability that the element is not covered is less than                     1                  /                (        2                              U                          )              {\displaystyle 1/(2{\mathcal {U}})}  .By the naive union bound,the probability that one of the                     1        +                  |                                      U                                    |                      {\displaystyle 1+|{\mathcal {U}}|}   bad events happensis less than                     1                  /                2        +                  |                                      U                                    |                          /                (        2                              U                          )        =        1              {\displaystyle 1/2+|{\mathcal {U}}|/(2{\mathcal {U}})=1}  .Thus, with positive probability there are no bad eventsand                               x          ′                      {\displaystyle x'}   is a set cover of cost at most                     2        λ        c        ⋅                  x                      ∗                                {\displaystyle 2\lambda c\cdot x^{*}}  .QED=== Derandomization using the method of conditional probabilities ===The lemma above shows the existence of a set coverof cost                     O        (        log        ⁡        (                  |                                      U                                    |                )        c        ⋅                  x                      ∗                                {\displaystyle O(\log(|{\mathcal {U}}|)c\cdot x^{*}}  ).In this context our goal is an efficient approximation algorithm,not just an existence proof, so we are not done.One approach would be to increase                     λ              {\displaystyle \lambda }  a little bit, then show that the probability of success is at least, say, 1/4.With this modification, repeating the random rounding step a few timesis enough to ensure a successful outcome with high probability.That approach weakens the approximation ratio.We next describe a different approach that yieldsa deterministic algorithm that is guaranteed tomatch the approximation ratio of the existence proof above.The approach is called the method of conditional probabilities.The deterministic algorithm emulates the randomized rounding scheme:it considers each set                     s        ∈                              S                                {\displaystyle s\in {\mathcal {S}}}   in turn,and chooses                               x                      s                    ′                ∈        {        0        ,        1        }              {\displaystyle x'_{s}\in \{0,1\}}  .But instead of making each choice randomly based on                               x                      ∗                                {\displaystyle x^{*}}  ,it makes the choice deterministically, so as tokeep the conditional probability of failure, given the choices so far, below 1.==== Bounding the conditional probability of failure ====We want to be able to set each variable                               x                      s                    ′                      {\displaystyle x'_{s}}   in turnso as to keep the conditional probability of failure below 1.To do this, we need a good bound on the conditional probability of failure.The bound will come by refining the original existence proof.That proof implicitly bounds the probability of failureby the expectation of the random variable                    F        =                                            c              ⋅                              x                ′                                                    2              λ              c              ⋅                              x                                  ∗                                                                    +                  |                                                    U                                            (            m            )                                    |                      {\displaystyle F={\frac {c\cdot x'}{2\lambda c\cdot x^{*}}}+|{\mathcal {U}}^{(m)}|}  ,where                                                        U                                            (            m            )                          =                              {                          e        :                  ∏                      s            ∋            e                          (        1        −                  x                      s                    ′                )        =        1                              }                                {\displaystyle {\mathcal {U}}^{(m)}={\Big \{}e:\prod _{s\ni e}(1-x'_{s})=1{\Big \}}}  is the set of elements left uncovered at the end.The random variable                     F              {\displaystyle F}   may appear a bit mysterious,but it mirrors the probabilistic proof in a systematic way.The first term in                     F              {\displaystyle F}   comes from applying Markov's inequalityto bound the probability of the first bad event (the cost is too high).It contributes at least 1 to                     F              {\displaystyle F}   if the cost of                               x          ′                      {\displaystyle x'}   is too high.The second termcounts the number of bad events of the second kind (uncovered elements).It contributes at least 1 to                     F              {\displaystyle F}   if                               x          ′                      {\displaystyle x'}   leaves any element uncovered.Thus, in any outcome where                     F              {\displaystyle F}   is less than 1,                              x          ′                      {\displaystyle x'}   must cover all the elementsand have cost meeting the desired bound from the lemma.In short, if the rounding step fails, then                     F        ≥        1              {\displaystyle F\geq 1}  .This implies (by Markov's inequality) that                    E        [        F        ]              {\displaystyle E[F]}   is an upper bound on the probability of failure.Note that the argument above is implicit already in the proof of the lemma,which also shows by calculation that                     E        [        F        ]        <        1              {\displaystyle E[F]<1}  .To apply the method of conditional probabilities,we need to extend the argument to bound the conditional probability of failureas the rounding step proceeds.Usually, this can be done in a systematic way,although it can be technically tedious.So, what about the conditional probability of failure as the rounding step iterates through the sets?Since                     F        ≥        1              {\displaystyle F\geq 1}   in any outcome where the rounding step fails,by Markov's inequality, the conditional probability of failureis at most the conditional expectation of                     F              {\displaystyle F}  .Next we calculate the conditional expectation of                     F              {\displaystyle F}  ,much as we calculated the unconditioned expectation of                     F              {\displaystyle F}   in the original proof.Consider the state of the rounding process at the end of some iteration                     t              {\displaystyle t}  .Let                               S                      (            t            )                                {\displaystyle S^{(t)}}   denote the sets considered so far(the first                     t              {\displaystyle t}   sets in                                           S                                {\displaystyle {\mathcal {S}}}  ).Let                               x                      (            t            )                                {\displaystyle x^{(t)}}   denote the (partially assigned) vector                               x          ′                      {\displaystyle x'}  (so                               x                      s                                (            t            )                                {\displaystyle x_{s}^{(t)}}   is determined only if                     s        ∈                  S                      (            t            )                                {\displaystyle s\in S^{(t)}}  ).For each set                     s        ∉                  S                      (            t            )                                {\displaystyle s\not \in S^{(t)}}  ,let                               p                      s                          =        min        (        λ                  x                      s                                ∗                          ,        1        )              {\displaystyle p_{s}=\min(\lambda x_{s}^{*},1)}  denote the probability with which                               x                      s                    ′                      {\displaystyle x'_{s}}   will be set to 1.Let                                                         U                                            (            t            )                                {\displaystyle {\mathcal {U}}^{(t)}}   contain the not-yet-covered elements.Then the conditional expectation of                     F              {\displaystyle F}  ,given the choices made so far, that is, given                               x                      (            t            )                                {\displaystyle x^{(t)}}  , is                    E        [        F                  |                          x                      (            t            )                          ]                 =                                                                     ∑                                  s                  ∈                                      S                                          (                      t                      )                                                                                  c              (              s              )                              x                                  s                                ′                            +                              ∑                                  s                  ∉                                      S                                          (                      t                      )                                                                                  c              (              s              )                              p                                  s                                                                    2              λ              c              ⋅                              x                                  ∗                                                                             +                           ∑                      e            ∈                                                            U                                                            (                t                )                                                              ∏                      s            ∉                          S                              (                t                )                                      ,            s            ∋            e                          (        1        −                  p                      s                          )        .              {\displaystyle E[F|x^{(t)}]~=~{\frac {\sum _{s\in S^{(t)}}c(s)x'_{s}+\sum _{s\not \in S^{(t)}}c(s)p_{s}}{2\lambda c\cdot x^{*}}}~+~\sum _{e\in {\mathcal {U}}^{(t)}}\prod _{s\not \in S^{(t)},s\ni e}(1-p_{s}).}  Note that                     E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}   is determined only after iteration                     t              {\displaystyle t}  .==== Keeping the conditional probability of failure below 1 ====To keep the conditional probability of failure below 1,it suffices to keep the conditional expectation of                     F              {\displaystyle F}   below 1.To do this, it suffices to keep the conditional expectation of                     F              {\displaystyle F}   from increasing.This is what the algorithm will do.It will set                               x                      s                    ′                      {\displaystyle x'_{s}}   in each iteration to ensure that                    E        [        F                  |                          x                      (            m            )                          ]        ≤        E        [        F                  |                          x                      (            m            −            1            )                          ]        ≤        ⋯        ≤        E        [        F                  |                          x                      (            1            )                          ]        ≤        E        [        F                  |                          x                      (            0            )                          ]        <        1              {\displaystyle E[F|x^{(m)}]\leq E[F|x^{(m-1)}]\leq \cdots \leq E[F|x^{(1)}]\leq E[F|x^{(0)}]<1}  (where                     m        =                  |                                      S                                    |                      {\displaystyle m=|{\mathcal {S}}|}  ).In the                     t              {\displaystyle t}  th iteration,how can the algorithm set                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  to ensure that                     E        [        F                  |                          x                      (            t            )                          ]        ≤        E        [        F                  |                          S                      (            t            −            1            )                          ]              {\displaystyle E[F|x^{(t)}]\leq E[F|S^{(t-1)}]}  ?It turns out that it can simply set                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  so as to minimize the resulting value of                     E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}  .To see why, focus on the point in time when iteration                     t              {\displaystyle t}   starts.At that time,                     E        [        F                  |                          x                      (            t            −            1            )                          ]              {\displaystyle E[F|x^{(t-1)}]}   is determined,but                     E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}   is not yet determined--- it can take two possible values depending on how                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  is set in iteration                     t              {\displaystyle t}  .Let                               E                      (            t            −            1            )                                {\displaystyle E^{(t-1)}}   denote the value of                     E        [        F                  |                          x                      ′                          (              t              −              1              )                                      ]              {\displaystyle E[F|x'^{(t-1)}]}  .Let                               E                      0                                (            t            )                                {\displaystyle E_{0}^{(t)}}   and                               E                      1                                (            t            )                                {\displaystyle E_{1}^{(t)}}  ,denote the two possible values of                      E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}  ,depending on whether                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}   is set to 0, or 1, respectively.By the definition of conditional expectation,                              E                      (            t            −            1            )                                   =                 Pr        [                  x                                    s              ′                                ′                =        0        ]                  E                      0                                (            t            )                          +        Pr        [                  x                                    s              ′                                ′                =        1        ]                  E                      1                                (            t            )                          .              {\displaystyle E^{(t-1)}~=~\Pr[x'_{s'}=0]E_{0}^{(t)}+\Pr[x'_{s'}=1]E_{1}^{(t)}.}  Since a weighted average of two quantitiesis always at least the minimum of those two quantities,it follows that                              E                      (            t            −            1            )                                   ≥                 min        (                  E                      0                                (            t            )                          ,                  E                      1                                (            t            )                          )        .              {\displaystyle E^{(t-1)}~\geq ~\min(E_{0}^{(t)},E_{1}^{(t)}).}  Thus, setting                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  so as to minimize the resulting value of                    E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}  will guarantee that                    E        [        F                  |                          x                      (            t            )                          ]        ≤        E        [        F                  |                          x                      (            t            −            1            )                          ]              {\displaystyle E[F|x^{(t)}]\leq E[F|x^{(t-1)}]}  .This is what the algorithm will do.In detail, what does this mean?Considered as a function of                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  (with all other quantities fixed)                    E        [        F                  |                          x                      (            t            )                          ]              {\displaystyle E[F|x^{(t)}]}  is a linear function of                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}  ,and the coefficient of                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}   in that function is                                                        c                                                s                  ′                                                                    2              λ              c              ⋅                              x                                  ∗                                                                             −                           ∑                      e            ∈                          s              ′                        ∩                                                            U                                                            t                −                1                                                              ∏                      s            ∉                          S                              (                t                )                                      ,            s            ∋            e                          (        1        −                  p                      s                          )        .              {\displaystyle {\frac {c_{s'}}{2\lambda c\cdot x^{*}}}~-~\sum _{e\in s'\cap {\mathcal {U}}_{t-1}}\prod _{s\not \in S^{(t)},s\ni e}(1-p_{s}).}  Thus, the algorithm should set                               x                                    s              ′                                ′                      {\displaystyle x'_{s'}}   to 0 if this expression is positive,and 1 otherwise.  This gives the following algorithm.=== Randomized-rounding algorithm for set cover ===input: set system                                           S                                {\displaystyle {\mathcal {S}}}  , universe                                           U                                {\displaystyle {\mathcal {U}}}  , cost vector                     c              {\displaystyle c}  output:  set cover                               x          ′                      {\displaystyle x'}   (a solution to the standard integer linear program for set cover)Compute a min-cost fractional set cover                               x                      ∗                                {\displaystyle x^{*}}   (an optimal solution to the LP relaxation).Let                     λ        ←        ln        ⁡        (        2                  |                                      U                                    |                )              {\displaystyle \lambda \leftarrow \ln(2|{\mathcal {U}}|)}  .  Let                               p                      s                          ←        min        (        λ                  x                      s                                ∗                          ,        1        )              {\displaystyle p_{s}\leftarrow \min(\lambda x_{s}^{*},1)}   for each                     s        ∈                              S                                {\displaystyle s\in {\mathcal {S}}}  .For each                               s          ′                ∈                              S                                {\displaystyle s'\in {\mathcal {S}}}   do:Let                                           S                          ←                              S                          −        {                  s          ′                }              {\displaystyle {\mathcal {S}}\leftarrow {\mathcal {S}}-\{s'\}}  .    (                                          S                                {\displaystyle {\mathcal {S}}}   contains the not-yet-decided sets.)If                                                            c                                                s                  ′                                                                    2              λ              c              ⋅                              x                                  ∗                                                                    >                  ∑                      e            ∈                          s              ′                        ∩                                          U                                                              ∏                      s            ∈                                          S                                      ,            s            ∋            e                          (        1        −                  p                      s                          )              {\displaystyle {\frac {c_{s'}}{2\lambda c\cdot x^{*}}}>\sum _{e\in s'\cap {\mathcal {U}}}\prod _{s\in {\mathcal {S}},s\ni e}(1-p_{s})}  then set                               x                      s                    ′                ←        0              {\displaystyle x'_{s}\leftarrow 0}  ,else set                               x                      s                    ′                ←        1              {\displaystyle x'_{s}\leftarrow 1}   and                                           U                          ←                              U                          −                  s          ′                      {\displaystyle {\mathcal {U}}\leftarrow {\mathcal {U}}-s'}  .  (                                          U                                {\displaystyle {\mathcal {U}}}   contains the not-yet-covered elements.)Return                               x          ′                      {\displaystyle x'}  .==== lemma (approximation guarantee for algorithm) ====The algorithm above returns a set cover                               x          ′                      {\displaystyle x'}   of cost at most                     2        ln        ⁡        (        2                  |                                      U                                    |                )              {\displaystyle 2\ln(2|{\mathcal {U}}|)}   times the minimum cost of any (fractional) set cover.==== proof ====The algorithm ensures that the conditional expectation of                     F              {\displaystyle F}  ,                    E        [        F                          |                                  x                      (            t            )                          ]              {\displaystyle E[F\,|\,x^{(t)}]}  , does not increase at each iteration.Since this conditional expectation is initially less than 1 (as shown previously),the algorithm ensures that the conditional expectation stays below 1.Since the conditional probability of failureis at most the conditional expectation of                     F              {\displaystyle F}  ,in this way the algorithmensures that the conditional probability of failure stays below 1.Thus, at the end, when all choices are determined,the algorithm reaches a successful outcome.That is, the algorithm above returns a set cover                               x          ′                      {\displaystyle x'}  of cost at most                     2        ln        ⁡        (        2                  |                                      U                                    |                )              {\displaystyle 2\ln(2|{\mathcal {U}}|)}   timesthe minimum cost of any (fractional) set cover.=== Remarks ===In the example above, the algorithm was guided by the conditional expectation of a random variable                     F              {\displaystyle F}  .In some cases, instead of an exact conditional expectation,an upper bound (or sometimes a lower bound)on some conditional expectation is used instead.This is called a pessimistic estimator.== See also ==Method of conditional probabilities== References ==Raghavan, Prabhakar; Tompson, Clark D. (1987), "Randomized rounding: A technique for provably good algorithms and algorithmic proofs", Combinatorica, 7 (4): 365–374, doi:10.1007/BF02579324.Raghavan, Prabhakar (1988), "Probabilistic construction of deterministic algorithms: approximating packing integer programs", Journal of Computer and System Sciences, 37 (2): 130–143, doi:10.1016/0022-0000(88)90003-7.== Further reading ==Althöfer, Ingo (1994), "On sparse approximations to randomized strategies and convex combinations", Linear Algebra and its Applications, 199: 339–355, doi:10.1016/0024-3795(94)90357-3, MR 1274423Hofmeister, Thomas; Lefmann, Hanno (1996), "Computing sparse approximations deterministically", Linear Algebra and its Applications, 240: 9–19, doi:10.1016/0024-3795(94)00175-8, MR 1387283Lipton, Richard J.; Young, Neal E. (1994), "Simple strategies for large zero-sum games with applications to complexity theory", STOC '94: Proceedings of the twenty-sixth annual ACM symposium on theory of computing, New York, NY: ACM, pp. 734–740, arXiv:cs.cc/0205035, doi:10.1145/195058.195447, ISBN 978-0-89791-663-9
	Label Propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications). These labels are propagated to the unlabeled points throughout the course of the algorithm.Within complex networks, real networks tend to have community structure. Label propagation is an algorithm  for finding communities. In comparison with other algorithms label propagation has advantages in its running time and amount of a priori information needed about the network structure (no parameter is required to be known beforehand). The disadvantage is that it produces no unique solution, but an aggregate of many solutions.== Functioning of the Algorithm ==At initial condition, the nodes carry a label that denotes the community they belong to. Membership in a community changes based on the labels that the neighboring nodes possess. This change is subject to the maximum number of labels within one degree of the nodes. Every node is initialized with a unique label, then the labels diffuse through the network. Consequently, densely connected groups reach a common label quickly. When many such dense (consensus) groups are created throughout the network, they continue to expand outwards until it is impossible to do so.The process has 5 steps:1. Initialize the labels at all nodes in the network. For a given node x, Cx (0) = x.2. Set t = 1.3. Arrange the nodes in the network in a random order and set it to X.4. For each x ∈ X chosen in that specific order, let Cx(t) = f(Cxi1(t), ...,Cxim(t),Cxi(m+1) (t − 1), ...,Cxik (t − 1)). f here returns the label occurring with the highest frequency among neighbours. Select a label at random if there are multiple highest frequency labels.5. If every node has a label that the maximum number of their neighbours have, then stop the algorithm. Else, set t = t + 1 and go to (3).== Multiple Community Structure ==In contrast with other algorithms label propagation can result in various community structures from the same initial condition. The range of solutions can be narrowed if some nodes are given preliminary labels while others are held unlabelled. Consequently, unlabelled nodes will be more likely to adapt to the labelled ones. For a more accurate finding of communities, Jaccard’s index is used to aggregate multiple community structures, containing all important information.== References ==== External links ==Python implementation of label propagation algorithm.
	Run-to-completion scheduling or nonpreemptive scheduling is a scheduling model in which each task runs until it either finishes, or explicitly yields control back to the scheduler. Run to completion systems typically have an event queue which is serviced either in strict order of admission by an event loop, or by an admission scheduler which is capable of scheduling events out of order, based on other constraints such as deadlines.Some preemptive multitasking scheduling systems behave as run-to-completion schedulers in regard to scheduling tasks at one particular process priority level, at the same time as those processes still preempt other lower priority tasks and are themselves preempted by higher priority tasks.== See also ==Deadline schedulingPreemptive multitaskingCooperative multitasking== References ==
	Shuffling is a procedure used to randomize a deck of playing cards to provide an element of chance in card games. Shuffling is often followed by a cut, to help ensure that the shuffler has not manipulated the outcome.== Shuffling techniques ===== Overhand shuffle ===One of the easiest shuffles to accomplish after a little practice is the overhand shuffle. Johan Jonasson wrote, "The overhand shuffle... is the shuffling technique where you gradually transfer the deck from, say, your right hand to your left hand by sliding off small packets from the top of the deck." In detail as normally performed, with the pack initially held in the left hand (say), most of the cards are grasped as a group from the bottom of the pack between the thumb and fingers of the right hand and lifted clear of the small group that remains in the left hand. Small packets are then released from the right hand a packet at a time so that they drop on the top of the pack accumulating in the left hand.  The process is repeated several times. The randomness of the whole shuffle is increased by the number of small packets in each shuffle and the number of repeat shuffles performed.The overhand shuffle offers sufficient opportunity for sleight of hand techniques to be used to affect the ordering of cards, creating a stacked deck. The most common way that players cheat with the overhand shuffle is by having a card at the top or bottom of the pack that they require, and then slipping it to the bottom at the start of a shuffle (if it was on top to start), or leaving it as the last card in a shuffle and just dropping it on top (if it was originally on the bottom of the deck).=== Riffle ===A common shuffling technique is called the riffle, or dovetail shuffle or leafing the cards, in which half of the deck is held in each hand with the thumbs inward, then cards are released by the thumbs so that they fall to the table interleaved. Many also lift the cards up after a riffle, forming what is called a bridge which puts the cards back into place; it can also be done by placing the halves flat on the table with their rear corners touching, then lifting the back edges with the thumbs while pushing the halves together. While this method is more difficult, it is often used in casinos because it minimizes the risk of exposing cards during the shuffle. There are two types of perfect riffle shuffles: if the top card moves to be second from the top then it is an in shuffle, otherwise it is known as an out shuffle (which preserves both the top and bottom cards).The Gilbert–Shannon–Reeds model provides a mathematical model of the random outcomes of riffling, that has been shown experimentally to be a good fit to human shuffling and that forms the basis for a recommendation that card decks be riffled seven times in order to randomize them thoroughly. Later, mathematicians Lloyd M. Trefethen and Lloyd N. Trefethen authored a paper using a tweaked version of the Gilbert-Shannon-Reeds model showing that the minimum number of riffles for total randomization could also be five, if the method of defining randomness is changed.=== Pile 'shuffle' ===Cards are simply dealt out into a number of piles, then the piles are stacked on top of each other. Though this is deterministic and does not randomize the cards at all, it ensures that cards that were next to each other are now separated. Some variations on the pile shuffle attempt to make it slightly random by dealing to the piles in a random order each circuit., Irish, scramble, beginner shuffle, smooshing, schwirsheling, or washing the cards, this involves simply spreading the 's hands. Then the cards are moved into one pile so that they begin to intertwine and are then arranged back into a stack. This method is useful for beginners and small children or if one is inept at shuffling cards. However, the beginner shuffle requires a large surface for spreading out the cards and takes=== Mongean shuffle ===The Mongean shuffle, or Monge's shuffle, is performed as follows (by a right-handed person): Start with the unshuffled deck in the left hand and transfer the top card to the right. Then repeatedly take the top card from the left hand and transfer it to the right, putting the second card at the top of the new deck, the third at the bottom, the fourth at the top, the fifth at the bottom, etc. The result, if one started with cards numbered consecutively                               1          ,          2          ,          3          ,          4          ,          5          ,          6          ,          …          ,          2          n                      {\displaystyle \scriptstyle 1,2,3,4,5,6,\dots ,2n}  , would be a deck with the cards in the following order:                               2          n          ,          2          n          −          2          ,          2          n          −          4          ,          …          ,          4          ,          2          ,          1          ,          3          ,          …          ,          2          n          −          3          ,          2          n          −          1                      {\displaystyle \scriptstyle 2n,2n-2,2n-4,\dots ,4,2,1,3,\dots ,2n-3,2n-1}  .For a deck of given size, the number of Mongean shuffles that it takes to return a deck to starting position, is known (sequence A019567 in the OEIS). Twelve perfect Mongean shuffles restore a 52-card deck.=== Weave and Faro shuffles ===is the procedure of pushing the ends of two halves of a deck against each other in such a way that they naturally intertwine.  Sometimes the deck is split into equal halves of 26 cards which are then pushed together in a certain way so as to make them perfectly interweave. This is known as a Faro Shuffle.The faro shuffle is performed by cutting the deck into two, preferably equal, packs in both hands as follows (right-handed):The cards are held from above in the right and from below in the left hand. Separation of the deck is done simply lifting up half the cards with the right hand thumb slightly and pushing the left hand's packet forward away from the right hand. The two packets are often crossed and slammed into each other as to align them. They are then pushed together by the short sides and bent (either up or down). The cards then alternately fall into each other, much like a zipper. A flourish can be added by springing the packets together by applying pressure and bending them from above, as called the bridge finish. The faro is a controlled shuffle which does not randomize a deck when performed properly.A perfect faro shuffle, where the cards are perfectly alternated, is considered one of the most difficult sleights by card magicians, simply because it requires the shuffler to be able to cut the deck into two equal packets and apply just the right amount of pressure when pushing the cards into each other. Performing eight perfect faro shuffles in a row restores the order of the deck to the original order only if there are 52 cards in the deck and if the original top and bottom cards remain in their positions (1st and 52nd) during the eight shuffles. If the top and bottom cards are weaved in during each shuffle, it takes 52 shuffles to return the deck back into original order (or 26 shuffles to reverse the order).=== Mexican spiral shuffle ===The Mexican spiral shuffle is performed by cyclic actions of moving the top card onto the table, then the new top card under the deck, the next onto the table, next under the deck, and so on until the last card is dealt onto the table. It takes quite a long time, compared with riffle or overhand shuffles, but allows other players to fully control cards which are on the table. The Mexican spiral shuffle was popular at the end of the 19th century in some areas of Mexico as a protection from gamblers and con men arriving from the United States.== False shuffles ==Magicians, sleight-of-hand artists, and card cheats employ various methods of shuffling whereby the deck appears to have been shuffled fairly, when in reality one or more cards (up to and including the entire deck) stays in the same position.  It is also possible, though generally considered very difficult, to "stack the deck" (place cards into a desirable order) by means of one or more riffle shuffles; this is called "riffle stacking".Both performance magicians and card sharps regard the Zarrow shuffle as a particularly effective example of the false shuffle.  In this shuffle, the entire deck remains in its original order, although spectators think they see an honest riffle shuffle.== Shuffling machines ==Casinos often equip their tables with shuffling machines instead of having croupiers shuffle the cards, as it gives the casino a few advantages, including an increased complexity to the shuffle and therefore an increased difficulty for players to make predictions, even if they are collaborating with croupiers. The shuffling machines are carefully designed to avoid biasing the shuffle and are typically computer-controlled. Shuffling machines also save time that would otherwise be wasted on manual shuffling, thereby increasing the profitability of the table. These machines are also used to lessen repetitive-motion-stress injuries to a dealer.Players with superstitions often regard with suspicion any electronic equipment, so casinos sometimes still have the croupiers perform the shuffling at tables that typically attract those crowds (Baccarat tables).== Randomization ==There are exactly 52 factorial (expressed in shorthand as 52!) possible orderings of the cards in a 52-card deck. In other words, there are 52 × 51 × 50 × 49 × ··· × 4 × 3 × 2 × 1 possible combinations of card sequence. This is approximately 8×1067 possible orderings or specifically 80,658,175,170,943,878,571,660,636,856,403,766,975,289,505,440,883,277,824,000,000,000,000. The magnitude of this number means that it is exceedingly improbable that two randomly selected, truly randomized decks will be the same. However, while the exact sequence of all cards in a randomized deck is unpredictable, it may be possible to make some probabilistic predictions about a deck that is not sufficiently randomized.=== Sufficient number of shuffles ===The number of shuffles which are sufficient for a "good" level of randomness depends on the type of shuffle and the measure of "good enough randomness", which in turn depends on the game in question. For most games, four to seven riffle shuffles are sufficient: for unsuited games such as blackjack, four riffle shuffles are sufficient, while for suited games, seven riffle shuffles are necessary. There are some games, however, for which even seven riffle shuffles are insufficient.In practice the number of shuffles required depends both on the quality of the shuffle and how significant non-randomness is, particularly how good the people playing are at noticing and using non-randomness. Two to four shuffles is good enough for casual play. But in club play, good bridge players take advantage of non-randomness after four shuffles, and top blackjack players supposedly track aces through the deck; this is known as "ace tracking", or more generally, as "shuffle tracking".=== Research ===Following early research at Bell Labs, which was abandoned in 1955, the question of how many shuffles was required remained open until 1990, when it was convincingly solved as seven shuffles, as elaborated below. Some results preceded this, and refinements have continued since.A leading figure in the mathematics of shuffling is mathematician and magician Persi Diaconis, who began studying the question around 1970, and has authored many papers in the 1980s, 1990s, and 2000s on the subject with numerous co-authors. Most famous is (Bayer & Diaconis 1992), co-authored with mathematician Dave Bayer, which analyzed the Gilbert–Shannon–Reeds model of random riffle shuffling and concluded that the deck did not start to become random until five good riffle shuffles, and was truly random after seven, in the precise sense of variation distance described in Markov chain mixing time; of course, you would need more shuffles if your shuffling technique is poor. Recently, the work of Trefethen et al. has questioned some of Diaconis' results, concluding that six shuffles are enough. The difference hinges on how each measured the randomness of the deck. Diaconis used a very sensitive test of randomness, and therefore needed to shuffle more. Even more sensitive measures exist, and the question of what measure is best for specific card games is still open. Diaconis released a response indicating that you only need four shuffles for un-suited games such as blackjack.On the other hand, variation distance may be too forgiving a measure and seven riffle shuffles may be many too few. For example, seven shuffles of a new deck leaves an 81% probability of winning New Age Solitaire where the probability is 50% with a uniform random deck. One sensitive test for randomness uses a standard deck without the jokers divided into suits with two suits in ascending order from ace to king, and the other two suits in reverse. (Many decks already come ordered this way when new.) After shuffling, the measure of randomness is the number of rising sequences that are left in each suit.== Shuffling algorithms ==If a computer has access to purely random numbers, it is capable of generating a "perfect shuffle", a random permutation of the cards; beware that this terminology (an algorithm that perfectly randomizes the deck) differs from "a perfectly executed single shuffle", notably a perfectly interleaving faro shuffle. The Fisher–Yates shuffle, popularized by Donald Knuth, is simple (a few lines of code) and efficient (O(n) on an n-card deck, assuming constant time for fundamental steps) algorithm for doing this. Shuffling can be seen as the opposite of sorting.There are other, less-desirable algorithms in common use. For example, one can assign a random number to each card, and then sort the cards in order of their random numbers. This will generate a random permutation, unless any of the random numbers generated are the same as any others (i.e. pairs, triplets etc.). This can be eliminated either by adjusting one of the pair's values randomly up or down by a small amount, or reduced to an arbitrarily low probability by choosing a sufficiently wide range of random number choices. If using efficient sorting such as mergesort or heapsort this is an O(n log n) average and worst-case algorithm.=== In online gambling ===These issues are of considerable commercial importance in online gambling, where the randomness of the shuffling of packs of simulated cards for online card games is crucial. For this reason, many online gambling sites provide descriptions of their shuffling algorithms and the sources of randomness used to drive these algorithms, with some gambling sites also providing auditors' reports of the performance of their systems.== See also ==Card manipulationMental pokerSolitaire (cipher)== References ===== Footnotes ===== External links ==Physical card shuffling:Illustrated guide to several shuffling methodsMagicians tool with lots of shuffling simulationMathematics of shuffling:Real World Shuffling In PracticeShuffle - MathWorld - Wolfram ResearchIvars Peterson's MathTrek: Card Shuffling ShenanigansReal world (historical) application:How We Learned to Cheat at Online Poker: A Study in Software Security
	Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.The algorithm is conceptually distinct from other methods for predicting microRNA:mRNA heteroduplexes in that it does not use experimentally validated heteroduplexes for training, instead relying only on the sequences ofknown mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.Rna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does not require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.Use of rna22 led to the discovery of "non-canonical" microRNA targets in the coding regions of the mouse Nanog, Oct4 and Sox2. Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.A recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that bona fide miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.== References ==
	Bürgi's Kunstweg is a set of algorithms invented by Jost Bürgi at the end of the 16th century. They can be used for the calculation of sines to an arbitrary precision. Bürgi used these algorithms to calculate a Canon Sinuum, a table of sines in steps of 2 arc seconds. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for navigation at sea. Johannes Kepler called the Canon Sinuum the most precise known table of sines (reference?). Bürgi explained his algorithms in his work Fundamentum Astronomiae which he presented to Emperor Rudolf II. in 1592.The principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same column. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate approximations of sines are obtained after few iterations.As recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines. According to Folkerts, this was the first step towards difference calculus.== References ==
	Invasion percolation is a mathematical model of realistic fluid distributions for slow immiscible fluid invasion in porous media,  in percolation theory.It was introduced by Wilkinson and Willemsen (1983).== References ==
	Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). These kind of problems are denoted as online problems and are seen as opposed to the classical optimization problems where complete information is assumed (offline). The research on online optimization can be distinguished into online problems where multiple decisions are made sequentially based on a piece-by-piece input and those where a decision is made only once. A famous online problem where a decision is made only once is the Ski rental problem. In general, the output of an online algorithm is compared to the solution of a corresponding offline algorithm which is necessarily always optimal and knows the entire input in advance (competitive analysis).In many situations, present decisions (for example, resources allocation) must be made with incomplete knowledge of the future or distributional assumptions on the future are not reliable. In such cases, online optimization  can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes.== Online problems ==A problem exemplifying the concepts of online algorithms is the Canadian traveller problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed (failed) is only revealed to the traveller when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual shortest path problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.There are many formal problems that offer more than one online algorithm as solution:K-server problemJob shop scheduling problemList update problemBandit problemSecretary problemSearch gamesSki rental problemLinear search problemPortfolio selection problem== References ==
	In theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be efficient if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.The proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.Implementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).== Examples ==Many examples of problems with checkable algorithms come from graph theory.For instance, a classical algorithm for testing whether a graph is bipartite would simply output a Boolean value: true if the graph is bipartite, false otherwise. In contrast, a certifying algorithm might output a 2-coloring of the graph in the case that it is bipartite, or a cycle of odd length if it is not. Any graph is bipartite if and only if it can be 2-colored, and non-bipartite if and only if it contains an odd cycle. Both checking whether a 2-coloring is valid and checking whether a given odd-length sequence of vertices is a cycle may be performed more simply than testing bipartiteness.Analogously, it is possible to test whether a given directed graph is acyclic by a certifying algorithm that outputs either a topological order or a directed cycle. It is possible to test whether an undirected graph is a chordal graph by a certifying algorithm that outputs either an elimination ordering (an ordering of all vertices such that, for every vertex, the neighbors that are later in the ordering form a clique) or a chordless cycle. And it is possible to test whether a graph is planar by a certifying algorithm that outputs either a planar embedding or a Kuratowski subgraph.The extended Euclidean algorithm for the greatest common divisor of two integers x and y is certifying: it outputs three integers g (the divisor), a, and b, such that ax + by = g. This equation can only be true of multiples of the greatest common divisor, so testing that g is the greatest common divisor may be performed by checking that g divides both x and y and that this equation is correct.== See also ==Sanity check, a simple test of the correctness of an output or intermediate result that is not required to be a complete proof of correctness== References ==
	Distributed Tree Search (DTS) algorithm is a class of algorithms for searching values in an efficient and distributed manner. Their purpose is to iterate through a tree by working along multiple branches in parallel and merging the results of each branch into one common solution, in order to minimize time spent searching for a value in a tree-like data structure.The original paper was written in 1988 by Chris Ferguson and Richard E. Korf, from the University of California's Computer Science Department. They used multiple other chess AIs to develop this wider range algorithm.== Overview ==The Distributed Tree Search Algorithm (also known as Korf-Ferguson algorithm) was created to solve the following problem: "Given a tree with non-uniform branching factor and depth, search it in parallel with an arbitrary number of processors as fast as possible."The top-level part of this algorithm is general and does not use a particular existing type of tree-search, but it can be easily specialized to fit any type of non-distributed tree-search.DTS consists of using multiple processes, each with a node and a set of processors attached, with the goal of searching the sub-tree below the said node. Each process then divides itself into multiple coordinated sub-processes which recursively divide themselves again until an optimal way to search the tree has been found based on the number of processors available to each process. Once a process finishes, DTS dynamically reassigns the processors to other processes as to keep the efficiency to a maximum through good load-balancing, especially in irregular trees.Once a process finishes searching, it recursively sends and merges a resulting signal to its parent-process, until all the different sub-answers have been merged and the entire problem has been solved.== Applications ==DTS is only applicable under two major conditions: the data structure to search through is a tree, and the algorithm can make use of at least one computation unit (Although it cannot be considered as distributed if there is only one).One major example of the everyday use of DTS is network routing. The Internet can be seen as a tree of IP addresses, and an analogy to a routing protocol could be how post offices work in the real world. Since there are over 4.3 billion IP addresses currently, society heavily depends on the time the data takes to find its way to its destination. As such, IP-routing divides the work into multiple sub-units which each have different scales of calculation capabilities and use each other's result to find the route in a very efficient manner. This is an instance of DTS that affects over 43% of the world's population, for reasons going from entertainment to national security.== Alternatives ==Although DTS is currently one of the most widely used algorithms, many of its applications have alternatives to them which could potentially develop into more efficient, less resource-demanding solutions, were they more researched.One of the more controversial examples is Big-Data processing. In applications like Google Search Engine, Facebook, YouTube, search needs to be optimized to keep waiting time inside a reasonable window. This could be achieved through the plain use of DTS, but other algorithms are used in place (for example data-hashing in SQL databases), or in conjunction (Facebook's Haystack algorithm groups parallel tree-search, data-hashing and memory-ordering/sorting).One of the more important limits of DTS is the fact that it requires a tree as input. Trees are a sub-instance of a data structure known as Graphs, which means every Graph can be converted into a tree. Although there currently exists no better way to search through trees than Korf-Ferguson's algorithm, each task has different particularities and in most cases, there will exist more efficient data structures to represent the problem and solve it than through tree-search. And so there exist instances of tree structures with cycles that cannot possibly be faster than a graph-search on the same structure with the same processing power.== Controversy ==There are few controversies around Korf-Ferguson's DTS algorithm, since it is recognized as very complete, but simple. It is very often used as a stepping stone for students to discover the fundamentals and key concepts of distributed problem-solving.The most important challenge to this algorithmic concept was an article by Kröll B, « Balanced Distributed Search Trees Do Not Exist », which does not attack the veracity or current efficiency of the algorithm, but rather the fact that DTS itself, no matter how many improvements are made to it (for example balancing the input tree before-hand), will never be able to reach optimal resolution-time. This opens a new view point: are too many resources used into the completion of DTS, which blocks new algorithms with higher efficiency-potential from getting researched and developed? Another limit of DTS is the fact that no matter how efficient the division, coordination and merging of the solutions is, it will always be limited by the material number or processors and their processing power. Until recently, this was admitted as being a limit to nearly every computation, but new-generation algorithms like Euclideon might one day be able to crush DFS's efficiency through processing-power-independent problem resolution.== See also ==Tree (data structure)Search treeBinary search treeTree traversalMonte Carlo tree searchParallel computingColbrook A., Brewer E., Dellarocas C., Weihl W., "Algorithms for Search Trees on Message-Passing Architectures" (1996)Colbrook A., Smythe C., Efficient implementations of search trees on parallel distributed memory architectures" (1990)Bayer R., McCreight E., Organization and Maintenance of Large Ordered Indices. Acta Informatica 1 (1972)Comer D., The Ubiquitous B-Tree (1979)== References ==
	Portability in high-level computer programming is the usability of the same software in different environments. The prerequirement for portability is the generalized abstraction between the application logic and system interfaces. When software with the same functionality is produced for several computing platforms, portability is the key issue for development cost reduction.== Strategies for portability ==Software portability may involve:Transferring installed program files to another computer of basically the same architecture.Reinstalling a program from distribution files on another computer of basically the same architecture.Building executable programs for different platforms from source code; this is what is usually understood by "porting".=== Similar systems ===When operating systems of the same family are installed on two computers with processors with similar instruction sets it is often possible to transfer the files implementing program files between them.In the simplest case the file or files may simply be copied from one machine to the other. However, in many cases, the software is installed on a computer in a way which depends upon its detailed hardware, software, and setup, with device drivers for particular devices, using installed operating system and supporting software components, and using different drives or directories.In some cases, software, usually described as "portable software", is specifically designed to run on different computers with compatible operating systems and processors, without any machine-dependent installation. Porting is no more than transferring specified directories and their contents. Software installed on portable mass storage devices such as USB sticks can be used on any compatible computer on simply plugging the storage device in, and stores all configuration information on the removable device. Hardware- and software-specific information is often stored in configuration files in specified locations (e.g. the registry on machines running Microsoft Windows).Software which is not portable in this sense will have to be transferred with modifications to support the environment on the destination machine.=== Different processors ===As of 2011 the majority of desktop and laptop computers used microprocessors compatible with the 32- and 64-bit x86 instruction sets. Smaller portable devices use processors with different and incompatible instruction sets, such as ARM. The difference between larger and smaller devices is such that detailed software operation is different; an application designed to display suitably on a large screen cannot simply be ported to a pocket-sized smartphone with a tiny screen even if the functionality is similar.Web applications are required to be processor independent, so portability can be achieved by using web programming techniques, writing in JavaScript. Such a program can run in a common web browser. Such web applications must, for security reasons, have limited control over the host computer, especially regarding reading and writing files. Non-web programs, installed upon a computer in the normal manner, can have more control, and yet achieve system portability by linking to portable libraries providing the same interface on different systems.== Source code portability ==Software can be compiled and linked from source code for different operating systems and processors if written in a programming language supporting compilation for the platforms. This is usually a task for the program developers; typical users have neither access to the source code nor the required skills.In open-source environments such as Linux the source code is available to all. In earlier days source code was often distributed in a standardised format, and could be built into executable code with a standard Make tool for any particular system by moderately knowledgeable users if no errors occurred during the build. Some Linux distributions distribute software to users in source form. In these cases there is usually no need for detailed adaptation of the software for the system; it is distributed in a way which modifies the compilation process to match the system.=== Effort to port source code ===Even with seemingly portable languages like C and C++ the effort to port source code can vary considerably. The authors of UNIX/32V (1979) reported that "[t]he (Bourne) shell [...] required by far the largest conversion effort of any supposedly portable program, for the simple reason that it is not portable."Sometimes the effort consists of recompiling the source code,  but sometimes it is necessary to rewrite major parts of the software. Many language specifications describe implementation defined behaviour (e.g. right shifting a signed integer in C can do a logical or an arithmetic shift). Operating system functions or third party libraries might not be available on the target system. Some functions can be available on a target system, but exhibit slightly different behaviour (E.g.: utime() fails under Windows with EACCES, when it is called for a directory). The program code itself can also contain unportable things, like the paths of include files. Drive letters and the backslash as path delimiter are not accepted on all operating systems. Implementation defined things like byte order and the size of an int can also raise the porting effort. In practice the claim of languages, like C and C++, to have the WOCA (write once, compile anywhere) is arguable.== See also ==Cross-platform softwareHardware-dependent SoftwareC (programming language)Language interoperabilityPortability testingSource-to-source compiler== References ==== Sources ==Mooney (1997). "Bringing Portability to the Software Process" (PDF). West Virginia University. Dept. of Statistics and Computer Science. Archived from the original (PDF) on 2008-07-25. Retrieved 2008-03-17. Garen (2007). "Software Portability: Weighing Options, Making Choices". The CPA Journal. 77 (11): 3.Lehey (1995). "Porting UNIX Software: From Download to Debug" (PDF). Retrieved 2010-05-27.
	Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems. The main goals are to create scalable and highly reliable software systems. According to Ben Treynor, founder of Google's Site Reliability Team, SRE is "what happens when a software engineer is tasked with what used to be called operations."== Roles ==A site reliability engineer (SRE) will spend up to 50% of their time doing "ops" related work such as issues, on-call, and manual intervention. Since the software system that an SRE oversees is expected to be highly automatic and self-healing, the SRE should spend the other 50% of their time on development tasks such as new features, scaling or automation. The ideal site reliability engineer candidate is either a software engineer with a good administration background or a highly skilled system administrator with knowledge of coding and automation.=== DevOps vs SRE ===Coined around 2008, DevOps is a philosophy of cross-team empathy and business alignment.  It's also been associated with a practice that encompasses automation of manual tasks, continuous integration and continuous delivery.  SRE and DevOps share the same foundational principles. SRE is viewed by many (as cited in the Google SRE book) as a "specific implementation of DevOps with some idiosyncratic extensions."  SREs, being developers themselves, will naturally bring solutions that help remove the barriers between development teams and operations teams.DevOps defines 5 key pillars of success:Reduce organizational silosAccept failure as normalImplement gradual changesLeverage tooling and automationMeasure everythingSRE satisfies the DevOps pillars as follows:Reduce organizational silosSRE shares ownership with developers to create shared responsibilitySREs use the same tools that developers use, and vice versaAccept failure as normalSREs embrace riskSRE quantifies failure and availability in a prescriptive manner using Service Level Indicators (SLIs) and Service Level Objectives (SLOs)SRE mandates blameless post mortemsImplement gradual changesSRE encourages developers and product owners to move quickly by reducing the cost of failureLeverage tooling and automationSREs have a charter to automate menial tasks (called "toil") awayMeasure everythingSRE defines prescriptive ways to measure valuesSRE fundamentally believes that systems operation is a software problem== See also ==Cloud computingData centerHigh availability softwareInfrastructure as codeOperations, administration and managementOperations managementReliability engineeringSystem administration== References ==GeneralSite Reliability Engineering: How Google Runs Production Systems, O'Reilly Media, April 2016, Betsy Beyer, Chris Jones, Jennifer Petoff, Niall Richard Murphy, ISBN 978-1-491-92912-4The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2, Thomas Limoncelli, ISBN 032194318X== External links ==Google - Site Reliability Engineering interview with Ben Treynor
	Systems modeling or system modeling is the interdisciplinary study of the use of models to conceptualize and construct systems in business and IT development.A common type of systems modeling is function modeling, with specific techniques such as the Functional Flow Block Diagram and IDEF0. These models can be extended using functional decomposition, and can be linked to requirements models for further systems partition.Contrasting the functional modeling, another type of systems modeling is architectural modeling which uses the systems architecture to conceptually model the structure, behavior, and more views of a system.The Business Process Modeling Notation (BPMN), a graphical representation for specifying business processes in a workflow, can also be considered to be a systems modeling language.== Overview ==In business and IT development the term "systems modeling" has multiple meanings. It can relate to:the use of model to conceptualize and construct systemsthe interdisciplinary study of the use of these modelsthe systems modeling, analysis, and design effortsthe systems modeling and simulation, such as system dynamicsany specific systems modeling languageAs a field of study systems modeling has emerged with the development of system theory and systems sciences.As a type of modeling systems modeling is based on systems thinking and the systems approach. In business and IT systems modeling contrasts other approaches such as:agent based modelingdata modeling andmathematical modelingIn "Methodology for Creating Business Knowledge" (1997) Arbnor and Bjerke the systems approach (systems modeling) was considered to be one of the three basic methodological approaches for gaining business knowledge, beside the analytical approach and the actor's approach (agent based modeling).== History ==The function model originates in the 1950s, after in the first half of the 20th century other types of management diagrams had already been developed. The first known Gantt chart was developed in 1896 by Karol Adamiecki, who called it a harmonogram. Because Adamiecki did not publish his chart until 1931 - and in any case his works were published in either Polish or Russian, languages not popular in the West - the chart now bears the name of  Henry Gantt (1861–1919), who designed his chart around the years 1910-1915 and popularized it in the West. One of the first well defined function models, was the Functional Flow Block Diagram (FFBD) developed by the defense-related TRW Incorporated in the 1950s. In the 1960s it was exploited by the NASA to visualize the time sequence of events in a space systems and flight missions. It is further widely used in classical systems engineering to show the order of execution of system functions.One of the earliest pioneering works in information systems modeling has been done by Young and Kent (1958), who argued:Since we may be called upon to evaluate different computers or to find alternative ways of organizing current systems it is necessary to have some means of precisely stating a data processing problem independently of mechanization.They aimed for a precise and abstract way of specifying the informational and time characteristics of a data processing problem, and wanted to create a notation that should enable the analyst to organize the problem around any piece of hardware. Their efforts was not so much focussed on independent systems analysis, but on creating abstract specification and invariant basis for designing different alternative implementations using different hardware components.A next step in IS modeling was taken by CODASYL, an IT industry consortium formed in 1959, who essentially aimed at the same thing as Young and Kent: the development of "a proper structure for machine independent problem definition language, at the system level of data processing". This led to the development of a specific IS information algebra.== Types of systems modeling ==In business and IT development systems are modeled with different scopes and scales of complexity, such as:Functional modelingSystems architectureBusiness process modelingEnterprise modelingFurther more like systems thinking, systems modeling in can be divided into:Systems analysisHard systems modeling or operational research modelingSoft system modelingAnd all other specific types of systems modeling, such as form example complex systems modeling, dynamical systems modeling, and critical systems modeling.== Specific types of modeling languages ==Framework-specific modeling languageSystems Modeling Language== See also ==Behavioral modelingDynamic systemsHuman visual system model –  a human visual system model used by image processing, video processing, and computer visionSEQUAL frameworkSoftware and Systems ModelingSolar system model –  a model that illustrate the relative positions and motions of the planets and starsStatistical modelSystems analysisSystems designSystems biology modelingViable system model –  a model of the organizational structure of any viable or autonomous system== References ==== Further reading ==Doo-Kwon Baik eds. (2005). Systems modeling and simulation: theory and applications : third Asian Simulation Conference, AsiaSim 2004, Jeju Island, Korea, October 4–6, 2004. Springer, 2005. ISBN 3-540-24477-8.Derek W. Bunn, Erik R. Larsen (1997). Systems modelling for energy policy. Wiley, 1997. ISBN 0-471-95794-1Hartmut Ehrig et al. (eds.) (2005). Formal methods in software and systems modeling. Springer, 2005 ISBN 3-540-24936-2D. J. Harris (1985). Mathematics for business, management, and economics: a systems modelling approach. E. Horwood, 1985. ISBN 0-85312-821-9Jiming Liu, Xiaolong Jin, Kwok Ching Tsui (2005). Autonomy oriented computing: from problem solving to complex systems modeling. Springer, 2005. ISBN 1-4020-8121-9Michael Pidd (2004). Systems Modelling: Theory and Practice. John Wiley & Sons, 2004. ISBN 0-470-86732-9Václav Pinkava (1988). Introduction to Logic for Systems Modelling. Taylor & Francis, 1988. ISBN 0-85626-431-8
	Traceability is the capability to trace something. In some cases, it is interpreted as the ability to verify the history, location, or application of an item by means of documented recorded identification.Other common definitions include the capability (and implementation) of keeping track of a given set or type of information to a given degree, or the ability to chronologically interrelate uniquely identifiable entities in a way that is verifiable.Traceability is applicable to measurement, supply chain, software development, healthcare and security.== Measurement ==The term measurement traceability is used to refer to an unbroken chain of comparisons relating an instrument's measurements to a known standard. Calibration to a traceable standard can be used to determine an instrument's bias, precision, and accuracy. It may also be used to show a chain of custody - from current interpretation of evidence to the actual evidence in a legal context, or history of handling of any information.In many countries, national standards for weights and measures are maintained by a National Metrological Institute (NMI) which provides the highest level of standards for the calibration / measurement traceability infrastructure in that country. Examples of government agencies include the National Physical Laboratory, UK (NPL) the National Institute of Standards and Technology (NIST) in the USA, the Physikalisch-Technische Bundesanstalt (PTB) in Germany, and the Istituto Nazionale di Ricerca Metrologica (INRiM) in Italy. As defined by NIST, "Traceability of measurement requires the establishment of an unbroken chain of comparisons to stated references each with a stated uncertainty."A clock providing traceable time is traceable to a time standard such as Coordinated Universal Time or International Atomic Time. The Global Positioning System is a source of traceable time.== Supply chain ==In the supply chain, traceability may be both a regulatory and an ethical or environmental issue. Environmentally friendly retailers may choose to make information regarding their supply chain freely available to customers, illustrating the fact that the products they sell are manufactured in factories with safe working conditions, by workers that earn a fair wage, using methods that do not damage the environment.=== Materials ===In regard to materials, traceability refers to the capability to associate a finished part with destructive test results performed on material from the same ingot with the same heat treatment, or to associate a finished part with results of a test performed on a sample from the same melt identified by the unique lot number of the material. Destructive tests typically include chemical composition and mechanical strength tests. A heat number is usually marked on the part or raw material which identifies the ingot it came from, and a lot number may identify the group of parts that experienced the same heat treatment (i.e., were in the same oven at the same time). Material traceability is important to the aerospace, nuclear, and process industry because they frequently make use of high strength materials that look identical to commercial low strength versions. In these industries, a part made of the wrong material is called "counterfeit," even if the substitution was accidental.This same practice extends throughout industries using military hardware, including the fastener industry.=== Logistics ===In logistics, traceability refers to the capability for tracing goods along the distribution chain on a batch number or series number basis. Traceability is an important aspect for example in the automotive industry, where it makes recalls possible, or in the food industry where it contributes to food safety.The international standards organization EPCglobal under GS1 has ratified the EPCglobal Network standards (especially the EPC Information Services EPCIS standard) which codify the syntax and semantics for supply chain events and the secure method for selectively sharing supply chain events with trading partners. These standards for traceability have been used in successful deployments in many industries and there are now a wide range of products that are certified as being compatible with these standards.=== Food processing ===In food processing (meat processing, fresh produce processing), the term traceability refers to the recording through means of barcodes or RFID tags & other tracking media, all movement of product and steps within the production process. One of the key reasons this is such a critical point is in instances where an issue of contamination arises, and a recall is required. Where traceability has been closely adhered to, it is possible to identify, by precise date/time & exact location which goods must be recalled, and which are safe, potentially saving millions of dollars in the recall process. Traceability within the food processing industry is also utilised to identify key high production & quality areas of a business, versus those of low return, and where points in the production process may be improved.In food processing software, traceability systems imply the use of a unique piece of data (e.g., order date/time or a serialized sequence number, generally through the use of a barcode / RFID) which can be traced through the entire production flow, linking all sections of the business, including suppliers & future sales through the supply chain. Messages and files at any point in the system can then be audited for correctness and completeness, using the traceability software to find the particular transaction and/or product within the supply chain.The European Union's General Food Law came into force in 2002, making traceability compulsory for food and feed operators and requiring those businesses to implement traceability systems. The EU introduced its Trade Control and Expert System, or TRACES, in April 2004. The system provides a central database to track movement of animals within the EU and from third countries. Australia has its National Livestock Identification System to keep track of livestock from birth to slaughterhouse.India has started taking initiatives for setting up traceability systems at Government and Corporate levels. Grapenet, an initiative by Agriculture and Processed Food Products Export Development Authority (APEDA), Ministry of Commerce, Government of India is an example in this direction. GrapeNet is an internet based traceability software system for monitoring fresh grapes exported from India to the European Union. GrapeNet is a first of its kind initiative in India that has put in place an end-to-end system for monitoring pesticide residue, achieve product standardization and facilitate tracing back from pallets to the farm of the Indian grower, through the various stages of sampling, testing, certification and packing. Grapenet won the National Award (Gold), in the winners announced for the best e-Governance initiatives undertaken in India in 2007. Grapenet was designed and developed by Logicsoft, award-winning traceability solutions company, based in New Delhi, India.The Directorate Generate Foreign Trade (DGFT), Government of India, through its notification  dated 04.02.2009 relating to Amendment in Foreign Trade Policy (RE2008)has mandated that Export to the European Union is permitted subject to registration with APEDA, thereby making Grapenet mandatory for all exports of fresh grapes from India to Europe.Uruguay has also designed a system called Traceability & Electronic Information System of the Beef Industry.=== Forest products ===Within the context of supporting legal and sustainable forest supply chains, traceability has emerged in the last decade as a new tool to verify claims and assure buyers about the source of their materials. Mostly led out of Europe, and targeting countries where illegal logging has been a key problem (FLEGT countries), timber tracking is now part of daily business for many enterprises and jurisdictions. Full traceability offers advantages for multiple partners along the supply chain beyond certification systems, including:Mechanism to comply with local and international policies and regulations.Reducing the risk of illegal or non-compliant material entering the supply chains.Providing coordination between authorities and relevant bodies.Allowing automatic reconciliation of batches and volumes available.Offering a method of stock control and monitoring.Triggering real-time alerts of non-compliance.Reducing likelihood of recording errors.Improving effectiveness and efficiency.Increasing transparency.Promoting company integrity.A number of timber tracking companies are in operation to service global demand.Enhanced traceability ensures that the supply chain data is 100% accurate from the forest to the point of export. Nowadays, there are techniques to predict geographical provenance of wood and contribute to the fight against illegal logging .== Systems and software development ==In systems and software development, the term traceability (or Requirements Traceability) refers to the ability to link product requirements back to stakeholders' rationales and forward to corresponding design artifacts, code, and test cases. Traceability supports numerous software engineering activities such as change impact analysis, compliance verification or traceback of code, regression test selection, and requirements validation. It is usually accomplished in the form of a matrix created for the verification and validation of the project. Unfortunately, the practice of constructing and maintaining a requirements trace matrix (RTM) can be very arduous and over time the traces tend to erode into an inaccurate state unless date/time stamped. Alternate automated approaches for generating traces using information retrieval methods have been developed.In transaction processing software, traceability implies use of a unique piece of data (e.g., order date/time or a serialized sequence number) which can be traced through the entire software flow of all relevant application programs. Messages and files at any point in the system can then be audited for correctness and completeness, using the traceability key to find the particular transaction. This is also sometimes referred to as the transaction footprint.== Health care ==Patient safety during healthcare service plays an important role in preventing delayed recovery or even mortality, by increasing and improving the quality of life of citizens, and is considered an indicator of the quality status of health services   Maintaining patient safety is a complex task and involves factors inherent to the environment and human actions.  New technologies facilitate the traceability tools of patients and medications. This is particularly relevant for drugs that are considered high risk and cost.The World Health Organization has recognized the importance of traceability for Medical Products of Human Origin (MPHO) and urged member states "to encourage the implementation of globally consistent coding systems to facilitate national and international traceability".== Security and crime-fighting ==To prevent theft, and assist in locating stolen objects, goods may be marked indelibly or undetectably so that they may be determined to be stolen, and in some cases identified. For example, it is sometimes arranged that stolen banknotes are marked with indelible dye to show that they are stolen; they can be identified by their unique serial numbers. Announcing that cash machines were fitted with sprayers of SmartWater, an invisible gel detectable for years, to mark thieves and their clothing when breaking into or tampering with the machine was found in a 2016 pilot scheme to reduce theft by 90%.== See also ==Provenance== References ==== External links ==National Institute of Standards and TechnologyNIST Policy on Traceability"Traceability" from the Global Legal Information Network Subject Term IndexVideo which explains the relationship between Calibration - Traceability - Accreditation with flow measuring deviceshttp://www.lughtechnology.com/en/ technology tools in the Health sector to control and improve the efficiency of their critical processes.
	From its beginnings in the 1960s, writing software has evolved into a profession concerned with how best to maximize the quality of software and of how to create it. Quality can refer to how maintainable software is, to its stability, speed, usability, testability, readability, size, cost, security, and number of flaws or "bugs", as well as to less measurable qualities like elegance, conciseness, and customer satisfaction, among many other attributes. How best to create high quality software is a separate and controversial problem covering software design principles, so-called "best practices" for writing code, as well as broader management issues such as optimal team size, process, how best to deliver software on time and as quickly as possible, work-place "culture", hiring practices, and so forth. All this falls under the broad rubric of software engineering.== Overview ==The evolution of software engineering is notable in a number of areas:Emergence as a profession: By the early 1980s, software engineering professionalism, to stand beside computer science and traditional engineering.Role of women: Before 1970 men filling the more prestigious and better paying hardware engineering roles often delegated the writing of software to women, and legends such as Grace Hopper or Margaret Hamilton filled many computer programming jobs. Today, fewer women work in software engineering than in other professions, a situation whose cause is not clearly identified. Many academic and professional organizations consider this situation unbalanced and are trying hard to solve it.Processes: Processes have become a big part of software engineering. They are hailed for their potential to improve software but sharply criticized for their potential to constrict programmers.Cost of hardware: The relative cost of software versus hardware has changed substantially over the last 50 years. When mainframes were expensive and required large support staffs, the few organizations buying them also had the resources to fund large, expensive custom software engineering projects. Computers are now much more numerous and much more powerful, which has several effects on software. The larger market can support large projects to create commercial off the shelf software, as done by companies such as Microsoft. The cheap machines allow each programmer to have a terminal capable of fairly rapid compilation. The programs in question can use techniques such as garbage collection, which make them easier and faster for the programmer to write. On the other hand, many fewer organizations are interested in employing programmers for large custom software projects, instead using commercial off the shelf software as much as possible.== 1945 to 1965: The origins ==Putative origins for the term software engineering include a 1965 letter from ACM president Anthony Oettinger,  lectures by Douglas T. Ross at MIT in the 1950s. Margaret H. Hamilton "is the person who came up with the idea of naming the discipline, software engineering, as a way of giving it legitimacy."The NATO Science Committee sponsored two conferences on software engineering in 1968 (Garmisch, Germany — see conference report) and 1969, which gave the field its initial boost. Many believe these conferences marked the official start of the profession of software engineering.== 1965 to 1985: The software crisis ==Software engineering was spurred by the so-called software crisis of the 1960s, 1970s, and 1980s, which identified many of the problems of software development. Many projects ran over budget and schedule. Some projects caused property damage. A few projects caused loss of life. The software crisis was originally defined in terms of productivity, but evolved to emphasize quality. Some used the term software crisis to refer to their inability to hire enough qualified programmers.Cost and Budget Overruns: The OS/360 operating system was a classic example. This decade-long project from the 1960s eventually produced one of the most complex software systems at the time. OS/360 was one of the first large (1000 programmers) software projects. Fred Brooks claims in The Mythical Man-Month that he made a multimillion-dollar mistake of not developing a coherent architecture before starting development.Property Damage: Software defects can cause property damage. Poor software security allows hackers to steal identities, costing time, money, and reputations.Life and Death: Software defects can kill. Some embedded systems used in radiotherapy machines failed so catastrophically that they administered lethal doses of radiation to patients.  The most famous of these failures is the Therac-25 incident.Peter G. Neumann has kept a contemporary list of software problems and disasters. The software crisis has been fading from view, because it is psychologically extremely difficult to remain in crisis mode for a protracted period (more than 20 years). Nevertheless, software – especially real-time embedded software – remains risky and is pervasive, and it is crucial not to give in to complacency. Over the last 10–15 years Michael A. Jackson has written extensively about the nature of software engineering, has identified the main source of its difficulties as lack of specialization, and has suggested that his problem frames provide the basis for a "normal practice" of software engineering, a prerequisite if software engineering is to become an engineering science.== 1985 to 1989: "No Silver Bullet" ==For decades, solving the software crisis was paramount to researchers and companies producing software tools.The cost of owning and maintaining software in the 1980s was twice as expensive as developing the software.During the 1990s, the cost of ownership and maintenance increased by 30% over the 1980s.In 1995, statistics showed that half of surveyed development projects were operational, but were not considered successful.The average software project overshoots its schedule by half.Three-quarters of all large software products delivered to the customer are failures that are either not used at all, or do not meet the customer's requirements.=== Software projects ===Seemingly, every new technology and practice from the 1970s through the 1990s was trumpeted as a silver bullet to solve the software crisis. Tools, discipline, formal methods, process, and professionalism were touted as silver bullets:Tools: Especially emphasized were tools: structured programming, object-oriented programming, CASE tools such as ICL's CADES CASE system,Ada, documentation, and standards were touted as silver bullets.Discipline: Some pundits argued that the software crisis was due to the lack of discipline of programmers.Formal methods: Some believed that if formal engineering methodologies would be applied to software development, then production of software would become as predictable an industry as other branches of engineering. They advocated proving all programs correct.Process: Many advocated the use of defined processes and methodologies like the Capability Maturity Model.Professionalism: This led to work on a code of ethics, licenses, and professionalism.In 1986, Fred Brooks published his No Silver Bullet article, arguing that no individual technology or practice would ever make a 10-fold improvement in productivity within 10 years.Debate about silver bullets raged over the following decade. Advocates for Ada, components, and processes continued arguing for years that their favorite technology would be a silver bullet. Skeptics disagreed. Eventually, almost everyone accepted that no silver bullet would ever be found. Yet, claims about silver bullets pop up now and again, even today.Some interpret no silver bullet to mean that software engineering failed. However, with further reading, Brooks goes on to say: "We will surely make substantial progress over the next 40 years; an order of magnitude over 40 years is hardly magical ..."The search for a single key to success never worked. All known technologies and practices have only made incremental improvements to productivity and quality. Yet, there are no silver bullets for any other profession, either. Others interpret no silver bullet as proof that software engineering has finally matured and recognized that projects succeed due to hard work.However, it could also be said that there are, in fact, a range of silver bullets today, including lightweight methodologies (see "Project management"), spreadsheet calculators, customized browsers, in-site search engines, database report generators, integrated design-test coding-editors with memory/differences/undo, and specialty shops that generate niche software, such as information web sites, at a fraction of the cost of totally customized web site development.  Nevertheless, the field of software engineering appears too complex and diverse for a single "silver bullet" to improve most issues, and each issue accounts for only a small portion of all software problems.== 1990 to 1999: Prominence of the Internet ==The rise of the Internet led to very rapid growth in the demand for international information display/e-mail systems on the World Wide Web.  Programmers were required to handle illustrations, maps, photographs, and other images, plus simple animation, at a rate never before seen, with few well-known methods to optimize image display/storage (such as the use of thumbnail images).The growth of browser usage, running on the HyperText Markup Language (HTML), changed the way in which information-display and retrieval was organized.  The widespread network connections led to the growth and prevention of international computer viruses on MS Windows computers, and the vast proliferation of spam e-mail became a major design issue in e-mail systems, flooding communication channels and requiring semi-automated pre-screening. Keyword-search systems evolved into web-based search engines, and many software systems had to be re-designed, for international searching, depending on search engine optimization (SEO) techniques.  Human natural-language translation systems were needed to attempt to translate the information flow in multiple foreign languages, with many software systems being designed for multi-language usage, based on design concepts from human translators.  Typical computer-user bases went from hundreds, or thousands of users, to, often, many-millions of international users.== 2000 to 2015: Lightweight methodologies ==With the expanding demand for software in many smaller organizations, the need for inexpensive software solutions led to the growth of simpler, faster methodologies that developed running software, from requirements to deployment, quicker & easier.  The use of rapid-prototyping evolved to entire lightweight methodologies, such as Extreme Programming (XP), which attempted to simplify many areas of software engineering, including requirements gathering and reliability testing for the growing, vast number of small software systems.  Very large software systems still used heavily documented methodologies, with many volumes in the documentation set; however, smaller systems had a simpler, faster alternative approach to managing the development and maintenance of software calculations and algorithms, information storage/retrieval and display.=== Current trends in software engineering ===Software engineering is a young discipline, and is still developing. The directions in which software engineering is developing include:==== Aspects ====Aspects help software engineers deal with quality attributes by providing tools to add or remove boilerplate code from many areas in the source code. Aspects describe how all objects or functions should behave in particular circumstances. For example, aspects can add debugging, logging, or locking control into all objects of particular types. Researchers are currently working to understand how to use aspects to design general-purpose code. Related concepts include generative programming and templates.==== Agile ====Agile software development guides software development projects that evolve rapidly with changing expectations and competitive markets. Proponents of this method believe that heavy, document-driven processes (like TickIT, CMM and ISO 9000) are fading in importance. Some people believe that companies and agencies export many of the jobs that can be guided by heavy-weight processes. Related concepts include extreme programming, scrum, and lean software development.==== Experimental ====Experimental software engineering is a branch of software engineering interested in devising experiments on software, in collecting data from the experiments, and in devising laws and theories from this data. Proponents of this method advocate that the nature of software is such that we can advance the knowledge on software through experiments only.==== Software product lines ====Software product lines, aka product family engineering, is a systematic way to produce families of software systems, instead of creating a succession of completely individual products. This method emphasizes extensive, systematic, formal code reuse, to try to industrialize the software development process.The Future of Software Engineering conference (FOSE), held at ICSE 2000, documented the state of the art of SE in 2000 and listed many problems to be solved over the next decade. The FOSE tracks at the  ICSE 2000  and the ICSE 2007 conferences also help identify the state of the art in software engineering.=== Software engineering today ===The profession is trying to define its boundary and content. The Software Engineering Body of Knowledge SWEBOK has been tabled as an ISO standard during 2006 (ISO/IEC TR 19759).In 2006, Money Magazine and Salary.com rated software engineering as the best job in America in terms of growth, pay, stress levels, flexibility in hours and working environment, creativity, and how easy it is to enter and advance in the field.== Sub-disciplines ===== Artificial intelligence ===A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.A 2011 McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.==== Languages ====Early symbolic AI inspired Lisp and Prolog, which dominated early AI programming. Modern AI development often uses mainstream languages such as Python or C++, or niche languages such as Wolfram Language.== Prominent figures in the history of software engineering ==Charles Bachman (1924-2017) is particularly known for his work in the area of databases.Laszlo Belady (born 1928) the editor-in-chief of the IEEE Transactions on Software Engineering in the 1980s.Fred Brooks (born 1931) best known for managing the development of OS/360.Peter Chen (born 1947) known for the development of entity-relationship modeling.Edsger Dijkstra (1930–2002) developed the framework for a form of structured programming.David Parnas (born 1941) developed the concept of information hiding in modular programming.Michael A. Jackson (born 1936) software engineering methodologist responsible for JSP method of program design; JSD method of system development (with John Cameron); and Problem Frames method for analysing and structuring software development problems.David Pearson (computer scientist) (born 1946) designed and developed the ICL CADES system 1968-1977 and went on to become a computer graphics pioneer.== See also ==History of softwareHistory of computer scienceHistory of programming languages== References ==== External links ==Oral history interview with Bruce H. Barnes, Charles Babbage Institute, University of Minnesota.  Barnes describes the National Science Foundation (NSF) and its support of research in theoretical computer science, computer architecture, numerical methods, and software engineering, and the development of networking.Oral history interview with Laszlo A. Belady, Charles Babbage Institute, University of Minnesota.
	Philippe Kruchten (born 1952) is a Canadian software engineer, and Professor of Software Engineering at University of British Columbia in Vancouver, Canada, known as Director of Process Development (RUP) at Rational Software, and developer of the 4+1 Architectural View Model.== Biography ==In 1975, Kruchten received a bachelor's degree in mechanical engineering at the Ecole Centrale de Lyon in France, an MA in Software Engineering in 1978 at the École nationale supérieure des télécommunications in Paris, and a PhD in computer science from the French Institute of Telecommunications in 1986. In the new millennium in Canada he received a Certificate in Intercultural Studies from the University of British Columbia in 2002.In 1974, Kruchten started working as a FORTRAN programmer in a French computer firm, stationed for three months at IBM in London. In 1976, he became an assistant professor at the Ecole Nationale Supérieure des Télécommunications, Paris for years, and has kept switching between the academic world and the computer industry ever since. In that time, he experienced the development of large, software-intensive systems in the areas such as telecommunication, defense, aerospace, transportation, and software development tools. From 1996, he was Director of Process Development (RUP) at Rational Software, and kept this position when Rational was acquired by IBM in 2003. Since 2004, he holds a position as Professor of Software Engineering at the University of British Columbia in Vancouver, Canada. Since 2009 he holds an NSERC Chair in Design Engineering.== See also ==Artifact (software development)== Publications ==Kruchten, Philippe. The Rational Unified Process-An Introduction, Addison-Wesley, 1998; 3rd ed. in 2003;Kruchten, Philippe, and Per Kroll. Rational Unified Process Made Easy-A Practitioner's Guide to the RUP, Addison-Wesley, 2003.Kruchten, Philippe and Pierre Robillard, UPEDU: Unified Process for Education, Addison-Wesley, 2003.Articles, a selection:Kruchten, Philippe. "Agile Architecture", https://philippe.kruchten.com/2013/12/11/agile-architecture/ (related presentation: http://www.sei.cmu.edu/library/assets/presentations/Kruchten%20100519%20agility%20architecture%20Saturn.pdf)Kruchten, Philippe, Patricia Lago, and Hans Van Vliet. "Building up and reasoning about architectural knowledge." Quality of Software Architectures. Springer Berlin Heidelberg, 2006. 43-58.Hofmeister, C., Kruchten, P., Nord, R. L., Obbink, H., Ran, A., & America, P. (2007). "A general model of software architecture design derived from five industrial approaches." Journal of Systems and Software, 80(1), 106-126.Kruchten, Philippe. "Voyage in the agile memeplex." Queue 5.5 (2007): 1.Kruchten, Philippe (1995, November). Architectural Blueprints — The “4+1” View Model of Software Architecture. IEEE Software 12 (6), pp. 42–50.== References ==== External links ==Official website Kruchten Engineering Services, Ltd.UBC page
	The World Wide Web has become a major delivery platform for a variety of complex and sophisticated enterprise applications in several domains. In addition to their inherent multifaceted functionality, these Web applications exhibit complex behaviour and place some unique demands on their usability, performance, security, and ability to grow and evolve. However, a vast majority of these applications continue to be developed in an ad-hoc way, contributing to problems of usability, maintainability, quality and reliability. While Web development can benefit from established practices from other related disciplines, it has certain distinguishing characteristics that demand special considerations. In recent years, there have been developments towards addressing these considerations.Web engineering focuses on the methodologies, techniques, and tools that are the foundation of Web application development and which support their design, development, evolution, and evaluation. Web application development has certain characteristics that make it different from traditional software, information system, or computer application development.Web engineering is multidisciplinary and encompasses contributions from diverse areas: systems analysis and design, software engineering, hypermedia/hypertext engineering, requirements engineering, human-computer interaction, user interface, information engineering, information indexing and retrieval, testing, modelling and simulation, project management, and graphic design and presentation. Web engineering is neither a clone nor a subset of software engineering, although both involve programming and software development. While Web Engineering uses software engineering principles, it encompasses new approaches, methodologies, tools, techniques, and guidelines to meet the unique requirements of Web-based applications.== As a discipline ==Proponents of Web engineering supported the establishment of Web engineering as a discipline at an early stage of Web. Major arguments for Web engineering as a new discipline are:Web-based Information Systems (WIS) development process is different and unique.Web engineering is multi-disciplinary; no single discipline (such as software engineering) can provide complete theory basis, body of knowledge and practices to guide WIS development.Issues of evolution and lifecycle management when compared to more 'traditional' applications.Web-based information systems and applications are pervasive and non-trivial. The prospect of Web as a platform will continue to grow and it is worth being treated specifically.However, it has been controversial, especially for people in other traditional disciplines such as software engineering, to recognize Web engineering as a new field. The issue is how different and independent Web engineering is, compared with other disciplines.Main topics of Web engineering include, but are not limited to, the following areas:=== Modeling disciplines ===Business Processes for Applications on the WebProcess Modelling of Web applicationsRequirements Engineering for Web applicationsB2B applications=== Design disciplines, tools, and methods ===UML and the WebConceptual Modeling of Web Applications (aka. Web modeling)Prototyping Methods and ToolsWeb design methodsCASE Tools for Web ApplicationsWeb Interface DesignData Models for Web Information Systems=== Implementation disciplines ===Integrated Web Application Development EnvironmentsCode Generation for Web ApplicationsSoftware Factories for/on the WebWeb 2.0, AJAX, E4X, ASP.NET, PHP and Other New DevelopmentsWeb Services Development and Deployment=== Testing disciplines ===Testing and Evaluation of Web systems and Applications.Testing Automation, Methods, and Tools.=== Applications categories disciplines ===Semantic Web applicationsDocument centric Web sitesTransactional Web applicationsInteractive Web applicationsWorkflow-based Web applicationsCollaborative Web applicationsPortal-oriented Web applicationsUbiquitous and Mobile Web ApplicationsDevice Independent Web DeliveryLocalization and Internationalization of Web ApplicationsPersonalization of Web Applications== Attributes ===== Web quality ===Web Metrics, Cost Estimation, and MeasurementPersonalisation and Adaptation of Web applicationsWeb QualityUsability of Web ApplicationsWeb accessibilityPerformance of Web-based applications=== Content-related ===Web Content ManagementContent Management System (CMS)Multimedia Authoring Tools and SoftwareAuthoring of adaptive hypermedia== Education ==Master of Science: Web Engineering as a branch of study within the MSc program Web Sciences at the Johannes Kepler University Linz, Austria Diploma in Web Engineering: Web Engineering as a study program at the International Webmasters College (iWMC), Germany == See also ==DevOpsWeb developerWeb modeling== References ==== Sources ==Robert L. Glass, "Who's Right in the Web Development Debate?" Cutter IT Journal, July 2001, Vol. 14, No.7, pp 6–0.S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera. "Designing Data-Intensive Web Applications". Morgan Kaufmann Publisher, Dec 2002, ISBN 1-55860-843-5=== Web engineering resources ===OrganizationsInternational Society for Web Engineering e.V.: http://www.iswe-ev.de/Web Engineering Community: http://www.webengineering.orgWISE Society: http://www.wisesociety.org/ACM SIGWEB: http://www.acm.org/sigwebWorld Wide Web Consortium: http://www.w3.orgBooks"Engineering Web Applications", by Sven Casteleyn, Florian Daniel, Peter Dolog and Maristella Matera, Springer, 2009, ISBN 978-3-540-92200-1"Web Engineering: Modelling and Implementing Web Applications", edited by Gustavo Rossi, Oscar Pastor, Daniel Schwabe and Luis Olsina, Springer Verlag HCIS, 2007, ISBN 978-1-84628-922-4"Cost Estimation Techniques for Web Projects", Emilia Mendes, IGI Publishing, ISBN 978-1-59904-135-3"Web Engineering - The Discipline of Systematic Development of Web Applications", edited by Gerti Kappel, Birgit Pröll, Siegfried Reich, and Werner Retschitzegger, John Wiley & Sons, 2006"Web Engineering", edited by Emilia Mendes and Nile Mosley, Springer-Verlag, 2005"Web Engineering: Principles and Techniques", edited by Woojong Suh, Idea Group Publishing, 2005"Form-Oriented Analysis -- A New Methodology to Model Form-Based Applications", by Dirk Draheim, Gerald Weber, Springer, 2005"Building Web Applications with UML" (2nd edition), by Jim Conallen, Pearson Education, 2003"Information Architecture for the World Wide Web" (2nd edition), by Peter Morville and Louis Rosenfeld, O'Reilly, 2002"Web Site Engineering: Beyond Web Page Design", by Thomas A. Powell, David L. Jones and Dominique C. Cutts, Prentice Hall, 1998"Designing Data-Intensive Web Applications", by S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera. Morgan Kaufmann Publisher, Dec 2002, ISBN 1-55860-843-5ConferencesWorld Wide Web Conference (by IW3C2, since 1994): http://www.iw3c2.orgInternational Conference on Web Engineering (ICWE) (since 2000)2018: http://icwe2018.webengineering.org/ (Caceres, Spain)2017: http://icwe2017.webengineering.org/ (Rome, Italy)2016: http://icwe2016.webengineering.org/ (Lugano, Switzerland)2007: http://www.icwe2007.org/2006: http://www.icwe2006.org2005: http://www.icwe2005.org2004: http://www.icwe2004.orgICWE Conference ProceedingsICWE2007: LNCS 4607 https://www.springer.com/computer/database+management+&+information+retrieval/book/978-3-540-73596-0ICWE2005: LNCS 3579 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-58872076-0ICWE2004: LNCS 3140 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-32445543-0ICWE2003: LNCS 2722 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-3092664-0Web Information Systems Engineering Conference (by WISE Society, since 2000): http://www.wisesociety.org/International Conference on Web Information Systems and Technologies (Webist) (since 2005): http://www.webist.org/International Workshop on Web Site Evolution (WSE): http://www.websiteevolution.org/International Conference on Software Engineering: http://www.icse-conferences.org/Book chapters and articlesPressman, R.S., 'Applying Web Engineering', Part 3, Chapters 16-20, in Software Engineering: A Practitioner's Perspective, Sixth Edition, McGraw-Hill, New York, 2004. http://www.rspa.com/'JournalsJournal of Web Engineering: http://www.rintonpress.com/journals/jwe/International Journal of Web Engineering and Technology: http://www.inderscience.com/browse/index.php?journalID=48ACM Transactions on Internet Technology: http://toit.acm.org/World Wide Web (Springer): https://link.springer.com/journal/11280Web coding journal: http://www.web-code.org/Special issuesWeb Engineering, IEEE MultiMedia, Jan.–Mar. 2001 (Part 1) and April–June 2001 (Part 2).  http://csdl2.computer.org/persagen/DLPublication.jsp?pubtype=m&acronym=muUsability Engineering, IEEE Software, January–February 2001.Web Engineering, Cutter IT Journal, 14(7), July 2001.*Testing E-business Applications, Cutter IT Journal, September 2001.Engineering Internet Software, IEEE Software, March–April 2002.Usability and the Web, IEEE Internet Computing, March–April 2002.
	Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.== Software construction fundamentals ===== Minimizing complexity ===The need to reduce complexity is mainly driven by limited ability of most people to hold complex structures and information in their working memories.  Reduced complexity is achieved through emphasizing the creation of code that is simple and readable rather than clever. Minimizing complexity is accomplished through making use of standards, and through numerous specific techniques in coding. It is also supported by the construction-focused quality techniques.=== Anticipating change ===Anticipating change helps software engineers build extensible software, which means they can enhance a software product without disrupting the underlying structure.  Research over 25 years showed that the cost of rework can be 10 to 100 times (5 to 10 times for smaller projects) more expensive than getting the requirements right the first time.  Given that 25% of the requirements change during development on average project, the need to reduce the cost of rework elucidates the need for anticipating change. === Constructing for verification ===Constructing for verification means building software in such a way that faults can be ferreted out readily by the software engineers writing the software, as well as during independent testing and operational activities. Specific techniques that support constructing for verification include following coding standards to support code reviews, unit testing, organizing code to support automated testing, and restricted use of complex or hard-to-understand language structures, among others.=== Reuse ===Systematic reuse can enable significant software productivity, quality, and cost improvements.  Reuse has two closely related facets:Construction for reuse: Create reusable software assets.Construction with reuse: Reuse software assets in the construction of a new solution.=== Standards in construction ===Standards, whether external (created by international organizations) or internal (created at the corporate level), that directly affect construction issues include:Communication methods: Such as standards for document formats and contents.Programming languagesCoding standardsPlatformsTools: Such as diagrammatic standards for notations like UML.== Managing construction ===== Construction model ===Numerous models have been created to develop software, some of which emphasize construction more than others. Some models are more linear from the construction point of view, such as the Waterfall and staged-delivery life cycle models. These models treat construction as an activity which occurs only after significant prerequisite work has been completed—including detailed requirements work, extensive design work, and detailed planning. Other models are more iterative, such as evolutionary prototyping, Extreme Programming, and Scrum. These approaches tend to treat construction as an activity that occurs concurrently with other software development activities, including requirements, design, and planning, or overlaps them.=== Construction planning ===The choice of construction method is a key aspect of the construction planning activity. The choice of construction method affects the extent to which construction prerequisites (e.g. Requirements analysis, Software design, .. etc.) are performed, the order in which they are performed, and the degree to which they are expected to be completed before construction work begins. Construction planning also defines the order in which components are created and integrated, the software quality management processes, the allocation of task assignments to specific software engineers, and the other tasks, according to the chosen method.=== Construction measurement ===Numerous construction activities and artifacts can be measured, including code developed, code modified, code reused, code destroyed, code complexity, code inspection statistics, fault-fix and fault-find rates, effort, and scheduling. These measurements can be useful for purposes of managing construction, ensuring quality during construction, improving the construction process, as well as for other reasons.== Practical considerations ==Software construction is driven by many practical considerations:=== Construction design ===In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design.Low Fan-out is one of the design characteristics found to be beneficial by researchers. Information hiding proved to be a useful design technique in large programs that made them easier to modify by a factor of 4. === Construction languages ===Construction languages include all forms of communication by which a human can specify an executable problem solution to a computer. They include configuration languages, toolkit languages, and programming languages:Configuration languages are languages in which software engineers choose from a limited set of predefined options to create new or custom software installations.Toolkit languages are used to build applications out of toolkits and are more complex than configuration languages.Scripting languages are kinds of application programming languages that supports scripts which are often interpreted rather than compiled.Programming languages are the most flexible type of construction languages which use three general kinds of notation:Linguistic notations which are distinguished in particular by the use of word-like strings of text to represent complex software constructions, and the combination of such word-like strings into patterns that have a sentence-like syntax.Formal notations which rely less on intuitive, everyday meanings of words and text strings and more on definitions backed up by precise, unambiguous, and formal (or mathematical) definitions.Visual notations which rely much less on the text-oriented notations of both linguistic and formal construction, and instead rely on direct visual interpretation and placement of visual entities that represent the underlying software.Programmers working in a language they have used for three years or more are about 30 percent more productive than programmers with equivalent experience who are new to a language.  High-level languages such as C++, Java, Smalltalk, and Visual Basic yield 5 to 15 times better productivity, reliability, simplicity, and comprehensibility than low-level languages such as assembly and C.  Equivalent code has been shown to need fewer lines to be implemented in high level languages than in lower level languages.=== Coding ===The following considerations apply to the software construction coding activity:Techniques for creating understandable source code, including naming and source code layout.  One study showed that the effort required to debug a program is minimized when the variables' names are between 10 and 16 characters.Use of classes, enumerated types, variables, named constants, and other similar entities. The following considerations should be noted:A study done by NASA showed that the putting the code into well-factored classes can double the code reusability compared to the code developed using functional design.One experiment showed that designs which access arrays sequentially, rather than randomly, result in fewer variables and fewer variable references.Use of control structures.  The following considerations should be noted:One experiment found that loops-with-exit are more comprehensible than other kinds of loops.Regarding the level of nesting in loops and conditionals, studies have shown that programmers have difficulty comprehending more than three levels of nesting.Control flow complexity has been shown to correlate with low reliability and frequent errors.Handling of error conditions—both planned errors and exceptions (input of bad data, for example)Prevention of code-level security breaches (buffer overruns or array index overflows, for example)Resource usage via use of exclusion mechanisms and discipline in accessing serially reusable resources (including threads or database locks)Source code organization (into statements and routines).  The following considerations should be noted regarding routines:Highly cohesive routines proved to be less error prone than routines with lower cohesion.  A study of 450 routines found that 50 percent of the highly cohesive routines were fault free compared to only 18 percent of routines with low cohesion.  Another study of a different 450 routines found that routines with the highest coupling-to-cohesion ratios had 7 times as many errors as those with the lowest coupling-to-cohesion ratios and were 20 times as costly to fix.Although studies showed inconclusive results regarding the correlation between routine sizes and the rate of errors in them, but one study found that routines with fewer than 143 lines of code were 2.4 times less expensive to fix than larger routines. Another study showed that the code needed to be changed least when routines averaged 100 to 150 lines of code.  Another study found that structural complexity and amount of data in a routine were correlated with errors regardless of its size.Interfaces between routines are some of the most error-prone areas of a program.  One study showed that 39 percent of all errors were errors in communication between routines.Unused parameters are correlated with an increased error rate. In one study, only 17 to 29 percent of routines with more than one unreferenced variable had no errors, compared to 46 percent in routines with no unused variables.The number of parameters of a routine should be 7 at maximum as research has found that people generally cannot keep track of more than about seven chunks of information at once.Source code organization (into classes, packages, or other structures).  When considering containment, the maximum number of data members in a class shouldn't exceed 7±2. Research has shown that this number is the number of discrete items a person can remember while performing other tasks.  When considering inheritance, the number of levels in the inheritance tree should be limited.  Deep inheritance trees have been found to be significantly associated with increased fault rates.   When considering the number of routines in a class, it should be kept as small as possible.  A study on C++ programs has found an association between the number of routines and the number of faults.Code documentationCode tuning=== Construction testing ===The purpose of construction testing is to reduce the gap between the time at which faults are inserted into the code and the time those faults are detected. In some cases, construction testing is performed after code has been written. In test-first programming, test cases are created before code is written. Construction involves two forms of testing, which are often performed by the software engineer who wrote the code:Unit testingIntegration testing=== Reuse ===Implementing software reuse entails more than creating and using libraries of assets. It requires formalizing the practice of reuse by integrating reuse processes and activities into the software life cycle. The tasks related to reuse in software construction during coding and testing are:The selection of the reusable units, databases, test procedures, or test data.The evaluation of code or test re-usability.The reporting of reuse information on new code, test procedures, or test data.=== Construction quality ===The primary techniques used to ensure the quality of code as it is constructed include:Unit testing and integration testing. One study found that the average defect detection rates of unit testing and integration testing are 30% and 35% respectively.Test-first developmentUse of assertions and defensive programmingDebuggingInspections. One study found that the average defect detection rate of formal code inspections is 60%. Regarding the cost of finding defects, a study found that code reading detected 80% more faults per hour than testing.  Another study shown that it costs six times more to detect design defects by using testing than by using inspections.  A study by IBM showed that only 3.5 hours where needed to find a defect through code inspections versus 15–25 hours through testing.  Microsoft has found that it takes 3 hours to find and fix a defect by using code inspections and 12 hours to find and fix a defect by using testing.  In a 700 thousand lines program, it was reported that code reviews were several times as cost-effective as testing.   Studies found that inspections result in 20% - 30% fewer defects per 1000 lines of code than less formal review practices and that they increase productivity by about 20%.  Formal inspections will usually take 10% - 15% of the project budget and will reduce overall project cost.  Researchers found that having more than 2 - 3 reviewers on a formal inspection doesn't increase the number of defects found, although the results seem to vary depending on the kind of material being inspected.Technical reviews. One study found that the average defect detection rates of informal code reviews and desk checking are 25% and 40% respectively.  Walkthroughs were found to have defect detection rate of 20% - 40%, but were found also to be expensive specially when project pressures increase.  Code reading was found by NASA to detect 3.3 defects per hour of effort versus 1.8 defects per hour for testing.  It also finds 20% - 60% more errors over the life of the project than different kinds of testing.  A study of 13 reviews about review meetings, found that 90% of the defects were found in preparation for the review meeting while only around 10% were found during the meeting.Static analysis (IEEE1028)Studies have shown that a combination of these techniques need to be used to achieve high defect detection rate.  Other studies showed that different people tend to find different defects.  One study found that the Extreme Programming practices of pair programming, desk checking, unit testing, integration testing, and regression testing can achieve a 90% defect detection rate.  An experiment involving experienced programmers found that on average they were able to find 5 errors (9 at best) out of 15 errors by testing.80% of the errors tend to be concentrated in 20% of the project's classes and routines. 50% of the errors are found in 5% of the project's classes.  IBM was able to reduce the customer reported defects by a factor of ten to one and to reduce their maintenance budget by 45% in its IMS system by repairing or rewriting only 31 out of 425 classes.  Around 20% of a project's routines contribute to 80% of the development costs.  A classic study by IBM found that few error-prone routines of OS/360 were the most expensive entities. They had around 50 defects per 1000 lines of code and fixing them costs 10 times what it took to develop the whole system.=== Integration ===A key activity during construction is the integration of separately constructed routines, classes, components, and subsystems. In addition, a particular software system may need to be integrated with other software or hardware systems. Concerns related to construction integration include planning the sequence in which components will be integrated, creating scaffolding to support interim versions of the software, determining the degree of testing and quality work performed on components before they are integrated, and determining points in the project at which interim versions of the software are tested.== Construction technologies ===== Object-oriented runtime issues ===Object-oriented languages support a series of runtime mechanisms that increase the flexibility and adaptability of the programs like data abstraction, encapsulation, modularity, inheritance, polymorphism, and reflection.Data abstraction is the process by which data and programs are defined with a representation similar in form to its meaning, while hiding away the implementation details. Academic research showed that data abstraction makes programs about 30% easier to understand than functional programs.=== Assertions, design by contract, and defensive programming ===Assertions are executable predicates which are placed in a program that allow runtime checks of the program.  Design by contract is a development approach in which preconditions and postconditions are included for each routine. Defensive programming is the protection a routine from being broken by invalid inputs.=== Error-handling, exception-handling, and fault tolerance ===Error-handling refers to the programming practice of anticipating and coding for error conditions that may arise when the program runs. Exception-handling is a programming-language construct or hardware mechanism designed to handle the occurrence of exceptions, special conditions that change the normal flow of program execution.  Fault tolerance is a collection of techniques that increase software reliability by detecting errors and then recovering from them if possible or containing their effects if recovery is not possible.=== State-based and table-driven construction techniques ===State-based programming is a programming technology using finite state machines to describe program behaviors.  A table-driven method is a schema that uses tables to look up information rather than using logic statements (such as if and case).=== Runtime configuration and internationalization ===Runtime configuration is a technique that binds variable values and program settings when the program is running, usually by updating and reading configuration files in a just-in-time mode.  Internationalization is the technical activity of preparing a program, usually interactive software, to support multiple locales.  The corresponding activity, localization, is the activity of modifying a program to support a specific local language.== See also ==Software engineeringSoftware development== Notes ==== References ==== External links ==Guide to the Software Engineering Body of Knowledge - 2004 Version By IEEE Computer SocietyGuide to the Software Engineering Body of Knowledge, Version 3.0, IEEE Computer Society, 2014
	Memory safety is the state of being protected from various software bugs and security vulnerabilities when dealing with memory access, such as buffer overflows and dangling pointers. For example, Java is said to be memory-safe because its runtime error detection checks array bounds and pointer dereferences. In contrast, C and C++ allow arbitrary pointer arithmetic with pointers implemented as direct memory addresses with no provision for bounds checking, and thus are termed memory-unsafe.== History ==Memory errors were first considered in the context of resource management and time-sharing systems, in an effort to avoid problems such as fork bombs. Developments were mostly theoretical until the Morris worm, which exploited a buffer overflow in fingerd. The field of computer security developed quickly thereafter, escalating with multitudes of new attacks such as the return-to-libc attack and defense techniques such as the non-executable stack and address space layout randomization. Randomization prevents most buffer overflow attacks and requires the attacker to use heap spraying or other application-dependent methods to obtain addresses, although its adoption has been slow. However, deployments of the technology are typically limited to randomizing libraries and the location of the stack.== Approaches ==DieHard, its redesign DieHarder, and the Allinea Distributed Debugging Tool are special heap allocators that allocate objects in their own random virtual memory page, allowing invalid reads and writes to be stopped and debugged at the exact instruction that causes them. Protection relies upon hardware memory protection and thus overhead is typically not substantial, although it can grow significantly if the program makes heavy use of allocation. Randomization provides only probabilistic protection against memory errors, but can often be easily implemented in existing software by relinking the binary.The memcheck tool of Valgrind uses an instruction set simulator and runs the compiled program in a memory-checking virtual machine, providing guaranteed detection of a subset of runtime memory errors. However, it typically slows the program down by a factor of 40, and furthermore must be explicitly informed of custom memory allocators.With access to the source code, libraries exist that collect and track legitimate values for pointers ("metadata") and check each pointer access against the metadata for validity, such as the Boehm garbage collector. In general, memory safety can be safely assured using tracing garbage collection and the insertion of runtime checks on every memory access; this approach has overhead, but less than that of Valgrind. All garbage-collected languages take this approach. For C and C++, many tools exist that perform a compile-time transformation of the code to do memory safety checks at runtime, such as CheckPointer and AddressSanitizer which imposes an average slowdown factor of 2.Another approach uses static program analysis and automated theorem proving to ensure that the program is free of memory errors. For example, the Rust programming language implements a borrow checker to ensure memory safety. Tools such as Coverity offer static memory analysis for C. C++'s smart pointers are a limited form of this approach.== Types of memory errors ==Many different types of memory errors can occur:Access errors: invalid read/write of a pointerBuffer overflow - out-of-bound writes can corrupt the content of adjacent objects, or internal data (like bookkeeping information for the heap) or return addresses.Buffer over-read - out-of-bound reads can reveal sensitive data or help attackers bypass address space layout randomization.Race condition - concurrent reads/writes to shared memoryInvalid page fault - accessing a pointer outside the virtual memory space. A null pointer dereference will often cause an exception or program termination in most environments, but can cause corruption in operating system kernels or systems without memory protection, or when use of the null pointer involves a large or negative offset.Use after free - dereferencing a dangling pointer storing the address of an object that has been deleted.Uninitialized variables - a variable that has not been assigned a value is used. It may contain an undesired or, in some languages, a corrupt value.Null pointer dereference - dereferencing an invalid pointer or a pointer to memory that has not been allocatedWild pointers arise when a pointer is used prior to initialization to some known state. They show the same erratic behaviour as dangling pointers, though they are less likely to stay undetected.Memory leak - when memory usage is not tracked or tracked incorrectlyStack exhaustion - occurs when a program runs out of stack space, typically because of too deep recursion. A guard page typically halts the program, preventing memory corruption, but functions with large stack frames may bypass the page.Heap exhaustion - the program tries to allocate more memory than the amount available. In some languages, this condition must be checked for manually after each allocation.Double free - repeated calls to free may prematurely free a new object at the same address. If the exact address has not been reused, other corruption may occur, especially in allocators that use free lists.Invalid free - passing an invalid address to free can corrupt the heap.Mismatched free - when multiple allocators are in use, attempting to free memory with a deallocation function of a different allocatorUnwanted aliasing - when the same memory location is allocated and modified twice for unrelated purposes.== References ==
	Search-based software engineering (SBSE) applies metaheuristic search techniques such as genetic algorithms, simulated annealing and tabu search to software engineering problems. Many activities in software engineering can be stated as optimization problems. Optimization techniques of operations research such as linear programming or dynamic programming are often impractical for large scale software engineering problems because of their computational complexity. Researchers and practitioners use metaheuristic search techniques to find near-optimal or "good-enough" solutions.SBSE problems can be divided into two types:black-box optimization problems, for example, assigning people to tasks (a typical combinatorial optimization problem).white-box problems where operations on source code need to be considered.== Definition ==SBSE converts a software engineering problem into a computational search problem that can be tackled with a metaheuristic. This involves defining a search space, or the set of possible solutions. This space is typically too large to be explored exhaustively, suggesting a metaheuristic approach. A metric  (also called a fitness function, cost function, objective function or quality measure) is then used to measure the quality of potential solutions. Many software engineering problems can be reformulated as a computational search problem.The term "search-based application", in contrast, refers to using search engine technology, rather than search techniques, in another industrial application.== Brief history ==One of the earliest attempts to apply optimization to a software engineering problem was reported by Webb Miller and David Spooner in 1976 in the area of software testing. In 1992, S. Xanthakis and his colleagues applied a search technique to a software engineering problem for the first time. The term SBSE was first used in 2001 by Harman and Jones. The research community grew to include more than 800 authors by 2013, spanning approximately 270 institutions in 40 countries.== Application areas ==Search-based software engineering is applicable to almost all phases of the software development process. Software testing has been one of the major applications. Search techniques have been applied to other software engineering activities, for instance, requirements analysis, design, development, and maintenance.=== Requirements engineering ===Requirements engineering is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches user requests amid constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a multiple-criteria decision-making problem and, generally involves presenting the decision maker with a set of good compromises between cost and user satisfaction as well as the requirements risk.=== Debugging and maintenance ===Identifying a software bug (or a code smell) and then debugging (or refactoring) the software is largely a manual and labor-intensive endeavor, though the process is tool-supported. One objective of SBSE is to automatically identify and fix bugs (for example via mutation testing).Genetic programming, a biologically-inspired technique that involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The GenProg Evolutionary Program Repair software repaired 55 out of 105 bugs for approximately $8 each in one test.Coevolution adopts a "predator and prey" metaphor in which a suite of programs and a suite of unit tests evolve together and influence each other.=== Testing ===Search-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. Regression testing has also received some attention.=== Optimizing software ===The use of SBSE in program optimization, or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of successful research. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.A recent work by Basios et al. shows that by optimising the data structure, Google Guava found 9% improvement on execution time, 13% improvement on memory consumption and 4% improvement on CPU usage separately.=== Project management ===A number of decisions that are normally made by a project manager can be done automatically, for example, project scheduling.== Tools ==Tools available for SBSE include OpenPAT. and EvoSuite  and Coverage, a code coverage measurement tool for Python== Methods and techniques ==A number of methods and techniques are available, including:Profiling via instrumentation in order to monitor certain parts of a program as it is executed.Obtaining an abstract syntax tree associated with the program, which can be automatically examined to gain insights into its structure.Applications of program slicing relevant to SBSE include software maintenance, optimization and program analysis.Code coverage allows measuring how much of the code is executed with a given set of input data.Static program analysis== Industry acceptance ==As a relatively new area of research, SBSE does not yet experience broad industry acceptance. Software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are unlike those that humans produce. In the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to support maintenance activities.Another concern is that SBSE might make the software engineer redundant. Supporters claim that the motivation for SBSE is to enhance the relationship between the engineer and the program.== See also ==Program analysis (computer science)Dynamic program analysisGenetic improvement== References ==== External links ==Repository of publications on SBSEMetaheuristics and Software EngineeringSoftware-artifact Infrastructure RepositoryInternational Conference on Software EngineeringGenetic and Evolutionary Computation (GECCO)Google Scholar page on Search-based software engineering
	Component-based software engineering (CBSE), also called components-based development (CBD), is a branch of software engineering that emphasizes the separation of concerns with respect to the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software.Software engineering practitioners regard components as part of the starting platform for service-orientation. Components play this role, for example, in web services, and more recently, in service-oriented architectures (SOA), whereby a component is converted by the web service into a service and subsequently inherits further characteristics beyond that of an ordinary component.Components can produce or consume events and can be used for event-driven architectures (EDA).== Definition and characteristics of components ==An individual software component is a software package, a web service, a web resource, or a module that encapsulates a set of related functions (or data).All system processes are placed into separate components so that all of the data and functions inside each component are semantically related (just as with the contents of classes). Because of this principle, it is often said that components are modular and cohesive.With regard to system-wide co-ordination, components communicate with each other via interfaces. When a component offers services to the rest of the system, it adopts a provided interface that specifies the services that other components can utilize, and how they can do so. This interface can be seen as a signature of the component - the client does not need to know about the inner workings of the component (implementation) in order to make use of it. This principle results in components referred to as encapsulated. The UML illustrations within this article represent provided interfaces by a lollipop-symbol attached to the outer edge of the component.However, when a component needs to use another component in order to function, it adopts a used interface that specifies the services that it needs. In the UML illustrations in this article, used interfaces are represented by an open socket symbol attached to the outer edge of the component.Another important attribute of components is that they are substitutable, so that a component can replace another (at design time or run-time), if the successor component meets the requirements of the initial component (expressed via the interfaces). Consequently, components can be replaced with either an updated version or an alternative without breaking the system in which the component operates.As a rule of thumb for engineers substituting components, component B can immediately replace component A, if component B provides at least what component A provided and uses no more than what component A used.Software components often take the form of objects (not classes) or collections of objects (from object-oriented programming), in some binary or textual form, adhering to some interface description language (IDL) so that the component may exist autonomously from other components in a computer. In other words, a component acts without changing its source code. Although, the behavior of the component's source code may change based on the application's extensibility, provided by its writer.When a component is to be accessed or shared across execution contexts or network links, techniques such as serialization or marshalling are often employed to deliver the component to its destination.Reusability is an important characteristic of a high-quality software component. Programmers should design and implement software components in such a way that many different programs can reuse them. Furthermore, component-based usability testing should be considered when software components directly interact with users.It takes significant effort and awareness to write a software component that is effectively reusable. The component needs to be:fully documentedthoroughly testedrobust - with comprehensive input-validity checkingable to pass back appropriate error messages or return codesdesigned with an awareness that it will be put to unforeseen usesIn the 1960s, programmers built scientific subroutine libraries that were reusable in a broad array of engineering and scientific applications. Though these subroutine libraries reused well-defined algorithms in an effective manner, they had a limited domain of application. Commercial sites routinely created application programs from reusable modules written in assembly language, COBOL, PL/1 and other second- and third-generation languages using both system and user application libraries.As of 2010, modern reusable components encapsulate both data structures and the algorithms that are applied to the data structures. Component-based software engineering builds on prior theories of software objects, software architectures, software frameworks and software design patterns, and the extensive theory of object-oriented programming and the object-oriented design of all these. It claims that software components, like the idea of hardware components, used for example in telecommunications, can ultimately be made interchangeable and reliable. On the other hand, it is argued that it is a mistake to focus on independent components rather than the framework (without which they would not exist).== History ==The idea that software should be componentized - built from prefabricated components - first became prominent with Douglas McIlroy's address at the NATO conference on software engineering in Garmisch, Germany, 1968, titled Mass Produced Software Components. The conference set out to counter the so-called software crisis. McIlroy's subsequent inclusion of pipes and filters into the Unix operating system was the first implementation of an infrastructure for this idea.Brad Cox of Stepstone largely defined the modern concept of a software component. He called them Software ICs and set out to create an infrastructure and market for these components by inventing the Objective-C programming language. (He summarizes this view in his book Object-Oriented Programming - An Evolutionary Approach 1986.)The software components are used in two different contexts and two kinds: i) using components as parts to build a single executable, or ii) each executable is treated as a component in a distributed environment, where components collaborate with each other using internet or intranet communication protocols for IPC (Inter Process Communications). The above belongs to former kind, while the below belongs to later kind.IBM led the path with their System Object Model (SOM) in the early 1990s. As a reaction, Microsoft paved the way for actual deployment of component software with Object linking and embedding (OLE) and Component Object Model (COM). As of 2010 many successful software component models exist.== Architecture ==A computer running several software components is often called an application server. This combination of application servers and software components is usually called distributed computing. Typical real-world application of this is in, e.g., financial applications or business software.== Component Models ==A component model is a definition of properties that components must satisfy, methods and mechanisms for the composition of components.During the last decades, researchers and practitioners have proposed several component models with different characteristics. A classification of the existing component models is given in. Examples of component models are: Enterprise JavaBeans (EJB) model, Component Object Model (COM) model, .NET model, X-MAN component model, and Common Object Request Broker Architecture  (CORBA) component model.== Technologies ==Business object technologiesNewiComponent-based software frameworks for specific domainsAdvanced Component FrameworkEarth System Modeling Framework (ESMF)MASH IoT Platform for Asset ManagementKOALA component model developed for software in consumer electronicsReact (JavaScript library)Software Communications Architecture (JTRS SCA)Component-oriented programmingBundles as defined by the OSGi Service PlatformComponent web platform for modular js, css, and other assetsComponent Object Model (OCX/ActiveX/COM) and DCOM from MicrosoftTASCS - SciDAC Center for Technology for Advanced Scientific Component SoftwareEiffel programming languageEnterprise JavaBeans from Sun Microsystems (now Oracle)Flow-based programmingFractal component model from ObjectWebMidCOM component framework for Midgard and PHPOberon, Component Pascal, and BlackBox Component BuilderrCOS method of component-based model driven design from UNU-IISTSOFA component system from ObjectWebThe System.ComponentModel namespace in Microsoft .NETUnity developed by Unity TechnologiesUnreal Engine developed by Epic GamesUNO from the OpenOffice.org office suiteVCL and CLX from Borland and similar free LCL library.XPCOM from Mozilla FoundationCompound document technologiesActive Documents in Oberon System and BlackBox Component BuilderKParts, the KDE compound document technologyObject linking and embedding (OLE)OpenDocDistributed computing software components.NET Remoting from Microsoft9P distributed protocol developed for Plan 9, and used by Inferno and other systems.CORBA and the CORBA Component Model from the Object Management GroupD-Bus from the freedesktop.org organizationDCOM and later versions of COM (and COM+) from MicrosoftDSOM and SOM from IBM (now scrapped)Ice from ZeroCJava EE from SunKompics from SICSUniversal Network Objects (UNO) from OpenOffice.orgWeb servicesRESTZope from Zope CorporationAXCIOMA (the component framework for distributed, real-time, and embedded systems) by Remedy ITCOHORTE the cross-platform runtime for executing and managing robust and reliable distributed Service-oriented Component-based applications, by isandlaTechDX-MAN Service ModelGeneric programming emphasizes separation of algorithms from data representationInterface description languages (IDLs)Open Service Interface Definitions (OSIDs)Part of both COM and CORBAPlatform-Independent Component Modeling LanguageSIDL - Scientific Interface Definition LanguagePart of the Babel Scientific Programming Language Interoperability System (SIDL and Babel are core technologies of the CCA and the SciDAC TASCS Center - see above.)SOAP IDL from World Wide Web Consortium (W3C)WDDXXML-RPC, the predecessor of SOAPInversion of Control (IoC) and Plain Old C++/Java Object (POCO/POJO) component frameworksPipes and filtersUnix operating system== See also ==Business logicModular programmingService Component Architecture (SCA)Software Communications Architecture (JTRS SCA)Third-party software componentWeb serviceWeb components== References ==== Further reading ==Brad J. Cox, Andrew J. Novobilski (1991). Object-Oriented Programming: An Evolutionary Approach. 2nd ed. Addison-Wesley, Reading ISBN 0-201-54834-8Bertrand Meyer (1997). Object-Oriented Software Construction. 2nd ed. Prentice Hall.George T. Heineman, William T. Councill (2001). Component-Based Software Engineering: Putting the Pieces Together. Addison-Wesley Professional, Reading 2001 ISBN 0-201-70485-4Richard Veryard (2001). Component-based business : plug and play. London : Springer. ISBN 1-85233-361-8Clemens Szyperski, Dominik Gruntz, Stephan Murer (2002). Component Software: Beyond Object-Oriented Programming. 2nd ed. ACM Press - Pearson Educational, London 2002 ISBN 0-201-74572-0== External links ==Why Software Reuse has Failed and How to Make It Work for You by Douglas C. SchmidtWhat is the True essence and reality of CBD? (Evidence to show existing CBD paradigm is flawed)comprehensive list of Component Systems on SourceForgeBrief Introduction to Real COP (Component Oriented Programming) by Using a small GUI application as an example
	In software engineering, software system safety optimizes system safety in the design, development, use, and maintenance of software systems and their integration with safety-critical hardware systems in an operational environment.== Overview ==Software system safety is a subset of system safety and system engineering and is synonymous with the software engineering aspects of Functional Safety. As part of the total safety and software development program, software cannot be allowed to function independently of the total effort. Both simple and highly integrated multiple systems are experiencing an extraordinary growth in the use of computers and software to monitor and/or control safety-critical subsystems or functions. A software specification error, design flaw, or the lack of generic safety-critical requirements can contribute to or cause a system failure or erroneous human decision. To achieve an acceptable level of safety for software used in critical applications, software system safety engineering must be given primary emphasis early in the requirements definition and system conceptual design process. Safety-critical software must then receive continuous management emphasis and engineering analysis throughout the development and operational lifecycles of the system. Functional Hazard Analyses (FHA) are often conducted early on - in parallel with or as part of system engineering Functional Analyses - to determine the safety-critical functions (SCF) of the systems for further analyses and verification. Software system safety is directly related to the more critical design aspects and safety attributes in software and system functionality, whereas software quality attributes are inherently different and require standard scrutiny and development rigor. Development Assurance levels (DAL) and associated Level of Rigor (LOR) is a graded approach to software quality and software design assurance as a pre-requisite that a suitable software process is followed for confidence. LOR concepts and standards such as DO-178C are NOT a substitute for software safety. Software safety per IEEE STD-1228 and MIL-STD-882E focuses on ensuring explicit safety requirements are met and verified using functional approaches from a safety requirements analysis and test perspective. Software safety hazard analysis required for more complex systems where software is controlling critical functions generally are in the following sequential categories and are conducted in phases as part of the system safety or safety engineering process: software safety requirements analysis; software safety design analyses (top level, detailed design and code level); software safety test analysis, and software safety change analysis. Once these "functional" software safety analyses are completed the software engineering team will know where to place safety emphasis and what functional threads, functional paths, domains and boundaries to focus on when designing in software safety attributes to ensure correct functionality and to detect malfunctions, failures, faults and to implement a host of mitigation strategies to control hazards. Software security and various software protection technologies are similar to software safety attributes in the design to mitigate various types of threats vulnerability and risks. Deterministic software is sought in the design by verifying correct and predictable behavior at the system level.== Goals ==Functional safety is achieved through engineering development to ensure correct execution and behavior of software functions as intendedSafety consistent with mission requirements, is designed into the software in a timely, cost effective manner.On complex systems involving many interactions safety-critical functionality should be identified and thoroughly analyzed before deriving hazards and design safeguards for mitigations.Safety-critical functions lists and preliminary hazards lists should be determined proactively and influence the requirements that will be implemented in software.Contributing factors and root causes of faults and resultant hazards associated with the system and its software are identified, evaluated and eliminated or the risk reduced to an acceptable level, throughout the lifecycle.Reliance on administrative procedures for hazard control is minimized.The number and complexity of safety critical interfaces is minimized.The number and complexity of safety critical computer software components is minimized.Sound human engineering principles are applied to the design of the software-user interface to minimize the probability of human error.Failure modes, including hardware, software, human and system are addressed in the design of the software.Sound software engineering practices and documentation are used in the development of the software.Safety issues and safety attributes are addressed as part of the software testing effort at all levels.Software is designed for human machine interface, ease of maintenance and modification or enhancementSoftware with safety-critical functionality must be thoroughly verified with objective analysis and preferably test evidence that all safety requirements have been met per established criteria.This article incorporates public domain material from the United States Government document Joint Software System Safety CommitteeSOFTWARE SYSTEM SAFETY HANDBOOK A Technical & Managerial Team Approach This document was originally obtained from the web side "http://www.monmouth.army.mil/cecom/safety/sys_service/". which is now a dead link since this base closed in 2011. A PDF of the document is available at http://www.system-safety.org/Documents/Software_System_Safety_Handbook.pdf 2.15MB. In addition, an updated version can be obtained from: https://www.acq.osd.mil/se/docs/Joint-SW-Systems-Safety-Engineering-Handbook.pdf 4.6MB.== See also ==Software assuranceIEC 61508 Functional SafetyIEEE STD-1228 http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=467427&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel1%2F3257%2F9808%2F00467427]]System accident
	A System Requirements Specification (SyRS) (abbreviated SysRS when need to be distinct from a software requirements specification (SRS) ) is a structured collection of information that embodies the requirements of a system.A business analyst, sometimes titled system analyst, is responsible for analyzing the business needs of their clients and stakeholders to help identify business problems and propose solutions. Within the systems development life cycle domain, the BA typically performs a liaison function between the business side of an enterprise and the information technology department or external service providers.== See also ==RequirementRequirements analysisHardware requirements specificationSoftware requirements specificationBusiness requirements engineeringConcept of operationsBusiness requirementsBusiness process reengineeringSystems analysisBusiness analysisInformation technologyUse caseProcess modelingData modelingCollege management== References ==== External links ==IEEE Guide for Developing System Requirements Specifications (IEEE Std 1233, 1999 Edition)IEEE Guide for Developing System Requirements Specifications (IEEE Std 1233, 1998 Edition)DAU description System/Subsystem Specification, Data Item Description (SSS-DID)System Requirements Specification for STEWARDS example SRS at USDA
	A mixed criticality system is a system containing computer hardware and software that can execute several applications of different criticality, such as safety-critical and non-safety critical, or of different Safety Integrity Level (SIL). Different criticality applications are engineered to different levels of assurance, with high criticality applications being the most costly to design and verify. These kinds of systems are typically embedded in a machine such as an aircraft whose safety must be ensured.== Principle ==Traditional safety-critical systems had to be tested and certified in their entirety to show that they were safe to use. However, many such systems are composed of a mixture of safety-critical and non-critical parts, as for example when an aircraft contains a passenger entertainment system that is isolated from the safety-critical flight systems. Some issues to address in mixed criticality systems include real-time behaviour, memory isolation, data and control coupling.Computer scientists have developed techniques for handling systems which thus have mixed criticality, but there are many challenges remaining especially for multi-core hardware.== Priority and Criticality ==Basically, most errors are currently committed when making confusion between priority attribution and criticality management. As priority defines an order between different tasks or messages to be transmitted inside a system, criticality define classes of messages which can have different parameters depending on the current use case. For example, in case of car crash avoidance or obstacle anticipation, camera sensors can suddenly emit messages more often, and so create an overload in the system. That is when we need to make Mixed-Criticality operate : to select messages to absolutely guarantee on the system in these overload cases.== Research projects ==EU funded research projects on mixed criticality include:MultiPARTESDREAMSPROXIMACONTREXSAFURECERTAINTYVIRTICALT-CRESTPROARTISACROSS (Artemis)EMC2 (Artemis)RECOMP ArtemisARAMIS (in German) and ARAMIS IIIMPReSSUK EPSRC funded research projects on mixed criticality include:MCCSeveral research projects have decided to present their research results at the EU-funded Mixed-Criticality Forum== Workshops and Seminars ==Workshops and seminars on Mixed Criticality Systems include:1st International Workshop on Mixed Criticality Systems (WMC 2013)2nd International Workshop on Mixed Criticality Systems (WMC 2014)3rd International Workshop on Mixed Criticality Systems (WMC 2015)4th International Workshop on Mixed Criticality Systems (WMC 2015)Dagstuhl Seminar on Mixed Criticality on Multicore/Manycore Platforms (2015)Dagstuhl Seminar on Mixed Criticality on Multicore/Manycore Platforms (2017)== References ==== External links ==Karlsruhe Institute of Technology: Mixed Criticality in Safety-Critical SystemsWashington University in St Louis: A Research Agenda for Mixed-Criticality Systems
	Software engineering is the systematic application of engineering approaches to the development of software.Notable definitions of software engineering include: "the systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software"—The Bureau of Labor Statistics—IEEE Systems and software engineering - Vocabulary"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software"—IEEE Standard Glossary of Software Engineering Terminology"an engineering discipline that is concerned with all aspects of software production"—Ian Sommerville"the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines"—Fritz Bauer"a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs"The term has also been used less formally:as the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;as the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;as the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.== History ==When the first digital computers appeared in the early 1940s, the instructions to make them operate were wired into the machine. Practitioners quickly realized that this design was not flexible and came up with the "stored program architecture" or von Neumann architecture. Thus the division between "hardware" and "software" began with abstraction being used to deal with the complexity of computing.Programming languages started to appear in the early 1950s and this was also another major step in abstraction. Major languages such as Fortran, ALGOL, and COBOL were released in the late 1950s to deal with scientific, algorithmic, and business problems respectively. David Parnas introduced the key concept of modularity and information hiding in 1972 to help programmers deal with the ever-increasing complexity of software systems.The origins of the term "software engineering" have been attributed to various sources. The term "software engineering" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger;,  it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering. Independently, Margaret Hamilton named the discipline "software engineering" during the Apollo missions to give what they were doing legitimacy.  At the time there was perceived to be a "software crisis". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of "Software Engineering" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton.In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).== Subdisciplines ==Software engineering can be divided into sub-disciplines. Some of them are:Software requirements (or Requirements engineering): The elicitation, analysis, specification, and validation of requirements for software.Software design: The process of defining the architecture, components, interfaces, and other characteristics of a system or component. It is also defined as the result of that process.Software construction: The detailed creation of working, meaningful software through a combination of programming (aka coding), verification, unit testing, integration testing, and debugging.Software testing: An empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test.Software maintenance: The totality of activities required to provide cost-effective support to software.Software configuration management: The identification of the configuration of a system at distinct points in time for the purpose of systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration throughout the system life cycle. Modern processes use software versioning.Software engineering management: The application of management activities—planning, coordinating, measuring, monitoring, controlling, and reporting—to ensure that the development and maintenance of software is systematic, disciplined, and quantified.Software development process: The definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself.Software engineering models and methods impose structure on software engineering with the goal of making that activity systematic, repeatable, and ultimately more success-orientedSoftware qualitySoftware engineering professional practice is concerned with the knowledge, skills, and attitudes that software engineers must possess to practice software engineering in a professional, responsible, and ethical mannerSoftware engineering economics is about making decisions related to software engineering in a business contextComputing foundationsMathematical foundationsEngineering foundations== Education ==Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2004, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.== Profession ==Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.The United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge - 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a "Software Engineering Code of Ethics".=== Employment ===The U. S. Bureau of Labor Statistics counted 1,256,200 software Developers (Engineers) holding jobs in the U.S. in 2016. Employment of computer and information technology occupations is projected to grow 13 percent from 2016 to 2026, faster than the average for all occupations. These occupations are projected to add about 557,100 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security. Yet, the BLS also says some employment in these occupations are slowing and computer programmers is projected to decline 7 percent from 2016 to 2026 since computer programming can be done from anywhere in the world, so companies sometimes hire programmers in countries where wages are lower. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry (analysts, architects, developers, testers, technical support, middleware analysts, managers) and in academia (educators, researchers).Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.=== Certification ===The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.Broader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.=== Impact of globalization ===The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / timezone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.While global outsourcing has several advantages, global - and generally distributed - development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.== Related fields ==Software engineering is a direct sub-field of engineering and has an overlap with computer science and management science. It is also considered a part of overall systems engineering.=== Computer Science ===Computer science focuses on the high-level aspects of computing and computer systems, such as the study of algorithms that process, store, and communicate digital information. Its fields can be divided into a variety of theoretical and practical disciplines, which include the study of fundamental properties of computational and intractable problems, and the application of software development techniques to real-world situations.Software engineering, on the other hand, focuses on techniques for the application of software development in industry, largely eschewing the abstract and high-level reasoning supporting such techniques.== Controversy ===== Criticism ===Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.Software craftsmanship has been proposed by a body of software developers as an alternative that emphasizes the coding skills and accountability of the software developers themselves without professionalism or any prescribed curriculum leading to ad-hoc problem-solving (craftmanship) without engineering (lack of predictability, precision, missing risk mitigation, methods are informal and poorly defined). The Software Craftsmanship Manifesto extends the Agile Software Manifesto and draws a metaphor between modern software development and the apprenticeship model of medieval Europe.Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.One of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."Edsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for whathe called the "radical novelty" of computer science:A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot."== See also ==Bachelor of Science in Information TechnologyBachelor of Software EngineeringList of software engineering conferencesList of software engineering publicationsSoftware craftsmanshipSoftware developmentSoftware Engineering Institute== Notes ==== References ==Abran, Alain; Moore, James W.; Bourque, Pierre; Dupuis, Robert; Tripp, Leonard L. (2004). Guide to the Software Engineering Body of Knowledge. IEEE. ISBN 978-0-7695-2330-9.Sommerville, Ian (2008). Software Engineering (7 ed.). Pearson Education. ISBN 978-81-7758-530-8. Retrieved 10 January 2013.== Further reading ==Pressman, Roger S (2009). Software Engineering: A Practitioner's Approach (7th ed.). Boston, Mass: McGraw-Hill. ISBN 978-0073375977.Sommerville, Ian (2010) [2010]. Software Engineering (9th ed.). Harlow, England: Pearson Education. ISBN 978-0137035151.Jalote, Pankaj (2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7.Bruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0136061250.Oshana, Robert. Software engineering for embedded systems : methods, practical techniques, and applications (Secondition ed.). Kidlington, Oxford, United Kingdom :. ISBN 9780128094334.CS1 maint: extra punctuation (link)== External links ==Guide to the Software Engineering Body of KnowledgeThe Open Systems Engineering and Software Development Life Cycle Framework OpenSDLC.org the integrated Creative Commons SDLCSoftware Engineering Institute Carnegie Mellon
	In software engineering, software configuration management (SCM or S/W CM) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine what was changed and who changed it.  If a configuration is working well, SCM can determine how to replicate it across many hosts.The acronym "SCM" is also expanded as source configuration management process and software change and configuration management.  However, "configuration" is generally understood to cover changes typically made by a system administrator.== Purposes ==The goals of SCM are generally:Configuration identification - Identifying configurations, configuration items and baselines.Configuration control - Implementing a controlled change process. This is usually achieved by setting up a change control board whose primary function is to approve or reject all change requests that are sent against any baseline.Configuration status accounting - Recording and reporting all the necessary information on the status of the development process.Configuration auditing - Ensuring that configurations contain all their intended parts and are sound with respect to their specifying documents, including requirements, architectural specifications and user manuals.Build management - Managing the process and tools used for builds.Process management - Ensuring adherence to the organization's development process.Environment management - Managing the software and hardware that host the system.Teamwork - Facilitate team interactions related to the process.Defect tracking - Making sure every defect has traceability back to the source.With the introduction of cloud computing the purposes of SCM tools have become merged in some cases.  The SCM tools themselves have become virtual appliances that can be instantiated as virtual machines and saved with state and version.  The tools can model and manage cloud-based virtual resources, including virtual appliances, storage units, and software bundles.  The roles and responsibilities of the actors have become merged as well with developers now being able to dynamically instantiate virtual servers and related resources.== History ==The history of software configuration management (SCM) in computing can be traced back as early as the 1950s, when CM (for Configuration Management), originally for hardware development and production control, was being applied to software development. Early software had a physical footprint, such as cards, tapes, and other media. The first software configuration management was a manual operation. With the advances in language and complexity, software engineering, involving configuration management and other methods, became a major concern due to issues like schedule, budget, and quality. Practical lessons, over the years, had led to the definition, and establishment, of procedures and tools. Eventually, the tools became systems to manage software changes. Industry-wide practices were offered as solutions, either in an open or proprietary manner (such as Revision Control System). With the growing use of computers, systems emerged that handled a broader scope, including requirements management, design alternatives, quality control, and more; later tools followed the guidelines of organizations, such as the Capability Maturity Model of the Software Engineering Institute.== See also ==Application lifecycle managementComparison of open source configuration management softwareComparison of version control softwareContinuous configuration automationList of revision control softwareInfrastructure as Code== References ==== Further reading ==828-2012 IEEE Standard for Configuration Management in Systems and Software Engineering. 2012. doi:10.1109/IEEESTD.2012.6170935. ISBN 978-0-7381-7232-3.Aiello, R. (2010). Configuration Management Best Practices: Practical Methods that Work in the Real World (1st ed.). Addison-Wesley. ISBN 0-321-68586-5.Babich, W.A. (1986). Software Configuration Management, Coordination for Team Productivity. 1st edition. Boston: Addison-WesleyBerczuk, Appleton; (2003). Software Configuration Management Patterns: Effective TeamWork, Practical Integration (1st ed.). Addison-Wesley. ISBN 0-201-74117-2.Bersoff, E.H. (1997). Elements of Software Configuration Management. IEEE Computer Society Press, Los Alamitos, CA, 1-32Dennis, A., Wixom, B.H. & Tegarden, D. (2002). System Analysis & Design: An Object-Oriented Approach with UML. Hoboken, New York: John Wiley & Sons, Inc.Department of Defense, USA (2001). Military Handbook: Configuration management guidance (rev. A) (MIL-HDBK-61A). Retrieved January 5, 2010, from http://www.everyspec.com/MIL-HDBK/MIL-HDBK-0001-0099/MIL-HDBK-61_11531/Futrell, R.T. et al. (2002). Quality Software Project Management. 1st edition. Prentice-Hall.International Organization for Standardization (2003). ISO 10007: Quality management systems – Guidelines for configuration management.Saeki M. (2003). Embedding Metrics into Information Systems Development Methods: An Application of Method Engineering Technique. CAiSE 2003, 374-389.Scott, J.A. & Nisse, D. (2001). Software configuration management. In: Guide to Software Engineering Body of Knowledge. Retrieved January 5, 2010, from http://www.computer.org/portal/web/swebok/htmlformatPaul M. Duvall, Steve Matyas, and Andrew Glover (2007). Continuous Integration: Improving Software Quality and Reducing Risk. (1st ed.). Addison-Wesley Professional. ISBN 0-321-33638-0.== External links ==SCM and ISO 9001 by Robert Bamford and William Deibler, SSQCUse Cases and Implementing Application Lifecycle Management Parallel Development Strategies for Software Configuration Management
	A view model or viewpoints framework in systems engineering, software engineering, and enterprise engineering is a framework which defines a coherent set of views to be used in the construction of a system architecture, software architecture, or  enterprise architecture. A view is a representation of a whole system from the perspective of a related set of concerns.Since the early 1990s there have been a number of efforts to prescribe approaches for describing and analyzing system architectures. These recent efforts define a set of views (or viewpoints). They are sometimes referred to as architecture frameworks or enterprise architecture frameworks, but are usually called "view models".Usually a view is a work product that presents specific architecture data for a given system. However, the same term is sometimes used to refer to a view definition, including the particular viewpoint and the corresponding guidance that defines each concrete view. The term view model is related to view definitions.== Overview ==The purpose of views and viewpoints is to enable humans to comprehend very complex systems, to organize the elements of the problem and the solution around domains of expertise and to separate concerns. In the engineering of physically intensive systems, viewpoints often correspond to capabilities and responsibilities within the engineering organization.Most complex system specifications are so extensive that no single individual can fully comprehend all aspects of the specifications. Furthermore, we all have different interests in a given system and different reasons for examining the system's specifications. A business executive will ask different questions of a system make-up than would a system implementer. The concept of viewpoints framework, therefore, is to provide separate viewpoints into the specification of a given complex system in order to facilitate communication with the stakeholders. Each viewpoint satisfies an audience with interest in a particular set of aspects of the system. Each viewpoint may use a specific viewpoint language that optimizes the vocabulary and presentation for the audience of that viewpoint. Viewpoint modeling has become an effective approach for dealing with the inherent complexity of large distributed systems.Architecture description practices, as described in IEEE Std 1471-2000, utilize multiple views to address several areas of concerns, each one focusing on a specific aspect of the system. Examples of architecture frameworks using multiple views include Kruchten's "4+1" view model, the Zachman Framework, TOGAF, DoDAF, and RM-ODP.== History ==In the 1970s, methods began to appear in software engineering for modeling with multiple views. Douglas T. Ross and K.E. Schoman in 1977 introduce the constructs context, viewpoint, and vantage point to organize the modeling process in systems requirements definition. According to Ross and Schoman, a viewpoint "makes clear what aspects are considered relevant to achieving ... the overall purpose [of the model]" and determines How do we look at [a subject being modelled]?As examples of viewpoints, the paper offers: Technical, Operational and Economic viewpoints. In 1992, Anthony Finkelstein and others published a very important paper on viewpoints. In that work: "A viewpoint can be thought of as a combination of the idea of an “actor”, “knowledge source”, “role” or “agent” in the development process and the idea of a “view” or “perspective” which an actor maintains." An important idea in this paper was to distinguish "a representation style, the scheme and notation by which the viewpoint expresses what it can see" and "a specification, the statements expressed in the viewpoint's style describing particular domains". Subsequent work, such as IEEE 1471, preserved this distinction by utilizing two separate terms: viewpoint and view, respectively.Since the early 1990s there have been a number of efforts to codify approaches for describing and analyzing system architectures. These are often termed architecture frameworks or sometimes viewpoint sets. Many of these have been funded by the United States Department of Defense, but some have sprung from international or national efforts in ISO or the IEEE. Among these, the IEEE Recommended Practice for Architectural Description of Software-Intensive Systems (IEEE Std 1471-2000) established useful definitions of view, viewpoint, stakeholder and concern and guidelines for documenting a system architecture through the use of multiple views by applying viewpoints to address stakeholder concerns. The advantage of multiple views is that hidden requirements and stakeholder disagreements can be discovered more readily. However, studies show that in practice, the added complexity of reconciling multiple views can undermine this advantage.IEEE 1471 (now ISO/IEC/IEEE 42010:2011, Systems and software engineering — Architecture description) prescribes the contents of architecture descriptions and describes their creation and use under a number of scenarios, including precedented and unprecedented design, evolutionary design, and capture of design of existing systems. In all of these scenarios the overall process is the same: identify stakeholders, elicit concerns, identify a set of viewpoints to be used, and then apply these viewpoint specifications to develop the set of views relevant to the system of interest. Rather than define a particular set of viewpoints, the standard provides uniform mechanisms and requirements for architects and organizations to define their own viewpoints. In 1996 the ISO Reference Model for Open Distributed Processing (RM-ODP) was published to provide a useful framework for describing the architecture and design of large-scale distributed systems.== View model topics ===== View ===A view of a system is a representation of the system from the perspective of a viewpoint. This viewpoint on a system involves a perspective focusing on specific concerns regarding the system, which suppresses details to provide a simplified model having only those elements related to the concerns of the viewpoint. For example, a security viewpoint focuses on security concerns and a security viewpoint model contains those elements that are related to security from a more general model of a system.A view allows a user to examine a portion of a particular interest area. For example, an Information View may present all functions, organizations, technology, etc. that use a particular piece of information, while the Organizational View may present all functions, technology, and information of concern to a particular organization. In the Zachman Framework views comprise a group of work products whose development requires a particular analytical and technical expertise because they focus on either the “what,” “how,” “who,” “where,” “when,” or “why” of the enterprise.  For example, Functional View work products answer the question “how is the mission carried out?”  They are most easily developed by experts in functional decomposition using process and activity modeling.  They show the enterprise from the point of view of functions.  They also may show organizational and information components, but only as they relate to functions.=== Viewpoints ===In systems engineering, a viewpoint is a partitioning or restriction of concerns in a system. Adoption of a viewpoint is usable so that issues in those aspects can be addressed separately. A good selection of viewpoints also partitions the design of the system into specific areas of expertise.Viewpoints provide the conventions, rules, and languages for constructing, presenting and analysing views. In ISO/IEC 42010:2007 (IEEE-Std-1471-2000) a viewpoint is a specification for an individual view. A view is a representation of a whole system from the perspective of a viewpoint. A view may consist of one or more architectural models. Each such architectural model is developed using the methods established by its associated architectural system, as well as for the system as a whole.=== Modeling perspectives ===Modeling perspectives is a set of different ways to represent pre-selected aspects of a system. Each perspective has a different focus, conceptualization, dedication and visualization of what the model is representing.In information systems, the traditional way to divide modeling perspectives is to distinguish the structural, functional and behavioral/processual perspectives. This together with rule, object, communication and actor and role perspectives is one way of classifying modeling approaches === Viewpoint model ===In any given viewpoint, it is possible to make a model of the system that contains only the objects that are visible from that viewpoint, but also captures all of the objects, relationships and constraints that are present in the system and relevant to that viewpoint. Such a model is said to be a viewpoint model, or a view of the system from that viewpoint.A given view is a specification for the system at a particular level of abstraction from a given viewpoint. Different levels of abstraction contain different levels of detail. Higher-level views allow the engineer to fashion and comprehend the whole design and identify and resolve problems in the large. Lower-level views allow the engineer to concentrate on a part of the design and develop the detailed specifications.In the system itself, however, all of the specifications appearing in the various viewpoint models must be addressed in the realized components of the system. And the specifications for any given component may be drawn from many different viewpoints. On the other hand, the specifications induced by the distribution of functions over specific components and component interactions will typically reflect a different partitioning of concerns than that reflected in the original viewpoints. Thus additional viewpoints, addressing the concerns of the individual components and the bottom-up synthesis of the system, may also be useful.=== Architecture description ===An architecture description is a representation of a system architecture, at any time, in terms of its component parts, how those parts function, the rules and constraints under which those parts function, and how those parts relate to each other and to the environment. In an architecture description the architecture data is shared across several views and products.At the data layer are the architecture data elements and their defining attributes and relationships. At the presentation layer are the products and views that support a visual means to communicate and understand the purpose of the architecture, what it describes, and the various architectural analyses performed. Products provide a way for visualizing architecture data as graphical, tabular, or textual representations. Views provide the ability to visualize architecture data that stem across products, logically organizing the data for a specific or holistic perspective of the architecture.== Types of system view models ===== Three-schema approach ===The Three-schema approach for data modeling, introduced in 1977, can be considered one of the first view models. It is an approach to building information systems and systems information management, that promotes the conceptual model as the key to achieving data integration. The Three schema approach defines three schemas and views:External schema for user viewsConceptual schema integrates external schemataInternal schema that defines physical storage structuresAt the center, the conceptual schema defines the ontology of the concepts as the users think of them and talk about them. The physical schema describes the internal formats of the data stored in the database, and the external schema defines the view of the data presented to the application programs. The framework attempted to permit multiple data models to be used for external schemata.Over the years, the skill and interest in building information systems has grown tremendously. However, for the most part, the traditional approach to building systems has only focused on defining data from two distinct views, the "user view" and the "computer view". From the user view, which will be referred to as the “external schema,” the definition of data is in the context of reports and screens designed to aid individuals in doing their specific jobs. The required structure of data from a usage view changes with the business environment and the individual preferences of the user. From the computer view, which will be referred to as the “internal schema,” data is defined in terms of file structures for storage and retrieval. The required structure of data for computer storage depends upon the specific computer technology employed and the need for efficient processing of data.=== 4+1 view model of architecture ===4+1 is a view model designed by Philippe Kruchten in 1995 for describing the architecture of software-intensive systems, based on the use of multiple, concurrent views. The views are used to describe the system in the viewpoint of different stakeholders, such as end-users, developers and project managers. The four views of the model are logical, development, process and physical view:The four views of the model are concerned with :Logical view: is concerned with the functionality that the system provides to end-users.Development view: illustrates a system from a programmers perspective and is concerned with software management.Process view: deals with the dynamic aspect of the system, explains the system processes and how they communicate, and focuses on the runtime behavior of the system.Physical view: depicts the system from a system engineer's point of view.  It is concerned with the topology of software components on the physical layer, as well as communication between these components.In addition selected use cases or scenarios are utilized to illustrate the architecture. Hence the model contains 4+1 views.== Types of enterprise architecture views ==Enterprise architecture framework defines how to organize the structure and views associated with an enterprise architecture. Because the discipline of Enterprise Architecture and Engineering is so broad, and because enterprises can be large and complex, the models associated with the discipline also tend to be large and complex. To manage this scale and complexity, an Architecture Framework provides tools and methods that can bring the task into focus and allow valuable artifacts to be produced when they are most needed.Architecture Frameworks are commonly used in Information technology and Information system governance. An organization may wish to mandate that certain models be produced before a system design can be approved. Similarly, they may wish to specify certain views be used in the documentation of procured systems - the U.S. Department of Defense stipulates that specific DoDAF views be provided by equipment suppliers for capital project above a certain value.=== Zachman Framework ===The Zachman Framework, originally conceived by John Zachman at IBM in 1987, is a framework for enterprise architecture, which provides a formal and highly structured way of viewing and defining an enterprise.The Framework is used for organizing architectural "artifacts" in a way that takes into account both who the artifact targets (for example, business owner and builder) and what particular issue (for example, data and functionality) is being addressed. These artifacts may include design documents, specifications, and models.The Zachman Framework is often referenced as a standard approach for expressing the basic elements of enterprise architecture. The Zachman Framework has been recognized by the U.S. Federal Government as having "... received worldwide acceptance as an integrated framework for managing change in enterprises and the systems that support them."=== RM-ODP views ===The International Organization for Standardization (ISO) Reference Model for Open Distributed Processing (RM-ODP)  specifies a set of viewpoints for partitioning the design of a distributed software/hardware system. Since most integration problems arise in the design of such systems or in very analogous situations, these viewpoints may prove useful in separating integration concerns. The RMODP viewpoints are:the enterprise viewpoint, which is concerned with the purpose and behaviors of the system as it relates to the business objective and the business processes of the organizationthe information viewpoint, which is concerned with the nature of the information handled by the system and constraints on the use and interpretation of that informationthe computational viewpoint, which is concerned with the functional decomposition of the system into a set of components that exhibit specific behaviors and interact at interfacesthe engineering viewpoint, which is concerned with the mechanisms and functions required to support the interactions of the computational componentsthe technology viewpoint, which is concerned with the explicit choice of technologies for the implementation of the system, and particularly for the communications among the componentsRMODP further defines a requirement for a design to contain specifications of consistency between viewpoints, including:the use of enterprise objects and processes in defining information unitsthe use of enterprise objects and behaviors in specifying the behaviors of computational components, and use of the information units in defining computational interfacesthe association of engineering choices with computational interfaces and behavior requirementsthe satisfaction of information, computational and engineering requirements in the chosen technologies=== DoDAF views ===The Department of Defense Architecture Framework (DoDAF) defines a standard way to organize an enterprise architecture (EA) or systems architecture into complementary and consistent views. It is especially suited to large systems with complex integration and interoperability challenges, and is apparently unique in its use of "operational views" detailing the external customer's operating domain in which the developing system will operate.The DoDAF defines a set of products that act as mechanisms for visualizing, understanding,and assimilating the broad scope and complexities of an architecture description through graphic,tabular, or textual means. These products are organized under four views: Overarching All View (AV),Operational View (OV),Systems View (SV), and theTechnical Standards View (TV).Each view depicts certain perspectives of an architecture as described below. Only a subset of the full DoDAF viewset is usually created for each system development. The figure represents the information that links the operational view, systems and services view, and technical standards view. The three views and their interrelationships driven – by common architecture data elements – provide the basis for deriving measures such as interoperability or performance, and for measuring the impact of the values of these metrics on operational mission and task effectiveness.=== Federal Enterprise Architecture views ===In the US Federal Enterprise Architecture enterprise, segment, and solution architecture provide different business perspectives by varying the level of detail and addressing related but distinct concerns. Just as enterprises are themselves hierarchically organized, so are the different views provided by each type of architecture. The Federal Enterprise Architecture Practice Guidance (2006) has defined three types of architecture:Enterprise architecture,Segment architecture, andSolution architecture.By definition, Enterprise Architecture (EA) is fundamentally concerned with identifying common or shared assets – whether they are strategies, business processes, investments, data, systems, or technologies. EA is driven by strategy; it helps an agency identify whether its resources are properly aligned to the agency mission and strategic goals and objectives. From an investment perspective, EA is used to drive decisions about the IT investment portfolio as a whole. Consequently, the primary stakeholders of the EA are the senior managers and executives tasked with ensuring the agency fulfills its mission as effectively and efficiently as possible.By contrast, segment architecture defines a simple roadmap for a core mission area, business service, or enterprise service. Segment architecture is driven by business management and delivers products that improve the delivery of services to citizens and agency staff. From an investment perspective, segment architecture drives decisions for a business case or group of business cases supporting a core mission area or common or shared service. The primary stakeholders for segment architecture are business owners and managers. Segment architecture is related to EA through three principles: structure, reuse, and alignment. First, segment architecture inherits the framework used by the EA, although it may be extended and specialized to meet the specific needs of a core mission area or common or shared service. Second, segment architecture reuses important assets defined at the enterprise level including: data; common business processes and investments; and applications and technologies. Third, segment architecture aligns with elements defined at the enterprise level, such as business strategies, mandates, standards, and performance measures.=== Nominal set of views ===In search of "Framework for Modeling Space Systems Architectures" Peter Shames and Joseph Skipper (2006) defined a "nominal set of views", Derived from CCSDS RASDS, RM-ODP, ISO 10746 and compliant with IEEE 1471.This "set of views", as described below, is a listing of possible modeling viewpoints. Not all of these views may be used for any one project and other views may be defined as necessary. Note that for some analyses elements from multiple viewpoints may be combined into a new view, possibly using a layered representation.In a latter presentation this nominal set of views was presented as an Extended RASDS Semantic Information Model Derivation. Hereby RASDS stands for Reference Architecture for Space Data Systems. see second image.Enterprise ViewpointOrganization view – Includes organizational elements and their structures and relationships. May include agreements, contracts, policies and organizational interactions.Requirements view – Describes the requirements, goals, and objectives that drive the system. Says what the system must be able to do.Scenario view – Describes the way that the system is intended to be used, see scenario planning. Includes user views and descriptions of how the system is expected to behave.Information viewpointMetamodel view – An abstract view that defines information model elements and their structures and relationships. Defines the classes of data that are created and managed by the system and the data architecture.Information view – Describes the actual data and information as it is realized and manipulated within the system. Data elements are defined by the metamodel view and they are referred to by functional objects in other views.Functional viewpointFunctional Dataflow view – An abstract view that describes the functional elements in the system, their interactions, behavior, provided services, constraints and data flows among them. Defines which functions the system is capable of performing, regardless of how these functions are actually implemented.Functional Control view – Describes the control flows and interactions among functional elements within the system. Includes overall system control interactions, interactions between control elements and sensor / effector elements and management interactions.Physical viewpointData System view – Describes instruments, computers, and data storage components, their data system attributes and the communications connectors (busses, networks, point to point links) that are used in the system.Telecomm view – Describes the telecomm components (antenna, transceiver), their attributes and their connectors (RF or optical links).Navigation view – Describes the motion of the major elements within the system (trajectory, path, orbit), including their interaction with external elements and forces that are outside of the control of the system, but that must be modeled with it to understand system behavior (planets, asteroids, solar pressure, gravity)Structural view – Describes the structural components in the system (s/c bus, struts, panels, articulation), their physical attributes and connectors, along with the relevant structural aspects of other components (mass, stiffness, attachment)Thermal view – Describes the active and passive thermal components in the system (radiators, coolers, vents) and their connectors (physical and free space radiation) and attributes, along with the thermal properties of other components (i.e. antenna as sun shade)Power view – Describes the active and passive power components in the system (solar panels, batteries, RTGs) within the system and their connectors, along with the power properties of other components (data system and propulsion elements as power sinks and structural panels as grounding plane)Propulsion view – Describes the active and passive propulsion components in the system (thrusters, gyros, motors, wheels) within the system and their connectors, along with the propulsive properties of other components Engineering viewpointAllocation view – Describes the allocation of functional objects to engineered physical and computational components within the system, permits analysis of performance and used to verify satisfaction of requirementsSoftware view - Describes the software engineering aspects of the system, software design and implementation of functionality within software components, select languages and libraries to be used, define APIs, do the engineering of abstract functional objects into tangible software elements. Some functional elements, described using a software language, may actually be implemented as hardware (FPGA, ASIC)Hardware views – Describes the hardware engineering aspects of the system, hardware design, selection and implementation of all of the physical components to be assembled into the system. There may be many of these views, each specific to a different engineering discipline.Communications Protocol view – Describes the end to end design of the communications protocols and related data transport and data management services, shows the protocol stacks as they are implemented on each of the physical components of the system.Risk view – Describes the risks associated with the system design, processes, and technologies, assigns additional risk assessment attributes to other elements described in the architectureControl Engineering view - Analyzes system from the perspective of its controllability, allocation of elements into system under control and control systemIntegration and Test view – Looks at the system from the perspective of what must be done to assemble, integrate and test system and sub-systems, and assemblies. Includes verification of proper functionality, driven by scenarios, in satisfaction of requirements.IV&V view – independent validation and verification of functionality and proper operation of the system in satisfaction of requirements. Does system as designed and developed meet goals and objectives.Technology viewpointStandards view – Defines the standards to be adopted during design of the system (e.g. communication protocols, radiation tolerance, soldering). These are essentially constraints on the design and implementation processes.Infrastructure view – Defines the infrastructure elements that are to support the engineering, design, and fabrication process. May include data system elements (design repositories, frameworks, tools, networks) and hardware elements (chip fabrication, thermal vacuum facility, machine shop, RF testing lab)Technology Development & Assessment view – Includes description of technology development programs designed to produce algorithms or components that may be included in a system development project. Includes evaluation of properties of selected hardware and software components to determine if they are at a sufficient state of maturity to be adopted for the mission being designed.In contrast to the previous listed view models, this "nominal set of views" lists a whole range of  views, possible to develop powerful and extensible approaches for describing a general class of software intensive system architectures.== See also ==Enterprise Architecture frameworkOrganizational architectureSoftware development methodologyTreasury Enterprise Architecture FrameworkTOGAFZachman FrameworkOntology (information science)Knowledge Acquisition== References ==Attribution This article incorporates public domain material from the National Institute of Standards and Technology website https://www.nist.gov.== External links == Media related to View models at Wikimedia Commons
	Software engineers form part of the workforce around the world. As of 2016, it is estimated that there are 21 million professional software developers.== United States ==As of 2016, it was estimated that 3.87 million professional software developers worked in the US out of a total employed workforce of 152 million (2.54%).=== Summary ===Based on data from the U.S. Bureau of Labor Statistics from 2002, about 612,000 software engineers worked in the U.S. - about one out of every 200 workers. There were 55% to 60% as many software engineers as all traditional engineers. This comparison holds whether one compares the number of practitioners, managers, educators, or technicians/programmers. Software engineering had 612,000 practitioners; 264,790 managers, 16,495 educators, and 457,320 programmers.=== Software Engineers Versus Traditional Engineers ===The following two tables compare the number of software engineers (611,900), versus the number of traditional engineers (1,157,020). The ratio is 53%.There are another 1,500,000 people in system analysis, system administration, and computer support, many of whom might be called software engineers. Many systems analysts manage software development teams and analysis is an important software engineering role, so many of them might be considered software engineers in the near future. This means that the number of software engineers may actually be much higher.Note also that the number of software engineers declined by 5% to 10% from 2000 to 2002.=== Computer Managers Versus Construction and Engineering Managers ===Computer and information system managers (264,790) manage software projects, as well as computer operations. Similarly, Construction and engineering managers (413,750) oversee engineering projects, manufacturing plants, and construction sites. Computer management is 64% the size of construction and engineering management.=== Software Engineering Educators Versus Engineering Educators ===Until now, computer science has been the main degree to acquire, whether one wanted to make software systems (software engineering) or study the theoretical and mathematical facets of software systems (computer science). The data shows that the number of chemistry and physics educators (29,610) nearly equals the number of engineering educators (29,310). It is estimated that similarly, ​1⁄2 of computer science educators emphasize the practical (software engineering) (16,495) and ​1⁄2 of computer science educators emphasize the theoretical (computer science) (16,495). This means that software engineering education is 56% the size of traditional engineering education. Computer science is larger than all engineering, and larger than all physics and chemistry.=== Other Software and Engineering Roles ====== Relation to IT demographics ===Software engineers are part of the much larger software, hardware, application, and operations community. In 2000 in the U.S., there were about 680,000 software engineers and about 10,000,000 IT workers.There are no numbers on testers in the BLS data.== India ==There has been a healthy growth in the number of India's IT professionals over the past few years. From a base of 6,800 knowledge workers in 1985-86, the number increased to 522,000 software and services professionals by the end of 2001-02. It is estimated that out of these 528,000 knowledge workers, almost 170,000 are working in the IT software and services export industry; nearly 106,000 are working in the IT enabled services and over 230,000 in user organizations.== References ==== See also ==Software engineeringList of software engineering topicsSoftware engineering economicsSoftware engineering professionalism
	In a software development team, a software analyst  is the person who studies the software application domain, prepares software requirements, and specification (Software Requirements Specification) documents. The software analyst is the seam between the software users and the software developers. They convey the demands of software users to the developers.== See also ==Systems analystApplication analyst== References ==
	This is a list of approaches, styles, and philosophies in software development. It also contains programming paradigms, software development methodologies, software development processes, and single practices, principles and laws.== Software development philosophies ==Large-scale programming styles:Behavior-driven developmentDesign-driven developmentDomain-driven designSecure by designTest-driven developmentAcceptance test–driven developmentContinuous test-driven developmentSpecification by exampleSpecification-related paradigms:Iterative and incremental developmentWaterfall modelFormal methodsComprehensive systems:Agile software developmentLean software developmentLightweight methodologyAdaptive software developmentExtreme programmingFeature-driven developmentICONIXKanban (development)Cowboy coding (a non-system)Rules of thumb:KISS principleMinimalism (computing)Open–closed principleRelease early, release oftenRule of least powerThere's more than one way to do itUnix philosophyWorse is betterYou aren't gonna need it (YAGNI)Other:The Cathedral and the Bazaar - book exampling top-down vs. bottom-up open-source software== Programming paradigms ==Agent-oriented programmingAspect-oriented programming (AOP)Component-based software engineeringFunctional programming (FP)Modular programmingObject-oriented programming (OOP)Reactive programming== Software development methodologies ==Agile Unified Process (AUP)Constructionist design methodology (CDM)Dynamic systems development method (DSDM)Extreme programming (XP)Iterative and incremental developmentKanbanLean software developmentOpen Unified ProcessPair programmingRapid application development (RAD)Rational Unified Process (RUP)ScrumStructured systems analysis and design method (SSADM)Unified Process (UP)== Software development processes ==Active-Admin-driven development (AADD)Behavior-driven development (BDD)Bug-driven development (BgDD)Configuration-driven development (CDD)Design-driven development (D3)Domain-driven design (DDD)Feature-driven development (FDD)Test-driven development (TDD)User-centered design (UCD)Value-driven design (VDD)== See also ==Anti-patternDesign patternProgramming paradigmSoftware development methodologySoftware development process== References ==Don't Make Me Think (book by Steve Krug about human computer interaction and web usability)
	A software map represents static, dynamic, and evolutionary information of software systems and their software development processes by means of 2D or 3D map-oriented information visualization. It constitutes a fundamental concept and tool in software visualization, software analytics, and software diagnosis. Its primary applications include risk analysis for and monitoring of code quality, team activity, or software development progress  and, generally, improving effectiveness of software engineering with respect to all related artifacts, processes, and stakeholders throughout the software engineering process and software maintenance.== Motivation and concepts ==Software maps are applied in the context of software engineering: Complex, long-term software development projects are commonly faced by manifold difficulties such as the friction between completing system features and, at the same time, obtaining a high degree of code quality and software quality to ensure software maintenance of the system in the future. In particular, "Maintaining complex software systems tends to be costly because developers spend a significant part of their time with trying to understand the system’s structure and behavior." The key idea of software maps is to cope with that challenge and optimization problems by providing effective communication means to close the communication gap among the various stakeholders and information domains within software development projects and obtaining insights in the sense of information visualization.Software maps take advantage of well-defined cartographic map techniques using the virtual 3D city model metaphor to express the underlying complex, abstract information space. The metaphor is required "since software has no physical shape, there is no natural mapping of software to a two-dimensional space". Software maps are non-spatial maps that have to convert the hierarchy data and its attributes into a spatial representation.== Applications ==Software maps generally allow for comprehensible and effective communication of course, risks, and costs of software development projects to various stakeholders such as management and development teams. They communicate the status of applications and systems currently being developed or further developed to project leaders and management at a glance. "A key aspect for this decision making is that software maps provide the structural context required for correct interpretation of these performance indicators". As an instrument of communication, software maps act as open, transparent information spaces which enable priorities of code quality and the creation of new functions to be balanced against one another and to decide upon and implement necessary measures to improve the software development process.For example, they facilitate decisions as to where in the code an increase of quality would be beneficial both for speeding up current development activities and for reducing risks of future maintenance problems.Due to their high degree of expressiveness (e.g., information density) and their instantaneous, automated generation, the maps additionally serve to reflect the current status of system and development processes, bridging an essential information gap between management and development teams, improve awareness about the status, and serve as early risk detection instrument.== Contents ==Software maps are based on objective information as determined by the KPI driven code analysis as well as by imported information from software repository systems, information from the source codes, or software development tools and programming tools. In particular, software maps are not bound to a specific programming language, modeling language, or software development process model.Software maps use the hierarchy of the software implementation artifacts such as source code files as base to build a tree mapping, i.e., a rectangular area that represents the whole hierarchy, subdividing the area into rectangular sub-areas. A software map, informally speaking, looks similar to a virtual 3D city model, whereby artifacts of the software system appear as virtual, rectangular 3D buildings or towers, which are placed according to their position in the software implementation hierarchy.Software maps can express and combine information about software development, software quality, and system dynamics by mapping that information onto visual variables of the tree map elements such as footprint size, height, color or texture. They can systematically be specified, automatically generated, and organized by templates.== Mapping software system example ==Software maps "combine thematic information about software development processes (evolution), software quality, structure, and dynamics and display that information in a cartographic manner". For example: The height of a virtual building can be proportional to the complexity of the code unit (e.g., single or combined software metrics).The ground area of a virtual 3D building can be proportional to the number of lines of code in the module or (e.g., non-comment lines-of-code NCLOC).The color can express the current development status, i.e., how many developers are changing/editing the code unit.With this exemplary configuration, the software map shows crucial points in the source code with relations to aspects of the software development process. For example, it becomes obvious at a glance what to change in order to:implement changes quickly;evaluate quickly the impact of changes in one place on functionality elsewhere;reduce entanglements that lead to uncontrolled processes in the application;find errors faster;discover and eliminate bad programming style.Software maps represent key tools in the scope of automated software diagnosis software diagnostics.== As business intelligence tools and recommendation systems ==Software maps can be used, in particular, as analysis and presentation tool of business intelligence systems, specialized in the analysis of software related data. Furthermore, software maps "serve as recommendation systems for software engineering".Software maps are not limited by software-related information: They can include any hierarchical system information as well, for example, maintenance information about complex technical artifacts.== Visualization techniques ==Software maps are investigated in the domain of software visualization. The visualization of software maps is commonly based on tree mapping, "a space-filling approach to the visualization of hierarchical information structures" or other hierarchy mapping approaches.=== Layout algorithms ===To construct software maps, different layout approaches are used to generate the basic spatial mapping of components such as: Tree-map algorithms that initially map the software hierarchy into a recursively nested rectangular area.Voronoi-map algorithms that initially map the software hierarchy by generating a Voronoi map.=== Layout stability ===The spatial arrangement computed by layouts such as defined by tree maps strictly depends on the hierarchy. If software maps have to be generated frequently for an evolving or changing system, the usability of software maps is affected by non-stable layouts, that is, minor changes to the hierarchy may cause significant changes to the layout.In contrast to regular Voronoi treemap algorithms, which do not provide deterministic layouts, layout algorithm for Voronoi treemaps can be extended to provides a high degree of layout similarity for varying hierarchies. Similar approaches exist for the tree-map based case.== History ==Software maps methods and techniques belong the scientific displine of Software visualization and information visualization. They form a key concept and technique within the fields of software diagnosis. They have applications also in software mining and software analytics. Software maps have been extensively developed and researched by, e.g., at the Hasso Plattner Institute for IT systems engineering, in particular for large-scale, complex IT systems and applications.== References ==== External links ==Scientific conference VISSOFT (IEEE Working Conference on Software Visualization) [1]Interactive Rendering of Complex 3D-TreemapsMultiscale Visual Comparison of Execution TracesInteractive Software Maps for Web-Based Source Code AnalysisExtending Recommendation Systems with Software MapsA Visual Analysis Approach to Support Perfective Software MaintenanceViewFusion: Correlating Structure and Activity Views for Execution TracesA Visual Analysis and Design Tool for Planning Software ReengineeringsInteractive Areal Annotations for 3D Treemaps of Large-Scale Software SystemsVisualization of Execution Traces and its Application to Software MaintenanceUnderstanding Complex Multithreaded Software Systems by Using Trace VisualizationVisualization of Multithreaded Behavior to Facilitate Maintenance of Complex Software SystemsVisualizing Massively Pruned Execution Traces to Facilitate Trace ExplorationProjecting Code Changes onto Execution Traces to Support Localization of Recently Introduced BugsSyncTrace: Visual Thread-Interplay Analysis
	Round-trip engineering (RTE) is a functionality of software development tools that synchronizes two or more related software artifacts, such as, source code, models, configuration files, and even documentation. The need for round-trip engineering arises when the same information is present in multiple artifacts and therefore an inconsistency may occur if not all artifacts are consistently updated to reflect a given change. For example, some piece of information was added to/changed in only one artifact and, as a result, it became missing in/inconsistent with the other artifacts.Round-trip engineering is closely related to traditional software engineering disciplines: forward engineering (creating software from specifications), reverse engineering (creating specifications from existing software), and reengineering (understanding existing software and modifying it). Round-trip engineering is often wrongly defined as simply supporting both forward and reverse engineering. In fact, the key characteristic of round-trip engineering that distinguishes it from forward and reverse engineering is the ability to synchronize existing artifacts that evolved concurrently by incrementally updating each artifact to reflect changes made to the other artifacts. Furthermore, forward engineering can be seen as a special instance of RTE in which only the specification is present and reverse engineering can be seen as a special instance of RTE in which only the software is present. Many reengineering activities can also be understood as RTE when the software is updated to reflect changes made to the previously reverse engineered specification.Another characteristic of round-trip engineering is automatic update of the artifacts in response to automatically detected inconsistencies. In that sense, it is different from forward- and reverse engineering which can be both manual (traditionally) and automatic (via automatic generation or analysis of the artifacts). The automatic update can be either instantaneous or on-demand. In instantaneous RTE, all related artifacts are immediately updated after each change made to one of them. In on-demand RTE, authors of the artifacts may concurrently evolve the artifacts (even in a distributed setting) and at some point choose to execute matching to identify inconsistencies and choose to propagate some of them and reconcile potential conflicts.Round trip engineering supports an iterative development process. After you have synchronized your model with revised code, you are still free to choose the best way to work – make further modifications to the code or make changes to your model. You can synchronize in either direction at any time and you can repeat the cycle as many times as necessary.== Examples of round-trip engineering ==Perhaps the most common form of round-trip engineering is synchronization between UML (Unified Modeling Language) models and the corresponding source code. Many commercial tools and research prototypes support this form of RTE; a 2007 book lists Rational Rose, Borland Together, ESS-Model, BlueJ, and Fujaba among those capable, with Fujaba said to be capable to also identify design patterns. Usually, UML class diagrams are supported to some degree; however, certain UML concepts, such as associations and containment do not have straightforward representations in many programming languages which limits the usability of the created code and accuracy of code analysis (e.g., containment is hard to recognize in the code). A 2005 book on Visual Studio notes for instance that a common problem in RTE tools is that the model reversed is not the same as the original one, unless the tools are helped by laborious annotations. The behavioral parts of UML impose even more challenges for RTE.A more tractable form of round-trip engineering is implemented in the context of framework application programming interfaces (APIs), whereby a model describing the usage of a framework API by an application is synchronized with that application's code. In this setting, the API prescribes all correct ways the framework can be used in applications, which allows precise and complete detection of API usages in the code as well as creation of useful code implementing correct API usages. Two prominent RTE implementations in this category are framework-specific modeling languages and Spring Roo.Round-trip engineering is critical for maintaining consistency among multiple models and between the models and the code in Object Management Group's (OMG) Model-driven architecture. OMG proposed the QVT (query/view/transformation) standard to handle model transformations required for MDA. To date, a few implementations of the standard have been created. (Need to present practical experiences with MDA in relation to RTE).== Examples in software engineering ==Round-trip engineering based on Unified Modeling Language (UML) needs three basic components for software development:Source Code Editor;UML Editor for the Attributes and Methods;Visualisation of UML structure.An example of basic round-trip engineering is accessible as a web-based Open Source tool is:JavaScript Class Creator allows integrated round-trip engineering for JavaScript Classes. UML Diagrams are generated with a diagram library JointJS. Editing of Javascript Source Code is realized with the editor ACE.== References ==
	In systems engineering, information systems and software engineering, the systems development life cycle (SDLC), also referred to as the application development life-cycle, is a process for planning, creating, testing, and deploying an information system. The systems development lifecycle concept applies to a range of hardware and software configurations, as a system can be composed of hardware only, software only, or a combination of both. There are usually six stages in this cycle: analysis, design, development and testing, implementation, documentation, and evaluation.== Overview ==A systems development life cycle is composed of a number of clearly defined and distinct work phases which are used by systems engineers and systems developers to plan for, design, build, test, and deliver information systems. Like anything that is manufactured on an assembly line, an SDLC aims to produce high-quality systems that meet or exceed customer expectations, based on customer requirements, by delivering systems which move through each clearly defined phase, within scheduled time frames and cost estimates.Computer systems are complex and often (especially with the recent rise of service-oriented architecture) link multiple traditional systems potentially supplied by different software vendors. To manage this level of complexity, a number of SDLC models or methodologies have been created, such as waterfall, spiral, Agile software development, rapid prototyping, incremental, and synchronize and stabilize.SDLC can be described along a spectrum of agile to iterative to sequential methodologies. Agile methodologies, such as XP and Scrum, focus on lightweight processes which allow for rapid changes (without necessarily following the pattern of SDLC approach) along the development cycle. Iterative methodologies, such as Rational Unified Process and dynamic systems development method, focus on limited project scope and expanding or improving products by multiple iterations. Sequential or big-design-up-front (BDUF) models, such as waterfall, focus on complete and correct planning to guide large projects and risks to successful and predictable results. Other models, such as anamorphic development, tend to focus on a form of development that is guided by project scope and adaptive iterations of feature development.In project management a project can be defined both with a project life cycle (PLC) and an SDLC, during which slightly different activities occur. According to Taylor (2004), "the project life cycle encompasses all the activities of the project, while the systems development life cycle focuses on realizing the product requirements".SDLC is used during the development of an IT project, it describes the different stages involved in the project from the drawing board, through the completion of the project.The SDLC is not a methodology per se, but rather a description of the phases in the life cycle of a software application. These phases (broadly speaking) are, investigation, analysis, design, build, test, implement, and maintenance and support. All software development methodologies (such as the more commonly known waterfall and scrum methodologies) follow the SDLC phases but the method of doing that varies vastly between methodologies. In the Scrum methodology, for example, one could say a single user story goes through all the phases of the SDLC within a single two-week sprint. Contrast this to the waterfall methodology, as another example, where every business requirement (recorded in the analysis phase of the SDLC in a document called the Business Requirements Specification) is translated into feature/functional descriptions (recorded in the design phase in a document called the Functional Specification) which are then all built in one go as a collection of solution features typically over a period of three to nine months, or more. These methodologies are obviously quite different approaches, yet they both contain the SDLC phases in which a requirement is born, then travels through the life cycle phases ending in the final phase of maintenance and support, after-which (typically) the whole life cycle starts again for a subsequent version of the software application.== History and details ==The product life cycle describes the process for building information systems in a very deliberate, structured and methodical way, reiterating each stage of the product's life. The systems development life cycle, according to Elliott & Strachan & Radford (2004), "originated in the 1960s, to develop large scale functional business systems in an age of large scale business conglomerates. Information systems activities revolved around heavy data processing and number crunching routines".Several systems development frameworks have been partly based on SDLC, such as the structured systems analysis and design method (SSADM) produced for the UK government Office of Government Commerce in the 1980s. Ever since, according to Elliott (2004), "the traditional life cycle approaches to systems development have been increasingly replaced with alternative approaches and frameworks, which attempted to overcome some of the inherent deficiencies of the traditional SDLC".== Phases ==The system development life cycle framework provides a sequence of activities for system designers and developers to follow. It consists of a set of steps or phases in which each phase of the SDLC uses the results of the previous one.The SDLC adheres to important phases that are essential for developers—such as planning, analysis, design, and implementation—and are explained in the section below. This includes evaluation of the currently used system, information gathering, feasibility studies, and request approval. A number of SDLC models have been created, including waterfall, fountain, spiral, build and fix, rapid prototyping, incremental, synchronize, and stabilize. The oldest of these, and the best known, is the waterfall model, a sequence of stages in which the output of each stage becomes the input for the next. These stages can be characterized and divided up in different ways, including the following:Preliminary analysis: Begin with a preliminary analysis, propose alternative solutions, describe costs and benefits, and submit a preliminary plan with recommendations.Conduct the preliminary analysis: Discover the organization's objectives and the nature and scope of the problem under study. Even if a problem refers only to a small segment of the organization itself, find out what the objectives of the organization itself are. Then see how the problem being studied fits in with them.Propose alternative solutions: After digging into the organization's objectives and specific problems, several solutions may have been discovered. However, alternate proposals may still come from interviewing employees, clients, suppliers, and/or consultants. Insight may also be gained by researching what competitors are doing.Cost benefit analysis: Analyze and describe the costs and benefits of implementing the proposed changes. In the end, the ultimate decision on whether to leave the system as is, improve it, or develop a new system will be guided by this and the rest of the preliminary analysis data.Systems analysis, requirements definition: Define project goals into defined functions and operations of the intended application. This involves the process of gathering and interpreting facts, diagnosing problems, and recommending improvements to the system. Project goals will be further aided by analysis of end-user information needs and the removal of any inconsistencies and incompleteness in these requirements.A series of steps followed by the developer include:Collection of facts: Obtain end user requirements through documentation, client interviews, observation, and questionnaires.Scrutiny of the existing system: Identify pros and cons of the current system in-place, so as to carry forward the pros and avoid the cons in the new system.Analysis of the proposed system: Find solutions to the shortcomings described in step two and prepare the specifications using any specific user proposals.Systems design: At this step desired features and operations are described in detail, including screen layouts, business rules, process diagrams, pseudocode, and other documentation.Development: The real code is written here.Integration and testing: All the pieces are brought together into a special testing environment, then checked for errors, bugs, and interoperability.Acceptance, installation, deployment: This is the final stage of initial development, where the software is put into production and runs actual business.Maintenance: During the maintenance stage of the SDLC, the system is assessed/evaluated to ensure it does not become obsolete. This is also where changes are made to initial software.Evaluation: Some companies do not view this as an official stage of the SDLC, while others consider it to be an extension of the maintenance stage, and may be referred to in some circles as post-implementation review. This is where the system that was developed, as well as the entire process, is evaluated. Some of the questions that need to be answered include if the newly implemented system meets the initial business requirements and objectives, if the system is reliable and fault-tolerant, and if it functions according to the approved functional requirements. In addition to evaluating the software that was released, it is important to assess the effectiveness of the development process. If there are any aspects of the entire process (or certain stages) that management is not satisfied with, this is the time to improve.Disposal: In this phase, plans are developed for discontinuing the use of system information, hardware, and software and making the transition to a new system. The purpose here is to properly move, archive, discard, or destroy information, hardware, and software that is being replaced, in a manner that prevents any possibility of unauthorized disclosure of sensitive data. The disposal activities ensure proper migration to a new system. Particular emphasis is given to proper preservation and archiving of data processed by the previous system. All of this should be done in accordance with the organization's security requirements.In the following diagram, these stages of the systems development life cycle are divided in ten steps, from definition to creation and modification of IT work products:Not every project will require that the phases be sequentially executed. However, the phases are interdependent. Depending upon the size and complexity of the project, phases may be combined or may overlap.=== System investigation ===First the IT system proposal is investigated. During this step, consider all current priorities that would be affected and how they should be handled. Before any system planning is done, a feasibility study should be conducted to determine if creating a new or improved system is a viable solution. This will help to determine the costs, benefits, resource requirements, and specific user needs required for completion. The development process can only continue once management approves of the recommendations from the feasibility study.The following represent different components of the feasibility study:Operational feasibilityFinancial feasibilityTechnical feasibilityHuman factors feasibilityLegal/Political feasibility=== Analysis ===The goal of analysis is to determine where the problem is, in an attempt to fix the system. This step involves breaking down the system in different pieces to analyze the situation, analyzing project goals, breaking down what needs to be created, and attempting to engage users so that definite requirements can be defined.=== Design ===In systems design, the design functions and operations are described in detail, including screen layouts, business rules, process diagrams, and other documentation. The output of this stage will describe the new system as a collection of modules or subsystems.The design stage takes as its initial input the requirements identified in the approved requirements document. For each requirement, a set of one or more design elements will be produced as a result of interviews, workshops, and/or prototype efforts.Design elements describe the desired system features in detail, and they generally include functional hierarchy diagrams, screen layout diagrams, tables of business rules, business process diagrams, pseudo-code, and a complete entity-relationship diagram with a full data dictionary. These design elements are intended to describe the system in sufficient detail, such that skilled developers and engineers may develop and deliver the system with minimal additional input design.=== Environments ===Environments are controlled areas where systems developers can build, distribute, install, configure, test, and execute systems that move through the SDLC. Each environment is aligned with different areas of the SDLC and is intended to have specific purposes. Examples of such environments include the:development environment, where developers can work independently of each other before trying to merge their work with the work of others;common build environment, where merged work can be built, together, as a combined system;systems integration testing environment, where basic testing of a system's integration points to other upstream or downstream systems can be tested;user acceptance testing environment, where business stakeholders can test against their original business requirements; andproduction environment, where systems finally get deployed for final use by their intended end users.=== Testing ===The code is tested at various levels in software testing. Unit, system, and user acceptance testings are often performed. This is a grey area as many different opinions exist as to what the stages of testing are and how much, if any iteration occurs. Iteration is not generally part of the waterfall model, but the means to rectify defects and validate fixes prior to deployment is incorporated into this phase.The following are types of testing that may be relevant, depending on the type of system under development:Defect testing the failed scenarios, includingPath testingData set testingUnit testingSystem testingIntegration testingBlack-box testingWhite-box testingRegression testingAutomation testingUser acceptance testingSoftware performance testing=== Training and transition ===Once a system has been stabilized through adequate testing, the SDLC ensures that proper training on the system is performed or documented before transitioning the system to its support staff and end users. Training usually covers operational training for those people who will be responsible for supporting the system as well as training for those end users who will be using the system after its delivery to a production operating environment.After training has been successfully completed, systems engineers and developers transition the system to its final production environment, where it is intended to be used by its end users and supported by its support and operations staff.=== Operations and maintenance ===The deployment of the system includes changes and enhancements before the decommissioning or sunset of the system. Maintaining the system is an important aspect of SDLC. As key personnel change positions in the organization, new changes will be implemented. There are two approaches to system development: the traditional approach (structured) and object oriented. Information engineering includes the traditional system approach, which is also called the structured analysis and design technique. The object oriented approach views information system as a collection of objects that are integrated with each other to make a full and complete information system.=== Evaluation ===The final phase of the SDLC is to measure the effectiveness of the system and evaluate potential enhancements.== Systems analysis and design ==The systems analysis and design (SAD) is the process of developing information systems (IS) that effectively use hardware, software, data, processes, and people to support the company's businesses objectives.  System analysis and design can be considered the meta-development activity, which serves to set the stage and bound the problem.  SAD can be leveraged to set the correct balance among competing high-level requirements in the functional and non-functional analysis domains.  System analysis and design interacts strongly with distributed enterprise architecture, enterprise I.T. Architecture, and business architecture, and relies heavily on concepts such as partitioning, interfaces, personae and roles, and deployment/operational modeling to arrive at a high-level system description.  This high level description is then further broken down into the components and modules which can be analyzed, designed, and constructed separately and integrated to accomplish the business goal.  SDLC and SAD are cornerstones of full life cycle product and system planning.== Object-oriented analysis ==Object-oriented analysis (OOA) is the process of analyzing a task (also known as a problem domain), to develop a conceptual model that can then be used to complete the task.  A typical OOA model would describe computer software that could be used to satisfy a set of customer-defined requirements.  During the analysis phase of problem-solving, a programmer might consider a written requirements statement, a formal vision document, or interviews with stakeholders or other interested parties.  The task to be addressed might be divided into several subtasks (or domains), each representing a different business, technological, or other areas of interest.  Each subtask would be analyzed separately.  Implementation constraints, (e.g., concurrency, distribution, persistence, or how the system is to be built) are not considered during the analysis phase; rather, they are addressed during object-oriented design (OOD).The conceptual model that results from OOA will typically consist of a set of use cases, one or more UML class diagrams, and a number of interaction diagrams. It may also include some kind of user interface mock-up.The input for object-oriented design is provided by the output of object-oriented analysis. Realize that an output artifact does not need to be completely developed to serve as input of object-oriented design; analysis and design may occur in parallel, and in practice the results of one activity can feed the other in a short feedback cycle through an iterative process. Both analysis and design can be performed incrementally, and the artifacts can be continuously grown instead of completely developed in one shot.Some typical (but common to all types of design analysis) input artifacts for object-oriented:Conceptual model: Conceptual model is the result of object-oriented analysis, it captures concepts in the problem domain. The conceptual model is explicitly chosen to be independent of implementation details, such as concurrency or data storage.Use case: Use case is a description of sequences of events that, taken together, lead to a system doing something useful. Each use case provides one or more scenarios that convey how the system should interact with the users called actors to achieve a specific business goal or function. Use case actors may be end users or other systems. In many circumstances use cases are further elaborated into use case diagrams. Use case diagrams are used to identify the actor (users or other systems) and the processes they perform.System Sequence Diagram: System Sequence diagram (SSD) is a picture that shows, for a particular scenario of a use case, the events that external actors generate, their order, and possible inter-system events.User interface documentations (if applicable): Document that shows and describes the look and feel of the end product's user interface. It is not mandatory to have this, but it helps to visualize the end-product and therefore helps the designer.Relational data model (if applicable): A data model is an abstract model that describes how data is represented and used. If an object database is not used, the relational data model should usually be created before the design, since the strategy chosen for object-relational mapping is an output of the OO design process. However, it is possible to develop the relational data model and the object-oriented design artifacts in parallel, and the growth of an artifact can stimulate the refinement of other artifacts.== Life cycle ===== Management and control ===The SDLC phases serve as a programmatic guide to project activity and provide a flexible but consistent way to conduct projects to a depth matching the scope of the project. Each of the SDLC phase objectives are described in this section with key deliverables, a description of recommended tasks, and a summary of related control objectives for effective management. It is critical for the project manager to establish and monitor control objectives during each SDLC phase while executing projects. Control objectives help to provide a clear statement of the desired result or purpose and should be used throughout the entire SDLC process. Control objectives can be grouped into major categories (domains), and relate to the SDLC phases as shown in the figure.To manage and control any SDLC initiative, each project will be required to establish some degree of a work breakdown structure (WBS) to capture and schedule the work necessary to complete the project. The WBS and all programmatic material should be kept in the "project description" section of the project notebook. The WBS format is mostly left to the project manager to establish in a way that best describes the project work.There are some key areas that must be defined in the WBS as part of the SDLC policy. The following diagram describes three key areas that will be addressed in the WBS in a manner established by the project manager. The diagram shows coverage spans numerous phases of the SDLC but the associated MCD has a subset of primary mappings to the SDLC phases. For example, Analysis and Design is primarily performed as part of the Acquisition and Implementation Domain and System Build and Prototype is primarily performed as part of delivery and support.=== Work breakdown structured organization ===The upper section of the work breakdown structure (WBS) should identify the major phases and milestones of the project in a summary fashion. In addition, the upper section should provide an overview of the full scope and timeline of the project and will be part of the initial project description effort leading to project approval. The middle section of the WBS is based on the seven systems development life cycle phases as a guide for WBS task development. The WBS elements should consist of milestones and "tasks" as opposed to "activities" and have a definitive period (usually two weeks or more). Each task must have a measurable output (e.x. document, decision, or analysis). A WBS task may rely on one or more activities (e.g. software engineering, systems engineering) and may require close coordination with other tasks, either internal or external to the project. Any part of the project needing support from contractors should have a statement of work (SOW) written to include the appropriate tasks from the SDLC phases. The development of a SOW does not occur during a specific phase of SDLC but is developed to include the work from the SDLC process that may be conducted by external resources such as contractors.=== Baselines ===Baselines are an important part of the systems development life cycle. These baselines are established after four of the five phases of the SDLC and are critical to the iterative nature of the model . Each baseline is considered as a milestone in the SDLC.functional baseline: established after the conceptual design phase.allocated baseline: established after the preliminary design phase.product baseline: established after the detail design and development phase.updated product baseline: established after the production construction phase.=== Complementary methodologies ===Complementary software development methods to systems development life cycle are:Software prototypingJoint applications development (JAD)Rapid application development (RAD)Extreme programming (XP);Open-source developmentEnd-user developmentObject-oriented programming== Strengths and weaknesses ==Few people in the modern computing world would use a strict waterfall model for their SDLC as many modern methodologies have superseded this thinking. Some will argue that the SDLC no longer applies to models like Agile computing, but it is still a term widely in use in technology circles. The SDLC practice has advantages in traditional models of systems development that lends itself more to a structured environment. The disadvantages to using the SDLC methodology is when there is need for iterative development or (i.e. web development or e-commerce) where stakeholders need to review on a regular basis the software being designed.A comparison of the strengths and weaknesses of SDLC:An alternative to the SDLC is rapid application development, which combines prototyping, joint application development and implementation of CASE tools. The advantages of RAD are speed, reduced development cost, and active user involvement in the development process.== System lifecycle ==The system lifecycle in systems engineering is a view of a system or proposed system that addresses all phases of its existence to include system conception, design and development, production and/or construction, distribution, operation, maintenance and support, retirement, phase-out and disposal.=== Conceptual design ===The conceptual design stage is the stage where an identified need is examined, requirements for potential solutions are defined, potential solutions are evaluated and a system specification is developed.  The system specification represents the technical requirements that will provide overall guidance for system design. Because this document determines all future development, the stage cannot be completed until a conceptual design review has determined that the system specification properly addresses the motivating need.Key steps within the conceptual design stage include:Need identificationFeasibility analysisSystem requirements analysisSystem specificationConceptual design review=== Preliminary system design ===During this stage of the system lifecycle, subsystems that perform the desired system functions are designed and specified in compliance with the system specification. Interfaces between subsystems are defined, as well as overall test and evaluation requirements. At the completion of this stage, a development specification is produced that is sufficient to perform detailed design and development.Key steps within the preliminary design stage include:Functional analysisRequirements allocationDetailed trade-off studiesSynthesis of system optionsPreliminary design of engineering modelsDevelopment specificationPreliminary design reviewFor example, as the system analyst of Viti Bank, you have been tasked to examine the current information system. Viti Bank is a fast growing bank in Fiji. Customers in remote rural areas are finding difficulty to access the bank services. It takes them days or even weeks to travel to a location to access the bank services. With the vision of meeting the customers needs, the bank has requested your services to examine the current system and to come up with solutions or recommendations of how the current system can be provided to meet its needs.=== Detail design and development ===This stage includes the development of detailed designs that brings initial design work into a completed with form of specifications. This work includes the specification of interfaces between the system and its intended environment and a comprehensive evaluation of the systems logistical, maintenance and support requirements.  The detail design and development is responsible for producing the product, process and material specifications and may result in substantial changes to the development specification.Key steps within the detail design and development stage include:Detailed designDetailed synthesisDevelopment of engineering and prototype modelsRevision of development specificationProduct, process and material specificationCritical design review=== Production and construction ===During the production and/or construction stage the product is built or assembled in accordance with the requirements specified in the product, process and material specifications and is deployed and tested within the operational target environment. System assessments are conducted in order to correct deficiencies and adapt the system for continued improvement.Key steps within the product construction stage include:Production and/or construction of system componentsAcceptance testingSystem distribution and operationOperational testing and evaluationSystem assessment=== Utilization and support ===Once fully deployed, the system is used for its intended operational role and maintained within its operational environment.Key steps within the utilization and support stage include:System operation in the user environmentChange managementSystem modifications for improvementSystem assessment=== Phase-out and disposal ===Effectiveness and efficiency of the system must be continuously evaluated to determine when the product has met its maximum effective lifecycle.  Considerations include: Continued existence of operational need, matching between operational requirements and system performance, feasibility of system phase-out versus maintenance, and availability of alternative systems.== See also ==Application lifecycle managementDecision cycleIPO ModelSoftware development methodologies== References ==== Further reading ==Cummings, Haag (2006). Management Information Systems for the Information Age. Toronto, McGraw-Hill RyersonBeynon-Davies P. (2009). Business Information Systems. Palgrave, Basingstoke. ISBN 978-0-230-20368-6Computer World, 2002, Retrieved on June 22, 2006 from the World Wide Web:Management Information Systems, 2005, Retrieved on June 22, 2006 from the World Wide Web:This article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.== External links ==The Agile System Development LifecyclePension Benefit Guaranty Corporation – Information Technology Solutions Lifecycle MethodologyFSA Life Cycle FrameworkHHS Enterprise Performance Life Cycle FrameworkThe Open Systems Development Life CycleSystem Development Life Cycle Evolution ModelingZero Deviation Life CycleIntegrated Defense AT&L Life Cycle Management Chart, the U.S. DoD form of this concept.
	A software engineer is a person who applies the principles of software engineering to the design, development, maintenance, testing, and evaluation of computer software.Prior to the mid-1970s, software practitioners generally called themselves computer scientists, computer programmers or software developers, regardless of their actual jobs. Many people prefer to call themselves software developer and programmer, because most widely agree what these terms mean, while the exact meaning of software engineer is still being debated.== Education ==Half of all practitioners today have degrees in computer science, information systems, or information technology. A small, but growing, number of practitioners have software engineering degrees. In 1987, Imperial College London introduced the first three-year software engineering Bachelor's degree in the UK and the world; in the following year, the University of Sheffield established a similar program.  In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs. In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees was recently defined by the CCSE. As of 2004, in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master's degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities.  Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.In 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.=== Other degrees ===In business, some software engineering practitioners have MIS or computer information systems degrees. In embedded systems, some have electrical engineering, electronics engineering, computer science with emphasis in "embedded systems" or computer engineering degrees, because embedded software often requires a detailed understanding of hardware. In medical software, practitioners may have medical informatics, general medical, or biology degrees.Some practitioners have mathematics, science, engineering, or technology (STEM) degrees. Some have philosophy (logic in particular) or other non-technical degrees. For instance, Barry Boehm earned degrees in mathematics. And, others have no degrees.== Profession ===== Employment ===Most software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work on their own as consulting software engineers. Some organizations have specialists to perform all of the tasks in the software development process. Other organizations separate software engineers based on specific software-engineering tasks. These companies sometimes hire interns (possibly university or college students) over a short time. In large projects, software engineers are distinguished from people who specialize in only one role because they take part in the design as well as the programming of the project. In small projects, software engineers will usually fill several or all roles at the same time. Specializations include:in industry (analysts, architects, developers, testers, technical support, managers)in academia (educators, researchers)=== Impact of globalization ===Most students in the developed world have avoided degrees related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although government statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Often one is expected to start out as a computer programmer before being promoted to software engineer. Thus, the career path to software engineering may be rough, especially during recessions.Some career counselors suggest a student also focus on "people skills" and business skills rather than purely technical skills because such "soft skills" are allegedly more difficult to offshore.Reasonable command over reading, writing & speaking English is asked by most of employers. It is the quasi-management aspects of software engineering that appear to be what has kept it from being impacted by globalization.=== Prizes ===There are several prizes in the field of software engineering:The Codie awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.Jolt Awards are awards in the software industry.Stevens Award is a software engineering award given in memory of Wayne Stevens.== Use of the title "Engineer" ===== Origin of the term ===Margaret Hamilton promoted the term "software engineering" during her work on the Apollo program. The term "engineering" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new "term" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.=== Suitability of the term ===One could argue that software engineering implies a certain level of academic training, professional discipline, adherence to formal processes, and especially legal liability that often are not applied in cases of software development. A common analogy is that working in construction does not make one a civil engineer, and so writing code does not make one a software engineer. Furthermore, because computing doesn't utilize the methods of mathematical physics common to all conventional engineering disciplines, it is more appropriate to call those engaged in this occupation as software developers or similar.In 1978, computer scientist E. W. Dijkstra wrote in a paper that the coining of the term software engineer was not useful since it was an inappropriate analogy: The existence of the mere term has been the base of a number of extremely shallow—and false—analogies, which just confuse the issue... Computers are such exceptional gadgets that there is good reason to assume that most analogies with other disciplines are too shallow to be of any positive value, are even so shallow that they are only confusing.In each of the last few decades, at least one radical new approach has entered the mainstream of software development (e.g. Structured Programming, Object Orientation), implying that the field is still changing too rapidly to be considered an engineering discipline. Proponents argue that the supposedly radical new approaches are evolutionary rather than revolutionary.Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused  and should be considered harmful, particularly in the United States.=== Regulatory classification ======= Canada ====In Canada the use of the job title Engineer is controlled in each province by self-regulating professional engineering organizations who are also tasked with enforcement of the governing legislation. The intent is that any individual holding themselves out as an engineer has been verified to have been educated to a certain accredited level and their professional practice is subject to a code of ethics and peer scrutiny. It is also illegal to use the title Engineer in Canada unless an individual is licensed.In Ontario, the Professional Engineers Act stipulates a minimum education level of a three-year diploma in technology from a College of Applied Arts and Technology or a degree in a relevant science area. However, engineering undergraduates and all other applicants are not allowed to use the title of engineer until they complete the minimum amount of work experience of four years in addition to completing the Professional Practice Examination (PPE). If the applicant does not hold an undergraduate engineering degree then they may have to take the Confirmatory Practice Exam or Specific Examination Program unless the exam requirements are waived by a committee.IT professionals with degrees in other fields (such as computer science or information systems) are restricted from using the title Software Engineer, or wording Software Engineer in a title, depending on their province or territory of residence.In some instances, cases have been taken to court regarding the illegal use of the protected title Engineer.==== Europe ====Throughout the whole of Europe, suitably qualified engineers may obtain the professional European Engineer qualification.==== France ====In France, the term ingénieur (engineer) is not a protected title and can be used by anyone, even by those who do not possess an academic degree.However, the title Ingénieur Diplomé (Graduate Engineer) is an official academic title that is protected by the government and is associated with the Diplôme d'Ingénieur, which is one of the most prestigious academic degrees in France.==== Iceland ====The use of the title tölvunarfræðingur (computer scientist) is protected by law in Iceland. Software engineering is taught in Computer Science departments in Icelandic universities. Icelandic law state that a permission must be obtained from the Minister of Industry when the degree was awarded abroad, prior to use of the title. The title is awarded to those who have obtained a BSc degree in Computer Science from a recognized higher educational institution.==== New Zealand ====In New Zealand, the Institution of Professional Engineers New Zealand (IPENZ), which licenses and regulates the country's chartered engineers (CPEng), recognizes software engineering as a legitimate branch of professional engineering and accepts application of software engineers to obtain chartered status provided he or she has a tertiary degree of approved subjects. Software Engineering is included whereas Computer Science is normally not.==== United States ====The Bureau of Labor Statistics (BLS) classifies computer software engineers as a subcategory of "computer specialists", along with occupations such as computer scientist, Programmer, Database administrator and Network administrator. The BLS classifies all other engineering disciplines, including computer hardware engineers, as engineers.Many states prohibit unlicensed persons from calling themselves an Engineer, or from indicating branches or specialties not covered licensing acts. In many states, the title Engineer is reserved for individuals with a Professional Engineering license indicating that they have shown minimum level of competency through accredited engineering education, qualified engineering experience, and engineering board's examinations.In April 2013 the National Council of Examiners for Engineering and Surveying (NCEES) began offering a Professional Engineer (PE) exam for Software Engineering. The exam was developed in association with the IEEE Computer Society. NCEES will end the exam after April 2019 due to lack of participation.== See also ==Bachelor of Science in Information TechnologyBachelor of Software EngineeringConsulting software engineerRelease EngineerSoftware Engineering Institute== References ==
	The following outline is provided as an overview of and topical guide to software engineering:Software engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.== Technologies and practices ==Skilled software engineers use technologies and practices from a variety of fields to improve their productivity in creating software and to improve the quality of the delivered product.=== Software applications ===Software engineers build software (applications, operating systems, system software) that people use.Applications influence software engineering by pressuring developers to solve problems in new ways. For example, consumer software emphasizes low cost, medical software emphasizes high quality, and Internet commerce software emphasizes rapid development.Business softwareAccounting softwareAnalyticsData mining closely related to databaseDecision support systemsAirline reservationsBankingAutomated teller machinesCheque processingCredit cardsCommerceTradeAuctions (e.g. eBay)Reverse auctions (procurement)Bar code scannersCompilersParsersCompiler optimizationInterpretersLinkersLoadersCommunicationE-mailInstant messengersVOIPCalendars — scheduling and coordinatingContact managersComputer graphicsAnimationSpecial effects for video and filmEditingPost-processingCryptographyDatabases, support almost every fieldEmbedded systems Both software engineers and traditional engineers write software control systems for embedded products.Automotive softwareAvionics softwareHeating ventilating and air conditioning (HVAC) softwareMedical device softwareTelephonyTelemetryEngineering All traditional engineering branches use software extensively. Engineers use spreadsheets, more than they ever used calculators. Engineers use custom software tools to design, analyze, and simulate their own projects, like bridges and power lines. These projects resemble software in many respects, because the work exists as electronic documents and goes through analysis, design, implementation, and testing phases. Software tools for engineers use the tenets of computer science; as well as the tenets of calculus, physics, and chemistry.Computer Aided Design (CAD)Electronic Design Automation (EDA)Numerical AnalysisSimulationFileFTPFile sharingFile synchronizationFinanceBond marketFutures marketStock marketGamesPokerMultiuser DungeonsVideo gamesInformation systems, support almost every fieldLIS Management of laboratory dataMIS Management of financial and personnel dataLogisticsSupply chain managementManufacturingComputer Aided Manufacturing (CAM)Distributed Control Systems (DCS)MusicMusic sequencersSound effectsMusic synthesisNetwork ManagementNetwork management systemElement Management SystemOperations Support SystemBusiness Support SystemsNetworks and InternetDomain Name SystemProtocolsRoutersOffice suitesWord processorsSpreadsheetsPresentationsOperating systemsEmbeddedGraphicalMultitaskingReal-timeRoboticsSignal processing, encoding and interpreting signalsImage processing, encoding and interpreting visual informationSpeech processingText recognitionHandwriting recognitionSimulation, supports almost every field.Engineering, A software simulation can be cheaper to build and more flexible to change than a physical engineering model.SciencesSciencesGenomicsTraffic ControlAir traffic controlShip traffic controlRoad traffic controlTrainingDrillSimulationTestingVisualization, supports almost every fieldArchitectureEngineeringSciencesVotingWorld wide webBrowsersServers=== Software engineering topics ===Many technologies and practices are (mostly) confined to software engineering,though many of these are shared with computer science.==== Programming paradigm, based on a programming language technology ====Object-oriented programmingAspect-oriented programmingFunctional decompositionStructured programmingRule-based programming==== Databases ====HierarchicalObjectRelationalSQL/XMLSQLMYSQLNoSQL==== Graphical user interfaces ====GTK+ GIMP ToolkitwxWidgetsUltimate++Qt toolkitFLTK==== Programming tools ====Configuration management and source code managementCVSSubversionGitMercurialRCSGNU ArchLibreSource SynchronizerTeam Foundation ServerVisual Studio Team ServicesBuild toolsMakeRakeCabalAntCADESNantMavenFinal BuilderGradleTeam Foundation ServerVisual Studio Team ServicesVisual Build ProEditorsIntegrated development environments (IDEs)Text editorsWord processorsParser creation toolsYacc/BisonStatic code analysis tools==== Libraries ====Component-based software engineering==== Design languages ====Unified Modeling Language (UML)==== Patterns, document many common programming and project management techniques ====Anti-patternsPatterns==== Processes and methodologies ====AgileAgile software developmentExtreme programmingLean software developmentRapid application development (RAD)Rational Unified ProcessScrum (in management)HeavyweightCleanroomISO/IEC 12207 — software life cycle processesISO 9000 and ISO 9001Process ModelsCMM and CMMI/SCAMPIISO 15504 (SPICE)MetamodelsISO/IEC 24744SPEM==== Platforms ====A platform combines computer hardware and an operating system. As platforms grow more powerful and less costly, applications and tools grow more widely available.BREWCray supercomputersDEC minicomputersIBM mainframesLinux PCsClassic Mac OS and macOS PCsMicrosoft .NETPalm PDAsSun Microsystems SolarisWindows PCs (Wintel)Symbian OS==== Other Practices ====CommunicationMethod engineeringPair programmingPerformance EngineeringProgramming productivityRefactoringSoftware inspections/Code reviewsSoftware reuseSystems integrationTeamwork==== Other tools ====Decision tablesFeatureUser storiesUse cases=== Computer science topics ===Skilled software engineers know a lot of computer science including what is possible and impossible, and what is easy and hard for software.Algorithms, well-defined methods for solving specific problems.SearchingSortingParsingNumerical analysisCompiler theoryYacc/BisonData structures, well-defined methods for storing and retrieving data.ListsTreesHash tablesComputability, some problems cannot be solved at allList of unsolved problems in computer scienceHalting problemComplexity, some problems are solvable in principle, yet unsolvable in practiceNP completenessComputational complexity theoryFormal methodsProof of correctnessProgram synthesisAdaptive SystemsNeural NetworksEvolutionary Algorithms=== Mathematics topics ===Discrete mathematics is a key foundation of software engineering.Number representationSet (computer science)BagsGraphsSequencesTreesGraph (data structure)LogicDeductionFirst-order logicHigher-order logicCombinatory logicInductionCombinatoricsOtherDomain knowledgeStatisticsDecision theoryType theory=== Life cycle phases ===Development life cycle phaseRequirements gathering / analysisSoftware architectureComputer programmingTesting, detects bugsBlack box testingWhite box testingQuality assurance, ensures compliance with process.Product Life cycle phase and Project lifecycleInceptionFirst developmentMajor releaseMinor releaseBug fix releaseMaintenanceObsolescenceRelease development stage, near the end of a release cycleAlphaBetaGold master1.0; 2.0Software development lifecycleWaterfall model — Structured programming and Stepwise refinementSSADMSpiral model — Iterative developmentV-modelAgile software developmentDSDMChaos model — Chaos strategy=== Deliverables ===Deliverables must be developed for many SE projects. Software engineers rarely make all of these deliverables themselves. They usually cooperate with the writers, trainers, installers, marketers, technical support people, and others who make many of these deliverables.Application software — the softwareDatabase — schemas and data.Documentation, online and/or print, FAQ, Readme, release notes, Help, for each roleUserAdministratorManagerBuyerAdministration and Maintenance policy, what should be backed-up, checked, configured, ...InstallersMigrationUpgrade from previous installationsUpgrade from competitor's installationsTraining materials, for each roleUserAdministratorManagerBuyerSupport info for computer support groups.Marketing and sales materialsWhite papers, explain the technologies used in the applications=== Business roles ===OperationsUsersAdministratorsManagersBuyersDevelopmentAnalystsProgrammersTestersManagersBusinessConsulting — customization and installation of applicationsSalesMarketingLegal — contracts, intellectual property rightsPrivacy and Privacy engineeringSupport — helping customers use applicationsPersonnel — hiring and training qualified personnelFinance — funding new developmentAcademeEducatorsResearchers=== Management topics ===LeadershipCoachingCommunicationListeningMotivationVision, SEs are good at thisExample, everyone follows a good example bestHuman resource managementHiring, getting people into an organizationTrainingEvaluationProject managementGoal settingCustomer interaction (Rethink)EstimationRisk managementChange managementProcess managementSoftware development processesMetrics=== Business topics ===Quality programsMalcolm Baldrige National Quality AwardSix SigmaTotal Quality Management (TQM)== Software engineering profession ==Software engineering demographicsSoftware engineering economicsCCSEHistory of software engineeringSoftware engineering professionalismEthicsLicensingLegalIntellectual propertyConsumer protection== History of software engineering ==History of software engineering=== Pioneers ===Many people made important contributions to SE technologies, practices, or applications.John Backus: Fortran, first optimizing compiler, BNFVic Basili: Experience factory.F.L. Bauer: Stack principle, popularized the term Software EngineeringKent Beck: Refactoring, extreme programming, pair programming, test-driven development.Tim Berners-Lee: World wide webBarry Boehm: SE economics, COCOMO, Spiral model.Grady Booch: Object-oriented design, UML.Fred Brooks: Managed System 360 and OS 360. Wrote The Mythical Man-Month and No Silver Bullet.Larry Constantine: Structured design, coupling, cohesionEdsger Dijkstra: Wrote Notes on Structured Programming, A Discipline of Programming and Go To Statement Considered Harmful, algorithms, formal methods, pedagogy.Michael Fagan: Software inspection.Tom Gilb: Software metrics, Software inspection, Evolutionary Delivery ("Evo").Adele Goldstine: Wrote the Operators Manual for the ENIAC, the first electronic digital computer, and trained some of the first human computersLois Haibt: FORTRAN, wrote the first parserMargaret Hamilton: Coined the term "software engineering", developed Universal Systems LanguageMary Jean Harrold: Regression testing, fault localizationGrace Hopper: The first compiler (Mark 1), COBOL, Nanoseconds.Watts Humphrey: Capability Maturity Model, Personal Software Process, fellow of the Software Engineering Institute.Jean Ichbiah: AdaMichael A. Jackson: Jackson Structured Programming, Jackson System DevelopmentBill Joy: Berkeley Unix, vi, Java.Alan Kay: SmalltalkBrian Kernighan: C and Unix.Donald Knuth: Wrote The Art of Computer Programming, TeX, algorithms, literate programmingNancy Leveson: System safetyBertrand Meyer: Design by Contract, Eiffel programming language.Peter G. Neumann: RISKS Digest, ACM Sigsoft.David Parnas: Module design, social responsibility, professionalism.David Pearson, Computer Scientist: Developed the ICL CADES software engineering system.Jef Raskin: Developed the original Macintosh GUI, authored The Humane InterfaceDennis Ritchie: C and Unix.Winston W. Royce: Waterfall model.Mary Shaw: Software architecture.Richard Stallman: Founder of the Free Software FoundationLinus Torvalds: Linux kernel, free software / open source development.Will Tracz: Reuse, ACM Software Engineering Notes.Gerald Weinberg: Wrote The Psychology of Computer Programming.Elaine Weyuker: Software testingJeannette Wing: Formal specifications.Ed Yourdon: Structured programming, wrote The Decline and Fall of the American Programmer.See alsoList of programmersList of computer scientists== Notable publications ==About Face: The Essentials of User Interface Design by Alan Cooper, about user interface design. ISBN 0-7645-2641-3The Capability Maturity Model by Watts Humphrey. Written for the Software Engineering Institute, emphasizing management and process. (See Managing the Software Process ISBN 0-201-18095-2)The Cathedral and the Bazaar by Eric Raymond about open source development.The Decline and Fall of the American Programmer by Ed Yourdon predicts the end of software development in the U.S. ISBN 0-13-191958-XDesign Patterns by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. ISBN 0-201-63361-2Extreme Programming Explained by Kent Beck ISBN 0-321-27865-8"Go To Statement Considered Harmful" by Edsger Dijkstra.Internet, Innovation and Open Source:Actors in the Network — First Monday article by Ilkka Tuomi (2000) sourceThe Mythical Man-Month by Fred Brooks, about project management. ISBN 0-201-83595-9Object-oriented Analysis and Design by Grady Booch. ISBN 0-8053-5340-2Peopleware by Tom DeMarco and Tim Lister.  ISBN 0-932633-43-9The pragmatic engineer versus the scientific designer by E. W. Dijkstra [1]Principles of Software Engineering Management by Tom Gilb about evolutionary processes. ISBN 0-201-19246-2The Psychology of Computer Programming by Gerald Weinberg. Written as an independent consultant, partly about his years at IBM. ISBN 0-932633-42-0Refactoring: Improving the Design of Existing Code by Martin Fowler, Kent Beck, John Brant, William Opdyke, and Don Roberts. ISBN 0-201-48567-2The Pragmatic Programmer: from journeyman to master by Andrew Hunt, and David Thomas. ISBN 0-201-61622-XSee also:Important publications in software engineering in CS.== Related fields ==Computer ScienceInformation engineeringInformation technologyTraditional engineeringComputer engineeringElectrical engineeringSoftware engineeringDomain engineeringInformation technology engineeringKnowledge engineeringUser interface engineeringWeb engineeringArts and SciencesMathematicsComputer scienceInformation scienceApplication softwareInformation systemsProgrammingSystems Engineering== See also ==Search-based software engineeringSWEBOK Software engineering body of knowledgeCCSE Computing curriculum for software engineeringComputer terms etymology, the origins of computer termsComplexity or scalingSecond system syndromeoptimizationSource code escrowFeature interaction problemCertification (software engineering)Engineering_disasters#Failure_due_to_softwareOutline of software development== References ==== External links ==Guide to the Software Engineering Body of Knowledge (SWEBOK)Professional organizationsBritish Computer SocietyAssociation for Computing MachineryIEEE Computer SocietyProfessionalismSE Code of EthicsProfessional licensing in TexasEducationCCSE Undergraduate curriculumStandardsIEEE Software Engineering StandardsInternet Engineering Task ForceISOGovernment organizationsEuropean Software InstituteSoftware Engineering InstituteAgileOrganization to promote Agile software developmentTest driven developmentExtreme programmingOther organizationsOnline community for software engineersSoftware Engineering SocietyDemographicsU.S. Bureau of Labor Statistics on SESurveysDavid Redmiles page from the University of California siteOtherFull text in PDF from the NATO conference in GarmischComputer Risks Peter G. Neumann's risks column.
	In computer science and software engineering, reusability is the use of existing assets in some form within the software product development process; these assets are products and by-products of the software development life cycle and include code, software components, test suites, designs and documentation. The opposite concept of reusability is leverage, which modifies existing assets as needed to meet specific system requirements. Because reuse implies the creation of a separately maintained version of the assets, it is preferred over leverage.Subroutines or functions are the simplest form of reuse. A chunk of code is regularly organized using modules or namespaces into layers. Proponents claim that objects and software components offer a more advanced form of reusability, although it has been tough to objectively measure and define levels or scores of reusability.The ability to reuse relies in an essential way on the ability to build larger things from smaller parts, and being able to identify commonalities among those parts. Reusability is often a required characteristic of platform software. Reusability brings several aspects to software development that do not need to be considered when reusability is not required.Reusability implies some explicit management of build, packaging, distribution, installation, configuration, deployment, maintenance and upgrade issues. If these issues are not considered, software may appear to be reusable from design point of view, but will not be reused in practice.Software reusability more specifically refers to design features of a software element (or collection of software elements) that enhance its suitability for reuse.Many reuse design principles were developed at the WISR workshops.Candidate design features for software reuse include:AdaptableBrief: small sizeConsistencyCorrectnessExtensibilityFastFlexibleGenericLocalization of volatile (changeable) design assumptions (David Parnas)ModularityOrthogonalityParameterizationSimple: low complexityStability under changing requirementsConsensus has not yet been reached on this list on the relative importance of the entries nor on the issues which make each one important for a particular class of applications.== See also ==Code reuse== References ==
	Quality engineering is the discipline of engineering concerned with the principles and practice of product and service quality assurance and control. In the software development, it is the management, development, operation and maintenance of IT systems and enterprise architectures with a high quality standard.== Description ==Quality engineering is the discipline of engineering that creates and implements strategies for quality assurance in product development and production as well as software development.Quality Engineers focus on optimizing product quality which W. Edwards Deming defined as:                              Quality                =                              Results of work efforts            Total costs                                {\displaystyle {\text{Quality}}={\frac {\text{Results of work efforts}}{\text{Total costs}}}}  Quality engineering body of knowledge includes:Management and leadershipThe quality systemElements of a quality systemProduct and process designClassification of quality characteristicsDesign inputs and reviewDesign verificationReliability and maintainabilityProduct and process controlContinuous improvementQuality control toolsQuality management and planning toolsContinuous improvement techniquesCorrective actionPreventive actionStatistical process control (SPC)Risk management== Roles ==Auditor: Quality engineers may be responsible for auditing their own companies or their suppliers for compliance to international quality standards such as ISO9000 and AS9100. They may also be independent auditors under an auditing body.Process quality: Quality engineers may be tasked with value stream mapping and statistical process control to determine if a process is likely to produce defective product. They may create inspection plans and criteria to ensure defective parts are detected prior to completion.Supplier quality: Quality engineers may be responsible for auditing suppliers or performing root cause and corrective action at their facility or overseeing such activity to prevent the delivery of defective product.== Software ==IT services are increasingly interlinked in workflows across platform boundaries, device and organisational boundaries, for example in cyber-physical systems, business-to-business workflows or when using cloud services. In such contexts, quality engineering facilitates the necessary all-embracing consideration of quality attributes.In such contexts an "end-to-end" view of quality from management to operation is vital. Quality engineering integrates methods and tools from enterprise architecture-management, Software product management, IT service management, software engineering and systems engineering, and from software quality management and information security management. This means that quality engineering goes beyond the classic disciplines of software engineering, information security management or software product management since it integrates management issues (such as business and IT strategy, risk management, business process views, knowledge and information management, operative performance management), design considerations (including the software development process, requirements analysis, software testing) and operative considerations (such as configuration, monitoring, IT service management). In many of the fields where it is used, quality engineering is closely linked to compliance with legal and business requirements, contractual obligations and standards. As far as quality attributes are concerned, reliability, security and safety of IT services play a predominant role.In quality engineering, quality objectives are implemented in a collaborative process. This process requires the interaction of largely independent actors whose knowledge is based on different sources of information.== Quality objectives ==Quality objectives describe basic requirements for software quality. In quality engineering they often address the quality attributes of availability, security, safety, reliability and performance. With the help of quality models like ISO/IEC 25000 and methods like the Goal Question Metric approach it is possible to attribute metrics to quality objectives. This allows measuring the degree of attainment of quality objectives. This is a key component of the quality engineering process and, at the same time, is a prerequisite for its continuous monitoring and control. To ensure effective and efficient measuring of quality objectives the integration of core numbers, which were identified manually (e.g. by expert estimates or reviews), and automatically identified metrics (e.g. by statistical analysis of source codes or automated regression tests) as a basis for decision-making is favourable.== Actors ==The end-to-end quality management approach to quality engineering requires numerous actors with different responsibilities and tasks, different expertise and involvement in the organisation.Different roles involved in quality engineering:Business architect,IT architect,Security officer,Requirements engineer,Software quality manager,Test manager,Project manager,Product manager andSecurity architect.Typically, these roles are distributed over geographic and organisational boundaries. Therefore, appropriate measures need to be taken to coordinate the heterogeneous tasks of the various roles in quality engineering and to consolidate and synchronize the data and information necessary in fulfilling the tasks, and to make them available to each actor in an appropriate form.== Knowledge management ==Knowledge management plays an important part in quality engineering. The quality engineering knowledge base comprises manifold structured and unstructured data, ranging from code repositories via requirements specifications, standards, test reports, enterprise architecture models to system configurations and runtime logs. Software and system models play an important role in mapping this knowledge. The data of the quality engineering knowledge base are generated, processed and made available both manually as well as tool-based in a geographically, organisationally and technically distributed context. Of prime importance is the focus on quality assurance tasks, early recognition of risks, and appropriate support for the collaboration of actors.This results in the following requirements for a quality engineering knowledge base:Knowledge is available in a quality as required. Important quality criteria include that knowledge is consistent and up-to-date as well as complete and adequate in terms of granularity in relation to the tasks of the appropriate actors.Knowledge is interconnected and traceable in order to support interaction between the actors and to facilitate analysis of data. Such traceability relates not only to interconnectedness of data across different levels of abstraction (e.g. connection of requirements with the services realizing them) but also to their traceability over time periods, which is only possible if appropriate versioning concepts exist. Data can be interconnected both manually as well as (semi-) automatically.Information has to be available in a form that is consistent with the domain knowledge of the appropriate actors. Therefore, the knowledge base has to provide adequate mechanisms for information transformation (e.g. aggregation) and visualization. The RACI concept is an example of an appropriate model for assigning actors to information in a quality engineering knowledge base.In contexts, where actors from different organisations or levels interact with each other, the quality engineering knowledge base has to provide mechanisms for ensuring confidentiality and integrity.Quality engineering knowledge bases offer a whole range of possibilities for analysis and finding information in order to support quality control tasks of actors.== Collaborative processes ==The quality engineering process comprises all tasks carried out manually and in a (semi-)automated way to identify, fulfil and measure any quality features in a chosen context. The process is a highly collaborative one in the sense that it requires interaction of actors, widely acting independently from each other.The quality engineering process has to integrate any existing sub-processes that may comprise highly structured processes such as IT service management and processes with limited structure such as agile software development. Another important aspect is change-driven procedure, where change events, such as changed requirements are dealt with in the local context of information and actors affected by such change. A pre-requisite for this is methods and tools, which support change propagation and change handling.The objective of an efficient quality engineering process is the coordination of automated and manual quality assurance tasks. Code review or elicitation of quality objectives are examples of manual tasks, while regression tests and the collection of code metrics are examples for automatically performed tasks. The quality engineering process (or its sub-processes) can be supported by tools such as ticketing systems or security management tools.== See also ==Seven Basic Tools of QualityEngineering managementManufacturing engineeringMission assuranceSystems engineeringW. Edwards DemingAssociationsAmerican Society for QualityINFORMSInstitute of Industrial Engineers== External links ==Txture is a tool for textual IT-Architecture documentation and analysis.mbeddr is a set of integrated and extensible languages for embedded software engineering, plus an integrated development environment (IDE).== References ==
	Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations. The architecture of a software system is a metaphor, analogous to the architecture of a building. It functions as a blueprint for the system and the developing project, laying out the tasks not necessary to be executed by the design teams.Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in the design of software. For example, the systems that controlled the space shuttle launch vehicle had the requirement of being very fast and very reliable. Therefore, an appropriate real-time computing language would need to be chosen. Additionally, to satisfy the need for reliability the choice could be made to have multiple redundant and independently produced copies of the program, and to run these copies on independent hardware while cross-checking results.Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design, and allows reuse of design components between projects.== Scope ==Opinions vary as to the scope of software architectures:Overall, macroscopic system structure; this refers to architecture as a higher level abstraction of a software system that consists of a collection of computational components together with connectors that describe the interaction between these components.The important stuff—whatever that is; this refers to the fact that software architects should concern themselves with those decisions that have high impact on the system and its stakeholders.That which is fundamental to understanding a system in its environment"Things that people perceive as hard to change; since designing the architecture takes place at the beginning of a software system's lifecycle, the architect should focus on decisions that "have to" be right the first time. Following this line of thought, architectural design issues may become non-architectural once their irreversibility can be overcome.A set of architectural design decisions; software architecture should not be considered merely a set of models or structures, but should include the decisions that lead to these particular structures, and the rationale behind them. This insight has led to substantial research into software architecture knowledge management.There is no sharp distinction between software architecture versus design and requirements engineering (see Related fields below). They are all part of a "chain of intentionality" from high-level intentions to low-level details.== Characteristics ==Software architecture exhibits the following:Multitude of stakeholders: software systems have to cater to a variety of stakeholders such as business managers, owners, users, and operators. These stakeholders all have their own concerns with respect to the system. Balancing these concerns and demonstrating how they are addressed is part of designing the system. This implies that architecture involves dealing with a broad variety of concerns and stakeholders, and has a multidisciplinary nature.Separation of concerns: the established way for architects to reduce complexity is to separate the concerns that drive the design. Architecture documentation shows that all stakeholder concerns are addressed by modeling and describing the architecture from separate points of view associated with the various stakeholder concerns. These separate descriptions are called architectural views (see for example the 4+1 Architectural View Model).Quality-driven: classic software design approaches (e.g. Jackson Structured Programming) were driven by required functionality and the flow of data through the system, but the current insight is that the architecture of a software system is more closely related to its quality attributes such as fault-tolerance, backward compatibility, extensibility, reliability, maintainability, availability, security, usability, and other such –ilities. Stakeholder concerns often translate into requirements on these quality attributes, which are variously called non-functional requirements, extra-functional requirements, behavioral requirements, or quality attribute requirements.Recurring styles: like building architecture, the software architecture discipline has developed standard ways to address recurring concerns. These "standard ways" are called by various names at various levels of abstraction. Common terms for recurring solutions are architectural style, tactic, reference architecture and architectural pattern.Conceptual integrity: a term introduced by Fred Brooks in The Mythical Man-Month to denote the idea that the architecture of a software system represents an overall vision of what it should do and how it should do it. This vision should be separated from its implementation. The architect assumes the role of "keeper of the vision", making sure that additions to the system are in line with the architecture, hence preserving conceptual integrity.Cognitive constraints: an observation first made in a 1967 paper by computer programmer Melvin Conway that organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. As with conceptual integrity, it was Fred Brooks who introduced it to a wider audience when he cited the paper and the idea in his elegant classic The Mythical Man-Month, calling it "Conway's Law."== Motivation ==Software architecture is an "intellectually graspable" abstraction of a complex system. This abstraction provides a number of benefits:It gives a basis for analysis of software systems' behavior before the system has been built. The ability to verify that a future software system fulfills its stakeholders' needs without actually having to build it represents substantial cost-saving and risk-mitigation. A number of techniques have been developed to perform such analyses, such as ATAM.It provides a basis for re-use of elements and decisions. A complete software architecture or parts of it, like individual architectural strategies and decisions, can be re-used across multiple systems whose stakeholders require similar quality attributes or functionality, saving design costs and mitigating the risk of design mistakes.It supports early design decisions that impact a system's development, deployment, and maintenance life. Getting the early, high-impact decisions right is important to prevent schedule and budget overruns.It facilitates communication with stakeholders, contributing to a system that better fulfills their needs. Communicating about complex systems from the point of view of stakeholders helps them understand the consequences of their stated requirements and the design decisions based on them. Architecture gives the ability to communicate about design decisions before the system is implemented, when they are still relatively easy to adapt.It helps in risk management. Software architecture helps to reduce risks and chance of failure.It enables cost reduction. Software architecture is a means to manage risk and costs in complex IT projects.== History ==The comparison between software design and (civil) architecture was first drawn in the late 1960s, but the term software architecture became prevalent only in the beginning of the 1990s.The field of computer science had encountered problems associated with complexity since its formation. Earlier problems of complexity were solved by developers by choosing the right data structures, developing algorithms, and by applying the concept of separation of concerns. Although the term "software architecture" is relatively new to the industry, the fundamental principles of the field have been applied sporadically by software engineering pioneers since the mid-1980s. Early attempts to capture and explain software architecture of a system were imprecise and disorganized, often characterized by a set of box-and-line diagrams.Software architecture as a concept has its origins in the research of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. These scientists emphasized that the structure of a software system matters and getting the structure right is critical. During the 1990s there was a concerted effort to define and codify fundamental aspects of the discipline, with research work concentrating on architectural styles (patterns), architecture description languages, architecture documentation, and formal methods.Research institutions have played a prominent role in furthering software architecture as a discipline.  Mary Shaw and David Garlan of Carnegie Mellon wrote a book titled Software Architecture: Perspectives on an Emerging Discipline in 1996, which promoted software architecture concepts such as components, connectors, and styles. The University of California, Irvine's Institute for Software Research's efforts in software architecture research is directed primarily in architectural styles, architecture description languages, and dynamic architectures.IEEE 1471-2000, Recommended Practice for Architecture Description of Software-Intensive Systems, was the first formal standard in the area of software architecture. It was adopted in 2007 by ISO as ISO/IEC 42010:2007. In November 2011, IEEE 1471–2000 was superseded by ISO/IEC/IEEE 42010:2011, Systems and software engineering – Architecture description (jointly published by IEEE and ISO).While in IEEE 1471, software architecture was about the architecture of "software-intensive systems", defined as "any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole", the 2011 edition goes a step further by including the ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also "humans, processes, procedures, facilities, materials and naturally occurring entities". This reflects the relationship between software architecture, enterprise architecture and solution architecture.== Architecture activities ==There are many activities that a software architect performs. A software architect typically works with project managers, discusses architecturally significant requirements with stakeholders, designs a software architecture, evaluates a design, communicates with designers and stakeholders, documents the architectural design and more. There are four core activities in software architecture design. These core architecture activities are performed iteratively and at different stages of the initial software development life-cycle, as well as over the evolution of a system.Architectural analysis is the process of understanding the environment in which a proposed system or systems will operate and determining the requirements for the system. The input or requirements to the analysis activity can come from any number of stakeholders and include items such as:what the system will do when operational (the functional requirements)how well the system will perform runtime non-functional requirements such as reliability, operability, performance efficiency, security, compatibility defined in ISO/IEC 25010:2011 standarddevelopment-time non-functional requirements such as maintainability and transferability defined in ISO 25010:2011 standardbusiness requirements and environmental contexts of a system that may change over time, such as legal, social, financial, competitive, and technology concernsThe outputs of the analysis activity are those requirements that have a measurable impact on a software system's architecture, called architecturally significant requirements.Architectural synthesis or design is the process of creating an architecture. Given the architecturally significant requirements determined by the analysis, the current state of the design and the results of any evaluation activities, the design is created and improved.Architecture evaluation is the process of determining how well the current design or a portion of it satisfies the requirements derived during analysis. An evaluation can occur whenever an architect is considering a design decision, it can occur after some portion of the design has been completed, it can occur after the final design has been completed or it can occur after the system has been constructed. Some of the available software architecture evaluation techniques include Architecture Tradeoff Analysis Method (ATAM) and TARA. Frameworks for comparing the techniques are discussed in frameworks such as SARA Report and Architecture Reviews: Practice and Experience.Architecture evolution is the process of maintaining and adapting an existing software architecture to meet changes in requirements and environment. As software architecture provides a fundamental structure of a software system, its evolution and maintenance would necessarily impact its fundamental structure. As such, architecture evolution is concerned with adding new functionality as well as maintaining existing functionality and system behavior.Architecture requires critical supporting activities. These supporting activities take place throughout the core software architecture process. They include knowledge management and communication, design reasoning and decision making, and documentation.=== Architecture supporting activities ===Software architecture supporting activities are carried out during core software architecture activities. These supporting activities assist a software architect to carry out analysis, synthesis, evaluation, and evolution. For instance, an architect has to gather knowledge, make decisions and document during the analysis phase.Knowledge management and communication is the act of exploring and managing knowledge that is essential to designing a software architecture. A software architect does not work in isolation. They get inputs, functional and non-functional requirements and design contexts, from various stakeholders; and provides outputs to stakeholders. Software architecture knowledge is often tacit and is retained in the heads of stakeholders. Software architecture knowledge management activity is about finding, communicating, and retaining knowledge. As software architecture design issues are intricate and interdependent, a knowledge gap in design reasoning can lead to incorrect software architecture design. Examples of knowledge management and communication activities include searching for design patterns, prototyping, asking experienced developers and architects, evaluating the designs of similar systems, sharing knowledge with other designers and stakeholders, and documenting experience in a wiki page.Design reasoning and decision making is the activity of evaluating design decisions. This activity is fundamental to all three core software architecture activities. It entails gathering and associating decision contexts, formulating design decision problems, finding solution options and evaluating tradeoffs before making decisions. This process occurs at different levels of decision granularity while evaluating significant architectural requirements and software architecture decisions, and software architecture analysis, synthesis, and evaluation. Examples of reasoning activities include understanding the impacts of a requirement or a design on quality attributes, questioning the issues that a design might cause, assessing possible solution options, and evaluating the tradeoffs between solutions.Documentation is the act of recording the design generated during the software architecture process. A system design is described using several views that frequently include a static view showing the code structure of the system, a dynamic view showing the actions of the system during execution, and a deployment view showing how a system is placed on hardware for execution. Kruchten's 4+1 view suggests a description of commonly used views for documenting software architecture; Documenting Software Architectures: Views and Beyond has descriptions of the kinds of notations that could be used within the view description. Examples of documentation activities are writing a specification, recording a system design model, documenting a design rationale, developing a viewpoint, documenting views.== Software architecture topics ===== Software architecture description ===Software architecture description involves the principles and practices of modeling and representing architectures, using mechanisms such as: architecture description languages, architecture viewpoints, and architecture frameworks.=== Architecture description languages ===An architecture description language (ADL) is any means of expression used to describe a software architecture (ISO/IEC/IEEE 42010).Many special-purpose ADLs have been developed since the 1990s, including AADL (SAE standard), Wright (developed by Carnegie Mellon), Acme (developed by Carnegie Mellon), xADL (developed by UCI), Darwin (developed by Imperial College London), DAOP-ADL (developed by University of Málaga), SBC-ADL (developed by National Sun Yat-Sen University), and ByADL (University of L'Aquila, Italy).=== Architecture viewpoints ===Software architecture descriptions are commonly organized into views, which are analogous to the different types of blueprints made in building architecture. Each view addresses a set of system concerns, following the conventions of its viewpoint, where a viewpoint is a specification that describes the notations, modeling, and analysis techniques to use in a view that express the architecture in question from the perspective of a given set of stakeholders and their concerns (ISO/IEC/IEEE 42010). The viewpoint specifies not only the concerns framed (i.e., to be addressed) but the presentation, model kinds used, conventions used and any consistency (correspondence) rules to keep a view consistent with other views.=== Architecture frameworks ===An architecture framework captures the "conventions, principles and practices for the description of architectures established within a specific domain of application and/or community of stakeholders" (ISO/IEC/IEEE 42010). A framework is usually implemented in terms of one or more viewpoints or ADLs.=== Architectural styles and patterns ===An architectural pattern is a general, reusable solution to a commonly occurring problem in software architecture within a given context. Architectural patterns are often documented as software design patterns.Following traditional building architecture, a 'software architectural style' is a specific method of construction, characterized by the features that make it notable" (architectural style).There are many recognized architectural patterns and styles, among them:BlackboardClient-server (2-tier, 3-tier, n-tier, cloud computing exhibit this style)Component-basedData-centricEvent-driven (or implicit invocation)Layered (or multilayered architecture)Microservices architectureMonolithic applicationPeer-to-peer (P2P)Pipes and filtersPlug-insReactive ArchitectureRepresentational state transfer (REST)Rule-basedService-orientedShared nothing architectureSpace-based architectureSome treat architectural patterns and architectural styles as the same, some treat styles as specializations of patterns. What they have in common is both patterns and styles are idioms for architects to use, they "provide a common language" or "vocabulary" with which to describe classes of systems.=== Software architecture and agile development ===There are also concerns that software architecture leads to too much Big Design Up Front, especially among proponents of agile software development. A number of methods have been developed to balance the trade-offs of up-front design and agility, including the agile method DSDM which mandates a "Foundations" phase during which "just enough" architectural foundations are laid. IEEE Software devoted a special issue to the interaction between agility and architecture.=== Software architecture erosion ===Software architecture erosion (or "decay") refers to the gap observed between the planned and actual architecture of a software system as realized in its implementation. Software architecture erosion occurs when implementation decisions either do not fully achieve the architecture-as-planned or otherwise violate constraints or principles of that architecture. The gap between planned and actual architectures is sometimes understood in terms of the notion of technical debt.As an example, consider a strictly layered system, where each layer can only use services provided by the layer immediately below it. Any source code component that does not observe this constraint represents an architecture violation. If not corrected, such violations can transform the architecture into a monolithic block, with adverse effects on understandability, maintainability, and evolvability.Various approaches have been proposed to address erosion. "These approaches, which include tools, techniques, and processes, are primarily classified into three general categories that attempt to minimize, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery, and reconciliation."There are two major techniques to detect architectural violations: reflexion models and domain-specific languages. Reflexion model (RM) techniques compare a high-level model provided by the system's architects with the source code implementation. There are also domain-specific languages with a focus on specifying and checking architectural constraints.=== Software architecture recovery ===Software architecture recovery (or reconstruction, or reverse engineering) includes the methods, techniques, and processes to uncover a software system's architecture from available information, including its implementation and documentation. Architecture recovery is often necessary to make informed decisions in the face of obsolete or out-of-date documentation and architecture erosion: implementation and maintenance decisions diverging from the envisioned architecture. Practices exist to recover software architecture as Static program analysis. This is a part of subjects covered by the Software intelligence practice.== Related fields ===== Design ===Architecture is design but not all design is architectural. In practice, the architect is the one who draws the line between software architecture (architectural design) and detailed design (non-architectural design). There are no rules or guidelines that fit all cases, although there have been attempts to formalize the distinction. According to the Intension/Locality Hypothesis, the distinction between architectural and detailed design is defined by the Locality Criterion, according to which a statement about software design is non-local (architectural) if and only if a program that satisfies it can be expanded into a program that does not. For example, the client–server style is architectural (strategic) because a program that is built on this principle can be expanded into a program that is not client–server—for example, by adding peer-to-peer nodes.=== Requirements engineering ===Requirements engineering and software architecture can be seen as complementary approaches: while software architecture targets the 'solution space' or the 'how', requirements engineering addresses the 'problem space' or the 'what'. Requirements engineering entails the elicitation, negotiation, specification, validation, documentation and management of requirements. Both requirements engineering and software architecture revolve around stakeholder concerns, needs and wishes.There is considerable overlap between requirements engineering and software architecture, as evidenced for example by a study into five industrial software architecture methods that concludes that "the inputs (goals, constraints, etc.) are usually ill-defined, and only get discovered or better understood as the architecture starts to emerge" and that while "most architectural concerns are expressed as requirements on the system, they can also include mandated design decisions". In short, the choice of required behavior given a particular problem impacts the architecture of the solution that addresses that problem, while at the same time the architectural design may impact the problem and introduce new requirements. Approaches such as the Twin Peaks model aim to exploit the synergistic relation between requirements and architecture.=== Other types of 'architecture' ===Computer architectureComputer architecture targets the internal structure of a computer system, in terms of collaborating hardware components such as the CPU – or processor – the bus and the memory.Systems architectureThe term systems architecture has originally been applied to the architecture of systems that consists of both hardware and software. The main concern addressed by the systems architecture is then the integration of software and hardware in a complete, correctly working device. In another common – much broader – meaning, the term applies to the architecture of any complex system which may be of technical, sociotechnical or social nature.Enterprise architectureThe goal of enterprise architecture is to "translate business vision and strategy into effective enterprise". Enterprise architecture frameworks, such as TOGAF and the Zachman Framework, usually distinguish between different enterprise architecture layers. Although terminology differs from framework to framework, many include at least a distinction between a business layer, an application (or information) layer, and a technology layer. Enterprise architecture addresses among others the alignment between these layers, usually in a top-down approach.== See also ==Architectural pattern (computer science)Anti-patternAttribute-driven designComputer architectureDistributed Data Management ArchitectureDistributed Relational Database ArchitectureSystems architectureSystems designSoftware Architecture Analysis MethodTime-triggered system== References ==== Further reading ==Paul Clements, Felix Bachmann, Len Bass, David Garlan, James Ivers, Reed Little, Paulo Merson, Robert Nord, Judith Stafford: Documenting Software Architectures: Views and Beyond, Second Edition. Addison-Wesley, 2010, ISBN 0-321-55268-7.This book describes what software architecture is and shows how to document it in multiple views, using UML and other notations. It also explains how to complement the architecture views with behavior, software interface, and rationale documentation. Accompanying the book is a wiki that contains an example of software architecture documentation.Len Bass, Paul Clements, Rick Kazman: Software Architecture in Practice, Third Edition. Addison Wesley, 2012, ISBN 0-321-81573-4 (This book, now in its third edition, eloquently covers the fundamental concepts of the discipline. The theme is centered on achieving quality attributes of a system.)Amnon H. Eden, Rick Kazman. Architecture, Design, Implementation. On the distinction between architectural design and detailed design.Garzás, Javier; Piattini, Mario (2005). "An ontology for micro-architectural design knowledge". IEEE Software. 22 (2): 28–33. doi:10.1109/MS.2005.26.Kruchten, Philippe (1995). "Architectural Blueprints – The '4+1' View Model of Software Architecture" (PDF). IEEE Software. 12 (6): 42–50. doi:10.1109/52.469759.Shan, Tony; Hua, Winnie (October 2006). "Solution Architecting Mechanism". Proceedings of the 10th IEEE International EDOC Enterprise Computing Conference: 23–32. doi:10.1109/EDOC.2006.54. ISBN 978-0-7695-2558-7.Martin Fowler (with Ralph Johnson) Who Needs an Architect? IEEE Software, Jul/Aug 2003Bell, Michael (2008). Service-Oriented Modeling: Service Analysis, Design, and Architecture. Wiley. ASIN 0470141115.CS1 maint: ASIN uses ISBN (link)== External links ==Explanation on IBM DeveloperworksCollection of software architecture definitions at Software Engineering Institute (SEI), Carnegie Mellon University (CMU)International Association of IT Architects (IASA Global), formerly known as the International Association for Software Architects (IASA)SoftwareArchitecturePortal.org – website of IFIP Working Group 2.10 on Software ArchitectureSoftwareArchitectures.com – independent resource of information on the disciplineSoftware Architecture, chapter 1 of Roy Fielding's REST dissertationWhen Good Architecture Goes BadThe Spiral Architecture Driven Development – the SDLC based on the Spiral model aims to reduce the risks of ineffective architectureSoftware Architecture Real Life Case Studies
	Software visualization or software visualisation refers to the visualization of information of and related to software systems—either the architecture of its source code or metrics of their runtime behavior- and their development process by means of static, interactive or animated 2-D or 3-D visual representations of their structure, execution, behavior, and evolution.== Software system information ==Software visualization uses a variety of information available about software systems. Key information categories include: implementation artifacts such as source codes,software metric data from measurements or from reverse engineering,traces that record execution behavior,software testing data (e.g., test coverage)software repository data that tracks changes.== Objectives ==The objectives of software visualization are to support the understanding of software systems (i.e., its structure) and algorithms (e.g., by animating the behavior of sorting algorithms) as well as the analysis and exploration of software systems and their anomalies (e.g., by showing classes with high coupling) and their development and evolution. One of the strengths of software visualization is to combine and relate information of software systems that are not inherently linked, for example by projecting code changes onto software execution traces.Software visualization can be used as tool and technique to explore and analyze software system information, e.g., to discover anomalies similar to the process of visual data mining. For example, software visualization is used to monitoring activities such as for code quality or team activity. Visualization is not inherently a method for software quality assurance. Software visualization participates to Software Intelligence in allowing to discover and take advantage of mastering inner components of software systems.== Types ==Tools for software visualization might be used to visualize source code and quality defects during software development and maintenance activities. There are different approaches to map source code to a visual representation such as by software maps Their objective includes, for example, the automatic discovery and visualization of quality defects in object-oriented software systems and services. Commonly, they visualize the direct relationship of a class and its methods with other classes in the software system and mark potential quality defects. A further benefit is the support for visual navigation through the software system.More or less specialized graph drawing software is used for software visualization. A small-scale 2003 survey of researchers active in the reverse engineering and software maintenance fields found that a wide variety of visualization tools were used, including general purpose graph drawing packages like GraphViz and GraphEd, UML tools like Rational Rose and Borland Together, and more specialized tools like Visualization of Compiler Graphs (VCG) and Rigi. The range of UML tools that can act as a visualizer by reverse engineering source is by no means short; a 2007 book noted that besides the two aforementioned tools, ESS-Model, BlueJ, and Fujaba also have this capability, and that Fujaba can also identify design patterns.== See also ==ProgramsImagix 4DNDependSotoarcSourcetrail[1]Softagram[2]Getaviz[3]SonarGraph[4]Related conceptsApplication discovery and understandingSoftware maintenanceSoftware mapsSoftware diagnosisCognitive dimensions of notationsSoftware archaeology== References ==== Further reading ==Roels, R., Mestereaga, P., and Signer, B. (2016). "An Interactive Source Code Visualisation Plug-in for the MindXpres Presentation Platform". Communications in Computer and Information Science (CCIS), 583, 2016Burch, M., Diehl, S., and Weißgerber, P. (2005). Visual data mining in software archives. Proceedings of the 2005 ACM symposium on Software visualization (SoftVis '05). ACM, New York, NY, USA, 37-46. doi:10.1145/1056018.1056024Diehl, S. (2002). Software Visualization. International Seminar. Revised Papers (LNCS Vol. 2269), Dagstuhl Castle, Germany, 20–25 May 2001 (Dagstuhl Seminar Proceedings).Diehl, S. (2007). Software Visualization — Visualizing the Structure, Behaviour, and Evolution of Software. Springer, 2007, ISBN 978-3-540-46504-1Eades, P. and Zhang, K. (1996). "Software Visualisation", Series on Software Engineering and Knowledge Engineering, Vol.7, World Scientific Co., Singapore, 1996, ISBN 981-02-2826-0, 268 pages.Gîrba, T., Kuhn, A., Seeberger, M., and Ducasse, S., "How Developers Drive Software Evolution," Proceedings of International Workshop on Principles of Software Evolution (IWPSE 2005), IEEE Computer Society Press, 2005, pp. 113–122. PDFKeim, D. A. (2002). Information visualization and visual data mining. IEEE Transactions on Visualization and Computer Graphics, USA * vol 8 (Jan. March 2002), no 1, p 1 8, 67 refs.Knight, C. (2002). System and Software Visualization. In Handbook of software engineering & knowledge engineering. Vol. 2, Emerging technologies (Vol. 2): World Scientific Publishing Company.Kuhn, A., and Greevy, O., "Exploiting the Analogy Between Traces and Signal Processing," Proceedings IEEE International Conference on Software Maintenance (ICSM 2006), IEEE Computer Society Press, Los Alamitos CA, September 2006. PDFLanza, M. (2004). CodeCrawler — polymetric views in action. Proceedings. 19th International Conference on Automated Software Engineering, Linz, Austria, 20 24 Sept. 2004 * Los Alamitos, CA, USA: IEEE Comput. Soc, 2004, p 394 5.Lopez, F. L., Robles, G., & Gonzalez, B. J. M. (2004). Applying social network analysis to the information in CVS repositories. "International Workshop on Mining Software Repositories (MSR 2004)" W17S Workshop 26th International Conference on Software Engineering, Edinburgh, Scotland, UK, 25 May 2004 * Stevenage, UK: IEE, 2004, p 101 5.Marcus, A., Feng, L., & Maletic, J. I. (2003). 3D representations for software visualization. Paper presented at the Proceedings of the 2003 ACM symposium on Software visualization, San Diego, California.Soukup, T. (2002). Visual data mining : techniques and tools for data visualization and mining. New York: Chichester.Staples, M. L., & Bieman, J. M. (1999). 3-D Visualization of Software Structure. In Advances in Computers (Vol. 49, pp. 96–143): Academic Press, London.Stasko, J. T., Brown, M. H., & Price, B. A. (1997). Software Visualization: MIT Press.Van Rysselberghe, F. (2004). Studying Software Evolution Information By Visualizing the Change History. Proceedings. 20th International Conference On Software Maintenance. pp 328–337, IEEE Computer Society Press, 2004Wettel, R., and Lanza, M., Visualizing Software Systems as Cities. In Proceedings of VISSOFT 2007 (4th IEEE International Workshop on Visualizing Software For Understanding and Analysis), pp. 92 – 99, IEEE Computer Society Press, 2007.Zhang, K. (2003). "Software Visualization - From Theory to Practice". Kluwer Academic Publishers, Boston, April 2003, ISBN 1-4020-7448-4, 468 pages.== External links ==SoftVis the ACM Symposium on Software VisualizationVISSOFT 2nd IEEE Working Conference on Software VisualizationEPDV Eclipse Project Dependencies Viewer
	A specification language is a formal language in computer science used during systems analysis, requirements analysis, and systems design to describe a system at a much higher level than a programming language, which is used to produce the executable code for a system.Specification languages are generally not directly executed. They are meant to describe the what, not the how. Indeed, it is considered as an error if a requirement specification is cluttered with unnecessary implementation detail.A common fundamental assumption of many specification approaches is that programs are modelled as algebraic or model-theoretic structures that include a collection of sets of data values together with functions over those sets.  This level of abstraction coincides with the view that the correctness of the input/output behaviour of a program takes precedence over all its other properties.In the property-oriented approach to specification (taken e.g. by CASL), specifications of programs consist mainly of logical axioms, usually in a logical system in which equality has a prominent role, describing the properties that the functions are required to satisfy - often just by their interrelationship.This is in contrast to so-called model-oriented specification in frameworks like VDM and Z, which consist of a simple realization of the required behaviour.Specifications must be subject to a process of refinement (the filling-in of implementation detail) before they can actually be implemented. The result of such a refinement process is an executable algorithm, which is either formulated in a programming language, or in an executable subset of the specification language at hand. For example, Hartmann pipelines, whenproperly applied, may be considered a dataflow specification which is directly executable.  Another example is the Actor model which has no specific application content and must be specialized to be executable.An important use of specification languages is enabling the creation of proofs of program correctness (see theorem prover).== Languages ==Attempto Controlled EnglishCASLVDMZ notationTLA+LePUS3 (a visual, object-oriented design description language)PerfectAlloyLOTOSE-LOTOSRefine LanguageSequenceLSMVSDL== See also ==Formal specificationLanguage-independent specificationUnified Modeling LanguageSDL== References ==
	A system context diagram (SCD) in engineering is a diagram that defines the boundary between the system, or part of a system, and its environment, showing the entities that interact with it. This diagram is a high level view of a system. It is similar to a block diagram.== Overview ==System context diagrams show a system, as a whole and its inputs and outputs from/to external factors. According to Kossiakoff and Sweet (2011):System Context Diagrams ... represent all external entities that may interact with a system ... Such a diagram pictures the system at the center, with no details of its interior structure, surrounded by all its interacting systems, environments and activities. The objective of the system context diagram is to focus attention on external factors and events that should be considered in developing a complete set of systems requirements and constraints.System context diagrams are used early in a project to get agreement on the scope under investigation. Context diagrams are typically included in a requirements document. These diagrams must be read by all project stakeholders and thus should be written in plain language, so the stakeholders can understand items within the document.== Building blocks ==Context diagrams can be developed with the use of two types of building blocks:Entities (Actors): labeled boxes; one in the center representing the system, and around it multiple boxes for each external actorRelationships: labeled lines between the entities and systemFor example, "customer places order." Context diagrams can also use many different drawing types to represent external entities. They can use ovals, stick figures, pictures, clip art or any other representation to convey meaning. Decision trees and data storage are represented in system flow diagrams.A context diagram can also list the classifications of the external entities as one of a set of simple categories (Examples:), which add clarity to the level of involvement of the entity with regards to the system. These categories include:Active: Dynamic to achieve some goal or purpose (Examples: "Article readers" or "customers").Passive: Static external entities which infrequently interact with the system (Examples: "Article editors" or "database administrator").Cooperative: Predictable external entities which are used by the system to bring about some desired outcome (Examples: "Internet service providers" or "shipping companies").Autonomous (Independent): External entities which are separated from the system, but affect the system indirectly, by means of imposed constraints or similar influences (Examples: "regulatory committees" or "standards groups").== Alternatives ==The best system context diagrams are used to display how a system interoperates at a very high level, or how systems operate and interact logically. The system context diagram is a necessary tool in developing a baseline interaction between systems and actors; actors and a system or systems and systems. Alternatives to the system context diagram are:Architecture Interconnect Diagram: The figure gives an example of an Architecture Interconnect Diagram: A representation of the Albuquerque regional ITS architecture interconnects for the Albuquerque Police Department that was generated using the Turbo Architecture tool is shown in the figure. Each block represents an ITS inventory element, including the name of the stakeholder in the top shaded portion. The interconnect lines between elements are solid or dashed, indicating existing or planned connections.Business Model Canvas, a strategic management template for developing new or documenting existing business models. It is a visual chart with elements describing a firm's value proposition, infrastructure, customers, and finances.[1] It assists firms in aligning their activities by illustrating potential trade-offs.Enterprise data model: this type of data model according to Simsion (2005) can contain up to 50 to 200 entity classes, which results from specific "high level of generalization in data modeling".IDEF0 Top Level Context Diagram: The IDEF0 process starts with the identification of the prime function to be decomposed. This function is identified on a "Top Level Context Diagram" that defines the scope of the particular IDEF0 analysis.Problem Diagrams (Problem Frames): In addition to the kinds of things shown on a context diagram, a problem diagram shows requirements and requirements references.Use case diagram: One of the Unified Modeling Language diagrams. They also represent the scope of the project at a similar level of abstraction. - Use Cases, however, tend to focus more on the goals of 'actors' who interact with the system, and do not specify any solution. Use Case diagrams represent a set of Use Cases, which are textual descriptions of how an actor achieves the goal of a use case. for Example Customer Places Order.ArchiMate: ArchiMate is an open and independent enterprise architecture modeling language to support the description, analysis and visualization of architecture within and across business domains in an unambiguous way.Most of these diagrams work well as long as a limited number of interconnects will be shown. Where twenty or more interconnects must be displayed, the diagrams become quite complex and can be difficult to read.== See also ==Data flow diagramInformation flow diagramEvent partitioningList of graphical methodsNetwork diagramRequirements analysisSoftware development processSystems analysis== References ==== External links ==Context Diagram TemplateSYSMOD's System Context Diagram
	In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.Some people consider a life-cycle "model" a more general term for a category of methodologies and a software development "process" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.== History ==The software development methodology (also known as SDM) framework didn't emerge until the 1960s. According to Elliott (2004) the systems development life cycle (SDLC) can be considered to be the oldest formalized methodology framework for building information systems. The main idea of the SDLC has been "to pursue the development of information systems in a very deliberate, structured and methodical way, requiring each stage of the life cycle––from inception of the idea to delivery of the final system––to be carried out rigidly and sequentially" within the context of the framework being applied. The main target of this methodology framework in the 1960s was "to develop large scale functional business systems in an age of large scale business conglomerates. Information systems activities revolved around heavy data processing and number crunching routines".Methodologies, processes, and frameworks range from specific proscriptive steps that can be used directly by an organization in day-to-day work, to flexible frameworks that an organization uses to generate a custom set of steps tailored to the needs of a specific project or group.  In some cases a "sponsor" or "maintenance" organization distributes an official set of documents that describe the process.  Specific examples include:1970sStructured programming since 1969Cap Gemini SDM, originally from PANDATA, the first English translation was published in 1974. SDM stands for System Development Methodology1980sStructured systems analysis and design method (SSADM) from 1980 onwardsInformation Requirement Analysis/Soft systems methodology1990sObject-oriented programming (OOP) developed in the early 1960s, and became a dominant programming approach during the mid-1990sRapid application development (RAD), since 1991Dynamic systems development method (DSDM), since 1994Scrum, since 1995Team software process, since 1998Rational Unified Process (RUP), maintained by IBM since 1998Extreme programming, since 19992000sAgile Unified Process (AUP) maintained since 2005 by Scott AmblerDisciplined agile delivery (DAD) Supersedes AUP2010sScaled Agile Framework (SAFe)Large-Scale Scrum (LeSS)DevOpsIt is notable that since DSDM in 1994, all of the methodologies on the above list except RUP have been agile methodologies - yet many organisations, especially governments, still use  pre-agile processes (often waterfall or similar). Software process and software quality are closely interrelated; some unexpected facets and effects have been observed in practice  Since the early 2000s scaling agile delivery processes has become the biggest challenge for teams using agile processes.Among these another software development process has been established in open source. The adoption of these best practices known and established processes within the confines of a company is called inner source.== Practices ==Several software development approaches have been used since the origin of information technology, in two main categories.  Typically an approach or a combination of approaches is chosen by management or a development team."Traditional" methodologies such as waterfall that have distinct phases are sometimes known as software development life cycle (SDLC) methodologies, though this term could also be used more generally to refer to any methodology.  A "life cycle" approach with distinct phases is in contrast to Agile approaches which define a process of iteration, but where design, construction, and deployment of different pieces can occur simultaneously.=== Continuous integration ===Continuous integration is the practice of merging all developer working copies to a shared mainline several times a day. Grady Booch first named and proposed CI in his 1991 method, although he did not advocate integrating several times a day. Extreme programming (XP) adopted the concept of CI and did advocate integrating more than once per day – perhaps as many as tens of times per day.=== Prototyping ===Software prototyping is about creating prototypes, i.e. incomplete versions of the software program being developed.The basic principles are:Prototyping is not a standalone, complete development methodology, but rather an approach to try out particular features in the context of a full methodology (such as incremental, spiral, or rapid application development (RAD)).Attempts to reduce inherent project risk by breaking a project into smaller segments and providing more ease-of-change during the development process.The client is involved throughout the development process, which increases the likelihood of client acceptance of the final implementation.While some prototypes are developed with the expectation that they will be discarded, it is possible in some cases to evolve from prototype to working system.A basic understanding of the fundamental business problem is necessary to avoid solving the wrong problems, but this is true for all software methodologies.=== Incremental development ===Various methods are acceptable for combining linear and iterative systems development methodologies, with the primary objective of each being to reduce inherent project risk by breaking a project into smaller segments and providing more ease-of-change during the development process.There are three main variants of incremental development:A series of mini-Waterfalls are performed, where all phases of the Waterfall are completed for a small part of a system, before proceeding to the next increment, orOverall requirements are defined before proceeding to evolutionary, mini-Waterfall development of individual increments of a system, orThe initial software concept, requirements analysis, and design of architecture and system core are defined via Waterfall, followed by incremental implementation, which culminates in installing the final version, a working system.=== Rapid application development ===Rapid application development (RAD) is a software development methodology, which favors iterative development and the rapid construction of prototypes instead of large amounts of up-front planning. The "planning" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster, and makes it easier to change requirements.The rapid development process starts with the development of preliminary data models and business process models using structured techniques. In the next stage, requirements are verified using prototyping, eventually to refine the data and process models. These stages are repeated iteratively; further development results in "a combined business requirements and technical design statement to be used for constructing new systems".The term was first used to describe a software development process introduced by James Martin in 1991.  According to Whitten (2003), it is a merger of various structured techniques, especially data-driven information technology engineering, with prototyping techniques to accelerate software systems development.The basic principles of rapid application development are:Key objective is for fast development and delivery of a high quality system at a relatively low investment cost.Attempts to reduce inherent project risk by breaking a project into smaller segments and providing more ease-of-change during the development process.Aims to produce high quality systems quickly, primarily via iterative Prototyping (at any stage of development), active user involvement, and computerized development tools. These tools may include Graphical User Interface (GUI) builders, Computer Aided Software Engineering (CASE) tools, Database Management Systems (DBMS), fourth-generation programming languages, code generators, and object-oriented techniques.Key emphasis is on fulfilling the business need, while technological or engineering excellence is of lesser importance.Project control involves prioritizing development and defining delivery deadlines or “timeboxes”. If the project starts to slip, emphasis is on reducing requirements to fit the timebox, not in increasing the deadline.Generally includes joint application design (JAD), where users are intensely involved in system design, via consensus building in either structured workshops, or electronically facilitated interaction.Active user involvement is imperative.Iteratively produces production software, as opposed to a throwaway prototype.Produces documentation necessary to facilitate future development and maintenance.Standard systems analysis and design methods can be fitted into this framework.== Methodologies ===== Agile development ==="Agile software development" refers to a group of software development methodologies based on iterative development, where requirements and solutions evolve via collaboration between self-organizing cross-functional teams. The term was coined in the year 2001 when the Agile Manifesto was formulated.Agile software development uses iterative development as a basis but advocates a lighter and more people-centric viewpoint than traditional approaches. Agile processes fundamentally incorporate iteration and the continuous feedback that it provides to successively refine and deliver a software system.There are many agile methodologies, including:Dynamic systems development method (DSDM)KanbanScrum=== Waterfall development ===The waterfall model is a sequential development approach, in which development is seen as flowing steadily downwards (like a waterfall) through several phases, typically:Requirements analysis resulting in a software requirements specificationSoftware designImplementationTestingIntegration, if there are multiple subsystemsDeployment (or Installation)MaintenanceThe first formal description of the method is often cited as an article published by Winston W. Royce in 1970 although Royce did not use the term "waterfall" in this article. Royce presented this model as an example of a flawed, non-working model.The basic principles are:Project is divided into sequential phases, with some overlap and splashback acceptable between phases.Emphasis is on planning, time schedules, target dates, budgets and implementation of an entire system at one time.Tight control is maintained over the life of the project via extensive written documentation, formal reviews, and approval/signoff by the user and information technology management occurring at the end of most phases before beginning the next phase.  Written documentation is an explicit deliverable of each phase.The waterfall model is a traditional engineering approach applied to software engineering.  A strict waterfall approach discourages revisiting and revising any prior phase once it is complete. This "inflexibility" in a pure waterfall model has been a source of criticism by supporters of other more "flexible" models.  It has been widely blamed for several large-scale government projects running over budget, over time and sometimes failing to deliver on requirements due to the Big Design Up Front approach. Except when contractually required, the waterfall model has been largely superseded by more flexible and versatile methodologies developed specifically for software development. See Criticism of Waterfall model.=== Spiral development ===In 1988, Barry Boehm published a formal software system development "spiral model," which combines some key aspect of the waterfall model and rapid prototyping methodologies, in an effort to combine advantages of top-down and bottom-up concepts.  It provided emphasis in a key area many felt had been neglected by other methodologies: deliberate iterative risk analysis, particularly suited to large-scale complex systems.The basic principles are:Focus is on risk assessment and on minimizing project risk by breaking a project into smaller segments and providing more ease-of-change during the development process, as well as providing the opportunity to evaluate risks and weigh consideration of project continuation throughout the life cycle."Each cycle involves a progression through the same sequence of steps, for each part of the product and for each of its levels of elaboration, from an overall concept-of-operation document down to the coding of each individual program."Each trip around the spiral traverses four basic quadrants: (1) determine objectives, alternatives, and constraints of the iteration; (2) evaluate alternatives; Identify and resolve risks; (3) develop and verify deliverables from the iteration; and (4) plan the next iteration.Begin each cycle with an identification of stakeholders and their "win conditions", and end each cycle with review and commitment.=== Offshore development ===Offshore custom software development aims at dispatching the software development process over various geographical areas to optimize project spending by capitalizing on countries with lower salaries and operating costs. Geographically distributed teams can be integrated at any point of the software development process through custom hybrid models.=== Other ===Other high-level software project methodologies include:Behavior-driven development and business process managementChaos model - The main rule is always resolve the most important issue first.Incremental funding methodology - an iterative approachLightweight methodology - a general term for methods that only have a few rules and practicesStructured systems analysis and design method - a specific version of waterfallSlow programming, as part of the larger Slow Movement, emphasizes careful and gradual work without (or minimal) time pressures. Slow programming aims to avoid bugs and overly quick release schedules.V-Model (software development) - an extension of the waterfall modelUnified Process (UP) is an iterative software development methodology framework, based on Unified Modeling Language (UML). UP organizes the development of software into four phases, each consisting of one or more executable iterations of the software at that stage of development: inception, elaboration, construction, and guidelines. Many tools and products exist to facilitate UP implementation. One of the more popular versions of UP is the Rational Unified Process (RUP).== Process meta-models ==Some "process models" are abstract descriptions for evaluating, comparing, and improving the specific process adopted by an organization.ISO/IEC 12207 is the international standard describing the method to select, implement, and monitor the life cycle for software.The Capability Maturity Model Integration (CMMI) is one of the leading models and based on best practice. Independent assessments grade organizations on how well they follow their defined processes, not on the quality of those processes or the software produced. CMMI has replaced CMM.ISO 9000 describes standards for a formally organized process to manufacture a product and the methods of managing and monitoring progress. Although the standard was originally created for the manufacturing sector, ISO 9000 standards have been applied to software development as well. Like CMMI, certification with ISO 9000 does not guarantee the quality of the end result, only that formalized business processes have been followed.ISO/IEC 15504 Information technology — Process assessment also known as Software Process Improvement Capability Determination (SPICE), is a "framework for the assessment of software processes". This standard is aimed at setting out a clear model for process comparison. SPICE is used much like CMMI. It models processes to manage, control, guide and monitor software development. This model is then used to measure what a development organization or project team actually does during software development. This information is analyzed to identify weaknesses and drive improvement. It also identifies strengths that can be continued or integrated into common practice for that organization or team.ISO/IEC 24744 Software Engineering — Metamodel for Development Methodologies, is a powertype-based metamodel for software development methodologies.SPEM 2.0 by the Object Management GroupSoft systems methodology - a general method for improving management processesMethod engineering - a general method for improving information system processes== In practice ==A variety of such frameworks have evolved over the years, each with its own recognized strengths and weaknesses. One software development methodology framework is not necessarily suitable for use by all projects. Each of the available methodology frameworks are best suited to specific kinds of projects, based on various technical, organizational, project and team considerations.Software development organizations implement process methodologies to ease the process of development. Sometimes, contractors may require methodologies employed, an example is the U.S. defense industry, which requires a rating based on process models to obtain contracts.  The international standard for describing the method of selecting, implementing and monitoring the life cycle for software is ISO/IEC 12207.A decades-long goal has been to find repeatable, predictable processes that improve productivity and quality. Some try to systematize or formalize the seemingly unruly task of designing software. Others apply project management techniques to designing software. Large numbers of software projects do not meet their expectations in terms of functionality, cost, or delivery schedule - see List of failed and overbudget custom software projects for some notable examples.Organizations may create a Software Engineering Process Group (SEPG), which is the focal point for process improvement. Composed of line practitioners who have varied skills, the group is at the center of the collaborative effort of everyone in the organization who is involved with software engineering process improvement.A particular development team may also agree to programming environment details, such as which integrated development environment is used, and one or more dominant programming paradigms, programming style rules, or choice of specific software libraries or software frameworks.  These details are generally not dictated by the choice of model or general methodology.== See also ==Systems development life cycleComputer-aided software engineering (some of these tools support specific methodologies)List of software development philosophiesOutline of software engineeringOpenUPProject managementSoftware developmentSoftware development effort estimationSoftware release life cycleTop-down and bottom-up design#Computer science== References ==== External links ==Selecting a development approach at cms.hhs.gov.Gerhard Fischer, "The Software Technology of the 21st Century: From Software Reuse to Collaborative Software Design", 2001Subway map of agile practices at Agile Alliance
	This is an alphabetical list of articles pertaining specifically to software engineering.== 0–9 ==2D computer graphics —3D computer graphics== A ==Abstract syntax tree —Abstraction —Accounting software —Ada —Addressing mode —Agile software development —Algorithm —Antipattern —Application framework —Application software —Artificial intelligence —Artificial neural network —ASCII —Aspect-oriented programming —Assembler —Assembly language —Assertion —Automata theory —Automotive software —Avionics software== B ==Backward compatibility —BASIC —BCPL —Berkeley Software Distribution —Beta test —Boolean logic —Business software== C ==C —C++ —C# —CAD —Canonical Model —Capability Maturity Model —Capability Maturity Model Integration —COBOL —Code coverage —Cohesion —Compilers —Complexity —Computation —Computational complexity theory —Computer —Computer-aided design —Computer-aided manufacturing —Computer architecture —Computer bug —Computer file —Computer graphics —Computer model —Computer multitasking —Computer programming —Computer science —Computer software —Computer term etymologies —Concurrent programming —Configuration management —Coupling —Cyclomatic complexity== D ==Data structure —Data-structured language —Database —Dead code —Decision table —Declarative programming —Design pattern —Development stage —Device driver —Disassembler —Disk image —Domain-specific language== E ==EEPROM —Electronic design automation —Embedded system —Engineering —Engineering model —EPROM —Even-odd rule —Expert system —Extreme programming== F ==FIFO (computing and electronics) —File system —Filename extension —Finite state machine —Firmware —Formal methods —Forth —Fortran —Forward compatibility —Functional decomposition —Functional design —Functional programming== G ==Game development —Game programming —Game tester —GIMP Toolkit —Graphical user interface== H ==Hierarchical database —High-level language —Hoare logic —Human–computer interaction —Hyperlink —Hyper-threading== I ==IEEE Software —Imperative programming —Information technology engineering —Information systems —Information technology —Instruction set —Interactive programming —Interface description language —Intermediate language —Interpreter —Invariant —ISO —ISO 9000 —ISO 9001 —ISO 9660 —ISO/IEC 12207 —ISO image —Iterative development== J ==Java —Java Modeling Language —Java virtual machine== K ==Kernel —Knowledge management== L ==Level design —Level designer —LIFO —Linux —List of programming languages —Literate programming== M ==Machine code —Machine language —Mainframe —Medical informatics —Medical software —Mesh networking —Metadata (computing) —Microcode —Microprogram —Microsoft Windows —Minicomputer —MIPS architecture —Multi-paradigm programming language== N ==Neural network software —Numerical analysis== O ==Object code —Object database —Object-oriented programming —Ontology —Opcode —Open implementation —Open-source software —Operating system== P ==Packet writing —Pair programming —Parallax scrolling —Pascal —p-code machine —Perl —PHP —Post-object programming —Privacy Engineering -Procedural programming —Processor register —Program specification —Programming language —Programming paradigm —Programming tool —Project lifecycle —Proprietary software —Python== Q ==Qt (toolkit) —Query optimizer —Queueing theory== R ==Rapid application development —Rational Unified Process —Real-time operating system —Refactoring —Reflection —Regression testing —Relational database —Release to manufacturing —Reliability (engineering) —Requirement —Requirements analysis —Revision control —Robotics== S ==Scripting language —Second-system effect —Signal analysis —Simulation —Software —Software architecture —Software bloat —Software brittleness —Software componentry —Software configuration management —Software development cycle —Software development process —Software engineering —Software framework —Software maintenance —Software metric —Source code —Source lines of code —Specification language —Sprite —SQL —Standard data model —SCAMPI —Stack (data structure) —Static code analysis —Static single assignment form —Statistical package —String —Structured programming —Structured Query Language —Subroutine —Supercomputer —System development life cycle —Systems architect —Systems design —SPICE (ISO15504)== T ==Tcl —Texture mapping —Theory of computation —Think aloud protocol —Thread —Threaded code —Three-address code —Timeboxing —TinyOS== U ==UCSD p-System —Unix —Usability —Usability testing —User interface== V ==Video games —Virtual finite state machine —Visual Basic== W ==Waterfall model —Wiki —Windows —Windows Vista== X ==Xerox PARC== Y ==YouTube-== Z ==Z notation
	In computer science, overhead is any combination of excess or indirect computation time, memory, bandwidth, or other resources that are required to perform a specific task. It is a special case of engineering overhead. Overhead can be a deciding factor in software design, with regard to structure, error correction, and feature inclusion.  Examples of computing overhead may be found in functional programming, data transfer, and data structures.== Software design ===== Choice of implementation ===A programmer/software engineer may have a choice of several algorithms, encodings, data types or data structures, each of which have known characteristics. When choosing among them, their respective overhead should also be considered.=== Tradeoffs ===In software engineering, overhead can influence the decision whether or not to include features in new products, or indeed whether to fix bugs. A feature that has a high overhead may not be included – or needs a big financial incentive to do so. Often, even though software providers are well aware of bugs in their products, the payoff of fixing them is not worth the reward, because of the overhead.For example, an implicit data structure or succinct data structure may provide low space overhead, but at the cost of slow performance (space/time tradeoff).=== Run-time complexity of software ===Algorithmic complexity is generally specified using Big O Notation. This makes no comment on how long something takes to run or how much memory it uses, but how its increase depends on the size of the input. Overhead is deliberately not part of this calculation, since it varies from one machine to another, whereas the fundamental running time of an algorithm does not.This should be contrasted with algorithmic efficiency, which takes into account all kinds of resources – a combination (though not a trivial one) of complexity and overhead.== Examples ===== Computer Programming (run-time and computational overhead) ===Invoking a function introduces a small run-time overhead. Sometimes the compiler can minimize this overhead by inlining some of these function calls.=== CPU Caches ===In a CPU cache, the "cache size" (or capacity) refers to how much data a cache stores. For instance, a "4KB cache" is a cache that holds 4KB of data. The "4KB" in this example excludes overhead bits such as frame, address, and tag information.=== Communications (data transfer overhead) ===Reliably sending a payload of data over a communications network requires sending more than just payload itself. It also involves sending various control and signalling data (TCP) required to reach the destination. This creates a so-called protocol overhead as the additional data does not contribute to the intrinsic meaning of the message.In telephony, number dialing and call set-up time are overheads.  In 2-way (but half-duplex) radios, the use of "over" and other signalling needed to avoid collisions is an overhead.Protocol overhead can be expressed as a percentage of non-application bytes (protocol and frame synchronization) divided by the total number of bytes in the message.=== Encodings and data structures (size overhead) ===The encoding of information and data introduces overhead too. The date and time "2011-07-12 07:18:47" can be expressed as Unix time with the 32-bit signed integer 1310447927, consuming only 4 bytes. Represented as ISO 8601 formatted UTF-8 encoded string 2011-07-12 07:18:47 the date would consume 19 byte, a size overhead of 375% over the binary integer representation. As XML this date can be written as follows with an overhead of 218 characters, while adding the semantic context that it is a CHANGEDATE with index 1.   <?xml version="1.0" encoding="UTF-8"?>     <DATETIME qualifier="CHANGEDATE" index="1">     <YEAR>2011</YEAR>     <MONTH>07</MONTH>     <DAY>12</DAY>     <HOUR>07</HOUR>     <MINUTE>18</MINUTE>     <SECOND>47</SECOND>  </DATETIME>The 349 byte, resulting from the UTF-8 encoded XML, correlates to a size overhead of 8625% over the original integer representation.== See also ==Rule of least powerUniversal Turing machine== References ==
	Prosa Structured Analysis Tool is a visual systems and software development environment which supports industry standard SA/SD/RT structured analysis and design with real-time extensions modeling method. Prosa supports data flow diagrams, state transition diagrams and entity relationship diagrams using Chen's and Bachmans ER notations. Prosa has integrated data dictionary.Prosa actively guides the designer to create correct and consistent graphic diagrams. Prosa offers interactive checking between diagrams. Concurrent documentation integration ensures real-time link from design to documentation.Prosa automates diagram creation and checking, and produces C++, C#, Java code headers and SQL DDL for implementation.Concurrent documentation ensures accurate documents which are consistent with the software design.Prosa has an established position in analysis and design tool business. Prosa is used in areas like system and software development, telecommunications, automation, car manufacturing, machinery, banking, insurance, defense/military, research, integrated circuit design, etc.== See also ==Structured Analysis== References ==== External links ==Official websiteSpecifying Systems and applications with SA/SD/RT method, TRAINING COURSE: MODELING/SA, Insoft Oy, 75 pages, 2013.
	Software engineering professionalism is a movement to make software engineering a profession, with aspects such as degree and certification programs, professional associations, professional ethics, and government licensing.  The field is a licensed discipline in Texas in the United States (Texas Board of Professional Engineers, since 2013), Engineers Australia(Course Accreditation since 2001, not Licensing), and many provinces in Canada.== History ==In 1993 the IEEE and ACM began a joint effort called JCESEP, which evolved into SWECC in 1998 to explore making software engineering into a profession. The ACM pulled out of SWECC in May 1999, objecting to its support for the Texas professionalization efforts, of having state licenses for software engineers.  ACM determined that the state of knowledge and practice in software engineering was too immature to warrant licensing,and that licensing would give false assurances of competence even if the body of knowledge were mature.The IEEE continued to support making software engineering a branch of traditional engineering.In Canada the Canadian Information Processing Society established the Information Systems Professional certification process.  Also, by the late 90's (1999 in British Columbia) the discipline of software engineering as a professional engineering discipline was officially created.  This has caused some disputes between the provincial engineering associations and companies who call their developers software engineers, even though these developers have not been licensed by any engineering association.In 1999, the Panel of Software Engineering was formed as part of the settlement between Engineering Canada and the Memorial University of Newfoundland over the school's use of the term "software engineering" in the name of a computer science program. Concerns were raised over inappropriate use of the name "software engineering" to describe non-engineering programs could lead to student and public confusion, and ultimately threaten public safety. The Panel issued recommendations to create a Software Engineering Accreditation Board, but the task force created to carry out the recommendations were unable to get the various stakeholders to agree to concrete proposals, resulting in separate accreditation boards.== Ethics ==Software engineering ethics is a large field. In some ways it began as an unrealistic attempt to define bugs as unethical.  More recently it has been defined as the application of both computer science and engineering philosophy, principles, and practices to the design and development of software systems.  Due to this engineering focus and the increased use of software in mission critical and human critical systems, where failure can result in large losses of capital but more importantly lives such as the Therac-25 system, many ethical codes have been developed by a number of societies, associations and organizations. These entities, such as the ACM, IEEE, APEGBC and Institute for Certification of Computing Professionals (ICCP) have formal codes of ethics. Adherence to the code of ethics is required as a condition of membership or certification. According to the ICCP, violation of the code can result in revocation of the certificate.  Also, all engineering societies require conformance to their ethical codes; violation of the code results in the revocation of the license to practice engineering in the society's jurisdiction.These codes of ethics usually have much in common.  They typically relate the need to act consistently with the client's interest, employer's interest, and most importantly the public's interest.  They also outline the need to act with professionalism and to promote an ethical approach to the profession.A Software Engineering Code of Ethics has been approved by the ACM and the IEEE-CS as the standard for teaching and practicing software engineering.=== Examples of codes of conduct ===The following are examples of codes of conduct for Professional Engineers.  These 2 have been chosen because both jurisdictions have a designation for Professional Software Engineers.Association of Professional Engineers and Geoscientists of British Columbia (APEGBC): All members in the association's code of Ethics must ensure that government, the public can rely on BC's professional engineers and Geoscientists to act at all times with fairness, courtesy and good faith to their employers, employee and customers, and to uphold the truth, honesty and trustworthiness, and to safe guard human life and the environment. This is just one of the many ways in which BC’s Professional Engineers and Professional Geoscientists maintain their competitive edge in today’s global marketplace.Association of Professional Engineers and Geoscientists of Alberta (APEGA): Different with British Columbia, the Alberta Government granted self governance to engineers, Geoscientists and geophysicists. All members in the APEGA have to accept legal and ethical responsibility for the work and to hold the interest of the public and society. The APEGA is a standards guideline of professional practice to uphold the protection of public interest for engineering, Geoscientists and geophysics in Alberta.=== Opinions on ethics ===Bill Joy argued that "better software" can only enable its privileged end users, make reality more power-pointy as opposed to more humane, and ultimately run away with itself so that "the future doesn't need us." He openly questioned the goals of software engineering in this respect, asking why it isn't trying to be more ethical rather than more efficient.  In his book Code and Other Laws of Cyberspace, Lawrence Lessig argues that computer code can regulate conduct in much the same way as the legal code.  Lessig and Joy urge people to think about the consequences of the software being developed, not only in a functional way, but also in how it affects the public and society as a whole.Overall, due to the youth of software engineering, many of the ethical codes and values have been borrowed from other fields, such as mechanical and civil engineering.  However, there are many ethical questions that even these, much older, disciplines have not encountered.  Questions about the ethical impact of internet applications, which have a global reach, have never been encountered until recently and other ethical questions are still to be encountered.  This means the ethical codes for software engineering are a work in progress, that will change and update as more questions arise.== Professional responsibilities in developing software ==Who's responsible?The developers work with clients and users to define system requirements. Once the system is built if any accidents occur, such as economical harm or other, who is responsible?If an independent QA team does integration testing and does not discover a critical fault in the system, who is ethically responsible for damage caused by that fault?Responsibilities for engineering and geoscience softwareDeveloping software is a highly risky proposition. The software development process is a complex undertaking consisting of specifying, designing, implementing, and testing. Any small mistake or fault will cause unlimited damage to society. Professional Members contribute to the success of software development projects. However, the Association of Professional Engineering and Geoscience is primarily concerned with their responsibility for minimizing the risk of failure and protecting the public interest.== Licensing ==The American National Society of Professional Engineers provides a model law and lobbies legislatures to adopt occupational licensing regulations. The model law requires:a four-year degree from a university program accredited by the Engineering Accreditation Committee (EAC) of the Accreditation Board for Engineering and Technology (ABET),an eight-hour examination on the fundamentals of engineering (FE) usually taken in the senior year of college,four years of acceptable experience,a second examination on principles and practice, andwritten recommendations from other professional engineers.Some states require continuing education.In Texas Donald Bagert of Texas became the first professional software engineer in the U.S. on September 4, 1998 or October 9, 1998. As of May 2002, Texas had issued 44 professional engineering licenses for software engineers. Rochester Institute of Technology granted the first Software Engineering bachelor’s degrees in 2001. Other universities have followed.Professional licensing has been criticized for many reasons.The field of software engineering is too immatureLicensing would give false assurances of competence even if the body of knowledge were matureSoftware engineers would have to study years of calculus, physics, and chemistry to pass the exams, which is irrelevant to most software practitioners. Many (most?) computer science majors don't earn degrees in engineering schools, so they are probably unqualified to pass engineering exams.In Canada, most people who earn professional software engineering licenses study software engineering, computer engineering or electrical engineering.  Many times these people already qualified to become professional engineers in their own fields but choose to be licensed as software engineers to differentiate themselves from computer scientists.In British Columbia, The Limited Licence is granted by the Association of Professional Engineers and Geoscientists of British Columbia. Fees are collected by APEGBC for the Limited Licence.=== Licensing and certification exams ===Since 2002 the IEEE Computer Society offered the Certified Software Development Professional (CSDP) certification exam (in 2015 this was replaced by several similar certifications). A group of experts from industry and academia developed the exam and maintained it. Donald Bagert, and at later period Stephen Tockey headed the certification committee. Contents of the exam centered around the SWEBOK (Software Engineering Body of Knowledge) guide, with the additional emphasis on Professional Practices and Software Engineering Economics knowledge areas (KAs). The motivation was to produce a structure at an international level for software engineering’s knowledge areas.=== Right to practise in Ontario ===A person must be granted the “professional engineer” licence to have the right to practise professional software engineering as a Professional Engineer in Ontario.To become licensed by Professional Engineers Ontario (PEO), you must:Be at least 18 years of age.Be a citizen or permanent resident of Canada.Be of good character.  You will be requested to answer questions and make a written declaration on your application form to test your ethics.Meet PEO’s stipulated academic requirements for licensure.Pass the Professional Practice Examination.Fulfill engineering work experience requirements.However, it's good to note that many graduates of Software Engineering programs are unable to obtain the PEO licence since the work they qualify for after graduation as entry-level is not related to engineering ie. working in a software company writing code or testing code would not qualify them as their work experience does not fulfill the work experience guidelines the PEO sets. Also Software Engineering programs in Ontario and other provinces involve a series of courses in electrical, electronics, and computers engineering qualifying the graduates to even work in those fields.=== Right to practise in Quebec ===A person must be granted the “engineer” licence to have the right to practise professional software engineering in Quebec. To become licensed by the Quebec order of engineers (in French : Ordre des ingénieurs du Québec - OIQ), you must:Be at least 18 years of age.Be of good character. You will be requested to answer questions and make a written declaration on your application form to test your ethics.Meet OIQ’s stipulated academic requirements for licensure. In this case, the academic program should be accredited by the Canadian Engineering Accreditation Board - CEAB)Pass the Professional Practice Examination.Fulfill engineering work experience requirements.Pass the working knowledge of French exam=== Software engineering (SEng) guidelines by Canadian provinces ===The term "engineer" in Canada is restricted to those who have graduated from a qualifying engineering programme. Some universities’ "software engineering" programmes are under the engineering faculty and therefore qualify, for example the University of Waterloo. Others, such as the University of Toronto have "software engineering" in the computer science faculty which does not qualify.  This distinction has to do with the way the profession is regulated.  Degrees in "Engineering" must be accredited by a national panel and have certain specific requirements to allow the graduate to pursue a career as a professional engineer. "Computer Science" degrees, even those with specialties in software engineering, do not have to meet these requirements so the computer science departments can generally teach a wider variety of topics and students can graduate without specific courses required to pursue a career as a professional engineer.== See also ==Bachelor of Science in Information TechnologyBachelor of Software EngineeringList of software engineering topicsSoftware engineering demographicsSoftware engineering economics== References ==== External links ==Professional licensing in TexasSE As Code of Ethics"A Review of the Professionalization of the Software Industry: Has it Made Software Engineering a Real Profession?", An academic article documenting the progress of SE professionalization
	Social software engineering (SSE) is a branch of software engineering that is concerned with the social aspects of software development and the developed software.SSE focuses on the socialness of both software engineering and developed software. On the one hand, the consideration of social factors in software engineering activities, processes and CASE tools is deemed to be useful to improve the quality of both development process and produced software. Examples include the role of situational awareness and multi-cultural factors in collaborative software development. On the other hand, the dynamicity of the social contexts in which software could operate (e.g., in a cloud environment) calls for engineering social adaptability as a runtime iterative activity. Examples include approaches which enable software to gather users' quality feedback and use it to adapt autonomously or semi-autonomously.SSE studies and builds socially-oriented tools to support collaboration and knowledge sharing in software engineering. SSE also investigates the adaptability of software to the dynamic social contexts in which it could operate and the involvement of clients and end-users in shaping software adaptation decisions at runtime. Social context includes norms, culture, roles and responsibilities, stakeholder's goals and interdependencies, end-users perception of the quality and appropriateness of each software behaviour, etc.The participants of the 1st International Workshop on Social Software Engineering and Applications (SoSEA 2008) proposed the following characterization:Community-centered: Software is produced and consumed by and/or for a community rather than focusing on individualsCollaboration/collectiveness: Exploiting the collaborative and collective capacity of human beingsCompanionship/relationship: Making explicit the various associations among peopleHuman/social activities: Software is designed consciously to support human activities and to address social problemsSocial inclusion: Software should enable social inclusion enforcing links and trust in communitiesThus, SSE can be defined as "the application of processes, methods, and tools to enable community-driven creation, management, deployment, and use of software in online environments".One of the main observations in the field of SSE is that the concepts, principles, and technologies made for social software applications are applicable to software development itself as software engineering is inherently a social activity. SSE is not limited to specific activities of software development. Accordingly, tools have been proposed supporting different parts of SSE, for instance, social system design or social requirements engineering. Consequently vertical market software, such as software development tools, engineering tools, marketing tools or software that helps users in a decision making process can profit from social components. Such vertical social software differentiates strongly in its user-base from traditional social software such as Yammer.== References ==
	Approximate computing is a computation technique which returns a possibly inaccurate result rather than a guaranteed accurate result, and can be used for applications where an approximate result is sufficient for its purpose. One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.The key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.== Strategies ==Several strategies can be used for performing approximate computing.Approximate circuitsApproximate adders, multipliers and other logical circuits can reduce hardware overhead. For example, an approximate multi-bit adder can ignore the carry chain and thus, allow all its sub-adders to perform addition operation in parallel.Approximate storageInstead of storing data values exactly, they can be stored approximately, e.g., by truncating the lower-bits in floating point data. Another method is accept less reliable memory. For this, in DRAM and eDRAM, refresh rate can be lowered and in SRAM, supply voltage can be lowered. In general, any error detection and correction mechanisms should be disabled.Software-level approximationThere are several ways to approximate at software level. Memoization can be applied. Some iterations of loops can be skipped (termed as loop perforation) to achieve a result faster. Some tasks can also be skipped, for example when a run-time condition suggests that those tasks are not going to be useful (task skipping). Monte Carlo algorithms and Randomized algorithms trade correctness for execution time guarantees. The computation can be reformulated according to paradigms that allow easily the acceleration on specialized hardware, e.g. a neural processing unit.Approximate systemIn an approximate system, different subsystems of the system such as the processor, memory, sensor, and communication modules are synergistically approximated to obtain a much better system-level Q-E trade-off curve compared to individual approximations to each of the subsystems.== Application areas ==Approximate computing has been used in a variety of domains where the applications are error-tolerant, such as multimedia processing, machine learning, signal processing, scientific computing, etc. Google is using this approach in their Tensor processing units (TPU, a custom ASIC).== Derived paradigms ==The main issue in approximate computing is the identification of the section of the application that can be approximated. In the case of large scale applications, it is very common to find people holding the expertise on approximate computing techniques not having enough expertise on the application domain (and vice versa). In order to solve this problem, programming paradigms have been proposed. They all have in common the clear role separation between application programmer and application domain expert. These approaches allow the spread of the most common optimizations and approximate computing techniques.== See also ==Artificial neural networkPCMOS== References ==
	Loop perforation is an approximate computing technique that allows to regularly skip some iterations of a loop.It relies on one parameter: the skip factor. The skip factor can either be interpreted as the number of iteration to skip each time or the number of iterations to perform before skipping one.== Code examples ==The examples that follows provide the result of loop perforation applied on this C-like source code=== Skip n iterations each time ====== Skip one iteration after n ===== See also ==Approximate computingTask skippingMemoization== Notes ==== References ==
	Task skipping is an approximate computing technique that allows to skip code blocks according to a specific boolean condition to be checked at run-time.This technique is usually applied on the most computational-intensive section of the code.It relies on the fact that a tuple of values sequentially computed are going to be useful only if the whole tuple meet certain conditions. Knowing that a value of the tuple invalides or probably will invalidate the whole tuple, it is possible to avoid the computation of the rest of the tuple.== Code example ==The example that follows provides the result of task skipping applied on this C-like source code=== Skipping a task ===== See also ==Loop perforationMemoization== Notes ==== References ==
	Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution. It can be applied during unit testing, component testing, integration testing,  system testing (automated/scripted or manual), or penetration testing.Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably. Defects commonly detected by runtime error detection include:Race conditionsExceptionsResource leaksMemory leaksSecurity attack vulnerabilities (e.g., SQL injection)Null pointersUninitialized memoryBuffer overflowsRuntime error detection tools can only detect errors in the executed control flow of the application.== See also ==Development TestingSoftware testingMemory debuggerBoundsCheckerRuntime verification== References ==
	System appreciation is an activity often included in the maintenance phase of software engineering projects.  Key deliverables from this phase include documentation that describes what the system does in terms of its functional features, and how it achieves those features in terms of its architecture and design.  Software architecture recovery is often the first step within System appreciation.== References ==== Further reading ==
	Structural synthesis of programs (SSP) is a special form of (automatic) program synthesis that is based on propositional calculus. More precisely, it uses intuitionistic logic for describing the structure of a program in such a detail that the program can be automatically composed from pieces like subroutines or even computer commands. It is assumed that these pieces have been implemented correctly, hence no correctness verification of these pieces is needed. SSP is well suited for automatic composition of services for service-oriented architectures and for synthesis of large simulation programs.== History ==Automatic program synthesis began in the artificial intelligence field, with software intended for automatic problem solving. The first program synthesizer was developed by Cordell Green in 1969. At about the same time, mathematicians including R. Constable, Z. Manna, and R. Waldinger explained the possible use of formal logic for automatic program synthesis. Practically applicable program synthesizers appeared considerably later.The idea of structural synthesis of programs was introduced at a conference on algorithms in modern mathematics and computer science  organized by Andrey Ershov and Donald Knuth in 1979. The idea originated from G. Pólya’s well-known book on problem solving. The method for devising a plan for solving a problem in SSP was presented as a formal system. The inference rules of the system were restructured and justified in logic by G. Mints and E. Tyugu  in 1982. A programming tool PRIZ   that uses SSP was developed in the 1980s.A recent Integrated development environment that supports SSP is CoCoViLa  — a model-based software development platform for implementing domain specific languages and developing large Java programs.== The logic of SSP ==Structural synthesis of programs is a method for composing programs from already implemented components (e.g. from computer commands or software object methods) that can be considered as functions.  A specification for synthesis is given in intuitionistic propositional logic by writing axioms about the applicability of functions. An axiom about the applicability of a function f is a logical implicationX1 ∧ X2 ∧ ... ∧ Xm  → Y1 ∧ Y2 ... Yn,where X1, X2, ... Xm are preconditions and Y1, Y2, ... Yn are postconditions of the application of the function f. In intuitionistic logic, the function f is called a realization of this formula. A precondition can be a proposition stating that input data exists, e.g. Xi  may have the meaning “variable xi has received a value”, but it may denote also some other condition, e.g. that resources needed for using the function f are available, etc. A precondition may also be an implication of the same form as the axiom given above; then it is called a subtask. A subtask denotes a function that must be available as an input when the function f is applied. This function itself must be synthesized in the process of SSP. In this case, realization of the axiom is a higher order function, i.e., a function that uses another function as an input. For instance, the formula(state  → nextState) ∧ initialState → resultcan specify a higher order function with two inputs and an output result. The first input is a function that has to be synthesized for computing nextState from state, and the second input is initialState. Higher order functions give generality to the SSP – any control structure needed in a synthesized program can be preprogrammed and used then automatically with a respective specification. In particular, the last axiom presented here is a specification of a complex program – a simulation engine for simulating dynamic systems on models where nextState can be computed from state of the system.== References ==== External links ==Automated Software Engineering Homepage
	Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behavior, and evolution. It serves as means to monitor, steer, observe and optimize software development, software maintenance, and software re-engineering in the sense of a business intelligence approach specific to software systems. It is generally based on the automatic extraction, analysis, and visualization of corresponding information sources of the software system. It can also be manually done and not automatic.== Applications ==Software diagnosis supports all branches of software engineering, in particular project management, quality management, risk management as well as implementation and test. Its main strength is to support all stakeholders of software projects (in particular during software maintenance and for software re-engineering tasks) and to provide effective communication means for software development projects. For example, software diagnosis facilitates "bridging an essential information gap between management and development, improve awareness, and serve as early risk detection instrument". Software diagnosis includes assessment methods for "perfective maintenance" that, for example, apply "visual analysis techniques to combine multiple indicators for low maintainability, including code complexity and entanglement with other parts of the system, and recent changes applied to the code".== Characteristics ==In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. Instead, software diagnosis aims at analyzing and evaluating the software system in its as-is state and based on system-generated information to bybass any subjective or potentially outdated information sources (e.g., initial software models). For it, software diagnosis combines and relates sources of information that are typically not directly linked. Examples: Source-code metrics are related with software developer activity to gain insight into developer-specific effects on software code quality.System structure and run-time execution traces are correlated to facilitate program comprehension through dynamic analysis in software maintenance tasks.== Principles ==The core principle of software diagnosis is to automatically extract information from all available information sources of a given software projects such as source code base, project repository, code metrics, execution traces, test results, etc. To combine information, software-specific data mining, analysis, and visualization techniques are applied. Its strength results, among various reasons, from integrating decoupled information spaces in the scope of a typical software project, for example development and developer activities (recorded by the repository) and code and quality metrics (derived by analyzing source code) or key performance indicators (KPIs).== Examples ==Examples of software diagnosis tools include software maps and software metrics.== Critics ==Software diagnosis—in contrast to many approaches in software engineering—does not assume that developer capabilities, development methods, programming or modeling languages are right or wrong (or better or worse compared to each other): Software diagnosis aims at giving insight into a given software system and its status regardless of the methods, languages, or models used to create and maintain the system.=== Related subjects ===Cost estimation in software engineeringProgramming productivityRapid application developmentSoftware designSoftware developmentSoftware documentationSoftware mapSoftware release life cycleSystems designSystems Development Life Cycle== References ==== External links ==A tool set based on software maps by Software Diagnostics, 2014Demo video interactive software maps for source-code analysis, 2013
	In software engineering, service virtualization or service virtualisation is a method to emulate the behavior of specific components in heterogeneous component-based applications such as API-driven applications, cloud-based applications and service-oriented architectures.It is used to provide software development and QA/testing teams access to dependent system components that are needed to exercise an application under test (AUT), but are unavailable or difficult-to-access for development and testing purposes. With the behavior of the dependent components "virtualized", testing and development can proceed without accessing the actual live components.Service virtualization is recognized by vendors, industry analysts, and industry publications as being different than mocking.== Overview ==Service virtualization emulates the behavior of software components to remove dependency constraints on development and testing teams. Such constraints occur in complex, interdependent environments when a component connected to the application under test is:Not yet completedStill evolvingControlled by a third-party or partnerAvailable for testing only in limited capacity or at inconvenient timesDifficult to provision or configure in a test environmentNeeded for simultaneous access by different teams with varied test data setup and other  requirementsRestricted or costly to use for load and performance testing Although the term "service virtualization" reflects the technique's initial focus on virtualizing web services, service virtualization extends across all aspects of composite applications: services, databases, mainframes, ESBs, and other components that communicate using common messaging protocols. Other similar tools are called API simulators, API mocking tools, over the wire test doubles.Service virtualization emulates only the behavior of the specific dependent components that developers or testers need to exercise in order to complete their end-to-end transactions. Rather than virtualizing entire systems, it virtualizes only specific slices of dependent behavior critical to the execution of development and testing tasks. This provides just enough application logic so that the developers or testers get what they need without having to wait for the actual service to be completed and readily available. For instance, instead of virtualizing an entire database (and performing all associated test data management as well as setting up the database for every test session), you monitor how the application interacts with the database, then you emulate the related database behavior (the SQL queries that are passed to the database, the corresponding result sets that are returned, and so forth).== Application ==Service virtualization involves creating and deploying a "virtual asset" that simulates the behavior of a real component which is required to exercise the application under test, but is difficult or impossible to access for development and testing purposes.A virtual asset stands in for a dependent component by listening for requests and returning an appropriate response—with the appropriate performance. For a database, this might involve listening for a SQL statement, then returning data source rows. For a web service, this might involve listening for an XML message over HTTP, JMS, or MQ, then returning another XML message. The virtual asset's functionality and performance might reflect the actual functionality/performance of the dependent component, or it might simulate exceptional conditions (such as extreme loads or error conditions) to determine how the application under test responds under those circumstances.Virtual assets are typically created by:Recording live communication among components as the system is exercised from the application under test (AUT)Providing logs representing historical communication among componentsAnalyzing service interface specifications (such as a WSDL)Defining the behavior manually with various interface controls and data source valuesThey are then further configured to represent specific data, functionality, and response times.Virtual assets are deployed locally or in the cloud (public or private). With development/test environments configured to use the virtual assets in place of dependent components, developers or testers can then exercise the application they are working on without having to wait for the dependent components to be completed or readily accessible.Industry analysts report that service virtualization is best suited for "IT shops with significant experience with 'skipping' integration testing due to 'dependent software', and with a reasonably sophisticated test harness.== Relation to stubbing and mocking ==An alternative approach to working around the test environment access constraints outlined in this article's introduction is for team members to develop method stubs or mock objects that substitute for dependent resources. The shortcoming of this approach became apparent in the early 2000s with the rise of Service-oriented architecture. The proliferation of Composite applications that rely on numerous dependent services, plus the rise of Agile software development following the 2001 publication of the Agile Manifesto, made it increasingly difficult for developers or testers to manually develop the number, scope, and complexity of stubs or mocks required to complete development and testing tasks for modern enterprise application development.The first step in the evolution from stubbing to service virtualization was the technology packaged in SOA testing tools since 2002.   The earliest implementations of service virtualization were designed to automate the process of developing simple stub-like emulations so that composite applications could be tested more efficiently. As enterprise systems continued to grow increasingly complex and distributed, software tool vendors shifted focus from stubbing to the more environment-focused service virtualization. While stubbing can still be completed through manual development and management of stubs, what has become known as "service virtualization" is completed by using one of the available commercial off the shelf (COTS) service virtualization technologies as a platform for the development and deployment of their "service virtualization assets".== Agile and DevOps ==The increasing popularity of Agile software development and DevOps has created demand for a new set of tools to deliver service virtualization to communities that work in this way. Practices such as Continuous delivery and moving away from mainframe and monolith development to more distributed microservice-based architectures fit well with the capabilities of service virtualization. Agile and DevOps teams prefer to work with lightweight tools that have less accumulated bloat and no cumbersome licensing restrictions.== See also ==Comparison of API simulation toolsTest double== References ==
	The ABC software metric was introduced by Jerry Fitzpatrick in 1997 to overcome the drawbacks of the LOC. The metric defines an ABC score as a triplet of values that represent the size of a set of source code statements. An ABC score is calculated by counting the number of assignments (A), number of branches (B), and number of conditionals (C) in a program. ABC score can be applied to individual methods, functions, classes, modules or files within a program.ABC score is represented by a 3-D vector < Assignments (A), Branches (B), Conditionals (C) >. It can also be represented as a scalar value, which is the magnitude of the vector < Assignments (A), Branches (B), Conditionals (C) >, and is calculated as follows:                              |                <        A        B        C        v        e        c        t        o        r        >                  |                =        √        (                  A                      2                          +                  B                      2                          +                  C                      2                          )              {\displaystyle |<ABCvector>|=\surd (A^{2}+B^{2}+C^{2})}  By convention, an ABC magnitude value is rounded to the nearest tenth.== History ==The concept of measuring software size was first introduced by Maurice Halstead from Purdue University in 1975. He suggested that every computer program consists mainly of tokens: operators and operands. He concluded that a count of the number of unique operators and operands gives us a measure of the size of the program. However, this was not adopted as a measure of the size of a program.Lines of code (LOC) was another popular measure of the size of a program. The LOC was not considered an accurate measure of the size of the program because even a program with identical functionality may have different numbers of lines depending on the style of coding.Another metric called the Function Point (FP) metric was introduced to calculate the number of user input and output transactions. The function point calculations did not give information about both the functionality of the program and about the routines that were involved in the program.The ABC metric is intended to overcome the drawbacks of the LOC, FP and token (operation and operand) counts. However, an FP score can also be used to supplement an ABC score.Though the author contends that the ABC metric measures size, some believe that it measures complexity. The ability of the ABC metric to measure complexity depends on how complexity is defined.== Definition ==The three components of the ABC score are defined as following:Assignment: storage or transfer of data into a variable.Branches: an explicit forward program branch out of scope.Conditionals: Boolean or logic test.Since basic languages such as C, C++, Java, etc. have operations like assignments of variables, function calls and test conditions only, the ABC score has these three components.If the ABC vector is denoted as <5,11,9> for a subroutine, it means that the subroutine has 5 assignments, 11 branches and 9 conditionals. For standardization purposes, the counts should be enclosed in angle brackets and written in the same order per the notation <A, B, C>.It is often more convenient to compare source code sizes using a scalar value. The individual ABC counts are distinct so, per Jerry Fitzpatrick, we consider the three components to be orthogonal, allowing a scalar ABC magnitude to be computed as shown above.Scalar ABC scores lose some of the benefits of the vector. Instead of computing a vector magnitude, the weighted sum of the vectors may support more accurate size comparison. ABC scalar scores should not be presented without the accompanying ABC vectors, since the scalar values are not the complete representation of the size.== Theory ==The specific rules for counting ABC vector values should be interpreted differently for different languages due to semantic differences between them.Therefore, the rules for calculating ABC vector slightly differ based on the language. We define the ABC metric calculation rules for C, C++ and Java below. Based on these rules the rules for other imperative languages can be interpreted.=== ABC rules for C ===The following rules give the count of Assignments, Branches, Conditionals in the ABC metric for C:Add one to the assignment count when:Occurrence of an assignment operator (=, *=, /=, %=, +=, <<=, >>=, &=, !=, ^=).Occurrence of an increment or a decrement operator (++, --).Add one to branch count when:Occurrence of a function call.Occurrence of any goto statement which has a target at a deeper level of nesting than the level to the goto.Add one to condition count when:Occurrence of a conditional operator (<, >, <=, >=, ==, !=).Occurrence of the following keywords (‘else’, ‘case’, ‘default’, ‘?’).Occurrence of a unary conditional operator.=== ABC rules for C++ ===The following rules give the count of Assignments, Branches, Conditionals in the ABC metric for C++:Add one to the assignment count when:Occurrence of an assignment operator (exclude constant declarations and default parameter assignments) (=, *=, /=, %=, +=, <<=, >>=, &=, !=, ^=).Occurrence of an increment or a decrement operator (prefix or postfix) (++, --).Initialization of a variable or a nonconstant class member.Add one to branch count when:Occurrence of a function call or a class method call.Occurrence of any goto statement which has a target at a deeper level of nesting than the level to the goto.Occurrence of ‘new’ or ‘delete’ operators.Add one to condition count when:Occurrence of a conditional operator (<, >, <=, >=, ==, !=).Occurrence of the following keywords (‘else’, ‘case’, ‘default’, ‘?’, ‘try’, ‘catch’).Occurrence of a unary conditional operator.=== ABC rules for Java ===The following rules give the count of Assignments, Branches, Conditionals in the ABC metric for Java:Add one to the assignment count when:Occurrence of an assignment operator (exclude constant declarations and default parameter assignments) (=, *=, /=, %=, +=, <<=, >>=, &=, !=, ^=, >>>=).Occurrence of an increment or a decrement operator (prefix or postfix) (++, --).Add one to branch count whenOccurrence of a function call or a class method call.Occurrence of a ‘new’ operator.Add one to condition count when:Occurrence of a conditional operator (<, >, <=, >=, ==, !=).Occurrence of the following keywords (‘else’, ‘case’, ‘default’, ‘?’, ‘try’, ‘catch’).Occurrence of a unary conditional operator.== Applications ===== Independent of style of coding ===Since the ABC score metric is built on the idea that tasks like data storage, branching and conditional testing, this metric is independent of the user’s style of coding.=== Project time estimation ===ABC score calculation helps in estimating the amount of time needed to complete a project. This can be done by roughly estimating the ABC score for the project, and by calculating the ABC score of the program in a particular day. The amount of time taken for the completion for the project can be obtained by dividing the ABC score of the project by the ABC score achieved in one day.=== Bug rate calculation ===The bug rate was originally calculated as Number of bugs / LOC. However, the LOC is not a reliable measure of the size of the program because it depends on the style of coding. A more accurate way of measuring bug rate is to count the - Number of bugs / ABC score.=== Program comparison ===Programs written in different languages can be compared with the help of ABC scores because most languages use assignments, branches and conditional statements.The information on the count of the individual parameters (number of assignments, branches and conditions) can help classify the program as ‘data strong’ or ‘function strong’ or ‘logic strong’. The vector form of an ABC score can provide insight into the driving principles behind the application, whereas the details are lost in the scalar form of the score.=== Linear metric ===ABC scores are linear, so any file, module, class, function or method can be scored. For example, the (vector) ABC score for a module is the sum of the scores of its sub-modules. Scalar ABC scores, however, are non-linear.== See also ==Software complexityComputer programmingHalstead complexity measuresCyclomatic complexitySynchronization complexitySoftware testingSoftware metric== References ==== External links ==Applying the ABC Metric to C, C++, and JavaABC MetricSize and Complexity RulesDeciphering Ruby Code MetricsThe ABC metric
	A data-flow diagram (DFD) is a way of representing a flow of a data of a process or a system (usually an information system). The DFD also provides information about the outputs and inputs of each entity and the process itself. A data-flow diagram has no control flow, there are no decision rules and no loops. Specific operations based on the data can be represented by a flowchart.There are several notations for displaying data-flow diagrams. The notation presented above was described in 1979 by Tom DeMarco as part of Structured Analysis.For each data flow, at least one of the endpoints (source and / or destination) must exist in a process. The refined representation of a process can be done in another data-flow diagram, which subdivides this process into sub-processes.The data-flow diagram is part of the structured-analysis modelling tools. When using UML, the activity diagram typically takes over the role of the data-flow diagram. A special form of data-flow plan is a site-oriented data-flow plan.Data-flow diagrams can be regarded as inverted Petri nets, because places in such networks correspond to the semantics of data memories. Analogously, the semantics of transitions from Petri nets and data flows and functions from data-flow diagrams should be considered equivalent.== History ==The DFD notation draws on graph theory, originally used in operational research to model workflow in organizations. DFD originated from the Activity Diagram used in the SADT (Structured Analysis and Design Technique) methodology at the end of the 1970s. DFD popularizers include Edward Yourdon, Larry Constantine, Tom DeMarco, Chris Gane and Trish Sarson.Data-flow diagrams (DFD) quickly became a popular way to visualize the major steps and data involved in software-system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to business process modeling. DFDs were useful to document the major data flows or to explore a new high-level design in terms of data flow.== DFD components ==DFD consists of processes, flows, warehouses, and terminators. There are several ways to view these DFD components.ProcessThe process (function, transformation) is part of a system that transforms inputs to outputs. The  symbol of a process is a circle, an oval, a rectangle or a rectangle with rounded corners (according to the type of notation). The process is named in one word, a short sentence, or a phrase that is clearly to express its essence.Data FlowData flow (flow, dataflow) shows the transfer of information (sometimes also material) from one part of the system to another. The symbol of the flow is the arrow. The flow should have a name that determines what information (or what material) is being moved. Exceptions are flows where it is clear what information is transferred through the entities that are linked to these flows. Material shifts are modeled in systems that are not merely informative. Flow should only transmit one type of information (material). The arrow shows the flow direction (it can also be bi-directional if the information to/from the entity is logically dependent - e.g. question and answer). Flows link processes, warehouses and terminators.WarehouseThe warehouse (datastore, data store, file, database) is used to store data for later use. The symbol of the store is two horizontal lines, the other way of view is shown in the DFD Notation. The name of the warehouse is a plural noun (e.g. orders) - it derives from the input and output streams of the warehouse. The warehouse does not have to be just a data file, for example, a folder with documents, a filing cabinet, and optical discs. Therefore, viewing the warehouse in DFD is independent of implementation. The flow from the warehouse usually represents the reading of the data stored in the warehouse, and the flow to the warehouse usually expresses data entry or updating (sometimes also deleting data). Warehouse is represented by two parallel lines between which the memory name is located (it can be modelled as a UML buffer node).TerminatorThe Terminator is an external entity that communicates with the system and stands outside of the system. It can be, for example, various organizations (eg a bank), groups of people (e.g. customers), authorities (e.g. a tax office) or a department (e.g. a human-resources department) of the same organization, which does not belong to the model system. The terminator may be another system with which the modeled system communicates.== Rules for creating DFD ==Entity names should be comprehensible without further comments. DFD is a system created by analysts based on interviews with system users. It is determined for system developers, on one hand, project contractor on the other, so the entity names should be adapted for model domain or amateur users or professionals. Entity names should be general (independent, e.g. specific individuals carrying out the activity), but should clearly specify the entity. Processes should be numbered for easier mapping and referral to specific processes. The numbering is random, however, it is necessary to maintain consistency across all DFD levels (see DFD Hierarchy). DFD should be clear, as the maximum number of processes in one DFD is recommended to be from 6 to 9, minimum is 3 processes in one DFD. The exception is the so-called contextual diagram where the only process symbolizes the model system and all terminators with which the system communicates.== DFD consistency ==DFD must be consistent with other models of the system - ERD, STD, Data Dictionary, and Process Specification models. Each process must have its name, inputs and outputs. Each flow should have its name (exception see Flow). Each Data store must have input and output flow. Input and output flows do not have to be displayed in one DFD - but they must exist in another DFD describing the same system. An exception is warehouse standing outside the system (external storage) with which the system communicates.== DFD hierarchy ==To make the DFD more transparent (i.e. not too many processes), multi-level DFDs can be created. DFDs that are at a higher level are less detailed (aggregate more detailed DFD at lower levels). The contextual DFD is the highest in the hierarchy (see DFD Creation Rules). The so-called zero level is followed by DFD 0, starting with process numbering (e.g., process 1, process 2). In the next, the so-called first level - DFD 1 - the numbering continues. E.g. process 1 is divided into the first three levels of the DFD, which are numbered 1.1, 1.2 and 1.3. Similarly, processes in the second level (DFD 2) are numbered eg 1.1.1, 1.1.2, 1.1.3 and 1.1.4. The number of levels depends on the size of the model system. DFD 0 processes may not have the same number of decomposition levels. DFD 0 contains the most important (aggregated) system functions. The lowest level should include processes that make it possible to create a process specification (Process Specification) for roughly one A4 page. If the mini-specification should be longer, it is appropriate to create an additional level for the process where it will be decomposed into multiple processes. For a clear overview of the entire DFD hierarchy, a vertical (cross-sectional) diagram can be created. The warehouse is displayed at the highest level where it is first used and at every lower level as well.== See also ==Activity diagramBusiness Process Model and NotationControl-flow diagramData islandDataflowDirected acyclic graphDrakon-chartFunctional flow block diagramFunction modelIDEF0PipelineStructured Analysis and Design TechniqueStructure chartSystem context diagramValue-stream mappingWorkflow== References ==== Bibliography ==Scott W. Ambler. The Object Primer 3rd Edition Agile Model Driven Development with UML 2Schmidt, G., Methode und Techniken der Organisation. 13. Aufl., Gießen 2003Stahlknecht, P., Hasenkamp, U.: Einführung in die Wirtschaftsinformatik. 12. Aufl., Berlin 2012Gane, Chris; Sarson, Trish. Structured Systems Analysis: Tools and Techniques. New York: Improved Systems Technologies, 1977. ISBN 978-0930196004. P. 373Demarco, Tom. Structured Analysis and System Specification. New York: Yourdon Press, 1979. ISBN 978-0138543808. P. 352.Yourdon, Edward. Structured Design: Fundamentals of a Discipline of Computer Program and Systems Design. New York: Yourdon Press, 1979. ISBN 978-0138544713. P. 473.Page-Jones, Meilir. Practical Guide to Structured Systems Design. New York: Yourdon Press, 1988. ISBN 978-8120314825. P. 384.Yourdon, Edward. Modern Structured Analysis. New York: Yourdon Press, 1988. ISBN 978-0135986240. P. 688.== External links == Media related to Data-flow diagram at Wikimedia Commons
	In software engineering, Human Trust for Serviceability means the satisfactory ability of software services for customer.== References ==
	In software engineering, software trustworthiness delivers a complete assurance that it will execute its required functions under all probable situations, will do so on time, and will never implement any activities that have significances of security risk in software for a specific time-duration.== References ==
	In software engineering, software durability means the solution ability of serviceability of software and to meet user's needs for a relatively long time. Software durability is important for user's satisfaction. For a software security to be durable, it must allow an organization to adjust the software to business needs that are constantly evolving, often in impulsive ways.Durability of software depends on four characteristics mainly; i.e. software trustworthiness, Human Trust for Serviceability, software dependability and software usability.== References ==
	Software prototyping is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed. It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.A prototype typically simulates only a few aspects of, and may be completely different from, the final product. Prototyping has several benefits: the software designer and implementer can get valuable feedback from the users early in the project. The client and the contractor can compare if the software made matches the software specification, according to which the software program is built. It also allows the software engineer some insight into the accuracy of initial project estimates and whether the deadlines and milestones proposed can be successfully met. The degree of completeness and the techniques used in prototyping have been in development and debate since its proposal in the early 1970s.== Overview ==The purpose of a prototype is to allow users of the software to evaluate developers' proposals for the design of the eventual product by actually trying them out, rather than having to interpret and evaluate the design based on descriptions. Software prototyping provides an understanding of the software's functions and potential threats or issues. Prototyping can also be used by end users to describe and prove requirements that have not been considered, and that can be a key factor in the commercial relationship between developers and their clients.  Interaction design in particular makes heavy use of prototyping with that goal.This process is in contrast with the 1960s and 1970s monolithic development cycle of building the entire program first and then working out any inconsistencies between design and implementation, which led to higher software costs and poor estimates of time and cost.  The monolithic approach has been dubbed the "Slaying the (software) Dragon" technique, since it assumes that the software designer and developer is a single hero who has to slay the entire dragon alone. Prototyping can also avoid the great expense and difficulty of having to change a finished software product.The practice of prototyping is one of the points Frederick P. Brooks makes in his 1975 book The Mythical Man-Month and his 10-year anniversary article "No Silver Bullet".An early example of large-scale software prototyping was the implementation of NYU's Ada/ED translator for the Ada programming language. It was implemented in SETL with the intent of producing an executable semantic model for the Ada language, emphasizing clarity of design and user interface over speed and efficiency. The NYU Ada/ED system was the first validated Ada implementation, certified on April 11, 1983.== Outline of the prototyping process ==The process of prototyping involves the following stepsIdentify basic requirementsDetermine basic requirements including the input and output information desired. Details, such as security, can typically be ignored.Develop initial prototypeThe initial prototype is developed that includes only user interfaces. (See Horizontal Prototype, below)ReviewThe customers, including end-users, examine the prototype and provide feedback on potential additions or changes.Revise and enhance the prototypeUsing the feedback both the specifications and the prototype can be improved.  Negotiation about what is within the scope of the contract/product may be necessary.  If changes are introduced then a repeat of steps #3 and #4 may be needed.== Dimensions of prototypes ==Nielsen summarizes the various dimensions of prototypes in his book Usability Engineering:=== Horizontal prototype ===A common term for a user interface prototype is the horizontal prototype.  It provides a broad view of an entire system or subsystem, focusing on user interaction more than low-level system functionality, such as database access.  Horizontal prototypes are useful for:Confirmation of user interface requirements and system scope,Demonstration version of the system to obtain buy-in from the business,Develop preliminary estimates of development time, cost and effort.=== Vertical prototype ===A vertical prototype is an enhanced complete elaboration of a single subsystem or function.  It is useful for obtaining detailed requirements for a given function, with the following benefits:Refinement database design,Obtain information on data volumes and system interface needs, for network sizing and performance engineering,Clarify complex requirements by drilling down to actual system functionality.== Types of prototyping ==Software prototyping has many variants. However, all of the methods are in some way based on two major forms of prototyping: throwaway prototyping and evolutionary prototyping.=== Throwaway prototyping ===Also called close-ended prototyping. Throwaway or rapid prototyping refers to the creation of a model that will eventually be discarded rather than becoming part of the final delivered software. After preliminary requirements gathering is accomplished, a simple working model of the system is constructed to visually show the users what their requirements may look like when they are implemented into a finished system.It is also a rapid prototyping. Rapid prototyping involves creating a working model of various parts of the system at a very early stage, after a relatively short investigation. The method used in building it is usually quite informal, the most important factor being the speed with which the model is provided. The model then becomes the starting point from which users can re-examine their expectations and clarify their requirements. When this goal has been achieved, the prototype model is 'thrown away', and the system is formally developed based on the identified requirements.The most obvious reason for using throwaway prototyping is that it can be done quickly. If the users can get quick feedback on their requirements, they may be able to refine them early in the development of the software.  Making changes early in the development lifecycle is extremely cost effective since there is nothing at that point to redo. If a project is changed after a considerable amount of work has been done then small changes could require large efforts to implement since software systems have many dependencies. Speed is crucial in implementing a throwaway prototype, since with a limited budget of time and money little can be expended on a prototype that will be discarded.Another strength of throwaway prototyping is its ability to construct interfaces that the users can test. The user interface is what the user sees as the system, and by seeing it in front of them, it is much easier to grasp how the system will function.…it is asserted that revolutionary rapid prototyping is a more effective manner in which to deal with user requirements-related issues, and therefore a greater enhancement to software productivity overall. Requirements can be identified, simulated, and tested far more quickly and cheaply when issues of evolvability, maintainability, and software structure are ignored. This, in turn, leads to the accurate specification of requirements, and the subsequent construction of a valid and usable system from the user's perspective, via conventional software development models. Prototypes can be classified according to the fidelity with which they resemble the actual product in terms of appearance, interaction and timing. One method of creating a low fidelity throwaway prototype is paper prototyping. The prototype is implemented using paper and pencil, and thus mimics the function of the actual product, but does not look at all like it. Another method to easily build high fidelity throwaway prototypes is to use a GUI Builder and create a click dummy, a prototype that looks like the goal system, but does not provide any functionality.The usage of storyboards, animatics or drawings is not exactly the same as throwaway prototyping, but certainly falls within the same family.  These are non-functional implementations but show how the system will look.Summary: In this approach the prototype is constructed with the idea that it will be discarded and the final system will be built from scratch. The steps in this approach are:Write preliminary requirementsDesign the prototypeUser experiences/uses the prototype, specifies new requirementsRepeat if necessaryWrite the final requirements=== Evolutionary prototyping ===Evolutionary prototyping (also known as breadboard prototyping) is quite different from throwaway prototyping. The main goal when using evolutionary prototyping is to build a very robust prototype in a structured manner and constantly refine it. The reason for this approach is that the evolutionary prototype, when built, forms the heart of the new system, and the improvements and further requirements will then be built.When developing a system using evolutionary prototyping, the system is continually refined and rebuilt. "…evolutionary prototyping acknowledges that we do not understand all the requirements and builds only those that are well understood."This technique allows the development team to add features, or make changes that couldn't be conceived during the requirements and design phase.For a system to be useful, it must evolve through use in its intended operational environment. A product is never "done;" it is always maturing as the usage environment changes…we often try to define a system using our most familiar frame of reference—where we are now. We make assumptions about the way business will be conducted and the technology base on which the business will be implemented. A plan is enacted to develop the capability, and, sooner or later, something resembling the envisioned system is delivered.Evolutionary prototypes have an advantage over throwaway prototypes in that they are functional systems. Although they may not have all the features the users have planned, they may be used on an interim basis until the final system is delivered."It is not unusual within a prototyping environment for the user to put an initial prototype to practical use while waiting for a more developed version…The user may decide that a 'flawed' system is better than no system at all."In evolutionary prototyping, developers can focus themselves to develop parts of the system that they understand instead of working on developing a whole system.To minimize risk, the developer does not implement poorly understood features. The partial system is sent to customer sites. As users work with the system, they detect opportunities for new features and give requests for these features to developers. Developers then take these enhancement requests along with their own and use sound configuration-management practices to change the software-requirements specification, update the design, recode and retest.=== Incremental prototyping ===The final product is built as separate prototypes. At the end, the separate prototypes are merged in an overall design. By the help of incremental prototyping the time gap between user and software developer is reduced.=== Extreme prototyping ===Extreme prototyping as a development process is used especially for developing web applications. Basically, it breaks down web development into three phases, each one based on the preceding one. The first phase is a static prototype that consists mainly of HTML pages. In the second phase, the screens are programmed and fully functional using a simulated services layer. In the third phase, the services are implemented. The process is called extreme prototyping to draw attention to the second phase of the process, where a fully functional UI is developed with very little regard to the services other than their contract.== Advantages of prototyping ==There are many advantages to using prototyping in software development – some tangible, some abstract.Reduced time and costs:  Prototyping can improve the quality of requirements and specifications provided to developers.  Because changes cost exponentially more to implement as they are detected later in development, the early determination of what the user really wants can result in faster and less expensive software.Improved and increased user involvement:  Prototyping requires user involvement and allows them to see and interact with a prototype allowing them to provide better and more complete feedback and specifications.  The presence of the prototype being examined by the user prevents many misunderstandings and miscommunications that occur when each side believe the other understands what they said.  Since users know the problem domain better than anyone on the development team does, increased interaction can result in a final product that has greater tangible and intangible quality.  The final product is more likely to satisfy the user's desire for look, feel and performance.== Disadvantages of prototyping ==Using, or perhaps misusing, prototyping can also have disadvantages.Insufficient analysis:  The focus on a limited prototype can distract developers from properly analyzing the complete project.  This can lead to overlooking better solutions, preparation of incomplete specifications or the conversion of limited prototypes into poorly engineered final projects that are hard to maintain.  Further, since a prototype is limited in functionality it may not scale well if the prototype is used as the basis of a final deliverable, which may not be noticed if developers are too focused on building a prototype as a model.User confusion of prototype and finished system: Users can begin to think that a prototype, intended to be thrown away, is actually a final system that merely needs to be finished or polished. (They are, for example, often unaware of the effort needed to add error-checking and security features which a prototype may not have.) This can lead them to expect the prototype to accurately model the performance of the final system when this is not the intent of the developers. Users can also become attached to features that were included in a prototype for consideration and then removed from the specification for a final system. If users are able to require all proposed features be included in the final system this can lead to conflict.Developer misunderstanding of user objectives: Developers may assume that users share their objectives (e.g. to deliver core functionality on time and within budget), without understanding wider commercial issues. For example, user representatives attending Enterprise software (e.g. PeopleSoft) events may have seen demonstrations of "transaction auditing" (where changes are logged and displayed in a difference grid view) without being told that this feature demands additional coding and often requires more hardware to handle extra database accesses. Users might believe they can demand auditing on every field, whereas developers might think this is feature creep because they have made assumptions about the extent of user requirements. If the developer has committed delivery before the user requirements were reviewed, developers are between a rock and a hard place, particularly if user management derives some advantage from their failure to implement requirements.Developer attachment to prototype: Developers can also become attached to prototypes they have spent a great deal of effort producing; this can lead to problems, such as attempting to convert a limited prototype into a final system when it does not have an appropriate underlying architecture. (This may suggest that throwaway prototyping, rather than evolutionary prototyping, should be used.)Excessive development time of the prototype: A key property to prototyping is the fact that it is supposed to be done quickly. If the developers lose sight of this fact, they very well may try to develop a prototype that is too complex. When the prototype is thrown away the precisely developed requirements that it provides may not yield a sufficient increase in productivity to make up for the time spent developing the prototype. Users can become stuck in debates over details of the prototype, holding up the development team and delaying the final product.Expense of implementing prototyping: the start up costs for building a development team focused on prototyping may be high. Many companies have development methodologies in place, and changing them can mean retraining, retooling, or both.  Many companies tend to just begin prototyping without bothering to retrain their workers as much as they should.A common problem with adopting prototyping technology is high expectations for productivity with insufficient effort behind the learning curve. In addition to training for the use of a prototyping technique, there is an often overlooked need for developing corporate and project specific underlying structure to support the technology. When this underlying structure is omitted, lower productivity can often result.== Best projects to use prototyping ==It has been argued that prototyping, in some form or another, should be used all the time. However, prototyping is most beneficial in systems that will have many interactions with the users.It has been found that prototyping is very effective in the analysis and design of on-line systems, especially for transaction processing, where the use of screen dialogs is much more in evidence. The greater the interaction between the computer and the user, the greater the benefit is that can be obtained from building a quick system and letting the user play with it.Systems with little user interaction, such as batch processing or systems that mostly do calculations, benefit little from prototyping. Sometimes, the coding needed to perform the system functions may be too intensive and the potential gains that prototyping could provide are too small.Prototyping is especially good for designing good human-computer interfaces. "One of the most productive uses of rapid prototyping to date has been as a tool for iterative user requirements engineering and human-computer interface design."=== Dynamic systems development method ===Dynamic Systems Development Method (DSDM) is a framework for delivering business solutions that relies heavily upon prototyping as a core technique, and is itself ISO 9001 approved.  It expands upon most understood definitions of a prototype.  According to DSDM the prototype may be a diagram, a business process, or even a system placed into production.  DSDM prototypes are intended to be incremental, evolving from simple forms into more comprehensive ones.DSDM prototypes can sometimes be throwaway or evolutionary.  Evolutionary prototypes may be evolved horizontally (breadth then depth) or vertically (each section is built in detail with additional iterations detailing subsequent sections).  Evolutionary prototypes can eventually evolve into final systems.The four categories of prototypes as recommended by DSDM are:Business prototypes – used to design and demonstrates the business processes being automated.Usability prototypes – used to define, refine, and demonstrate user interface design usability, accessibility, look and feel.Performance and capacity prototypes –  used to define, demonstrate, and predict how systems will perform under peak loads as well as to demonstrate and evaluate other non-functional aspects of  the system (transaction rates, data storage volume, response time, etc.)Capability/technique prototypes – used to develop, demonstrate, and evaluate a design approach or concept.The DSDM lifecycle of a prototype is to:Identify prototypeAgree to a planCreate the prototypeReview the prototype=== Operational prototyping ===Operational prototyping was proposed by Alan Davis as a way to integrate throwaway and evolutionary prototyping with conventional system development. "It offers the best of both the quick-and-dirty and conventional-development worlds in a sensible manner. Designers develop only well-understood features in building the evolutionary baseline, while using throwaway prototyping to experiment with the poorly understood features."Davis' belief is that to try to "retrofit quality onto a rapid prototype" is not the correct method when trying to combine the two approaches. His idea is to engage in an evolutionary prototyping methodology and rapidly prototype the features of the system after each evolution.The specific methodology follows these steps: An evolutionary prototype is constructed and made into a baseline using conventional development strategies, specifying and implementing only the requirements that are well understood.Copies of the baseline are sent to multiple customer sites along with a trained prototyper.At each site, the prototyper watches the user at the system.Whenever the user encounters a problem or thinks of a new feature or requirement, the prototyper logs it. This frees the user from having to record the problem, and allows him to continue working.After the user session is over, the prototyper constructs a throwaway prototype on top of the baseline system.The user now uses the new system and evaluates. If the new changes aren't effective, the prototyper removes them.If the user likes the changes, the prototyper writes feature-enhancement requests and forwards them to the development team.The development team, with the change requests in hand from all the sites, then produce a new evolutionary prototype using conventional methods.Obviously, a key to this method is to have well trained prototypers available to go to the user sites. The operational prototyping methodology has many benefits in systems that are complex and have few known requirements in advance.=== Evolutionary systems development ===Evolutionary Systems Development is a class of methodologies that attempt to formally implement evolutionary prototyping. One particular type, called Systemscraft is described by John Crinnion in his book Evolutionary Systems Development.Systemscraft was designed as a 'prototype' methodology that should be modified and adapted to fit the specific environment in which it was implemented.Systemscraft was not designed as a rigid 'cookbook' approach to the development process. It is now generally recognised[sic] that a good methodology should be flexible enough to be adjustable to suit all kinds of environment and situation…The basis of Systemscraft, not unlike evolutionary prototyping, is to create a working system from the initial requirements and build upon it in a series of revisions. Systemscraft places heavy emphasis on traditional analysis being used throughout the development of the system.=== Evolutionary rapid development ===Evolutionary Rapid Development (ERD) was developed by the Software Productivity Consortium, a technology development and integration agent for the Information Technology Office of the Defense Advanced Research Projects Agency (DARPA).Fundamental to ERD is the concept of composing software systems based on the reuse of components, the use of software templates and on an architectural template. Continuous evolution of system capabilities in rapid response to changing user needs and technology is highlighted by the evolvable architecture, representing a class of solutions. The process focuses on the use of small artisan-based teams integrating software and systems engineering disciplines working multiple, often parallel short-duration timeboxes with frequent customer interaction.Key to the success of the ERD-based projects is parallel exploratory analysis and development of features, infrastructures, and components with and adoption of leading edge technologies enabling the quick reaction to changes in technologies, the marketplace, or customer requirements.To elicit customer/user input, frequent scheduled and ad hoc/impromptu meetings with the stakeholders are held. Demonstrations of system capabilities are held to solicit feedback before design/implementation decisions are solidified. Frequent releases (e.g., betas) are made available for use to provide insight into how the system could better support user and customer needs. This assures that the system evolves to satisfy existing user needs.The design framework for the system is based on using existing published or de facto standards. The system is organized to allow for evolving a set of capabilities that includes considerations for performance, capacities, and functionality. The architecture is defined in terms of abstract interfaces that encapsulate the services and their implementation (e.g., COTS applications). The architecture serves as a template to be used for guiding development of more than a single instance of the system. It allows for multiple application components to be used to implement the services. A core set of functionality not likely to change is also identified and established.The ERD process is structured to use demonstrated functionality rather than paper products as a way for stakeholders to communicate their needs and expectations. Central to this goal of rapid delivery is the use of the "timebox" method. Timeboxes are fixed periods of time in which specific tasks (e.g., developing a set of functionality) must be performed. Rather than allowing time to expand to satisfy some vague set of goals, the time is fixed (both in terms of calendar weeks and person-hours) and a set of goals is defined that realistically can be achieved within these constraints.  To keep development from degenerating into a "random walk," long-range plans are defined to guide the iterations. These plans provide a vision for the overall system and set boundaries (e.g., constraints) for the project. Each iteration within the process is conducted in the context of these long-range plans.Once an architecture is established, software is integrated and tested on a daily basis. This allows the team to assess progress objectively and identify potential problems quickly. Since small amounts of the system are integrated at one time, diagnosing and removing the defect is rapid. User demonstrations can be held at short notice since the system is generally ready to exercise at all times.== Tools ==Efficiently using prototyping requires that an organization have the proper tools and a staff trained to use those tools.  Tools used in prototyping can vary from individual tools, such as 4th generation programming languages used for rapid prototyping to complex integrated CASE tools. 4th generation visual programming languages like Visual Basic and ColdFusion are frequently used since they are cheap, well known and relatively easy and fast to use.  CASE tools, supporting requirements analysis, like the Requirements Engineering Environment (see below) are often developed or selected by the military or large organizations. Object oriented tools are also being developed like LYMB from the GE Research and Development Center. Users may prototype elements of an application themselves in a spreadsheet.As web-based applications continue to grow in popularity, so too, have the tools for prototyping such applications. Frameworks such as Bootstrap, Foundation, and AngularJS provide the tools necessary to quickly structure a proof of concept. These frameworks typically consist of a set of controls, interactions, and design guidelines that enable developers to quickly prototype web applications.=== Screen generators, design tools, and software factories ===Screen generating programs are also commonly used and they enable prototypers to show user's systems that do not function, but show what the screens may look like. Developing Human Computer Interfaces can sometimes be the critical part of the development effort, since to the users the interface essentially is the system.Software factories can generate code by combining ready-to-use modular components. This makes them ideal for prototyping applications, since this approach can quickly deliver programs with the desired behaviour, with a minimal amount of manual coding.=== Application definition or simulation software ===A new class of software called  Application definition or simulation software  enables users to rapidly build lightweight, animated simulations of another computer program, without writing code.  Application simulation software allows both technical and non-technical users to experience, test, collaborate and validate the simulated program, and provides reports such as annotations, screenshot and schematics. As a solution specification technique, Application Simulation falls between low-risk, but limited, text or drawing-based mock-ups (or wireframes) sometimes called paper-based prototyping, and time-consuming, high-risk code-based prototypes, allowing software professionals to validate requirements and design choices early on, before development begins. In doing so, the risks and costs associated with software implementations can be dramatically reduced.To simulate applications one can also use software that simulates real-world software programs for computer-based training, demonstration, and customer support, such as screencasting software as those areas are closely related. There are also more specialised tools.=== Requirements Engineering Environment ==="The Requirements Engineering Environment (REE), under development at Rome Laboratory since 1985, provides an integrated toolset for rapidly representing, building, and executing models of critical aspects of complex systems."Requirements Engineering Environment is currently used by the United States Air Force to develop systems. It is:an integrated set of tools that allows systems analysts to rapidly build functional, user interface, and performance prototype models of system components. These modeling activities are performed to gain a greater understanding of complex systems and lessen the impact that inaccurate requirement specifications have on cost and scheduling during the system development process. Models can be constructed easily, and at varying levels of abstraction or granularity, depending on the specific behavioral aspects of the model being exercised.REE is composed of three parts. The first, called proto is a CASE tool specifically designed to support rapid prototyping. The second part is called the Rapid Interface Prototyping System or RIP, which is a collection of tools that facilitate the creation of user interfaces. The third part of REE is a user interface to RIP and proto that is graphical and intended to be easy to use.Rome Laboratory, the developer of REE, intended that to support their internal requirements gathering methodology. Their method has three main parts: Elicitation from various sources (users, interfaces to other systems), specification, and consistency checkingAnalysis that the needs of diverse users taken together do not conflict and are technically and economically feasibleValidation that requirements so derived are an accurate reflection of user needs.In 1996, Rome Labs contracted Software Productivity Solutions (SPS) to further enhance REE to create "a commercial quality REE that supports requirements specification, simulation, user interface prototyping, mapping of requirements to hardware architectures, and code generation…" This system is named the Advanced Requirements Engineering Workstation or AREW.=== LYMB ===LYMB is an object-oriented development environment aimed at developing applications that require combining graphics-based user interfaces, visualization, and rapid prototyping.=== Non-relational environments ===Non-relational definition of data (e.g. using Caché or associative models) can help make end-user prototyping more productive by delaying or avoiding the need to normalize data at every iteration of a simulation. This may yield earlier/greater clarity of business requirements, though it does not specifically confirm that requirements are technically and economically feasible in the target production system.=== PSDL ===PSDL is a prototype description language to describe real-time software.The associated tool set is CAPS (Computer Aided Prototyping System).Prototyping software systems with hard real-time requirements is challenging because timing constraints introduce implementation and hardware dependencies.PSDL addresses these issues by introducing control abstractions that include declarative timing constraints. CAPS uses this information to automatically generate code and associated real-time schedules, monitor timing constraints during prototype execution, and simulate execution in proportional real time relative to a set of parameterized hardware models. It also provides default assumptions that enable execution of incomplete prototype descriptions, integrates prototype construction with a software reuse repository for rapidly realizing efficient implementations, and provides support for rapid evolution of requirements and designs.== See also ==Comparison of software prototyping tools== Notes ==^  C. Melissa Mcclendon, Larry Regot, Gerri Akers: The Analysis and Prototyping of Effective Graphical User Interfaces. October 1996. [3]^  D.A. Stacy, professor, University of Guelph. Guelph, Ontario. Lecture notes on Rapid Prototyping. August, 1997. [4]^  Stephen J. Andriole: Information System Design Principles for the 90s: Getting it Right. AFCEA International Press, Fairfax, Virginia. 1990. Page 13.^  R. Charette, Software Engineering Risk Analysis and Management. McGraw Hill, New York, 1989.^  Alan M. Davis: Operational Prototyping: A new Development Approach. IEEE Software, September 1992. Page 71.^  Todd Grimm: The Human Condition: A Justification for Rapid Prototyping. Time Compression Technologies, vol. 3 no. 3. Accelerated Technologies, Inc. May 1998 . Page 1. [5]^  John Crinnion: Evolutionary Systems Development, a practical guide to the use of prototyping within a structured systems methodology. Plenum Press, New York, 1991. Page 18.^  S. P. Overmyer: Revolutionary vs. Evolutionary Rapid Prototyping: Balancing Software Productivity and HCI Design Concerns. Center of Excellence in Command, Control, Communications and Intelligence (C3I), George Mason University, 4400 University Drive, Fairfax, Virginia.^  Software Productivity Consortium: Evolutionary Rapid Development. SPC document SPC-97057-CMC, version 01.00.04, June 1997. Herndon, Va. Page 6.^  Davis. Page 72-73. Citing: E. Bersoff and A. Davis, Impacts of Life Cycle Models of Software Configuration Management. Comm. ACM, Aug. 1991, pp. 104–118^  Adapted from C. Melissa Mcclendon, Larry Regot, Gerri Akers.^  Adapted from Software Productivity Consortium. PPS 10–13.^  Joseph E. Urban: Software Prototyping and Requirements Engineering. Rome Laboratory, Rome, NY.^  Paul W. Parry. Rapid Software Prototyping. Sheffield Hallam University, Sheffield, UK. [6]^  Dr. Ramon Acosta, Carla Burns, William Rzepka, and James Sidoran. Applying Rapid Prototyping Techniques in the Requirements Engineering Environment. IEEE, 1994. [7]^  Software Productivity Solutions, Incorporated. Advanced Requirements Engineering Workstation (AREW). 1996. [8]^  from GE Research and Development. https://web.archive.org/web/20061013220422/http://www.crd.ge.com/esl/cgsp/fact_sheet/objorien/index.html^  Dynamic Systems Development Method Consortium. https://web.archive.org/web/20060209072841/http://na.dsdm.org/^  Alan Dix, Janet Finlay, Gregory D. Abowd, Russell Beale; Human-Computer Interaction, Third edition== References ==
	In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. It is not a finished design that can be transformed directly into source or machine code. It is a description or template for how to solve a problem that can be used in many different situations. Design patterns are formalized best practices that the programmer can use to solve common problems when designing an application or system.Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved. Patterns that imply mutable state may be unsuited for functional programming languages, some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.Design patterns may be viewed as a structured approach to computer programming intermediate between the levels of a programming paradigm and a concrete algorithm.== History ==Patterns originated as an architectural concept by Christopher Alexander (1977/78). In 1987, Kent Beck and Ward Cunningham began experimenting with the idea of applying patterns to programming – specifically pattern languages – and presented their results at the OOPSLA conference that year. In the following years, Beck, Cunningham and others followed up on this work.Design patterns gained popularity in computer science after the book Design Patterns: Elements of Reusable Object-Oriented Software was published in 1994  by the so-called "Gang of Four" (Gamma et al.), which is frequently abbreviated as "GoF". That same year, the first Pattern Languages of Programming Conference was held, and the following year the Portland Pattern Repository was set up for documentation of design patterns. The scope of the term remains a matter of dispute. Notable books in the design pattern genre include:Gamma, Erich; Helm, Richard; Johnson, Ralph; Vlissides, John (1995). Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley. ISBN 978-0-201-63361-0.Brinch Hansen, Per (1995). Studies in Computational Science: Parallel Programming Paradigms. Prentice Hall. ISBN 978-0-13-439324-7.Buschmann, Frank; Meunier, Regine; Rohnert, Hans; Sommerlad, Peter (1996). Pattern-Oriented Software Architecture, Volume 1: A System of Patterns. John Wiley & Sons. ISBN 978-0-471-95869-7.Beck, Kent (1997). Smalltalk Best Practice Patterns. Prentice Hall. ISBN 978-0134769042.Schmidt, Douglas C.; Stal, Michael; Rohnert, Hans; Buschmann, Frank (2000). Pattern-Oriented Software Architecture, Volume 2: Patterns for Concurrent and Networked Objects. John Wiley & Sons. ISBN 978-0-471-60695-6.Fowler, Martin (2002). Patterns of Enterprise Application Architecture. Addison-Wesley. ISBN 978-0-321-12742-6.Hohpe, Gregor; Woolf, Bobby (2003). Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions. Addison-Wesley. ISBN 978-0-321-20068-6.Freeman, Eric T; Robson, Elisabeth; Bates, Bert; Sierra, Kathy (2004). Head First Design Patterns. O'Reilly Media. ISBN 978-0-596-00712-6.Although design patterns have been applied practically for a long time, formalization of the concept of design patterns languished for several years.== Practice ==Design patterns can speed up the development process by providing tested, proven development paradigms. Effective software design requires considering issues that may not become visible until later in the implementation. Freshly written code can often have hidden subtle issues that take time to be detected, issues that sometimes can cause major problems down the road. Reusing design patterns helps to prevent such subtle issues, and it also improves code readability for coders and architects who are familiar with the patterns.In order to achieve flexibility, design patterns usually introduce additional levels of indirection, which in some cases may complicate the resulting designs and hurt application performance.By definition, a pattern must be programmed anew into each application that uses it. Since some authors see this as a step backward from software reuse as provided by components, researchers have worked to turn patterns into components. Meyer and Arnout were able to provide full or partial componentization of two-thirds of the patterns they attempted.Software design techniques are difficult to apply to a broader range of problems. Design patterns provide general solutions, documented in a format that does not require specifics tied to a particular problem.== Structure ==Design patterns are composed of several sections (see § Documentation below). Of particular interest are the Structure, Participants, and Collaboration sections. These sections describe a design motif: a prototypical micro-architecture that developers copy and adapt to their particular designs to solve the recurrent problem described by the design pattern. A micro-architecture is a set of program constituents (e.g., classes, methods...) and their relationships. Developers use the design pattern by introducing in their designs this prototypical micro-architecture, which means that micro-architectures in their designs will have structure and organization similar to the chosen design motif.=== Domain-specific patterns ===Efforts have also been made to codify design patterns in particular domains, including use of existing design patterns as well as domain specific design patterns. Examples include user interface design patterns, information visualization, secure design, "secure usability", Web design  and business model design.The annual Pattern Languages of Programming Conference proceedings  include many examples of domain-specific patterns.== Classification and list ==Design patterns were originally grouped into the categories: creational patterns, structural patterns, and behavioral patterns, and described using the concepts of delegation, aggregation, and consultation. For further background on object-oriented design, see coupling and cohesion, inheritance, interface, and polymorphism. Another classification has also introduced the notion of architectural design pattern that may be applied at the architecture level of the software such as the Model–View–Controller pattern.=== Creational patterns ====== Structural patterns ====== Behavioral patterns ====== Concurrency patterns ===== Documentation ==The documentation for a design pattern describes the context in which the pattern is used, the forces within the context that the pattern seeks to resolve, and the suggested solution. There is no single, standard format for documenting design patterns. Rather, a variety of different formats have been used by different pattern authors. However, according to Martin Fowler, certain pattern forms have become more well-known than others, and consequently become common starting points for new pattern-writing efforts. One example of a commonly used documentation format is the one used by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides in their book Design Patterns. It contains the following sections:Pattern Name and Classification: A descriptive and unique name that helps in identifying and referring to the pattern.Intent: A description of the goal behind the pattern and the reason for using it.Also Known As: Other names for the pattern.Motivation (Forces): A scenario consisting of a problem and a context in which this pattern can be used.Applicability: Situations in which this pattern is usable; the context for the pattern.Structure: A graphical representation of the pattern. Class diagrams and Interaction diagrams may be used for this purpose.Participants: A listing of the classes and objects used in the pattern and their roles in the design.Collaboration: A description of how classes and objects used in the pattern interact with each other.Consequences: A description of the results, side effects, and trade offs caused by using the pattern.Implementation: A description of an implementation of the pattern; the solution part of the pattern.Sample Code: An illustration of how the pattern can be used in a programming language.Known Uses: Examples of real usages of the pattern.Related Patterns: Other patterns that have some relationship with the pattern; discussion of the differences between the pattern and similar patterns.== Criticism ==It has been observed that design patterns may just be a sign that some features are missing in a given programming language (Java or C++ for instance). Peter Norvig demonstrates that 16 out of the 23 patterns in the Design Patterns book (which is primarily focused on C++) are simplified or eliminated (via direct language support) in Lisp or Dylan. Related observations were made by Hannemann and Kiczales who implemented several of the 23 design patterns using an aspect-oriented programming language (AspectJ) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns. See also Paul Graham's essay "Revenge of the Nerds".Inappropriate use of patterns may unnecessarily increase complexity.== See also ==== References ==== Further reading ==
	XcodeGhost (and variant XcodeGhost S) are modified versions of Apple's Xcode development environment that are considered malware. The software first gained widespread attention in September 2015, when a number of apps originating from China harbored the malicious code. It was thought to be the "first large-scale attack on Apple's App Store", according to the BBC. The problems were first identified by researchers at Alibaba, a leading e-commerce firm in China. Over 4000 apps are infected, according to FireEye, far more than the 25 initially acknowledged by Apple, including apps from authors outside China.Security firm Palo Alto Networks surmised that because network speeds were slower in China, developers in the country looked for local copies of the Apple Xcode development environment, and encountered altered versions that had been posted on domestic web sites. This opened the door for the malware to be inserted into high profile apps used on iOS devices.Even two months after the initial reports, security firm FireEye reported that hundreds of enterprises were still using infected apps and that XcodeGhost remained "a persistent security risk". The firm also identified a new variant of the malware and dubbed it XcodeGhost S; among the apps that were infected were the popular messaging app WeChat and a Netease app Music 163.== Discovery ==On September 16, 2015, a Chinese iOS developer mentioned on the social network Sina Weibo that a malware in Xcode injects third party code into apps compiled with it.Alibaba researchers then published detailed information on the malware and called it XcodeGhost.On September 17, 2015, Palo Alto Networks published several reports on the malware.== Operation ===== Propagation ===Because of the slow download speed from Apple servers, Chinese iOS developers would download Xcode from third party websites, such as Baidu Yun (now called Baidu WangPan), a cloud storage service hosted by Baidu, or get copies from co-workers. Attackers took advantage of this situation by distributing compromised versions on such file hosting websites.Palo Alto Networks suspects that the malware was available in March 2015.=== Attack vector ======= Origins ====The attacker used a compiler backdoor attack. The novelty of this attack is the modification of the Xcode compiler. However, according to documents leaked by Edward Snowden, CIA security researchers from Sandia National Laboratories claimed that they "had created a modified version of Apple’s proprietary software development tool, Xcode, which could sneak surveillance backdoors into any apps or programs created using the tool."==== Modified files ====Known versions of XcodeGhost add extra files to the original Xcode application:Core service framework on iOS, iOS simulator and OS X platformsIDEBundleInjection framework added on iOS, iOS simulator and OS X platformsXcodeGhost also modified the linker to link the malicious files into the compiled app. This step is reported on the compiling log but not on the Xcode IDE.Both iOS and OS X apps are vulnerable to XcodeGhost.==== Deployment ====XcodeGhost compromised the CoreServices layer, which contains highly used features and frameworks used by the app. When a developer compiles their application with a compromised version of Xcode, the malicious CoreServices are automatically integrated into the app without the developer's knowledge.Then the malicious files will add extra code in UIWindow class and UIDevice class. The UIWindow class is "an object that manages and coordinates the views an app displays on a device screen".The UIDevice class provides a singleton instance representing the current device. From this instance the attacker can obtain information about the device such as assigned name, device model, and operating-system name and version.=== Behavior on infected devices ======= Remote control security risks ====XcodeGhost can be remotely controlled via commands sent by an attacker from a Command and control server through HTTP. This data is encrypted using the DES algorithm in ECB mode. Not only is this encryption mode known to be weak, the encryption keys can also be found using reverse engineering. An attacker could perform a man in the middle attack and transmit fake HTTP traffic to the device (to open a dialog box or open specific app for example).==== Stealing user device information ====When the infected app is launched, either by using an iPhone or the simulator inside Xcode, XcodeGhost will automatically collect device information such as:Current timeCurrent infected app's nameThe app's bundle identifierCurrent device's name and typeCurrent system's language and countryCurrent device's UUIDNetwork typeThen the malware will encrypt those data and send it to a command and control server. The server differs from version to version of XcodeGhost; Palo Alto Networks was able to find three server URLs:http://init.crash-analytics.comhttp://init.icloud-diagnostics.comhttp://init.icloud-analysis.comThe last domain was also used in the iOS malware KeyRaider.==== Read and write from clipboard ====XcodeGhost is also able, each time an infected app is launched, to store the data written in the iOS clipboard. The malware is also able to modify this data. This can be particularly dangerous if the user uses a password management app.==== Hijack opening specific URLs ====XcodeGhost is also able to open specific URLs when the infected app is launched. Since Apple iOS and OS X work with Inter-App Communication URL mechanism (e.g. 'whatsapp://', 'Facebook://', 'iTunes://'?), the attacker can open any apps installed on the compromised phone or computer (in case of an infected OS X app). Such mechanism could be harmful with password management apps or even on phishing websites.==== Prompting alert dialog ====In its current known version XcodeGhost cannot prompt alert dialogs on the user device. However, since it only requires to minor changes.By using a UIAlertView class with the UIAlertViewStyleLoginAndPasswordInput property, the infected app can display a fake alert dialog box that looks like a normal Apple ID user credential check and send the input to the Command and control server.=== Infected apps ===Among all the Chinese apps, IMs app, banking apps, mobile carrier's app, maps, stock trading apps, SNS apps and games were infected. Popular apps used all over the world were also infected such as WeChat, a popular instant messaging app, CamScanner, an app to scan document using the smartphone camera or WinZip.Pangu Team claimed that they counted 3,418 infected apps.Fox-it, a Netherland-based security company reports that they found thousand of malicious traffic outside China.== Removal ===== Neutralizing command and control servers and compromised versions of Xcode ===Since the article of Alibaba and Palo Alto Networks, Amazon took down all the servers that were used by XcodeGhost. Baidu also removed all malicious Xcode installers from its cloud storage service.=== Removing malicious apps from the App Store ===On September 18, 2015 Apple admitted the existence of the malware and began asking all developers with compromised apps to compile their apps with a clean version of Xcode before submitting them for review again.Pangu Team released a tool to detect infected apps on a device, but like other antivirus apps it won't run on a device that hasn't been jailbroken.  Apple does not allow antivirus apps into the iOS App Store.=== Checking Xcode version ===Apple advises Xcode developers to verify their version of Xcode and to always have Gatekeeper activated on their machine.== References ==
	A software map represents static, dynamic, and evolutionary information of software systems and their software development processes by means of 2D or 3D map-oriented information visualization. It constitutes a fundamental concept and tool in software visualization, software analytics, and software diagnosis. Its primary applications include risk analysis for and monitoring of code quality, team activity, or software development progress  and, generally, improving effectiveness of software engineering with respect to all related artifacts, processes, and stakeholders throughout the software engineering process and software maintenance.== Motivation and concepts ==Software maps are applied in the context of software engineering: Complex, long-term software development projects are commonly faced by manifold difficulties such as the friction between completing system features and, at the same time, obtaining a high degree of code quality and software quality to ensure software maintenance of the system in the future. In particular, "Maintaining complex software systems tends to be costly because developers spend a significant part of their time with trying to understand the system’s structure and behavior." The key idea of software maps is to cope with that challenge and optimization problems by providing effective communication means to close the communication gap among the various stakeholders and information domains within software development projects and obtaining insights in the sense of information visualization.Software maps take advantage of well-defined cartographic map techniques using the virtual 3D city model metaphor to express the underlying complex, abstract information space. The metaphor is required "since software has no physical shape, there is no natural mapping of software to a two-dimensional space". Software maps are non-spatial maps that have to convert the hierarchy data and its attributes into a spatial representation.== Applications ==Software maps generally allow for comprehensible and effective communication of course, risks, and costs of software development projects to various stakeholders such as management and development teams. They communicate the status of applications and systems currently being developed or further developed to project leaders and management at a glance. "A key aspect for this decision making is that software maps provide the structural context required for correct interpretation of these performance indicators". As an instrument of communication, software maps act as open, transparent information spaces which enable priorities of code quality and the creation of new functions to be balanced against one another and to decide upon and implement necessary measures to improve the software development process.For example, they facilitate decisions as to where in the code an increase of quality would be beneficial both for speeding up current development activities and for reducing risks of future maintenance problems.Due to their high degree of expressiveness (e.g., information density) and their instantaneous, automated generation, the maps additionally serve to reflect the current status of system and development processes, bridging an essential information gap between management and development teams, improve awareness about the status, and serve as early risk detection instrument.== Contents ==Software maps are based on objective information as determined by the KPI driven code analysis as well as by imported information from software repository systems, information from the source codes, or software development tools and programming tools. In particular, software maps are not bound to a specific programming language, modeling language, or software development process model.Software maps use the hierarchy of the software implementation artifacts such as source code files as base to build a tree mapping, i.e., a rectangular area that represents the whole hierarchy, subdividing the area into rectangular sub-areas. A software map, informally speaking, looks similar to a virtual 3D city model, whereby artifacts of the software system appear as virtual, rectangular 3D buildings or towers, which are placed according to their position in the software implementation hierarchy.Software maps can express and combine information about software development, software quality, and system dynamics by mapping that information onto visual variables of the tree map elements such as footprint size, height, color or texture. They can systematically be specified, automatically generated, and organized by templates.== Mapping software system example ==Software maps "combine thematic information about software development processes (evolution), software quality, structure, and dynamics and display that information in a cartographic manner". For example: The height of a virtual building can be proportional to the complexity of the code unit (e.g., single or combined software metrics).The ground area of a virtual 3D building can be proportional to the number of lines of code in the module or (e.g., non-comment lines-of-code NCLOC).The color can express the current development status, i.e., how many developers are changing/editing the code unit.With this exemplary configuration, the software map shows crucial points in the source code with relations to aspects of the software development process. For example, it becomes obvious at a glance what to change in order to:implement changes quickly;evaluate quickly the impact of changes in one place on functionality elsewhere;reduce entanglements that lead to uncontrolled processes in the application;find errors faster;discover and eliminate bad programming style.Software maps represent key tools in the scope of automated software diagnosis software diagnostics.== As business intelligence tools and recommendation systems ==Software maps can be used, in particular, as analysis and presentation tool of business intelligence systems, specialized in the analysis of software related data. Furthermore, software maps "serve as recommendation systems for software engineering".Software maps are not limited by software-related information: They can include any hierarchical system information as well, for example, maintenance information about complex technical artifacts.== Visualization techniques ==Software maps are investigated in the domain of software visualization. The visualization of software maps is commonly based on tree mapping, "a space-filling approach to the visualization of hierarchical information structures" or other hierarchy mapping approaches.=== Layout algorithms ===To construct software maps, different layout approaches are used to generate the basic spatial mapping of components such as: Tree-map algorithms that initially map the software hierarchy into a recursively nested rectangular area.Voronoi-map algorithms that initially map the software hierarchy by generating a Voronoi map.=== Layout stability ===The spatial arrangement computed by layouts such as defined by tree maps strictly depends on the hierarchy. If software maps have to be generated frequently for an evolving or changing system, the usability of software maps is affected by non-stable layouts, that is, minor changes to the hierarchy may cause significant changes to the layout.In contrast to regular Voronoi treemap algorithms, which do not provide deterministic layouts, layout algorithm for Voronoi treemaps can be extended to provides a high degree of layout similarity for varying hierarchies. Similar approaches exist for the tree-map based case.== History ==Software maps methods and techniques belong the scientific displine of Software visualization and information visualization. They form a key concept and technique within the fields of software diagnosis. They have applications also in software mining and software analytics. Software maps have been extensively developed and researched by, e.g., at the Hasso Plattner Institute for IT systems engineering, in particular for large-scale, complex IT systems and applications.== References ==== External links ==Scientific conference VISSOFT (IEEE Working Conference on Software Visualization) [1]Interactive Rendering of Complex 3D-TreemapsMultiscale Visual Comparison of Execution TracesInteractive Software Maps for Web-Based Source Code AnalysisExtending Recommendation Systems with Software MapsA Visual Analysis Approach to Support Perfective Software MaintenanceViewFusion: Correlating Structure and Activity Views for Execution TracesA Visual Analysis and Design Tool for Planning Software ReengineeringsInteractive Areal Annotations for 3D Treemaps of Large-Scale Software SystemsVisualization of Execution Traces and its Application to Software MaintenanceUnderstanding Complex Multithreaded Software Systems by Using Trace VisualizationVisualization of Multithreaded Behavior to Facilitate Maintenance of Complex Software SystemsVisualizing Massively Pruned Execution Traces to Facilitate Trace ExplorationProjecting Code Changes onto Execution Traces to Support Localization of Recently Introduced BugsSyncTrace: Visual Thread-Interplay Analysis
	SPADE (SMART Process Acceleration Development Environment) is a software development productivity and quality tool used to create professional software in a short time and with little effort. As seen in the diagram SPADE (green icon) automates many manual activities of the Software development process. It therefore takes less time and effort to perform the full software development cycle.With SPADE the remaining manual steps are:  Reqs: gathering the wishes of the customer and documenting them in clear requirements, user stories or similar.Test cases: creating integration tests cases that will be run automatically during Test.Test: usability testing and testing integration with external (non-SPADE) systems.Accept: accepting the created solutionAutomation of the other steps is possible because the (business) requirements are specified clearly by using the method SMART Requirements 2.0. SPADE then uses intelligent algorithms that incorporate dependency analysis and a set of design patterns to transform these requirements to an optimized business process design, including the flow of user interactive and fully automated steps that are part of this business process.SPADE is a domain specific model based software development tool that is well suited for creating both complex as well as simple information processing systems. It is currently less suitable for creating software that can control hardware in real time or advanced graphical user interfaces. One can however add plug-ins for accessing functionality that is not created by SPADE.== Details ==We will explain creating clear requirements using the language SMART notation which is part of the method SMART Requirements 2.0 followed by explaining how and what SPADE will automatically create from these requirements. We will also explain creating and running test cases and the typical architecture of the software solution that is created by SPADE.=== Creating clear requirements ===The input of SPADE are end result oriented business requirement specifications. These explain:This information is placed in a document, usually a file of some sort and is written down using a formal specification language. Below is an example in gray with explanation in italic text.Start by naming the process and its most important piece of information as its subjectProcess 'Order products' with subject #('Order': ORDER)Sum up the high level results. Double quotes are used to define requirements and help to create a result oriented break down structure.  The following applies:     "Customer has ordered products"     and     "Customer has an invoice if approved"     and     "The order needs to be approved if needed"Define the requirements clearly. Use if-then-else to define 'When' results should apply or should be produced. 'Where' the information is coming from is defined using references. For instance ORDER.approved is a piece of available information that is either produced during the process or is already an available piece of information. Some requirements (results) can be specified visually. To your right the "Customer has an invoice" is specified as an e-mail.  "Customer has an invoice if approved" =     if ORDER.approved then "Customer has an invoice" "The order needs to be approved if needed" =     if   "too expensive"     then "The order needs to be approved"     else ORDER.approved = true "too expensive" =     ORDER.total > 500A person can also be a source of information by stating 'input from' followed by a name that identifies the role of this person or the actual user. In the example below a person with the role CONTROLLER. If this persons in turn need information to be able to give this input, you need to state that this input can be given 'based on' certain other information. In this case the date, the BUYER and the LINES of the ORDER.  "The order needs to be approved" =     ORDER.approved = input from CONTROLLER based on #(ORDER.date, ORDER.BUYER, ORDER.LINES)The actual person that is giving the input at the time the system is used (the current user), can also be used as a piece of information. The example below defines the ORDER and its attributes. One if the attributes is called BUYER and this is filled with the actual CUSTOMER that (at the time the actual process runs) is playing that role, in other words giving the input.  "Customer has ordered products" =     One ORDER exists in ORDERS with:         date        = currentDate()         BUYER       = CUSTOMER         LINES       = "order lines" "order lines" =     Several LINE exist in ORDER_LINES with:         PRODUCT = input from CUSTOMER         number  = input from CUSTOMERThe requirements also require a business or logical data model. Most of the logical data model can be derived from the requirements. For instance it knows which entities are needed (ORDERS, ORDER_LINES and PRODUCST) and in some cases it also can derive the type of an attribute. For instance __approved__ can only be true or false because it is used as a condition and LINES should be a relation to ORDER_LINES. Some types however cannot be derived and need to be defined explicitly in this data model. Below is an example of this data model.   ORDERS =       date                 : date       BUYER                : USERS(1)       LINES                : ORDER_LINES(*) opposite of ORDER       approved             : boolean       total                : decimal(10,2) = sum(LINES.total)       summary              : text displayed = '{total} euro by {BUYER.firstName} {BUYER.lastName} on {date}'   ORDER_LINES =       PRODUCT     : PRODUCTS(1)       number      : integer       ORDER       : ORDERS(1) opposite of LINES       total       : decimal(10,2) = number * PRODUCT.price   PRODUCTS =       name         : text       price        : decimal(10,2)       summary      : text displayed = '{name} ({price} euro)'Most of this data model is pretty straight forward and resemble other data modelling techniques. Some things stand out:Relational attributes: relations are specified using relational attributes. For instance BUYER, which contains 1 instance in the standard entity USERS and LINES which contain multiple (*) instances of the entity ORDER_LINES and is the opposite of the relation ORDER (which is a relational attribute of the entity ORDER_LINES).Calculated attributes: attributes can be calculated which means they are not stored but calculated when needed. For instance the total of one instance of ORDERS is the sum of the total of its LINES. The summary is a textual value that is a template text with some placeholders inside that contain total, the first and last name of the BUYER and the date.Displayed: which means that if the system needs to render instances from ORDERS and it doesn't know how to do that, it will use the attribute marked with displayed.=== SPADE automates design and the creation of code ===SPADE perform the following steps: Parse: in other words read the business requirementsAnalyse dependencies: the dependencies between the different parts of the business requirements are analysed.Create process designs: an intelligent algorithm transform dependencies to process designs. It uses a set of design patterns and several optimization techniques to create an optimized process design that has no waste in it. The design is both a high level design (e.g. chains of business processes) as well as a low level design (e.g. at statement level).Generate sources: for the work flow and all the screens and steps in the process design.To your right is an example process design that was created by SPADE. The whole process is the business process, the yellow steps are the user interactive steps or the steps in which the system interacts with an external actor, for instance an external system. The blue steps are the fully automated steps. Example screen captures of the forms are added below the process diagram.=== Creating and running test cases ===When you are using the created solution, you are also recording test cases at the same time. Those test cases are then expanded with asserts that verify the outcome of the process. Below is an example in gray with explanation in italic text.Each test scenario starts with stating which process is started by which user. In this case process 'Order products' for user 'edwinh'.START_PROCESS = Order products, edwinhThe next part describes which roles and users will claim and then enter data in which task. In this case a customer with user name marcusk will enter 2 LINEs and each line will have a selected product and a number of products. The second task is for the manager with user name edwinh and he will fill approved with true.   # -------- FIRST CLAIM AND ENTER THE 1ST TASK ----------   task01.CLAIM_NEXT_GROUP_TASK = customer, marcusk   task01.LINEs = 2   task01.LINEs[0]-c-product = 1   task01.LINEs[0]-c-number = 10   task01.LINEs[1]-c-product = 2   task01.LINEs[1]-c-number = 20      # -------- FIRST CLAIM AND ENTER THE 2ND TASK ----------   task02.CLAIM_NEXT_GROUP_TASK = manager, edwinh   task02.approved = trueThe next part are the asserts the check if the process achieved the predicted end result. These are not recorded and need to be added manually. In this example we have added 2 asserts. The first checks if there is +1 more instance of ORDERS with attribute approved filled with TRUE. The second checks if +2 new instances of ORDER_LINES have been added.   ASSERT_PROCESS_VALUE_COUNT_01 = ORDERS.approved = TRUE, +1   ASSERT_PROCESS_ROW_COUNT_02   = ORDER_LINES, +2=== Deploying the solution ===SPADE can run on its own but it often runs as an Apache Maven plugin and is therefore part of a Maven build cycle. This build cycle also includes running the test scenarios, which in turndeploys the generated functionality as a .jar file,loads tests data,executes the test scenario's andverifies the result.The Maven build cycle can be used in daily builds all the way up to continuous delivery / deployment. For demo purposes, the steps mentioned can also be executed in the standard front-end of the resulting software solution. With the standard front end it is also possible to automate the following:analyze the existing database to check if the database already complies to the generated functionality;if there is no database present, a compliant database can be created automatically;if the database does not yet comply, tables and relations can be create or updated automatically.Migrating data from an old database or from the old release to the new release is also performed automated. However, the migration software (e.g. by using SQL or ETL) is created manually.Note that automation that SPADE provides during deployment is often used for smaller systems and for sprint demos. For deploying bigger projects, other more advanced deployment tools are more commonly used. === The resulting software solution ===The diagram to your right shows how SPADE relates to the created solution, as well as a global architecture of this solution. Below the different elements of the diagram are explained:SMART Business requirements: are (manually) gathered and documented using the requirements specification language SMART notation. This is a Domain-specific language that can be used to define information based end results that business or organizations would want to produce.Automatically creates designs, docs, source code: from the requirements SPADE then automatically creates designs, documentation and source code that can be compiled to the software solution.Users and GUI: the solution can interact with role based authorized users by different GUI's. The solution will already have standard GUI's for all functionality but can be expanded with Custom GUI's. GUI's of both types can be mixed if needed.REST/SOAP: all functionality will always have a matching REST or SOAP service that are used by the different GUI's but can also be used by authorized external systems.DBAL: the server also has a hibernate or similar database abstraction layer to communicate with the database.Plug-ins: can be used or added to the server to communicate with either external systems or with external devices. This enables solution is also able to use devices from the Internet Of Things domain. All plug-ins can be called upon from the business requirements but always in a non-technical way. For instance, if you define a DOCUMENT as a result, SPADE will know to call the plug-in associated with the entity DOCUMENTS. The plug-in will actually create and store a physical document.Specific functionality: this is the functionality that is created based upon the business requirements. With it you can create a wide variety of functionality. SPADE users can use a library of off-the-shelf requirements for example CRM, HR, profile matching and financial functionality. This can be inserted and adjusted to fit the specific needs of the client. The specific functionality can use all plug-ins as well as all generic functionality to extend the domain of available functionality.Generic functionality: by default, the solution is already equipped with a lot of standard generic functionality. For instance DMS, document generation, auto e-mails, SMS, messaging, advanced search, browse through data and export.=== Which software development activities are automated and which are manual? ===The next table shows which software development activities are automated by SPADE and which exceptions apply.== History ==2000: in 2000 Edwin Hendriks of the company CMG (company) (now part of CGI Group) developed a process improvement method called Process Acceleration. At the core of this method was a way to define the desired end result of a business process fully unambiguous as well as a structured approach to deduce the most optimal business process that would achieve this end result. This was the birth of the first version of SMART notation (at that time called PA notation) which is a formal language that can be used to specify end results of entire process chains (versus specifying the process chain itself). CMG (company) used this method and SMART notation for several of their projects and their clients.2007: although successful, CMG (company) at that time was not known for delivering process improvement consultancy. That was the reason for CMG (company) (at that time merged with Logica) to focus on the core of Process Acceleration, thus resulting in 2007 in a method that improves software development called PA SMART Requirements (now called SMART Requirements 2.0). Since that time SMART Requirements 2.0 has been used by CGI Group and their customers as well as other companies and organizations.2008: having an unambiguous definition of the end result of a process chain, and having a structured approach to deduce the most optimal process from this end result, sprung the idea to create a tool that could read the end result, deduce the most optimal process from it, and generate the software for each step in the process. Edwin Hendriks, Marcus Klimstra and Niels Bergsma developed a first working prototype of the SPADE (at that time called the PA Generator) using [.NET] and also producing systems using a [.NET] architecture.2010: Logica decided to start the development of a commercial usable version of the SPADE.2011: version 2.12 of the SPADE was used to create the first 2 systems that were made operational. Being a cross departmental time tracking system and an anonymous voting system both used by Logica itself.2012: version 3 of the SPADE was created. This was the first commercial usable version of the SPADE. From that time SPADE was used to create solutions for the clients. It was often used to recreate existing legacy systems because of the short time and cost associated when creating solutions using SPADE. Despite the increased development speed and low costs, SPADE still had teething problems. This made it difficult to estimate the actual time needed to (re)create solutions making it hard to plan projects. This was the same year that Logica was acquired by CGI Group.2015: version 4 of SPADE was used for the first time by elementary school children to create an exam system. It showed that creating SMART requirements and then asking SPADE to create a professional system for them was relatively easy when compared to other ways of creating software. In the same year a small rocket was launched which interacted with SPADE created ground station software. It showed that SPADE could in fact interact with external devices pretty fast (but still not yet fast enough to be usable to create real-time systems).2016: in version 4.4 SPADE most teething problems were solved making it possible to (re)create large and complex systems in a short time. SPADE is currently being expanded to provide an easier way to create and change requirements as well as an easier way to customize the standard GUI. This will make it possible for more non-developers to use SPADE to create solutions.== Advantages, disadvantages and considerations ==On the upside SPADE shows remarkable development speeds. International benchmarks show that the complete development cycle will be completed on average 20 times faster when compared to conventional techniques and in many cases it is even faster to completely recreate the functionality of existing software solutions compared to buying and configuring them. This development speed of course makes it easier for clients to see and try out the newly created solution. Of course by automating design and coding there will be almost no design and coding errors. The fact that the resulting solutions has no vendor-lock and is completely based on free to use open source components is also a big plus. Of course SPADE is also easy to learn.On the downside SPADE will remains a domain specific language and will therefore not be suitable for any type of functionality. This will require conventional development or other tools. Besides this real-time performance and the ability to change the GUI more easily is something that needs extra development. SPADE is also rather new and is not yet considered a mainstream development tool. Of course creating SMART requirements takes more effort and skill compared to just describing them in a couple of sentences.One should always consider that in normal software development the requirements define a fixed "contract" of the functionality that should be created. For instance the user story in a Scrum development team should also be fixed before the user story can be developed during a sprint. This is the same for SPADE projects. However, when the requirements or the user stories are ready to be developed, the sprint will be performed by SPADE and this will take only a couple of minutes. This has resulted in the tendency to move the requirements phase (the creation of the user stories) to the sprint. This is therefore considered to be a bad practice in both normal Agile development as well as Agile development using SPADE.Another consideration is that it is so easy to large and complex functionality. Although this poses no problem for SPADE, it does make it hard for certain people to handle the sheer size and complexity of the functionality of system. It is therefore advisable to still tackle size and complexity in the same way as you would in normal system development. By chopping up and structuring functionality in comprehensible pieces.== See also ==Disciplined Agile Delivery== References ==== External links ==
	A language workbench is a tool or set of tools that enables software development in the language-oriented programming software development paradigm. A language workbench will typically include tools to support the definition, reuse and composition of domain-specific languages together with their integrated development environment.  Language workbenches were introduced and popularized by Martin Fowler in 2005.Language workbenches usually support:Specification of the language concepts or metamodelSpecification of the editing environments for the domain-specific languageSpecification of the execution semantics, e.g. through interpretation and code generation== Examples ==Racket is a cross platform language development workbench including compiler, JIT compiler, IDE and command line tools designed to accommodate creating both domain-specific languages and completely new languages with facilities to add new notation, constrain constructs, and create IDE tools. JetBrains MPS is a tool for designing domain-specific languages. It uses projectional editing which allows overcoming the limits of language parsers, and building DSL editors, such as ones with tables and diagrams. It implements language-oriented programming. MPS combines an environment for language definition, a language workbench, and an Integrated Development Environment (IDE) for such languages.Kermeta is an open-source academic language workbench.  The Kermeta workbench uses three different meta-languages: one meta-language for the abstract syntax (aligned with Emof); one for the static semantics (aligned with OCL) and one for the behavioral semantics (called the Kermeta Language itself).Melange is a language workbench that provides a modular approach for customizing, assembling and integrating multiple domain-specific language (DSL) specifications and implementations.Spoofax is an open-source language workbench for generating parsers, type checkers, compilers, interpreters, as well as IDE plugins for Eclipse and IntelliJ. It uses SDF and a scannerless GLR parser for syntax, and formalisms derived from Stratego/XT for semantics.Xtext is an open-source software framework for developing programming languages and domain-specific languages (DSLs). Unlike standard parser generators, Xtext generates not only a parser, but also a class model for the abstract syntax tree. In addition, it provides a fully featured, customizable Eclipse-based IDE.== See also ==Language-oriented programmingCompiler-compilerIntentional programmingCategory:Language workbench== References ==== External links ==Martin Fowler, Language WorkbenchLanguage Workbench Challenge
	An artifact is one of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g., use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.The term artifact in connection with software development is largely associated with specific development methods or processes e.g., Unified Process. This usage of the term may have originated with those methods.Build tools often refer to source code compiled for testing as an artifact, because the executable is necessary to carrying out the testing plan.  Without the executable to test, the testing plan artifact is limited to non-execution based testing.  In non-execution based testing, the artifacts are the walkthroughs, inspections and correctness proofs.  On the other hand, execution based testing requires at minimum two artifacts: a test suite and the executable.  An artifact occasionally may be used to refer to the released code (in the case of a code library) or released executable (in the case of a program) produced but the more common usage is in referring to the byproducts of software development rather than the product itself. Open source code libraries often contain a testing harness to allow contributors to ensure their changes do not cause regression bugs in the code library.Much of what are considered artifacts is software documentation.In end-user development an artifact is either an application or a complex data object that is created by an end-user without the need to know a general programming language. Artifacts describe automated behavior or control sequences, such as database requests or grammar rules, or user-generated content.Artifacts vary in their maintainability.  Maintainability is primarily affected by the role the artifact fulfills.  The role can be either practical or symbolic.  In the earliest stages of software development, artifacts may be created by the design team to serve a symbolic role to show the project sponsor how serious the contractor is about meeting the project's needs.  Symbolic artifacts often convey information poorly, but are impressive-looking. Symbolic enhance understanding.  Generally speaking, Illuminated Scrolls are also considered unmaintainable due to the diligence it requires to preserve the symbolic quality.  For this reason, once Illuminated Scrolls are shown to the project sponsor and approved, they are replaced by artifacts which serve a practical role.  Practical artifacts usually need to be maintained throughout the project lifecycle, and, as such, are generally highly maintainable.Artifacts are significant from a project management perspective as deliverables. The deliverables of a software project are likely to be the same as its artifacts with the addition of the software itself.The sense of artifacts as byproducts is similar to the use of the term artifact in science to refer to something that arises from the process in hand rather than the issue itself, i.e., a result of interest that stems from the means rather than the end.To collect, organize and manage artifacts, a Software development folder may be utilized.// POST: api/Todo[HttpPost]public async Task<ActionResult<TodoItem>> PostTodoItem(TodoItem item){    _context.TodoItems.Add(item);    await _context.SaveChangesAsync();     return CreatedAtAction(nameof(GetTodoItem), new { id = item.Id }, item);}== See also ==Artifact (UML)Software development folder== References ==== Further reading ==Per Kroll & Philippe Kruchten (2003). The Rational Unified Process Made Easy: A Practitioner's Guide to the RUP. ISBN 0-321-16609-4.
	Software development is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.Software can be developed for a variety of purposes, the three most common being to meet specific needs of a specific client/business (the case with custom software), to meet a perceived need of some set of potential users (the case with commercial and open source software), or for personal use (e.g. a scientist may write software to automate a mundane task). Embedded software development, that is, the development of embedded software, such as used for controlling consumer products, requires the development process to be integrated with the development of the controlled physical product. System software underlies applications and the programming process itself, and is often developed separately.The need for better quality control of the software development process has given rise to the discipline of software engineering, which aims to apply the systematic approach exemplified in the engineering paradigm to the process of software development.There are many approaches to software project management, known as software development life cycle models, methodologies, processes, or models. The waterfall model is a traditional version, contrasted with the more recent innovation of agile software development.== Methodologies ==A software development process (also known as a software development methodology, model, or life cycle) is a framework that is used to structure, plan, and control the process of developing information systems. A wide variety of such frameworks has evolved over the years, each with its own recognized strengths and weaknesses. There are several different approaches to software development: some take a more structured, engineering-based approach to develop business solutions, whereas others may take a more incremental approach, where software evolves as it is developed piece-by-piece. One system development methodology is not necessarily suitable for use by all projects. Each of the available methodologies is best suited to specific kinds of projects, based on various technical, organizational, project and team considerations.Most methodologies share some combination of the following stages of software development:Analyzing the problemMarket researchGathering requirements for the proposed business solutionDevising a plan or design for the software-based solutionImplementation (coding) of the softwareTesting the softwareDeploymentMaintenance and bug fixingThese stages are often referred to collectively as the software development life-cycle, or SDLC. Different approaches to software development may carry out these stages in different orders, or devote more or less time to different stages. The level of detail of the documentation produced at each stage of software development may also vary. These stages may also be carried out in turn (a “waterfall” based approach), or they may be repeated over various cycles or iterations (a more "extreme" approach). The more extreme approach usually involves less time spent on planning and documentation, and more time spent on coding and development of automated tests. More “extreme” approaches also promote continuous testing throughout the development life-cycle, as well as having a working (or bug-free) product at all times. More structured or “waterfall” based approaches attempt to assess the majority of risks and develop a detailed plan for the software before implementation (coding) begins, and avoid significant design changes and re-coding in later stages of the software development life-cycle planning.There are significant advantages and disadvantages to the various methodologies, and the best approach to solving a problem using software will often depend on the type of problem. If the problem is well understood and a solution can be effectively planned out ahead of time, the more "waterfall" based approach may work the best. If, on the other hand, the problem is unique (at least to the development team) and the structure of the software solution cannot be easily envisioned, then a more "extreme" incremental approach may work best.== Software development activities ===== Identification of need ===The sources of ideas for software products are plentiful. These ideas can come from market research including the demographics of potential new customers, existing customers, sales prospects who rejected the product, other internal software development staff, or a creative third party. Ideas for software products are usually first evaluated by marketing personnel for economic feasibility, for fit with existing channels distribution, for possible effects on existing product lines, required features, and for fit with the company's marketing objectives. In a marketing evaluation phase, the cost and time assumptions become evaluated. A decision is reached early in the first phase as to whether, based on the more detailed information generated by the marketing and development staff, the project should be pursued further.In the book "Great Software Debates", Alan M. Davis states in the chapter "Requirements", sub-chapter "The Missing Piece of Software Development"Students of engineering learn engineering and are rarely exposed to finance or marketing. Students of marketing learn marketing and are rarely exposed to finance or engineering. Most of us become specialists in just one area. To complicate matters, few of us meet interdisciplinary people in the workforce, so there are few roles to mimic. Yet, software product planning is critical to the development success and absolutely requires knowledge of multiple disciplines.Because software development may involve compromising or going beyond what is required by the client, a software development project may stray into less technical concerns such as human resources, risk management, intellectual property, budgeting, crisis management, etc. These processes may also cause the role of business development to overlap with software development.=== Planning ===Planning is an objective of each and every activity, where we want to discover things that belong to the project.An important task in creating a software program is extracting the requirements or requirements analysis. Customers typically have an abstract idea of what they want as an end result but do not know what software should do. Skilled and experienced software engineers recognize incomplete, ambiguous, or even contradictory requirements at this point.  Frequently demonstrating live code may help reduce the risk that the requirements are incorrect."Although much effort is put in the requirements phase to ensure that requirements are complete and consistent, rarely that is the case; leaving the software design phase as the most influential one when it comes to minimizing the effects of new or changing requirements. Requirements volatility is challenging because they impact future or already going development efforts."Once the general requirements are gathered from the client, an analysis of the scope of the development should be determined and clearly stated.  This is often called a scope document.=== Designing ===Once the requirements are established, the design of the software can be established in a software design document.  This involves a preliminary or high-level design of the main modules with an overall picture (such as a block diagram) of how the parts fit together.  The language, operating system, and hardware components should all be known at this time.  Then a detailed or low-level design is created, perhaps with prototyping as proof-of-concept or to firm up requirements.=== Implementation, testing and documenting ===Implementation is the part of the process where software engineers actually program the code for the project.Software testing is an integral and important phase of the software development process. This part of the process ensures that defects are recognized as soon as possible. In some processes, generally known as test-driven development, tests may be developed just before implementation and serve as a guide for the implementation's correctness.Documenting the internal design of software for the purpose of future maintenance and enhancement is done throughout development. This may also include the writing of an API, be it external or internal. The software engineering process chosen by the developing team will determine how much internal documentation (if any) is necessary. Plan-driven models (e.g., Waterfall) generally produce more documentation than Agile models.=== Deployment and maintenance ===Deployment starts directly after the code is appropriately tested, approved for release, and sold or otherwise distributed into a production environment. This may involve installation, customization (such as by setting parameters to the customer's values), testing, and possibly an extended period of evaluation.Software training and support is important, as software is only effective if it is used correctly.Maintaining and enhancing software to cope with newly discovered faults or requirements can take substantial time and effort, as missed requirements may force redesign of the software.. In most cases maintenance is required on regular basis to fix reported issues and keep the software running.== Subtopics ===== View model ===A view model is a framework that provides the viewpoints on the system and its environment, to be used in the software development process. It is a graphical representation of the underlying semantics of a view.The purpose of viewpoints and views is to enable human engineers to comprehend very complex systems and to organize the elements of the problem and the solution around domains of expertise. In the engineering of physically intensive systems, viewpoints often correspond to capabilities and responsibilities within the engineering organization.Most complex system specifications are so extensive that no one individual can fully comprehend all aspects of the specifications. Furthermore, we all have different interests in a given system and different reasons for examining the system's specifications. A business executive will ask different questions of a system make-up than would a system implementer. The concept of viewpoints framework, therefore, is to provide separate viewpoints into the specification of a given complex system. These viewpoints each satisfy an audience with interest in some set of aspects of the system. Associated with each viewpoint is a viewpoint languagethat optimizes the vocabulary and presentation for the audience of that viewpoint.=== Business process and data modelling ===Graphical representation of the current state of information provides a very effective means for presenting information to both users and system developers.A business model illustrates the functions associated with the business process being modeled and the organizations that perform these functions. By depicting activities and information flows, a foundation is created to visualize, define, understand, and validate the nature of a process.A data model provides the details of information to be stored and is of primary use when the final product is the generation of computer software code for an application or the preparation of a functional specification to aid a computer software make-or-buy decision. See the figure on the right for an example of the interaction between business process and data models.Usually, a model is created after conducting an interview, referred to as business analysis. The interview consists of a facilitator asking a series of questions designed to extract required information that describes a process. The interviewer is called a facilitator to emphasize that it is the participants who provide the information. The facilitator should have some knowledge of the process of interest, but this is not as important as having a structured methodology by which the questions are asked of the process expert. The methodology isimportant because usually a team of facilitators is collecting information across the facility and the results of the information from all the interviewers must fit together once completed.The models are developed as defining either the current state of the process, in which case the final product is called the "as-is" snapshot model, or a collection of ideas of what the process should contain, resulting in a "what-can-be" model. Generation of process and data models can be used to determine if the existing processes and information systems are sound and only need minor modifications or enhancements, or if re-engineering is required as a corrective action. The creation of business models is more than a way to view or automate your information process. Analysis can be used to fundamentally reshape the way your business or organization conducts its operations.=== Computer-aided software engineering ===Computer-aided software engineering (CASE), in the field software engineering, is the scientific application of a set of software tools and methods to the development of software which results in high-quality, defect-free, and maintainable software products. It also refers to methods for the development of information systems together with automated tools that can be used in the software development process. The term "computer-aided software engineering" (CASE) can refer to the software used for the automated development of systems software, i.e., computer code. The CASE functions include analysis, design, and programming. CASE tools automate methods for designing, documenting, and producing structured computer code in the desired programming language.Two key ideas of Computer-aided Software System Engineering (CASE) are:Foster computer assistance in software development and software maintenance processes, andAn engineering approach to software development and maintenance.Typical CASE tools exist for configuration management, data modeling, model transformation, refactoring, source code generation.=== Integrated development environment ===An integrated development environment (IDE) also known as integrated design environment or integrated debugging environment is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a:Source code editor,Compiler or interpreter,Build automation tools, andDebugger (usually).IDEs are designed to maximize programmer productivity by providing tight-knit components with similar user interfaces. Typically an IDE is dedicated to a specific programming language, so as to provide a feature set which most closely matches the programming paradigms of the language.=== Modeling language ===A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure. A modeling language can be graphical or textual. Graphical modeling languages use a diagram techniques with named symbols that represent concepts and lines that connect the symbols and that represent relationships and various other graphical annotation to represent constraints. Textual modeling languages typically use standardised keywords accompanied by parameters to make computer-interpretable expressions.Examples of graphical modelling languages in the field of software engineering are:Business Process Modeling Notation (BPMN, and the XML form BPML) is an example of a process modeling language.EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language.Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across layers.Flowchart is a schematic representation of an algorithm or a stepwise process,Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems.IDEF is a family of modeling languages, the most notable of which include IDEF0 for functional modeling, IDEF1X for information modeling, and IDEF5 for modeling ontologies.LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modelling large object-oriented (Java, C++, C#) programs and design patterns.Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behaviour of reactive and distributed systems.Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0, the current version, supports thirteen different diagram techniques and has widespread tool support.Not all modeling languages are executable, and for those that are, using them doesn't necessarily mean that programmers are no longer needed. On the contrary, executable modeling languages are intended to amplify the productivity of skilled programmers, so that they can address more difficult problems, such as parallel computing and distributed systems.=== Programming paradigm ===A programming paradigm is a fundamental style of computer programming, which is not generally dictated by the project management methodology (such as waterfall or agile). Paradigms differ in the concepts and abstractions used to represent the elements of a program (such as objects, functions, variables, constraints) and the steps that comprise a computation (such as assignations, evaluation, continuations, data flows).  Sometimes the concepts asserted by the paradigm are utilized cooperatively in high-level system architecture design; in other cases, the programming paradigm's scope is limited to the internal structure of a particular program or module.A programming language can support multiple paradigms. For example, programs written in C++ or Object Pascal can be purely procedural, or purely object-oriented, or contain elements of both paradigms. Software designers and programmers decide how to use those paradigm elements. In object-oriented programming, programmers can think of a program as a collection of interacting objects, while in functional programming a program can be thought of as a sequence of stateless function evaluations. When programming computers or systems with many processors, process-oriented programming allows programmers to think about applications as sets of concurrent processes acting upon logically shared data structures.Just as different groups in software engineering advocate different methodologies, different programming languages advocate different programming paradigms. Some languages are designed to support one paradigm (Smalltalk supports object-oriented programming, Haskell supports functional programming), while other programming languages support multiple paradigms (such as Object Pascal, C++, C#, Visual Basic, Common Lisp, Scheme, Python, Ruby, and Oz).Many programming paradigms are as well known for what methods they forbid as for what they enable. For instance, pure functional programming forbids using side-effects; structured programming forbids using goto statements. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles. Avoiding certain methods can make it easier to prove theorems about a program's correctness, or simply to understand its behavior.Examples of high-level paradigms include:Aspect-oriented software developmentDomain-specific modelingModel-driven engineeringObject-oriented programming methodologiesGrady Booch's object-oriented design (OOD), also known as object-oriented analysis and design (OOAD). The Booch model includes six diagrams: class, object, state transition, interaction, module, and process.Search-based software engineeringService-oriented modelingStructured programmingTop-down and bottom-up designTop-down programming: evolved in the 1970s by IBM researcher Harlan Mills (and Niklaus Wirth) in developed structured programming.=== Reuse of solutions ===A definition of software reuse is the process of creating software from predefined software components. These are few software reuse methods.A software framework is a re-usable design or implementation for a software system or subsystem.Existing components (Component-based software engineering) can be reused, assembled together to create a larger application.API (Application programming interface, Web service) establish a set of "subroutine definitions, protocols, and tools for building application software" which can be utilized in future builds.Open Source documentations, via libraries such as GitHub, provide free code for software developers to re-use and implement into new applications or designs.== See also ==Continuous integrationCustom softwareDevOpsFunctional specificationProgramming productivitySoftware blueprintSoftware designSoftware development effort estimationSoftware development processSoftware project managementSpecification and Description LanguageUser experienceSoftware industry=== Roles and industry ===Bachelor of Science in Information TechnologyComputer programmerConsulting software engineerOffshore software developmentSoftware developerSoftware engineerSoftware publisher=== Specific applications ===Video game developmentWeb application developmentWeb engineeringMobile Application Development== References ==== Further reading ==Kit, Edward (1992). Software Testing in The Real World. Addison-Wesley Professional. ISBN 0201877562.McCarthy, Jim (1995). Dynamics of Software Development. Microsoft Press. ISBN 1556158238.Conde, Dan (2002). Software Product Management: Managing Software Development from Idea to Product to Marketing to Sales. Aspatore Books. ISBN 1587622025.Davis, A. M. (2005). Just enough requirements management: Where software development meets marketing. Dorset House Publishing Company, Incorporated. ISBN 0932633641.Hasted, Edward (2005). Software That Sells: A Practical Guide to Developing and Marketing Your Software Project. Wiley Publishing. ISBN 0764597833.Hohmann, Luke (2003). Beyond Software Architecture: Creating and Sustaining Winning Solutions. Addison-Wesley Professional. ISBN 0201775948.John W. Horch (2005). "Two Orientations On How To Work With Objects." In: IEEE Software. vol. 12, no. 2, pp. 117–118, Mar., 1995.Rittinghouse, John (2003). Managing Software Deliverables: A Software Development Management Methodology. Digital Press. ISBN 155558313X.Wiegers, Karl E. (2005). More About Software Requirements: Thorny Issues and Practical Advice. Microsoft Press. ISBN 0735622671.Wysocki, Robert K. (2006). Effective Software Project Management. Wiley. ISBN 0764596365.
	Software visualization or software visualisation refers to the visualization of information of and related to software systems—either the architecture of its source code or metrics of their runtime behavior- and their development process by means of static, interactive or animated 2-D or 3-D visual representations of their structure, execution, behavior, and evolution.== Software system information ==Software visualization uses a variety of information available about software systems. Key information categories include: implementation artifacts such as source codes,software metric data from measurements or from reverse engineering,traces that record execution behavior,software testing data (e.g., test coverage)software repository data that tracks changes.== Objectives ==The objectives of software visualization are to support the understanding of software systems (i.e., its structure) and algorithms (e.g., by animating the behavior of sorting algorithms) as well as the analysis and exploration of software systems and their anomalies (e.g., by showing classes with high coupling) and their development and evolution. One of the strengths of software visualization is to combine and relate information of software systems that are not inherently linked, for example by projecting code changes onto software execution traces.Software visualization can be used as tool and technique to explore and analyze software system information, e.g., to discover anomalies similar to the process of visual data mining. For example, software visualization is used to monitoring activities such as for code quality or team activity. Visualization is not inherently a method for software quality assurance. Software visualization participates to Software Intelligence in allowing to discover and take advantage of mastering inner components of software systems.== Types ==Tools for software visualization might be used to visualize source code and quality defects during software development and maintenance activities. There are different approaches to map source code to a visual representation such as by software maps Their objective includes, for example, the automatic discovery and visualization of quality defects in object-oriented software systems and services. Commonly, they visualize the direct relationship of a class and its methods with other classes in the software system and mark potential quality defects. A further benefit is the support for visual navigation through the software system.More or less specialized graph drawing software is used for software visualization. A small-scale 2003 survey of researchers active in the reverse engineering and software maintenance fields found that a wide variety of visualization tools were used, including general purpose graph drawing packages like GraphViz and GraphEd, UML tools like Rational Rose and Borland Together, and more specialized tools like Visualization of Compiler Graphs (VCG) and Rigi. The range of UML tools that can act as a visualizer by reverse engineering source is by no means short; a 2007 book noted that besides the two aforementioned tools, ESS-Model, BlueJ, and Fujaba also have this capability, and that Fujaba can also identify design patterns.== See also ==ProgramsImagix 4DNDependSotoarcSourcetrail[1]Softagram[2]Getaviz[3]SonarGraph[4]Related conceptsApplication discovery and understandingSoftware maintenanceSoftware mapsSoftware diagnosisCognitive dimensions of notationsSoftware archaeology== References ==== Further reading ==Roels, R., Mestereaga, P., and Signer, B. (2016). "An Interactive Source Code Visualisation Plug-in for the MindXpres Presentation Platform". Communications in Computer and Information Science (CCIS), 583, 2016Burch, M., Diehl, S., and Weißgerber, P. (2005). Visual data mining in software archives. Proceedings of the 2005 ACM symposium on Software visualization (SoftVis '05). ACM, New York, NY, USA, 37-46. doi:10.1145/1056018.1056024Diehl, S. (2002). Software Visualization. International Seminar. Revised Papers (LNCS Vol. 2269), Dagstuhl Castle, Germany, 20–25 May 2001 (Dagstuhl Seminar Proceedings).Diehl, S. (2007). Software Visualization — Visualizing the Structure, Behaviour, and Evolution of Software. Springer, 2007, ISBN 978-3-540-46504-1Eades, P. and Zhang, K. (1996). "Software Visualisation", Series on Software Engineering and Knowledge Engineering, Vol.7, World Scientific Co., Singapore, 1996, ISBN 981-02-2826-0, 268 pages.Gîrba, T., Kuhn, A., Seeberger, M., and Ducasse, S., "How Developers Drive Software Evolution," Proceedings of International Workshop on Principles of Software Evolution (IWPSE 2005), IEEE Computer Society Press, 2005, pp. 113–122. PDFKeim, D. A. (2002). Information visualization and visual data mining. IEEE Transactions on Visualization and Computer Graphics, USA * vol 8 (Jan. March 2002), no 1, p 1 8, 67 refs.Knight, C. (2002). System and Software Visualization. In Handbook of software engineering & knowledge engineering. Vol. 2, Emerging technologies (Vol. 2): World Scientific Publishing Company.Kuhn, A., and Greevy, O., "Exploiting the Analogy Between Traces and Signal Processing," Proceedings IEEE International Conference on Software Maintenance (ICSM 2006), IEEE Computer Society Press, Los Alamitos CA, September 2006. PDFLanza, M. (2004). CodeCrawler — polymetric views in action. Proceedings. 19th International Conference on Automated Software Engineering, Linz, Austria, 20 24 Sept. 2004 * Los Alamitos, CA, USA: IEEE Comput. Soc, 2004, p 394 5.Lopez, F. L., Robles, G., & Gonzalez, B. J. M. (2004). Applying social network analysis to the information in CVS repositories. "International Workshop on Mining Software Repositories (MSR 2004)" W17S Workshop 26th International Conference on Software Engineering, Edinburgh, Scotland, UK, 25 May 2004 * Stevenage, UK: IEE, 2004, p 101 5.Marcus, A., Feng, L., & Maletic, J. I. (2003). 3D representations for software visualization. Paper presented at the Proceedings of the 2003 ACM symposium on Software visualization, San Diego, California.Soukup, T. (2002). Visual data mining : techniques and tools for data visualization and mining. New York: Chichester.Staples, M. L., & Bieman, J. M. (1999). 3-D Visualization of Software Structure. In Advances in Computers (Vol. 49, pp. 96–143): Academic Press, London.Stasko, J. T., Brown, M. H., & Price, B. A. (1997). Software Visualization: MIT Press.Van Rysselberghe, F. (2004). Studying Software Evolution Information By Visualizing the Change History. Proceedings. 20th International Conference On Software Maintenance. pp 328–337, IEEE Computer Society Press, 2004Wettel, R., and Lanza, M., Visualizing Software Systems as Cities. In Proceedings of VISSOFT 2007 (4th IEEE International Workshop on Visualizing Software For Understanding and Analysis), pp. 92 – 99, IEEE Computer Society Press, 2007.Zhang, K. (2003). "Software Visualization - From Theory to Practice". Kluwer Academic Publishers, Boston, April 2003, ISBN 1-4020-7448-4, 468 pages.== External links ==SoftVis the ACM Symposium on Software VisualizationVISSOFT 2nd IEEE Working Conference on Software VisualizationEPDV Eclipse Project Dependencies Viewer
	Joseph Robinette Biden Jr. (; born November 20, 1942) is an American politician who served as the 47th vice president of the United States from 2009 to 2017. Biden also represented Delaware in the U.S. Senate from 1973 to 2009. A member of the Democratic Party, Biden is a candidate for president in the 2020 election.Biden was born in Scranton, Pennsylvania, and lived there for ten years before moving with his family to Delaware. He became a lawyer in 1969 and was elected to the New Castle County Council in 1970. He was first elected to the U.S. Senate in 1972, when he became the sixth-youngest senator in American history. Biden was re-elected six times and was the fourth most senior senator when he resigned to assume the vice presidency in 2009. Biden was a long-time member and former chairman of the Foreign Relations Committee. He opposed the Gulf War in 1991, but advocated U.S. and NATO intervention in the Bosnian War in 1994 and 1995. He voted in favor of the resolution authorizing the Iraq War in 2002, but opposed the surge of U.S. troops in 2007. He has also served as chairman of the Senate Judiciary Committee, dealing with issues related to drug policy, crime prevention, and civil liberties. Biden led the efforts to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also chaired the Judiciary Committee during the contentious U.S. Supreme Court nominations of Robert Bork and Clarence Thomas. Biden unsuccessfully sought the Democratic presidential nomination in 1988 and in 2008.In 2008, Biden was the running mate of Democratic presidential nominee Barack Obama and became the first Roman Catholic to serve as vice president of the United States. As vice president, Biden oversaw infrastructure spending aimed at counteracting the Great Recession and helped formulate U.S. policy toward Iraq through the withdrawal of U.S. troops in 2011. His ability to negotiate with congressional Republicans helped the Obama administration pass legislation such as the Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010, which resolved a taxation deadlock; the Budget Control Act of 2011, which resolved that year's debt ceiling crisis; and the American Taxpayer Relief Act of 2012, which addressed the impending fiscal cliff. Obama and Biden were re-elected in 2012.In October 2015, after months of speculation, Biden announced he would not seek the presidency in the 2016 election. In January 2017, Obama awarded Biden the Presidential Medal of Freedom with distinction. After completing his second term as vice president, Biden joined the faculty of the University of Pennsylvania, where he was named the Benjamin Franklin Professor of Presidential Practice. He announced his 2020 run for president on April 25, 2019.== Early life (1942–1965) ==Biden was born on November 20, 1942, at St. Mary's Hospital in Scranton, Pennsylvania, to Catherine Eugenia Biden (née Finnegan) and Joseph Robinette Biden Sr. He was the first of four siblings in a Catholic family, with a sister and two brothers. His mother was of Irish descent, with roots variously attributed to County Louth or County Londonderry. His paternal grandparents, Mary Elizabeth (Robinette) and Joseph H. Biden, an oil businessman from Baltimore, Maryland, were of English, French, and Irish ancestry. His paternal third great-grandfather, William Biden, was born in Sussex, England, and immigrated to the United States. His maternal great-grandfather, Edward Francis Blewitt, was a member of the Pennsylvania State Senate.Biden's father had been wealthy earlier in his life but suffered several financial setbacks by the time his son was born. For several years, the family had to live with Biden's maternal grandparents, the Finnegans. When the Scranton area fell into economic decline during the 1950s, Biden's father could not find sustained work. In 1953, the Biden family moved into an apartment in Claymont, Delaware, where they lived for several years before again moving to a house in Wilmington, Delaware. Joe Biden Sr. became a successful used car salesman, and the family's circumstances were middle class.Biden attended the Archmere Academy in Claymont where he was a standout halfback/wide receiver on the high school football team; he helped lead a perennially losing team to an undefeated season in his senior year. He played on the baseball team as well. During these years, he participated in an anti-segregation sit-in at a Wilmington theatre. Academically, he was an above-average student, was considered a natural leader among the students, and was elected class president during his junior and senior years. He graduated in 1961.He earned his bachelor's in 1965 from the University of Delaware, with a double major in history and political science, graduating with a class rank of 506 out of 688. His classmates were impressed by his cramming abilities, and he played halfback with the Blue Hens freshman football team. In 1964, while on spring break in the Bahamas, he met and began dating Neilia Hunter, who was from an affluent background in Skaneateles, New York, and attended Syracuse University. He told her that he aimed to become a senator by the age of 30 and then president. He dropped a junior year plan to play for the varsity football team as a defensive back, enabling him to spend more time visiting out of state with her.He then entered Syracuse University College of Law, receiving a half scholarship based on financial need with some additional assistance based on academics. By his own description, he found law school to be "the biggest bore in the world" and pulled many all-nighters to get by. During his first year there, he was accused of having plagiarized five of 15 pages of a law review article. Biden said it was inadvertent due to his not knowing the proper rules of citation, and he was permitted to retake the course after receiving an "F" grade, which was subsequently dropped from his record. This incident would later attract attention when further plagiarism accusations emerged in 1987. He received his Juris Doctor in 1968, graduating 76th of 85 in his class. Biden was admitted to the Delaware bar in 1969.Biden received student draft deferments during this period, at the peak of the Vietnam War, and in 1968, he was reclassified by the Selective Service System as not available for service due to having had asthma as a teenager. He never took part in anti-war demonstrations, later saying that at the time he was preoccupied with marriage and law school, and "wore sports coats ... not tie-dyed".Negative impressions of drinking alcohol in the Biden and Finnegan families and in the neighborhood led to Biden becoming a teetotaler. He suffered from stuttering through much of his childhood and into his twenties, and says he overcame it by spending many hours reciting poetry in front of a mirror.== Early political career and family life (1966–1972) ==On August 27, 1966, while Biden was still a law student, he married Neilia Hunter. They overcame her parents' initial reluctance for her to wed a Roman Catholic, and the ceremony was held in a Catholic church in Skaneateles. They had three children, Joseph R. "Beau" Biden III in 1969, Robert Hunter in 1970, and Naomi Christina in 1971.During 1968, Biden clerked for six months at a Wilmington law firm headed by prominent local Republican William Prickett and, as he later said, "thought of myself as a Republican". He disliked the conservative racial politics of incumbent Democratic Governor of Delaware Charles L. Terry and supported a more liberal Republican, Russell W. Peterson, who defeated Terry in 1968. The local Republicans tried to recruit him, but he resisted due to his distaste for Republican presidential candidate Richard Nixon, and registered as an Independent instead.In 1969, Biden resumed practicing law in Wilmington, first as a public defender and then at a firm headed by Sid Balick, a locally active Democrat. Balick named him to the Democratic Forum, a group trying to reform and revitalize the state party, and Biden switched his registration to Democrat. He also started his own firm, Biden and Walsh. Corporate law, however, did not appeal to him and criminal law did not pay well. He supplemented his income by managing properties.Later in 1969, Biden ran as a Democrat for the New Castle County Council on a liberal platform that included support for public housing in the suburban area. He won by a solid, two-thousand vote margin in the usually Republican district and in a bad year for Democrats in the state. Even before taking his seat, he was already talking about running for the U.S. Senate in a couple of years. He served on the County Council from 1970 to 1972 while continuing his private law practice. Biden represented the 4th district on the county council. Among issues he addressed on the council was his opposition to large highway projects that might disrupt Wilmington neighborhoods, including those related to Interstate 95.=== 1972 U.S. Senate campaign ===Biden's entry into the 1972 U.S. Senate election in Delaware presented a unique circumstance. Longtime Delaware political figure and Republican incumbent Senator J. Caleb Boggs was considering retirement, which would likely have left U.S. Representative Pete du Pont and Wilmington Mayor Harry G. Haskell Jr. in a divisive primary fight. To avoid that, U.S. president Richard Nixon helped convince Boggs to run again with full party support. No other Democrat wanted to run against Boggs. Biden's campaign had virtually no money and was given no chance of winning. It was managed by his sister Valerie Biden Owens (who would go on to manage his future campaigns) and staffed by other family members, and relied upon handed-out newsprint position papers and meeting voters face-to-face; the small size of the state and lack of a major media market made the approach feasible. He did receive some assistance from the AFL–CIO and Democratic pollster Patrick Caddell. His campaign issues focused on withdrawal from Vietnam, the environment, civil rights, mass transit, more equitable taxation, health care, the public's dissatisfaction with politics-as-usual, and "change". During the summer, he trailed by almost 30 percentage points, but his energy level, his attractive young family, and his ability to connect with voters' emotions gave the surging Biden an advantage over the ready-to-retire Boggs.  He won the November 7, 1972 election in an upset by a margin of 3,162 votes.=== Family deaths ===On December 18, 1972, a few weeks after the election, Biden's wife and one-year-old daughter Naomi were killed in an automobile accident while Christmas shopping in Hockessin, Delaware. Neilia Biden's station wagon was hit by a tractor-trailer truck as she pulled out from an intersection. Biden's sons Beau and Hunter survived the accident and were taken to the hospital in fair condition, Beau with a broken leg and other wounds, and Hunter with a minor skull fracture and other head injuries. Doctors soon said both would make full recoveries. Biden considered resigning to care for them, but was persuaded not to by Senate Majority Leader Mike Mansfield. In later years, Biden commented that the truck driver had been drinking alcohol before the collision, but these allegations have been denied by the driver's family and were never substantiated by the police.== United States Senate (1973–2009) ===== Recovery and remarriage ===Biden was sworn into office on January 5, 1973, by Francis R. Valeo, the Secretary of the Senate in a small chapel at the Delaware Division of the Wilmington Medical Center. Beau was wheeled in with his leg still in traction; Hunter, who had already been released, was also there, as were other members of the extended family. Witnesses and television cameras were also present and the event received national attention.At age 30 (the minimum age required to hold the office), Biden became the sixth-youngest senator in U.S. history, and one of only 18 senators who took office before reaching the age of 31. But the accident that killed his wife and daughter left him filled with both anger and religious doubt: "I liked to [walk around seedy neighborhoods] at night when I thought there was a better chance of finding a fight ... I had not known I was capable of such rage ... I felt God had played a horrible trick on me." To be at home every day for his young sons, Biden began the practice of commuting every day by Amtrak train for 90 minutes each way from his home in the Wilmington suburbs to Washington, D.C., which he continued to do throughout his Senate career. In the aftermath of the accident, he had trouble focusing on work and appeared to just go through the motions of being a senator. In his memoirs, Biden notes that staffers were taking bets on how long he would last. A single father for five years, he left standing orders that he be interrupted in the Senate at any time if his sons called. In remembrance of his wife and daughter, Biden does not work on December 18, the anniversary of the accident.Biden's elder son, Beau, became Delaware Attorney General and an Army Judge Advocate who served in Iraq; his younger son, Hunter, became a Washington attorney and lobbyist. On May 30, 2015, Beau died at the age of 46 after a two-year battle with brain cancer. At the time of his death, Beau had been widely seen as the frontrunner to be the Democratic nominee for Governor of Delaware in 2016.In 1975, Biden met Jill Tracy Jacobs, who grew up in Willow Grove, Pennsylvania, and would become a teacher in Delaware. They had met on a blind date arranged by Biden's brother, although it turned out that Biden had already noticed a photograph of her in an advertisement for a local park in Wilmington, Delaware. Biden would credit her with renewing his interest both in politics and in life. On June 17, 1977, Biden and Jacobs were married by a Catholic priest at the Chapel at the United Nations in New York. Jill Biden has a bachelor's degree from the University of Delaware; two master's degrees, one from West Chester University and the other from Villanova University; and a doctorate in education from the University of Delaware. They have one daughter together, Ashley Blazer (born 1981), who became a social worker and staffer at the Delaware Department of Services for Children, Youth, and Their Families. Biden and his wife are Roman Catholics and regularly attend Mass at St. Joseph's on the Brandywine in Greenville, Delaware.=== Early Senate activities ===During his first years in the Senate, Biden focused on legislation regarding consumer-protection and environmental issues and called for greater accountability on the part of government. In mid-1974, freshman Senator Biden was named one of the 200 Faces for the Future by Time magazine, in a profile that mentioned what had happened to his family and characterized Biden as "self-confident" and "compulsively ambitious".Biden was one of the Senate's leading opponents of desegregation busing. In his Senate campaign, Biden expressed support for the 1971 Swann decision of the Supreme Court, which supported busing programs to integrate school districts to remedy de jure segregation, but opposed it to remedy de facto segregation, as in Delaware. He said Republicans were using busing as a scare tactic to court Southern white votes, and alongside Boggs voiced opposition to a House of Representatives constitutional amendment banning busing. In 1974, Biden voted to table an amendment to an omnibus education bill promoted by Edward Gurney (R-FL) which contained anti-busing measures and anti-school desegregation clauses. In May, Senator Robert Griffin (R-MI) attempted to revive an amended version of this amendment. Minority Leader Hugh Scott (R-PA) and Majority Leader Mike Mansfield (D-MT) offered a compromise to leave intact the text of Griffin's amendment but added the qualifier that such legislation was not intended to weaken the judiciary's power to enforce the 5th and 14th Amendments of the U.S. Constitution. Biden voted for this compromise, which angered his local voters.Following this, an organization of Delaware residents met at the Krebs School in Newport to protest integration. Biden spoke to the auditorium and said that his position on school busing was evolving, emphasizing that busing in Delaware was (in his opinion) beyond the restriction of the courts. The crowd was unconvinced, and heckled him until he yielded the microphone. This, alongside the prospect of a busing plan in Wilmington (see "Delaware matters"), led to Biden aligning himself with civil rights opponent Senator Jesse Helms (R-NC) in opposing busing. Biden and anti-busing senators wanted to limit the scope of Title VI of Civil Rights Act of 1964 as relating to the power of federal government to enforce school integration policies. After 1975, Biden took a harsher line on further legislative action to limit busing. That year, Helms proposed an anti-integration amendment to an education bill which would stop the Department of Health, Education, and Welfare (HEW) collecting data about the race of students or teachers and therefore could not stop funding districts which refused to integrate. Biden supported this amendment, saying: "I am sure it comes as a surprise to some of my colleagues ... that a senator with a voting record such as mine stands up and supports" the amendment. He said busing was a "bankrupt idea [that violated] the cardinal rule of common sense," and that his opposition would make it easier for other liberals to follow suit. However, he had also supported integrationist Senator Edward Brooke's (R-MA) initiatives on housing, job opportunities and voting rights. Biden's support was criticized by civil rights lawyer Jack Greenberg (director of the NAACP Legal Defense Fund), who said that the bill "heave[d] a brick through the window of school integration", with Biden having his hand on the brick.Biden supported a measure sponsored by Senator Robert Byrd (D-WV), that forbad the use of federal funds to transport students beyond their closest school. This was adopted as part of the Labor-HEW Appropriations Act of 1976. In 1977, Biden co-sponsored an amendment alongside segregationist Thomas Eagleton (D-MO) to close loopholes in Byrd's amendment. A 1977 status report on school desegregation by the federal Civil Rights Commission in Washington, D.C. said, "the enactment of Eagleton-Biden would be an actual violation, on the part of the Federal Government, of the fifth amendment and Title VI" of the Civil Rights Act. The amendment was signed into law by the Carter administration in 1978. Biden repeatedly asked for, and received, Eastland's support on anti-busing measures.Such opposition to busing later led his party to mostly abandon school desegregation policies.Biden became ranking minority member of the U.S. Senate Committee on the Judiciary in 1981. In 1984, he was Democratic floor manager for the successful passage of the Comprehensive Crime Control Act. Over time, the tough-on-crime provisions of the legislation became controversial on the left and among criminal justice reform proponents, and in 2019, Biden characterized his role in passing the legislation as a "big mistake". Biden and his supporters praised him for modifying some of the Act's worst provisions, and it was his most important legislative accomplishment at that point in time. He first considered running for president in that year, after he gained notice for giving speeches to party audiences that simultaneously scolded and encouraged Democrats.Regarding foreign policy, during his first decade in the Senate, Biden focused on arms control issues. In response to the refusal of the U.S. Congress to ratify the SALT II Treaty signed in 1979 by Soviet leader Leonid Brezhnev and President Jimmy Carter, he took the initiative to meet the Soviet Foreign Minister Andrei Gromyko, educated him about American concerns and interests, and secured several changes to address objections of the Foreign Relations Committee. When the Reagan administration wanted to interpret the 1972 SALT I Treaty loosely in order to allow the Strategic Defense Initiative to proceed, Biden argued for strict adherence to the treaty's terms. He clashed again with the Reagan administration in 1986 over economic sanctions against South Africa; he received considerable attention when he excoriated Secretary of State George P. Shultz at a Senate hearing because of the administration's support of that country, which continued to practice the apartheid system.=== 1988 presidential campaign ===Biden ran for the 1988 Democratic presidential nomination, formally declaring his candidacy at the Wilmington train station on June 9, 1987. He was attempting to become the youngest president since John F. Kennedy. When the campaign began, he was considered a potentially strong candidate because of his moderate image, his speaking ability on the stump, his appeal to Baby Boomers, his high-profile position as chair of the Senate Judiciary Committee at the upcoming Robert Bork Supreme Court nomination hearings, and his fundraising appeal. He raised $1.7 million in the first quarter of 1987, more than any other candidate.By August 1987, Biden's campaign, whose messaging was confused due to staff rivalries, had begun to lag behind those of Michael Dukakis and Dick Gephardt, although he had still raised more funds than all candidates but Dukakis, and was seeing an upturn in Iowa polls. In September 1987, the campaign ran into trouble when he was accused of plagiarizing a speech that had been made earlier that year by Neil Kinnock, leader of the British Labour Party. Kinnock's speech included the lines:Why am I the first Kinnock in a thousand generations to be able to get to university? [Then pointing to his wife in the audience] Why is Glenys the first woman in her family in a thousand generations to be able to get to university? Was it because all our predecessors were thick?While Biden's speech included the lines:I started thinking as I was coming over here, why is it that Joe Biden is the first in his family ever to go to a university? [Then pointing to his wife in the audience] Why is it that my wife who is sitting out there in the audience is the first in her family to ever go to college? Is it because our fathers and mothers were not bright? Is it because I'm the first Biden in a thousand generations to get a college and a graduate degree that I was smarter than the rest?Biden had in fact cited Kinnock as the source for the formulation on previous occasions. But he made no reference to the original source at the August 23 Democratic debate at the Iowa State Fair being reported on, nor in an August 26 interview for the National Education Association. Moreover, while political speeches often appropriate ideas and language from each other, Biden's use came under more scrutiny because he fabricated aspects of his own family's background in order to match Kinnock's. Biden was soon found to have earlier that year lifted passages from a 1967 speech by Robert F. Kennedy (for which his aides took the blame), and a short phrase from the 1961 inaugural address of John F. Kennedy; and in two prior years to have done the same with a 1976 passage from Hubert H. Humphrey.A few days later, Biden's plagiarism incident in law school came to public light. Video was also released showing that when earlier questioned by a New Hampshire resident about his grades in law school, he had stated that he had graduated in the "top half" of his class, that he had attended law school on a full scholarship, and that he had received three degrees in college, each of which was untrue or exaggerations of his actual record. Advisers and reporters pointed out that he falsely claimed to have marched in the Civil Rights Movement.The Kinnock and school revelations were magnified by the limited amount of other news about the nomination race at the time, when most of the public were not yet paying attention to any of the campaigns; Biden thus fell into what The Washington Post writer Paul Taylor described as that year's trend, a "trial by media ordeal". He lacked a strong demographic or political group of support to help him survive the crisis. He withdrew from the nomination race on September 23, 1987, saying his candidacy had been overrun by "the exaggerated shadow" of his past mistakes.After Biden withdrew from the race, it was revealed that the Dukakis campaign had secretly made a video highlighting the Biden–Kinnock comparison and distributed it to news outlets. Later in 1987, the Delaware Supreme Court's Board of Professional Responsibility cleared Biden of the law school plagiarism charges regarding his standing as a lawyer, saying Biden had "not violated any rules".In February 1988, after suffering from several episodes of increasingly severe neck pain, Biden was taken by long-distance ambulance to Walter Reed Army Medical Center and given lifesaving surgery to correct an intracranial berry aneurysm that had begun leaking; the situation was serious enough that a priest had administered last rites at the hospital. While recuperating, he suffered a pulmonary embolism, which represented a major complication. Another operation to repair a second aneurysm, which had caused no symptoms but was also at risk from bursting, was performed in May 1988. The hospitalization and recovery kept Biden from his duties in the U.S. Senate for seven months. Biden has had no recurrences or effects from the aneurysms since then. In retrospect, Biden's family came to believe that the early end to his presidential campaign had been a blessing in disguise, for had he still been campaigning in the midst of the primaries in early 1988, he might well have not stopped to seek medical attention and the condition might have become unsurvivable.=== Senate Judiciary Committee ===Biden was a long-time member of the U.S. Senate Committee on the Judiciary. He chaired it from 1987 until 1995 and he served as ranking minority member on it from 1981 until 1987 and again from 1995 until 1997.While chairman, Biden presided over two of the most contentious U.S. Supreme Court confirmation hearings in history, those for Robert Bork in 1987 and Clarence Thomas in 1991. In the Bork hearings, he stated his opposition to Bork soon after the nomination, reversing an approval in an interview of a hypothetical Bork nomination he had made the previous year and angering conservatives who thought he could not conduct the hearings dispassionately. At the close, he won praise for conducting the proceedings fairly and with good humor and courage, as his 1988 presidential campaign collapsed in the middle of the hearings. Rejecting some of the less intellectually honest arguments that other Bork opponents were making, Biden framed his discussion around the belief that the U.S. Constitution provides rights to liberty and privacy that extend beyond those explicitly enumerated in the text, and that Bork's strong originalism was ideologically incompatible with that view. Bork's nomination was rejected in the committee by a 9–5 vote, and then rejected in the full Senate by a 58–42 margin.In the Thomas hearings, Biden's questions on constitutional issues were often long and convoluted, sometimes such that Thomas forgot the question being asked. Viewers of the high-profile hearings were often annoyed by Biden's style. Thomas later wrote that despite earlier private assurances from the senator, Biden's questions had been akin to a beanball. The nomination came out of the committee without a recommendation, with Biden opposed. In part due to his own bad experiences in 1987 with his presidential campaign, Biden was reluctant to let personal matters enter into the hearings. Biden initially shared with the committee, but not the public, Anita Hill's sexual harassment charges, on the grounds she was not yet willing to testify. After she did, Biden did not permit other witnesses to testify further on her behalf, such as Angela Wright (who made a similar charge) and experts on harassment. Biden said he was striving to preserve Thomas's right to privacy and the decency of the hearings. The nomination was approved by a 52–48 vote in the full Senate, with Biden again opposed. During and afterward, Biden was strongly criticized by liberal legal groups and women's groups for having mishandled the hearings and having not done enough to support Hill. Biden subsequently sought out women to serve on the Judiciary Committee and emphasized women's issues in the committee's legislative agenda. In April 2019, Biden called Hill to express regret over his treatment of her; after the conversation, Hill said that she remained deeply unsatisfied.Biden was involved in crafting many federal crime laws. He spearheaded the Violent Crime Control and Law Enforcement Act of 1994, also known as the Biden Crime Law, which included the Federal Assault Weapons Ban, which expired in 2004 after its ten-year sunset period and was not renewed. It also included the landmark Violence Against Women Act (VAWA), which contains a broad array of measures to combat domestic violence. In 2000, the Supreme Court ruled in United States v. Morrison that the section of VAWA allowing a federal civil remedy for victims of gender-motivated violence exceeded Congress's authority and therefore was unconstitutional. Congress reauthorized VAWA in 2000 and 2005. Biden has said, "I consider the Violence Against Women Act the single most significant legislation that I've crafted during my 35-year tenure in the Senate." In 2004 and 2005, Biden enlisted major American technology companies in diagnosing the problems of the Austin, Texas-based National Domestic Violence Hotline, and to donate equipment and expertise to it in a successful effort to improve its services.Biden was critical of the actions of Independent Counsel Kenneth Starr during the 1990s Whitewater controversy and Lewinsky scandal investigations, and said "it's going to be a cold day in hell" before another Independent Counsel is granted the same powers. Biden voted to acquit on both charges during the impeachment of President Clinton.As chairman of the International Narcotics Control Caucus, Biden wrote the laws that created the U.S. "Drug Czar", who oversees and coordinates national drug control policy. In April 2003, he introduced the controversial Reducing Americans' Vulnerability to Ecstasy Act, also known as the RAVE Act. He continued to work to stop the spread of "date rape drugs" such as flunitrazepam, and drugs such as Ecstasy and Ketamine. In 2004, he worked to pass a bill outlawing steroids like androstenedione, the drug used by many baseball players.Biden's "Kids 2000" legislation established a public/private partnership to provide computer centers, teachers, Internet access, and technical training to young people, particularly to low-income and at-risk youth.=== Senate Foreign Relations Committee ===Biden was a long-time member of the U.S. Senate Committee on Foreign Relations. In 1997, he became the ranking minority member and chaired the committee in January 2001 and from June 2001 through 2003. When Democrats re-took control of the Senate following the 2006 elections, Biden again assumed the top spot on the committee in 2007. Biden was generally a liberal internationalist in foreign policy. He collaborated effectively with important Republican Senate figures such as Richard Lugar and Jesse Helms and sometimes went against elements of his own party. Biden was also co-chairman of the NATO Observer Group in the Senate. A partial list covering this time showed Biden meeting with some 150 leaders from nearly 60 countries and international organizations. Biden held frequent hearings as chairman of the committee, as well as holding many subcommittee hearings during the three times he chaired the Subcommittee on European Affairs.Biden became interested in the Yugoslav Wars after hearing about Serbian abuses during the Croatian War of Independence in 1991. Once the Bosnian War broke out, Biden was among the first to call for the "lift and strike" policy of lifting the arms embargo, training Bosnian Muslims and supporting them with NATO air strikes, and investigating war crimes. Both the George H. W. Bush administration and Clinton administration were reluctant to implement the policy, fearing Balkan entanglement. In April 1993, Biden spent a week in the Balkans and held a tense three-hour meeting with Serbian leader Slobodan Milošević. Biden related that he told Milošević, "I think you're a damn war criminal and you should be tried as one." Biden wrote an amendment in 1992 to compel the Bush administration to arm the Bosnians, but deferred in 1994 to a somewhat softer stance preferred by the Clinton administration, before signing on the following year to a stronger measure sponsored by Bob Dole and Joe Lieberman. The engagement led to a successful NATO peacekeeping effort. Biden has called his role in affecting Balkans policy in the mid-1990s his "proudest moment in public life" that related to foreign policy. In 1999, during the Kosovo War, Biden supported the NATO bombing campaign against Serbia and Montenegro, and co-sponsored with his friend John McCain the McCain-Biden Kosovo Resolution, which called on President Clinton to use all necessary force, including ground troops, to confront Milošević over Serbian actions in Kosovo. In 1998, Congressional Quarterly named Biden one of "Twelve Who Made a Difference" for playing a lead role in several foreign policy matters, including NATO enlargement and the successful passage of bills to streamline foreign affairs agencies and punish religious persecution overseas.Biden had voted against authorization for the Gulf War in 1991, siding with 45 of the 55 Democratic senators; he said the U.S. was bearing almost all the burden in the anti-Iraq coalition. Biden was a strong supporter of the 2001 war in Afghanistan, saying "Whatever it takes, we should do it." Regarding Iraq, Biden stated in 2002 that Saddam Hussein was a threat to national security, and that there was no option but to eliminate that threat. In October 2002, Biden voted in favor of the Authorization for Use of Military Force Against Iraq, justifying the Iraq War. While he soon became a critic of the war and viewed his vote as a "mistake", he did not push to require a U.S. withdrawal. He supported the appropriations to pay for the occupation, but argued repeatedly that the war should be internationalized, that more soldiers were needed, and that the Bush administration should "level with the American people" about the cost and length of the conflict.By late 2006, Biden's stance had shifted, and he opposed the troop surge of 2007, saying General David Petraeus was "dead, flat wrong" in believing the surge could work. Biden was instead a leading advocate for dividing Iraq into a loose federation of three ethnic states. In November 2006, Biden and Leslie H. Gelb, President Emeritus of the Council on Foreign Relations, released a comprehensive strategy to end sectarian violence in Iraq. Rather than continuing the present approach or withdrawing, the plan called for "a third way": federalizing Iraq and giving Kurds, Shiites, and Sunnis "breathing room" in their own regions. In September 2007, a non-binding resolution passed the Senate endorsing such a scheme. However, the idea was unfamiliar, had no political constituency, and failed to gain traction. Iraq's political leadership united in denouncing the resolution as a de facto partitioning of the country, and the U.S. Embassy in Baghdad issued a statement distancing itself.In March 2004, Biden secured the brief release of Libyan democracy activist and political prisoner Fathi Eljahmi, after meeting with leader Muammar Gaddafi in Tripoli. In May 2008, Biden sharply criticized President George W. Bush for his speech to Israel's Knesset in which he suggested that some Democrats were acting in the same way some Western leaders did when they appeased Hitler in the runup to World War II. Biden stated: "This is bullshit. This is malarkey. This is outrageous. Outrageous for the president of the United States to go to a foreign country, sit in the Knesset ... and make this kind of ridiculous statement." Biden later apologized for using the expletive. Biden further stated, "Since when does this administration think that if you sit down, you have to eliminate the word 'no' from your vocabulary?"=== Delaware matters ===Biden was a familiar figure to his Delaware constituency, by virtue of his daily train commuting from there, and generally sought to attend to state needs. Biden was a strong supporter of increased Amtrak funding and rail security; he hosted barbecues and an annual Christmas dinner for the Amtrak crews, and they would sometimes hold the last train of the night a few minutes so he could catch it. He earned the nickname "Amtrak Joe" as a result (and in 2011, Amtrak's Wilmington Station was named the Joseph R. Biden Jr. Railroad Station, in honor of the over 7,000 trips he made from there). He was an advocate for Delaware military installations, including Dover Air Force Base and New Castle Air National Guard Base.In 1978, when Biden was seeking re-election as Senator, Wilmington's federally mandated cross-district busing plan generated much turmoil. Biden's compromise solution between his white constituents and African-American leaders was to introduce legislation to outlaw the court's power to enforce certain types of busing, while allowing it to end segregation which had been deliberately imposed by school districts. White anti-integrationists seized onto a comment Biden made saying that he would support the use of federal helicopters if Wilmington's schools could not be voluntarily integrated, and Littleton P. Mitchell (head of the Delaware NAACP) later said that Biden "adequately represented our community for many years, but he quivered that one time on busing". This compromise nearly alienated him both from working-class whites and from African-Americans, but tensions ended following the end of a teachers' strike which began over pay issues raised by the busing plan.Beginning in 1991, Biden served as an adjunct professor at the Widener University School of Law, Delaware's only law school, teaching a seminar on constitutional law. The seminar was one of Widener's most popular, often with a waiting list for enrollment. Biden typically co-taught the course with another professor, taking on at least half the course minutes and sometimes flying back from overseas to make one of the classes.Biden was a sponsor of bankruptcy legislation during the 2000s, which was sought by MBNA, one of Delaware's largest companies, and other credit card issuers. Biden allowed an amendment to the bill to increase the homestead exemption for homeowners declaring bankruptcy and fought for an amendment to forbid anti-abortion felons from using bankruptcy to discharge fines; the overall bill was vetoed by Bill Clinton in 2000 but then finally passed as the Bankruptcy Abuse Prevention and Consumer Protection Act in 2005, with Biden supporting it.Biden held up trade agreements with Russia when that country stopped importing U.S. chickens. The downstate Sussex County region is the nation's top chicken-producing area.In 2007, Biden requested and gained $67 million worth of projects for his constituents through congressional earmarks.=== Reputation ===Following his initial election in 1972, Biden was re-elected to six additional terms, in the elections of 1978, 1984, 1990, 1996, 2002, and 2008, usually getting about 60 percent of the vote. He did not face strong opposition; Pete du Pont, then governor, chose not to run against him in 1984. Biden spent 28 years as a junior senator due to the two-year seniority of his Republican colleague William V. Roth Jr. After Roth was defeated for re-election by Tom Carper in 2000, Biden became Delaware's senior senator. He then became the longest-serving senator in Delaware history and, as of 2018, was the 18th longest serving senator in the history of the United States. In May 1999, Biden became the youngest senator to cast 10,000 votes.With a net worth between $59,000 and $366,000, and almost no outside income or investment income, Biden was consistently ranked as one of the least wealthy members of the Senate. Biden stated that he was listed as the second-poorest member in Congress; he was not proud of the distinction, but attributed it to having been elected early in his career. Biden realized early in his senatorial career how vulnerable poorer public officials are to offers of financial contributions in exchange for policy support, and he pushed campaign finance reform measures during his first term. Biden earned $15.6 million in 2017-2018. By 2019, Biden's middle class status was referred to as a "state of mind". Biden's assets increased to between $2.2 million and $8 million.During his years as a senator, Biden amassed a reputation for loquaciousness, with his questions and remarks during Senate hearings being known as long-winded. He has been a strong speaker and debater and a frequent and effective guest on Sunday morning talk shows. In public appearances, he is known to deviate from prepared remarks. According to political analyst Mark Halperin, he has shown "a persistent tendency to say silly, offensive, and off-putting things"; The New York Times writes that Biden's "weak filters make him capable of blurting out pretty much anything". Journalist James Traub has written that "Biden's vanity and his regard for his own gifts seem considerable even by the rarefied standards of the U.S. Senate."The political writer Howard Fineman has said, "Biden is not an academic, he's not a theoretical thinker, he's a great street pol. He comes from a long line of working people in Scranton—auto salesmen, car dealers, people who know how to make a sale. He has that great Irish gift." Political columnist David S. Broder has viewed Biden as having grown since he came to Washington and since his failed 1988 presidential bid: "He responds to real people—that's been consistent throughout. And his ability to understand himself and deal with other politicians has gotten much much better." Traub concludes that "Biden is the kind of fundamentally happy person who can be as generous toward others as he is to himself."=== 2008 presidential campaign ===Biden had thought about running for president again ever since his failed 1988 bid.Biden declared his candidacy for president on January 31, 2007, after having discussed running for months prior. Biden made a formal announcement to Tim Russert on Meet the Press, stating he would "be the best Biden I can be". In January 2006, Delaware newspaper columnist Harry F. Themal wrote that Biden "occupies the sensible center of the Democratic Party". Themal concludes that this is the position Biden desires, and that in a campaign "he plans to stress the dangers to the security of the average American, not just from the terrorist threat, but from the lack of health assistance, crime, and energy dependence on unstable parts of the world".During his campaign, Biden focused on the war in Iraq and his support for the implementation of the Biden-Gelb plan to achieve political success. He touted his record in the Senate as the head of major congressional committees and his experience on foreign policy. Despite speculation to the contrary, Biden rejected the notion of accepting the position of Secretary of State, focusing only on the presidency. At a 2007 campaign event, Biden said, "I know a lot of my opponents out there say I'd be a great Secretary of State. Seriously, every one of them. Do you watch any of the debates? 'Joe's right, Joe's right, Joe's right.'" Other candidates' comments that "Joe is right" in the Democratic debates were converted into a Biden campaign theme and ad. In mid-2007, Biden stressed his foreign policy expertise compared to Obama's, saying of the latter, "I think he can be ready, but right now I don't believe he is. The presidency is not something that lends itself to on-the-job training." Biden also said that Obama was copying some of his foreign policy ideas. Biden was noted for his one-liners on the campaign trail, saying of Republican then-frontrunner Rudy Giuliani at the debate on October 30, 2007, in Philadelphia, "There's only three things he mentions in a sentence: a noun, and a verb and 9/11." Overall, Biden's debate performances were an effective mixture of humor, and sharp and surprisingly disciplined comments.Biden made remarks during the campaign that attracted controversy. On the day of his January 2007 announcement, he spoke of fellow Democratic candidate and Senator Barack Obama: "I mean, you got the first mainstream African-American who is articulate and bright and clean and a nice-looking guy, I mean, that's a storybook, man." This comment undermined his campaign as soon as it began and significantly damaged his fund-raising capabilities; it later took second place on Time magazine's list of Top 10 Campaign Gaffes for 2007. Biden had earlier been criticized in July 2006 for a remark he made about his support among Indian Americans: "I've had a great relationship. In Delaware, the largest growth in population is Indian-Americans moving from India. You cannot go to a 7-Eleven or a Dunkin' Donuts unless you have a slight Indian accent. I'm not joking." Biden later said the remark was not intended to be derogatory.In an unusual move, Biden shared campaign planes with one of his rivals for the nomination, US Senator Chris Dodd of Connecticut. Dodd and Biden were friends, and seeking to save funds during somewhat long-shot efforts at obtaining the nomination.Overall, Biden had difficulty raising funds, struggled to draw people to his rallies, and failed to gain traction against the high-profile candidacies of Obama and Senator Hillary Clinton; he never rose above single digits in the national polls of the Democratic candidates. In the initial contest on January 3, 2008, Biden placed fifth in the Iowa caucuses, garnering slightly less than one percent of the state delegates. Biden withdrew from the race that evening, saying "There is nothing sad about tonight. ... I feel no regret."Despite the lack of success, Biden's stature in the political world rose as the result of his 2008 campaign. In particular, it changed the relationship between Biden and Obama. Although the two had served together on the Senate Foreign Relations Committee, they had not been close, with Biden having resented Obama's quick rise to political stardom, and Obama having viewed Biden as garrulous and patronizing. Now, having gotten to know each other during 2007, Obama appreciated Biden's campaigning style and appeal to working class voters, and Biden was convinced that Obama was "the real deal".== 2008 vice presidential campaign ==Since shortly following Biden's withdrawal from the presidential race, Obama had been privately telling Biden that he was interested in finding an important place for him in a possible Obama administration. Biden declined Obama's first request to vet him for the vice presidential slot, fearing the vice presidency would represent a loss in status and voice from his Senate position, but subsequently changed his mind. In a June 22, 2008, interview on NBC's Meet the Press, Biden confirmed that, although he was not actively seeking a spot on the ticket, he would accept the vice presidential nomination if offered. In early August, Obama and Biden met in secret to discuss a possible vice-presidential relationship, and the two developed a strong personal rapport. On August 22, 2008, Barack Obama announced that Biden would be his running mate. The New York Times reported that the strategy behind the choice reflected a desire to fill out the ticket with someone who has foreign policy and national security experience—and not to help the ticket win a swing state or to emphasize Obama's "change" message. Other observers pointed out Biden's appeal to middle class and blue-collar voters, as well as his willingness to aggressively challenge Republican nominee John McCain in a way that Obama seemed uncomfortable doing at times. In accepting Obama's offer, Biden ruled out to him the possibility of running for president again in 2016 (although comments by Biden in subsequent years seemed to back off that stance, with Biden not wanting to diminish his political power by appearing uninterested in advancement). Biden was officially nominated for vice president on August 27 by voice vote at the 2008 Democratic National Convention in Denver, Colorado.After his selection as a vice presidential candidate, his own Roman Catholic Diocese of Wilmington confirmed that even if elected vice president, Biden would not be allowed to speak at Catholic schools. Biden was soon barred from receiving Holy Communion by the bishop of his original hometown of Scranton, Pennsylvania, because of his support for abortion rights; however, Biden did continue to receive Communion at his local Delaware parish. Scranton became a flash point in the competition for swing state Catholic voters between the Democratic campaign and liberal Catholic groups, who stressed that other social issues should be considered as much or more than abortion, and many bishops and conservative Catholics, who maintained abortion was paramount. Biden said he believed that life began at conception but that he would not impose his personal religious views on others. Bishop Saltarelli had previously stated regarding stances similar to Biden's: "No one today would accept this statement from any public servant: 'I am personally opposed to human slavery and racism but will not impose my personal conviction in the legislative arena.' Likewise, none of us should accept this statement from any public servant: 'I am personally opposed to abortion but will not impose my personal conviction in the legislative arena.'"Biden's vice presidential campaigning gained little media visibility, as far greater press attention was focused on the Republican running mate, Alaskan Governor Sarah Palin. During one week in September 2008, for instance, the Pew Research Center's Project for Excellence in Journalism found that Biden was included in only five percent of the news coverage of the race, far less than for the other three candidates on the tickets. Biden nevertheless focused on campaigning in economically challenged areas of swing states and trying to win over blue-collar Democrats, especially those who had supported Hillary Clinton. Biden attacked McCain heavily, despite a long-standing personal friendship; he would say, "That guy I used to know, he's gone. It literally saddens me." As the financial crisis of 2007–2010 reached a peak with the liquidity crisis of September 2008 and the proposed bailout of the United States financial system became a major factor in the campaign, Biden voted in favor of the $700 billion Emergency Economic Stabilization Act of 2008, which passed the Senate 74–25.On October 2, 2008, Biden participated in the campaign's one vice presidential debate with Palin at Washington University in St. Louis. Post-debate polls found that while Palin exceeded many voters' expectations, Biden had won the debate overall. On October 5, Biden suspended campaign events for a few days after the death of his mother-in-law. During the final days of the campaign, Biden focused on less-populated, older, less well-off areas of battleground states, especially in Florida, Ohio, and Pennsylvania, where polling indicated he was popular and where Obama had not campaigned or performed well in the Democratic primaries. He also campaigned in some normally Republican states, as well as in areas with large Catholic populations.Under instructions from the Obama campaign, Biden kept his speeches succinct and tried to avoid off-hand remarks, such as one about Obama's being tested by a foreign power soon after taking office, which had attracted negative attention. Privately, Obama was frustrated by Biden's remarks, saying "How many times is Biden gonna say something stupid?" Obama campaign staffers referred to Biden blunders as "Joe bombs" and kept Biden uninformed about strategy discussions, which in turn irked Biden. Relations between the two campaigns became strained for a month, until Biden apologized on a call to Obama and the two built a stronger partnership. Publicly, Obama strategist David Axelrod said that any unexpected comments had been outweighed by Biden's high popularity ratings. Nationally, Biden had a 60 percent favorability rating in a Pew Research Center poll, compared to Palin's 44 percent.On November 4, 2008, Obama was elected president and Biden was elected vice president. The Obama–Biden ticket won 365 Electoral College votes to McCain–Palin's 173, and had a 53–46 percent edge in the nationwide popular vote.Biden had continued to run for his Senate seat as well as for vice president, as permitted by Delaware law. On November 4 Biden was also re-elected as senator, defeating Republican Christine O'Donnell.Having won both races, Biden made a point of holding off his resignation from the Senate so that he could be sworn in for his seventh term on January 6, 2009. He became the youngest senator ever to start a seventh full term, and said, "In all my life, the greatest honor bestowed upon me has been serving the people of Delaware as their United States senator." Biden cast his last Senate vote on January 15, supporting the release of the second $350 billion for the Troubled Asset Relief Program. Biden resigned from the Senate later that day; in emotional farewell remarks on the Senate floor, where he had spent most of his adult life, Biden said, "Every good thing I have seen happen here, every bold step taken in the 36-plus years I have been here, came not from the application of pressure by interest groups, but through the maturation of personal relationships."Delaware Gov. Ruth Ann Minner appointed long-time Biden adviser, Ted Kaufman, to complete the term. Kaufman chose not to fun for a full term and, after a special election in 2010, was succeeded by Democrat Chris Coons.== Vice presidency (2009–2017) ===== Post-election transition ===On November 4, 2008, Biden was elected Vice President of the United States as Obama's running mate.Soon after the election, he was appointed chairman of president-elect Obama's transition team. During the transition phase of the Obama administration, Biden said he was in daily meetings with Obama and that McCain was still his friend. The U.S. Secret Service codename given to Biden is "Celtic", referencing his Irish roots.Biden chose veteran Democratic lawyer and aide Ron Klain to be his chief of staff, and Time Washington bureau chief Jay Carney to be his director of communications. Biden intended to eliminate some of the explicit roles assumed by the vice presidency of his predecessor, Dick Cheney, who had established himself as an autonomous power center. Otherwise, Biden said he would not emulate any previous vice presidency, but would instead seek to provide advice and counsel on every critical decision Obama would make. Biden said he was closely involved in all the cabinet appointments that were made during the transition. Biden was also named to head the new White House Task Force on Working Families, an initiative aimed at improving the economic well being of the middle class. In his last act as Chairman of the Foreign Relations Committee, Biden went on a trip to Iraq, Afghanistan and Pakistan during the second week of January 2009, meeting with the leadership of those countries.=== First term (2009–2013) ===At noon on January 20, 2009, Joe Biden became the 47th vice president of the United States, sworn into the office by Supreme Court Justice John Paul Stevens. Biden is the first United States vice president from Delaware and the first Roman Catholic to attain that office.In the early months of the Obama administration, Biden assumed the role of an important behind-the-scenes counselor. One role was to adjudicate disputes between Obama's "team of rivals". The president compared Biden's efforts to a basketball player "who does a bunch of things that don't show up in the stat sheet". Biden played a key role in gaining Senate support for several major pieces of Obama legislation, and was a main factor in convincing Senator Arlen Specter to switch from the Republican to the Democratic party. Biden lost an internal debate to Secretary of State Hillary Clinton regarding his opposition to sending 21,000 new troops to the war in Afghanistan. His skeptical voice was still considered valuable within the administration, however, and later in 2009 Biden's views achieved more prominence within the White House as Obama reconsidered his Afghanistan strategy.Biden made visits to Iraq about once every two months, including trips to Baghdad in August and September 2009 to listen to Prime Minister Nouri al-Maliki and reiterate U.S. stances on Iraq's future; by this time he had become the administration's point man in delivering messages to Iraqi leadership about expected progress in the country. More generally, overseeing Iraq policy became Biden's responsibility: the President is said to have put it as "Joe, you do Iraq". Biden said Iraq "could be one of the great achievements of this administration". Biden's January 2010 visit to Iraq in the midst of turmoil over banned candidates from the upcoming Iraqi parliamentary election resulted in 59 of the several hundred candidates being reinstated by the Iraqi government two days later. By 2012, Biden had made eight trips there, but his oversight of U.S. policy in Iraq receded with the exit in 2011 of U.S. troops.Biden was also in charge of the oversight role for infrastructure spending from the Obama stimulus package intended to help counteract the ongoing recession, and stressed that only worthy projects should get funding. He talked with hundreds of governors, mayors, and other local officials in this role. During this period, Biden was satisfied that no major instances of waste or corruption had occurred, and when he completed that role in February 2011, he said that the number of fraud incidents with stimulus monies had been less than one percent.It took some time for the cautious Obama and the blunt, rambling Biden to work out ways of dealing with each other. In late April 2009, Biden's off-message response to a question during the beginning of the swine flu outbreak, that he would advise family members against travelling on airplanes or subways, led to a swift retraction from the White House. The remark revived Biden's reputation for gaffes, and led to a spate of late-night television jokes themed on him being a loose-talking buffoon. In the face of persistently rising unemployment through July 2009, Biden acknowledged that the administration had "misread how bad the economy was" but maintained confidence that the stimulus package would create many more jobs once the pace of expenditures picked up. The same month, Secretary of State Clinton quickly disavowed Biden's remarks disparaging Russia as a power, but despite any missteps, Biden still retained Obama's confidence and was increasingly influential within the administration. On March 23, 2010, a microphone picked up Biden telling the President that his signing of the Patient Protection and Affordable Care Act was "a big ... deal", using an adjective beginning with "f", during live national news telecasts. White House press secretary Robert Gibbs replied via Twitter "And yes Mr. Vice President, you're right ..." Despite their different personalities, Obama and Biden formed a friendship, partly based around Obama's daughter Sasha and Biden's granddaughter Maisy, who attended Sidwell Friends School together.Biden's most important role within the administration was to question assumptions, playing a contrarian role. Obama said that "The best thing about Joe is that when we get everybody together, he really forces people to think and defend their positions, to look at things from every angle, and that is very valuable for me." Another senior Obama advisor said Biden "is always prepared to be the skunk at the family picnic to make sure we are as intellectually honest as possible". On June 11, 2010, Biden represented the United States at the opening ceremony of the World Cup, attended the England v. U.S. game which was tied 1–1, and visited Egypt, Kenya, and South Africa.Throughout, Joe and Jill Biden maintained a relaxed atmosphere at their official residence in Washington, often entertaining some of their grandchildren, and regularly returned to their home in Delaware.Biden campaigned heavily for Democrats in the 2010 midterm elections, maintaining an attitude of optimism in the face of general predictions of large-scale losses for the party. Following large-scale Republican gains in the elections and the departure of White House Chief of Staff Rahm Emanuel, Biden's past relationships with Republicans in Congress became more important. He led the successful administration effort to gain Senate approval for the New START treaty. In December 2010, Biden's advocacy within the White House for a middle ground, followed by his direct negotiations with Senate Minority Leader Mitch McConnell, were instrumental in producing the administration's compromise tax package that revolved around a temporary extension of the Bush tax cuts. Biden then took the lead in trying to sell the agreement to a reluctant Democratic caucus in Congress, which was passed as the Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010.In foreign policy, Biden supported the NATO-led military intervention in Libya in 2011. Biden has supported closer economic ties with Russia.In March 2011, Obama detailed Biden to lead negotiations among both houses of Congress and the White House in resolving federal spending levels for the rest of the year, and avoiding a government shutdown. By May 2011, a "Biden panel" with six congressional members was trying to reach a bipartisan deal on raising the U.S. debt ceiling as part of an overall deficit reduction plan. The U.S. debt ceiling crisis developed over the next couple of months, but it was again Biden's relationship with McConnell that proved to be a key factor in breaking a deadlock and finally bringing about a bipartisan deal to resolve it, in the form of the Budget Control Act of 2011, signed on August 2, 2011, the same day that an unprecedented U.S. default had loomed. Biden had spent the most time bargaining with Congress on the debt question of anyone in the administration, and one Republican staffer said, "Biden's the only guy with real negotiating authority, and [McConnell] knows that his word is good. He was a key to the deal."It has been reported that Biden was opposed to going forward with the May 2011 U.S. mission to kill Osama bin Laden, lest failure adversely affect Obama's chances for a second term. He took the lead in notifying Congressional leaders of the successful outcome.=== 2012 re-election campaign ===In October 2010, Biden stated that Obama had asked him to remain as his running mate for the 2012 presidential election. With Obama's popularity on the decline, however, in late 2011 White House Chief of Staff William M. Daley conducted some secret polling and focus group research into the idea of Secretary of State Clinton replacing Biden on the ticket. The notion was dropped when the results showed no appreciable improvement for Obama, and White House officials later said that Obama had never entertained the idea.Biden's May 2012 statement that he was "absolutely comfortable" with same-sex marriage gained considerable public attention in comparison to President Obama's position, which had been described as "evolving". Biden made his statement without administration consent, and Obama and his aides were quite irked, since Obama had planned to shift position several months later, in the build-up to the party convention, and since Biden had previously counseled the President to avoid the issue lest key Catholic voters be offended. Gay rights advocates seized upon the Biden stance, and within days, Obama announced that he too supported same-sex marriage, an action in part forced by Biden's unexpected remarks. Biden apologized to Obama in private for having spoken out, while Obama acknowledged publicly it had been done from the heart. The incident showed that Biden still struggled at times with message discipline; as Time wrote, "everyone knows [that] Biden's greatest strength is also his greatest weakness." Relations were also strained between the campaigns when Biden appeared to use his to bolster fundraising contacts for a possible run on his own in the 2016 presidential election, and the vice president ended up being excluded from Obama campaign strategy meetings.The Obama campaign nevertheless still valued Biden as a retail-level politician who could connect with disaffected, blue collar workers and rural residents, and he had a heavy schedule of appearances in swing states as the Obama re-election campaign began in earnest in spring 2012. An August 2012 remark before a mixed-race audience that proposed Republican relaxation of Wall Street regulations would "put y'all back in chains" led to a similar analysis of Biden's face-to-face campaigning abilities versus tendency to go off track. The Los Angeles Times wrote, "Most candidates give the same stump speech over and over, putting reporters if not the audience to sleep. But during any Biden speech, there might be a dozen moments to make press handlers cringe, and prompt reporters to turn to each other with amusement and confusion." Time magazine wrote that Biden often goes too far and that "Along with the familiar Washington mix of neediness and overconfidence, Biden's brain is wired for more than the usual amount of goofiness."Biden was officially nominated for a second term as vice president on September 6 by voice vote at the 2012 Democratic National Convention in Charlotte, North Carolina. He faced his Republican counterpart, Representative Paul Ryan, in the lone 2012 vice presidential debate on October 11 in Danville, Kentucky. There he made a feisty, emotional defense of the Obama administration's record and energetically attacked the Republican ticket, in an effort to regain campaign momentum lost by Obama's unfocused debate performance against Republican nominee Mitt Romney the week before.On November 6, 2012, the President and the Vice President were elected to second terms. The Obama–Biden ticket won 332 Electoral College votes to Romney–Ryan's 206 and had a 51–47 percent edge in the nationwide popular vote.==== Post-election ====In December 2012, Biden was named by Obama to head the Gun Violence Task Force, created to address the causes of gun violence in the United States in the aftermath of the Sandy Hook Elementary School shooting. Later that month, during the final days before the country fell off the "fiscal cliff", Biden's relationship with McConnell once more proved important as the two negotiated a deal that led to the American Taxpayer Relief Act of 2012 being passed at the start of 2013. It made permanent much of the Bush tax cuts but raised rates on upper income levels.=== Second term (2013–2017) ===Biden was inaugurated to a second term in the early morning of January 20, 2013, at a small ceremony in his official residence with Justice Sonia Sotomayor presiding (a public ceremony took place on January 21). He continued to be in the forefront as, in the wake of the Sandy Hook Elementary School shooting, the Obama administration put forth executive orders and proposed legislation towards new gun control measures (the legislation failed to pass).During the discussions that led to the October 2013 passage of the Continuing Appropriations Act, 2014, which resolved the U.S. federal government shutdown of 2013 and the U.S. debt-ceiling crisis of 2013, Biden played little role. This was due to Senate Majority Leader Harry Reid and other Democratic leaders cutting the vice president out of any direct talks with Congress, feeling that Biden had given too much away during previous negotiations.Biden's Violence Against Women Act was reauthorized again in 2013. The act led to further related developments in the creation of the White House Council on Women and Girls, begun in the first term, as well as the White House Task Force to Protect Students from Sexual Assault, begun in January 2014 with Biden as co-chair along with Valerie Jarrett. Biden has a strong stance on sexual assault. For example, Biden stated to a victim of sexual assault at Stanford University, "you did it ... in the hope that your strength might prevent this crime from happening to someone else. Your bravery is breathtaking." He has also taken legality into the situation. Biden issued federal guidelines while presenting a speech at the University of New Hampshire. He stated that "No means no, if you're drunk or you're sober. No means no if you're in bed, in a dorm or on the street. No means no even if you said yes at first and you changed your mind. No means no."Biden favored arming Syria's rebel fighters. As Iraq fell apart during 2014, renewed attention was paid to the Biden-Gelb Iraqi federalization plan of 2006, with some observers suggesting that Biden had been right all along. Biden himself said that the U.S. would follow ISIL "to the gates of hell". In October 2014, Biden said that Turkey, Saudi Arabia and the United Arab Emirates had "poured hundreds of millions of dollars and tens of thousands of tons of weapons into anyone who would fight against Al-Assad, except that the people who were being supplied were al-Nusra, and al Qaeda, and the extremist elements of jihadis coming from other parts of the world."By 2015, a series of swearings-in and other events where Biden had placed his hands on women and girls and talked closely to them attracted the attention of both the press and social media. In one case, a senator issued a statement afterward saying about his daughter, "No, she doesn't think the vice president is creepy." On January 17, 2015, Secret Service agents heard shots were fired as a vehicle drove near Biden's Delaware residence at 8:28 p.m. outside the security perimeter, but the vice president and his wife Jill were not home. A vehicle was observed by an agent speeding away.On December 8, 2015, Biden spoke in Ukraine's parliament in Kiev in one of his many visits to set USA aid and policy stance for Ukraine.On February 28, 2016, Biden gave a speech at the 88th Academy Awards to do with awareness for sexual assault; he also introduced Lady Gaga.In March 2016, Biden spoke at the American Israel Public Affairs Committee (AIPAC) Policy Conference in Washington, D.C. In his speech, he stated, "We're all united by our unyielding—I mean literally unyielding—commitment to the survival, the security, and the success of the Jewish State of Israel."On December 8, 2016, Biden went to Ottawa to meet with the Prime Minister of Canada, Justin Trudeau.During his two full terms, Joe Biden never cast a tie-breaking vote in the Senate, making him the longest serving vice president with this distinction.==== Death of Beau Biden ====On May 30, 2015, Biden's son, Beau Biden, died at age 46 after having battled brain cancer for several years. In a statement, the Vice President's office said, "The entire Biden family is saddened beyond words." The nature and seriousness of the illness had not been previously disclosed to the public, and Biden had quietly reduced his public schedule in order to spend more time with his son. At the time of his death, Beau Biden had been widely seen as the frontrunner to be the Democratic nominee for Governor of Delaware in 2016.=== Role in the 2016 presidential campaign ===During much of his second term, Biden was said to be preparing for a possible bid for the 2016 Democratic presidential nomination. At age 74 on Inauguration Day in January 2017, he would have been the oldest president on inauguration in history. With his family, many friends, and donors encouraging him in mid-2015 to enter the race, and with Hillary Clinton's favorability ratings in decline at that time, Biden was reported to again be seriously considering the prospect and a "Draft Biden 2016" PAC was established.As of September 11, 2015, Biden was still uncertain whether or not to run. Biden cited the recent death of his son being a large drain on his emotional energy, and that "nobody has a right ... to seek that office unless they're willing to give it 110% of who they are".On October 21, speaking from a podium in the Rose Garden with his wife and President Obama by his side, Biden announced his decision not to enter the race for the Democratic presidential nomination in the 2016 election. In January 2016, Biden affirmed that not running was the right decision, but admitted to regretting not running for president "every day."As of the end of January 2016, neither Biden nor President Barack Obama had endorsed any candidate in the 2016 presidential election. Biden did miss his annual Thanksgiving tradition of going to Nantucket, opting instead to travel abroad and meet with several European leaders. He took time to meet with Martin O'Malley, having previously met with Bernie Sanders, both 2016 candidates. Neither of these meetings was considered an endorsement, as Biden had said that he would meet with any candidate who asked.After Obama endorsed Hillary Clinton on June 9, 2016, Biden endorsed her later the same day. Though Biden and Clinton were scheduled to campaign together in Scranton on July 8, the appearance was canceled by Clinton in light of the shooting of Dallas police officers the previous day.Following his endorsement of Clinton, Biden publicly displayed his disagreements with the policies of Republican presidential nominee Donald Trump. On June 20, Biden critiqued Trump's proposal to temporarily ban Muslims from entering the country as well as his stated intent to build a wall between the United States and Mexico border, furthering that Trump's suggestion to either torture and or kill family members of terrorists was both damaging to American values and "deeply damaging to our security". During an interview with George Stephanopoulos at the 2016 Democratic National Convention on July 26, Biden asserted that "moral and centered" voters would not vote for Trump. On October 21, the anniversary of his choice to not run, Biden said he wished he was still in high school so he could take Trump "behind the gym". On October 24, Biden clarified he would have fought Trump only if he was still in high school, and the following day, October 25, Trump responded that he would "love that".== Post–vice presidency (2017–present) ==In 2017, Biden was named the Benjamin Franklin Presidential Practice professor at the University of Pennsylvania, where he intended to focus on foreign policy, diplomacy, and national security while leading the Penn Biden Center for Diplomacy and Global Engagement. He also wanted to pursue his "cancer moonshot" agenda, calling the fight against cancer "the only bipartisan thing left in America" in March 2017.Biden had been close friends with Sen. John McCain for over 30 years. In 2018, Sen. McCain died at the age of 81 after dealing with the same cancer that Joe Biden's late son Beau Biden died of. Biden gave the eulogy at McCain's funeral service in Phoenix, Arizona. He opened with "My name's Joe Biden. I'm a Democrat. And I loved John McCain.", he also called him a "brother". Biden also served as a pallbearer at Sen. McCain's memorial service at the Washington National Cathedral alongside Warren Beatty, and Michael Bloomberg.=== Comments on President Trump ===While attending the launch of the Penn Biden Center for Diplomacy and Global Engagement on March 30, 2017, a student asked Biden what "piece of advice" he would give to President Trump. Biden responded that the President should grow up and cease his tweeting so he could focus on the office. During a speech at a May 29, 2017 gathering of Philip D. Murphy supporters at a community center gymnasium, Biden said, "There are a lot of people out there who are frightened. Trump played on their fears. What we haven't done, in my view—and this is a criticism of all us—we haven't spoken enough to the fears and aspirations of the people we come from." On June 17, 2017, Biden predicted the "state the nation is today will not be sustained by the American people" while speaking at a Florida Democratic Party fundraiser in Hollywood. Biden told CBS This Morning that Trump's administration "seems to feel the need to coddle autocrats and dictators" like Saudi Arabian leaders, Russian president Putin, North Korean leader Kim Jong-un or Philippine president Rodrigo Duterte. In October 2018, Biden said if Democrats retake the House of Representatives, "I hope they don't [impeach Trump]. I don't think there's a basis for doing that right now." On June 11, 2019, Biden criticized Trump's "damaging" trade war with China. Biden criticized Trump's decision to withdraw U.S. troops from Syria, which critics say gave Turkey the green light to launch the military offensive against Syrian Kurds.==== Climate change ====During an appearance at the Brainstorm Health Conference in San Diego, California on May 2, 2017, Biden said the public "has moved ahead of the administration [on science]". On May 31, Biden tweeted that climate change was an "existential threat to our future" and remaining in the Paris Agreement was the "best way to protect our children and global leadership." The following day, after President Trump announced his withdrawal of the US from the Paris Agreement, Biden tweeted that the choice "imperils US security and our ability to own the clean energy future." While appearing at the Concordia Europe Summit in Athens, Greece on June 7, Biden said, referring to the Paris Agreement, "The vast majority of the American people do not agree with the decision the president made."==== Healthcare ====On March 22, 2017, Biden referred to the Republican healthcare bill as a "tax bill" meant to transfer nearly US$1 trillion used for health benefits for the lower classes to wealthy Americans during his first appearance on Capitol Hill since Trump's inauguration.On May 4, after the House of Representatives narrowly voted for the American Health Care Act, Biden tweeted that it was a "Day of shame for Congress", lamenting the loss of pre-existing condition protections. On June 24, in response to Senate Republicans revealing an American Health Care Act draft the previous day, Biden tweeted that the bill "isn't about health care at all—it's a wealth transfer: slashes care to fund tax cuts for the wealthy & corporations". On July 28, in response to the Republican Senate healthcare bill falling through, Biden tweeted, "Thank you to everyone who tirelessly worked to protect the healthcare of millions."==== Immigration ====On September 5, 2017, after Attorney General Jeff Sessions announced that the Trump Administration is rescinding the Deferred Action for Childhood Arrivals, Biden tweeted, "Brought by parents, these children had no choice in coming here. Now they'll be sent to countries they've never known. Cruel. Not America."==== LGBT rights ====On April 14, 2017, Biden released a statement denouncing the authorities in Chechnya for their rounding up, torturing, and murdering of "individuals who are believed to be gay", also stating his hope that the Trump administration honor a prior pledge to advance human rights by confronting Chechen leader Ramzan Kadyrov and Russian leaders over "these egregious violations of human rights". On June 21, during a speech at a Democratic National Committee LGBT gala in New York City, Biden said, "Hold President Trump accountable for his pledge to be your friend."On July 26, 2017, after Trump announced a ban of transgender people serving in the military, Biden tweeted, "Every patriotic American who is qualified to serve in our military should be able to serve. Full stop."In March 2019, Biden condemned Brunei's new LGBT death penalty law, tweeting: "Stoning people to death for homosexuality or adultery is appalling and immoral. There is no excuse – not culture, not tradition – for this kind of hate and inhumanity." Biden suggested that the Trump administration's hostility to the rights of LGBT people was sending a poor example to countries like Brunei.=== Allegations of inappropriate physical contact ===There have been multiple photographs and videos of Biden engaged in what commentators considered to be inappropriate proximity to women and children, including kissing and or touching. Biden has described himself as a "tactile politician" and admitted that this behavior has caused trouble for him in the past. An image of Biden in close proximity to Stephanie Carter during her husband's swearing in as Secretary of Defense in 2015 resulted in a mocking epithet that was widely repeated. Carter defended Biden's depicted behavior in a 2019 interview.In March 2019, former Nevada assemblywoman Lucy Flores alleged that Biden kissed her without her consent at a 2014 campaign rally in Las Vegas. In a New York magazine op-ed for The Cut, Flores wrote that Biden had walked up behind her, put his hands on her shoulders, smelled her hair, and kissed the back of her head. Adding that the way he touched her was "an intimate way reserved for close friends, family, or romantic partners – and I felt powerless to do anything about it." In an interview with HuffPost, Flores stated she believed Biden's behavior to be disqualifying for a 2020 presidential run. Biden's spokesman stated that Biden did not recall the behavior described. Two days after Flores, Amy Lappos, a former congressional aide to Jim Himes, said Biden crossed a line of decency and respect when he touched her in a non-sexual, but inappropriate way by holding her head to rub noses with her at a political fundraiser in Greenwich in 2009. The next day, two additional women came forward with allegations of inappropriate conduct. One woman said that Biden placed his hand on her thigh, and the other said he ran his hand from her shoulder down her back.By early April 2019, a total of seven women had made allegations of inappropriate physical contact regarding Biden. At a conference on April 5, Biden apologized for not understanding how individuals would react to his actions, but stated that his intentions were honorable; he went on to say that he was not sorry for anything that he had ever done, which led critics to accuse him of sending a mixed message. He also proclaimed—with each public embrace he gave during the event—that he had received permission for it. Some critics interpreted this as Biden jokingly deflecting criticism, while other observers considered his change in tone responsive to the criticisms received.=== 2020 presidential campaign ===During a tour of the U.S. Senate with reporters before leaving office, on December 5, 2016, Biden refused to rule out a bid for the presidency in the 2020 presidential election, after leaving office as vice president. If he were to run in 2020, Biden would be 77 years old on election day and 78 on inauguration day in 2021. He reasserted his ambivalence about running on an appearance of The Late Show with Stephen Colbert on December 7, in which he stated "never say never" about running for president in 2020, while also acknowledging that he did not see a scenario in which he would run for office again. He seemingly announced on January 13, 2017, exactly one week prior to the expiration of his vice presidential term, that he would not run. He then appeared to backtrack four days later, on January 17, stating "I'll run if I can walk." A political action committee known as Time for Biden was formed in January 2018, seeking Biden's entry into the race.Between 2016 and 2019, Biden was mentioned by various media outlets as a potential candidate. He told a forum held in Bogota, Colombia, on July 17, 2018, that he would decide whether or not to formally declare as a candidate by January 2019. On February 4, 2019, with no decision having been forthcoming from Biden, Edward-Isaac Dovere of The Atlantic wrote that Biden was "very close to saying yes" but that some close to him are worried he would have a last-minute change of heart, as he did in 2016. Dovere reported that Biden was concerned about the effect another presidential run could have on his family and reputation, as well as fundraising struggles and perceptions about his age and relative centrism, compared to other declared and potential candidates. Conversely, his "sense of duty," offense at the Trump presidency, the lack of foreign policy experience amongst other Democratic hopefuls and his desire to foster "bridge-building progressivism" in the Party, were said to be factors prompting him to run. In March 2019, he indicated he may run, and ultimately launched his campaign on April 25, 2019. In May 2019, Biden chose Philadelphia, Pennsylvania, to be his 2020 U.S. presidential campaign headquarters.While at a fundraiser on June 18, 2019, Biden said that one of his greatest strengths was "bringing people together" and pointed to his relationships with Senators James Eastland and Herman Talmadge, two segregationists as examples. While imitating a Southern drawl, Biden remarked "I was in a caucus with James O. Eastland. He never called me 'boy,' he always called me 'son'." New Jersey Senator Cory Booker was one of many Democrats to criticize Biden for the remarks, issuing a statement that said "You don't joke about calling black men 'boys.' Men like James O. Eastland used words like that, and the racist policies that accompanied them, to perpetuate white supremacy and strip black Americans of our very humanity." In response, Biden said that he was not meaning to use the term "boy" in its derogatory racial context.During the first Democratic presidential debate, Kamala Harris criticized Biden for his comments regarding his past work with segregationist senators and his past opposition to desegregation busing that allowed black children like her to attend integrated schools. Biden was widely criticized for his debate performance and support for him dropped 10 points. President Trump defended Biden, saying Harris was given "too much credit" for her debate with Biden.Biden's history of "verbal flubs" have been suggested as possibly being linked to his age. In 2018, Biden said he was "a gaffe machine" but disagreed with comparisons to Trump, saying his gaffe-prone nature was "a wonderful thing compared to a guy who can't tell the truth". On August 26, 2019 Biden subtly addressed the controversy. Following a minor mishap in which he stumbled over where he had spoken earlier that day at Dartmouth College, he directly looked at the assembled press and said that "I want to be clear: I'm not going nuts."On July 15, 2019, the non-profit Biden Cancer Initiative announced the foundation was ceasing operations for the foreseeable future. Biden and his wife left the initiative's board in April as an ethics precaution before starting his 2020 presidential campaign.In September 2019, it was reported that President Trump had been pressuring Ukrainian President Volodymyr Zelensky to investigate alleged wrongdoing by Biden and his son Hunter Biden in correlation with a whistleblowing report regarding a phone call between Trump and Zelensky on July 25, 2019. Despite the allegations, as of September 2019, there has been no evidence produced of any wrongdoing by the Bidens. This pressure to investigate the Bidens was widely interpreted by the media to be an attempt to hurt Biden's chances of winning the presidency, resulting in a major political scandal and an impeachment inquiry against Trump.== Political positions ==Biden has been characterized as a moderate Democrat. He has supported deficit spending for fiscal stimulus in the American Recovery and Reinvestment Act of 2009; the increased infrastructure spending proposed by the Obama administration; mass transit, including Amtrak, bus, and subway subsidies; same-sex marriage; and the reduced military spending proposed in the Obama Administration's fiscal year 2014 budget.A method that political scientists use for gauging ideology is to compare the annual ratings by the Americans for Democratic Action (ADA) with the ratings by the American Conservative Union (ACU). Biden has a lifetime liberal 72 percent score from the ADA through 2004, while the ACU awarded Biden a lifetime conservative rating of 13 percent through 2008. Using another metric, Biden has a lifetime average liberal score of 77.5 percent, according to a National Journal analysis that places him ideologically among the center of Senate Democrats as of 2008. The Almanac of American Politics rates congressional votes as liberal or conservative on the political spectrum, in three policy areas: economic, social, and foreign. For 2005–2006, Biden's average ratings were as follows: the economic rating was 80 percent liberal and 13 percent conservative, the social rating was 78 percent liberal and 18 percent conservative, and the foreign rating was 71 percent liberal and 25 percent conservative. This has not changed much over time; his liberal ratings in the mid-1980s were also in the 70–80 percent range.Various advocacy groups have given Biden scores or grades as to how well his votes align with the positions of each group. The American Civil Liberties Union gives him an 80 percent lifetime score, with a 91 percent score for the 110th Congress. Biden opposes drilling for oil in the Arctic National Wildlife Refuge and supports governmental funding to find new energy sources. Biden believes action must be taken on global warming. He co-sponsored the Sense of the Senate resolution calling on the United States to be a part of the United Nations climate negotiations and the Boxer–Sanders Global Warming Pollution Reduction Act, the most stringent climate bill in the United States Senate. Biden was given an 85 percent lifetime approval rating from the AFL–CIO, and he voted for the North American Free Trade Agreement (NAFTA).== Distinctions ==Biden has received honorary degrees from the University of Scranton (1976), Saint Joseph's University (LL.D 1981), Widener University School of Law (2000), Emerson College (2003), his alma mater the University of Delaware (2004), Suffolk University Law School (2005), and his other alma mater Syracuse University (LL.D 2009)  University of Pennsylvania (LL.D 2013)  Miami Dade College (2014)  Trinity College, Dublin (LL.D 2016)  Colby College (LL.D 2017)  Morgan State University (DPS 2017)  University of South Carolina (DPA 2017) Biden also received the Chancellor Medal (1980) and the George Arents Pioneer Medal (2005) from his alma mater, Syracuse University.In 2008, Biden received Working Mother magazine's Best of Congress Award for "improving the American quality of life through family-friendly work policies." Also in 2008, Biden shared with fellow Senator Richard Lugar the Hilal-i-Pakistan award from the Government of Pakistan "in recognition of their consistent support for Pakistan". In 2009, Biden received the Golden Medal of Freedom award from Kosovo, that region's highest award, for his vocal support for their independence in the late 1990s.Biden is an inductee of the Delaware Volunteer Firemen's Association Hall of Fame. He was named to the Little League Hall of Excellence in 2009.On June 25, 2016, Joe Biden received the freedom of County Louth in the Republic of Ireland.On January 12, 2017, Obama surprised Biden by awarding him the Presidential Medal of Freedom with Distinction during a farewell press conference at the White House honoring Biden and his wife. Obama said he was awarding the Medal of Freedom to Biden for "faith in your fellow Americans, for your love of country and a lifetime of service that will endure through the generations". It was the first and only time Obama awarded the Medal of Freedom with the additional honor of distinction, an honor which his three predecessors had reserved for only President Ronald Reagan, Colin Powell and Pope John Paul II, respectively.On December 11, 2018, the University of Delaware renamed their School of Public Policy and Administration after Biden, naming it the Joseph R. Biden, Jr. School of Public Policy and Administration, which also houses the Biden Institute.== Electoral history ==== Writings by Biden ==== See also ==Trump–Ukraine scandal== Notes ==== References ===== Footnotes ====== Books ===== External links ==Joe Biden President campaign websiteArchive of Obama White House official biographyAppearances on C-SPANJoe Biden at CurlieSenate campaign website (archived)Biography at the Biographical Directory of the United States CongressFinancial information (federal office) at the Federal Election CommissionJoe Biden on IMDbIn the Words of Joe Biden—slideshow by Life magazine
	Faceware Technologies is an American company that designs facial animation and motion capture technology. The company was established under Image Metrics and became its own company at the beginning of 2012.Faceware produces software used to capture an actor's performance and transfer it onto an animated character, as well as hardware needed to capture the performances. The software line includes Faceware Analyzer, Faceware Retargeter, and Faceware Live.Faceware software is used by film studios and video game developers including Rockstar Games, Bungie, Cloud Imperium Games, and 2K in games such as Grand Theft Auto V, Destiny, Star Citizen, and Halo: Reach.Through its application in the video game industry, Faceware won the Develop Award while it was still part of Image Metrics for Technical Innovation in 2008. It won the Develop Award again for Creative Contribution: Visuals in 2014. Faceware received Best of Show recognition at the Game Developers Conference 2011 in San Francisco as well as Computer Graphics World's Silver Edge Award at SIGGRAPH 2014 and 2016. Finally, Faceware won the XDS Gary Award in 2016 for its contributions to the Faceware-EA presentation at the 2016 XDS Summit.== History ==Image Metrics, founded in 2000, is a provider of facial animation and motion capture technology within the video game and entertainment industries. In 2008, Image Metrics offered a beta version of its facial animation technology to visual effects and film studios. The technology captured an actor's performance on video, analyzed it, and mapped it onto a CG model. The release of the beta allowed studios to incorporate the facial animation technology into internal pipelines rather than going to the Image Metrics studio as they had in the past. The first studio to beta test Image Metric's software in 2009 was the visual effects studio Double Negative out of London.In 2010, Image Metrics launched the facial animation technology platform Faceware. Faceware focused on increasing creative control, efficiency and production speed for animators. The software could be integrated into any pipeline or used with any game engine. Image Metrics provided training to learn the Faceware platform. The first studio to sign on as a Faceware customer was Bungie, which incorporated the software into its in-house production. Image Metrics acquired FacePro in 2010, a company that provided automated lip synchronization which could be altered for accurate results, and Image Metrics integrated the acquired technology into its facial animation software. Also in 2010, Image Metrics bought Character-FX, a character animation company. Character-FX produced tools for use in Autodesk’s Maya and 3DS Max which aide in the creation of character facial rigs using an automated weighting transfer system that rapidly shifts facial features on a character to create lifelike movement.Image Metrics raised $8 million in funding and went public through a reverse merger in 2010 with International Cellular Industries. Image Metrics became wholly owned by International Cellular industries, which changed its name and took on facial animation technology as its sole line of business. Faceware 3.0 was announced in March 2011. The upgrade included auto-pose, a shared pose database, and curve refinement. Image Metrics led a workshop and presentation about Faceware 3.0 at the CTN Animation Expo 2011 titled "Faceware: Creating an Immersive Experience through Facial Animation." Faceware's technology was displayed at Edinburgh Interactive in August 2011 to show its ability to add player facial animation from webcam or Kinect sensor into a game in real time.Image Metrics sold the Faceware software to its spinoff company, Faceware Technologies, in January 2012. Following the spinoff, Faceware Technologies focused on producing and distributing its technology to professional animators. The technology was tested through Universities, including the University of Portsmouth.Faceware launched its 3D facial animation tools, software packages Faceware Analyzer and Faceware Retargeter with the Head-Mounted Camera System (HMCS). Analyzer tracks and processes live footage of an actor and Retargeter transfers that movement onto the face of a computer-generated character. The Head-Mounted Camera System is not required to use the software. Six actors can be captured simultaneously.Faceware Live was shown for the first time at SIGGRAPH 2013. It was created to enable the real-time capture and retargeting of facial movements. The live capture of facial performance can use any video source to track and translate facial expressions into a set of animation values and transfer the captured data onto a 3D animated character in real time. In 2014, Faceware released Faceware Live 2.0. The update included the option to stream multiple characters simultaneously, instant calibration, improved facial tracking, consistent calibration, and support for high-frame-rate cameras.In 2015, Faceware launched a plugin for Unreal Engine 4 called Faceware Live. The company co-developed the plugin with Australia-based Opaque Multimedia. It makes motion capture of  expressions and other facial movements possible with any video camera through Faceware's markerless 3D facial motion capture software.In 2016, Faceware announced the launch of Faceware Interactive, which is focused on the development of software and hardware that can be used in the creation of digital characters with whom real people can interact.== Partners ==Faceware Technologies partnered with Binari Sonori in 2014 to develop a video-based localization service. Also in 2014, Faceware Technologies entered a global partnership with Vicon, a company focused on motion capture. The partnership would focus on developing new technology to expand into full-body motion capture data. The first step of the integration was to make the Faceware software compatible with Vicon's head rig, Cara, to allow data acquired from Cara to be processed and transferred into Faceware products.== Overview ==Faceware Technologies has two main aspects of facial animation software.Faceware Analyzer is a stand-alone single-camera facial tracking software that converts videos of facial motion into files that can be used for Faceware Retargeter. The Lite version of the software can automatically track facial movements which can be applied to 3D models with Faceware Retargeter. The Pro version can perform shot specific custom calibrations, import and export actor data, auto indicate tracking regions, and has server and local licensing options. The data captured by Faceware Analyzer is then processed in Faceware Retargeter.Faceware Retargeter 4.0 was announced in 2014. Faceware Retargeter uses facial tracking data created in Analyzer to create facial animation in a pose-based workflow. The upgrade has a plug-in for Autodesk animation tools, advanced character expression sets, visual tracking data, shared pose thumbnails, and batch processing. The Lite version of the Retargeter software transfers actor's performances onto animated characters and reduces and smooths key frames. The Pro version includes custom poses, intelligent pose suggestions, shared pose libraries, and the ability to backup and restore jobs.Faceware Live aims to create natural looking faces and facial expressions in real-time. Any video source can be used with the software's one-button calibration. The captured video is transferred onto a 3D animated character. This process combines image processing and data streaming to translate facial expressions into a set of animation values.Faceware has hardware options that can be rented or purchased. Available hardware is the entry level GoPro Headcam Kit and the Professional Headcam System. The Indie Facial Mo-cap package includes hardware, a camera and headmount, and the tools to use it.== Selected works ==Faceware software is used by companies such as Activision-Blizzard, Bethesda, Ubisoft, Electronic Arts, Sony, Cloud Imperium Games, and Microsoft. Rockstar Games used the software in games such as Grand Theft Auto V and Red Dead Redemption and Bungie used Faceware in games including Destiny and Halo: Reach. Faceware has also been used in other games like XCOM2, Dying Light: The Following, Hitman, EA Sports UFC 2, Fragments for Microsoft's HoloLens, DOOM, Mirror's Edge Catalyst, Kingsglaive, F1 2016, ReCore, Destiny: Rise of Iron, Mafia III, Call of Duty Infinite Warfare, Killzone:Shadow Fall, NBA 2K10-2K17, Sleeping Dogs, Crysis 2 and 3, Star Citizen, and in movies like The Curious Case of Benjamin Button and Robert Zemeckis's The Walk.== References ==== External links ==Official site
	Menthor Editor is a free ontology engineering tool for dealing with OntoUML models. It also includes OntoUML syntax validation, Alloy simulation, Anti-Pattern verification, and MDA transformations from OntoUML to OWL, SBVR and Natural Language (Brazilian Portuguese).Menthor Editor emerged from OLED. OLED was developed at the Ontology & Conceptual Modeling Research Group (NEMO) located at the Federal University of Espírito Santo (UFES) in Vitória city, state of Espírito Santo, BrazilMenthor Editor is being developed by Menthor using Java. Menthor Editor is available in English and it is a multiplaform software, i.e., it is compatible with Windows, Linux and OS X.== References ==== External links ==Official website
	Kiwix is a free and open-source offline web browser created by Emmanuel Engelhart and Renaud Gaudin in 2007. It was first launched to allow offline access to Wikipedia, but has since expanded to include other projects from the Wikimedia Foundation as well as public domain texts from Project Gutenberg. Available in more than 100 languages, Kiwix has been included in several high-profile projects, from smuggling operations in North Korea and encyclopedic access in Cuba to Google Impact Challenge's recipient Bibliothèques Sans Frontières.== History ==Founder Emmanuel Engelhart sees Wikipedia as a common good, saying "The contents of Wikipedia should be available for everyone! Even without Internet access. This is why I have launched the Kiwix project."After becoming a Wikipedia editor in 2004, Emmanuel Engelhart became interested in developing offline versions of Wikipedia. A project to make a Wikipedia CD, initiated in 2003, was a trigger for the project.In 2012 Kiwix won a grant from Wikimedia France to build kiwix-plug, which was deployed to universities in eleven countries known as the Afripedia Project. In February 2013 Kiwix won SourceForge's Project of the Month award and an Open Source Award in 2015.== Description ==The software is designed as an offline reader for web content. It can be used on computers without an internet connection, computers with a slow or expensive connection, or to avoid censorship. It can also be used while traveling (e.g. on a plane or train).Users first download Kiwix, then download content for offline viewing with Kiwix. Compression saves disk space and bandwidth. All of English-language Wikipedia, with pictures, fits on a USB stick (54 GB as of May 2016, or 16 GB with no pictures).All content files are compressed in ZIM format, which makes them smaller, but leaves them easy to index, search, and selectively decompress.The ZIM files are then opened with Kiwix, which looks and behaves like a web browser. Kiwix offers full text search, tabbed navigation, and the option to export articles to PDF and HTML.There is an HTTP server version called kiwix-serve; this allows a computer to host Kiwix content, and make it available to other computers on a network. The other computers see an ordinary website. Kiwix-plug is an HTTP server version for plug computers, which is often used to provide a Wi-Fi server.Kiwix uses the deprecated XULRunner Mozilla application framework localised on Translatewiki.net, but plans to replace it.== Available content ==A list of content available on Kiwix is available for download, including language-specific sublists. Content can be loaded through Kiwix itself.Since 2014, most Wikipedia versions are available for download in various different languages. For English Wikipedia, a full version containing pictures as well as an alternative version containing text only can be downloaded from the archive. The servers are updated every two to ten months, depending on the size of the file. For English Wikipedia, the update frequency is thus substantially lower than the bzip2 database downloads by the Wikimedia Foundation, which are updated twice a month.Besides Wikipedia, content from the Wikimedia foundation such as Wikisource, Wikiquote, Wikivoyage, Wikibooks, and Wikiversity are also available for offline viewing in various different languages.In November 2014 a ZIM version of all open texts forming part of Project Gutenberg was made available.Besides public domain content, works licensed under a Creative Commons license are available for download as well. For example, offline versions of the Ubuntu wiki containing user documentation for the Ubuntu operating system, ZIM editions of TED conference talks and videos from Crash Course are available in the Kiwix archive as ZIM file formats.== Deployments ==Kiwix can be installed on a desktop computer as a stand-alone program, installed on a tablet or smartphone, or can create its own WLAN environment from a Raspberry plug.As a software development project, Kiwix itself is not directly involved in deployment projects. However, third party organisations do use the software as a component of their own projects. Examples include:Universities and libraries that can't afford broadband Internet access.The Afripedia Project set up kiwix servers in French-speaking universities, some of them with no Internet access, in 11 African countries.Schools in developing countries, where access to the internet is difficult or too expensive.Installed on computers used for the One Laptop per Child project.Installed on Raspberry Pis for use in schools with no electricity in Tanzania by the Tanzania Development Trust.Installed on tablets in schools in Mali as part of the MALebooks project.Used by school teachers and university professors, as well as students, in Senegal.Deployed in Benin during teacher training seminars run by Zedaga, a Swiss NGO specialized in education.The Fondation Orange has used kiwix-serve in its own French language technological knowledge product they have deployed in Africa.A special version for the organisation SOS Children's Villages was developed, initially for developing countries, but it is also used in the developed world.At sea and in other remote areas:Aboard ships in Antarctic waters.By the Senegalese Navy in their patrol ships.Included in Navigatrix, a Linux distribution for people on boats.On a train or plane.In European and US prison education programs.=== Package managers and app stores ===Kiwix was formerly available in the native package managers of some Linux distributions. However, Kiwix is currently not available in most package databases, due to XULRunner, a program on which Kiwix depends, being deprecated by Mozilla and removed from the package databases. Kiwix is available in the Sugar and ArchLinux Linux distributions. It is also available on Android.Kiwix is available in the Microsoft Store, on Google Play, and Apple's iOS App Store. Since 2015, a series of "customized apps" have also been released, of which Medical Wikipedia and PhET simulations are the two largest.== See also ==Internet-in-a-Box== References ==== External links ==Official websiteKiwix on SourceForge.netExplanation of Kiwix from Wikimania 2013Kiwix stories on the Wikimedia Blog
	In the context of software quality, defect criticality is a measure of the impact of a software defect. It is defined as the product of severity, likelihood, and class.Defects are different from user stories, and therefore the priority (severity) should be calculated as follows.== Severity/Impact ==0 - Affects critical data or functionality and leaves users with no workaround1 - Affects critical data or functionality and forces users to employ a workaround2 - Affects non-critical data or functionality and forces users to employ a workaround3 - Affects non-critical data or functionality and does not force users to employ a workaround4 - Affects aesthetics, professional look and feel, “quality” or “usability”== Likelihood/Visibility ==1 - Seen by all or almost all users who use the application (>=95% of users)2 - Seen by more than 2/3 of the users who use the application (>67% and <95%)3 - Seen by about half the users who use the application (>33% and <66%)4 - Seen by about 1/3 or less of the users who use the application (>0% and <32%)== Class of defect ===== Class 0 ===Stability, Reliability and AvailabilitySecurityLegal (Liability, ADA, Copyright)TestabilityStorage (data loss/corruption)=== Class 1 ===Performance and Efficiency (use of resources: memory, disk, CPU)Scalability=== Class 2 ===FunctionalityLogic or CalculationCompatibilityInteroperability=== Class 3 ===UsabilityLearn abilityReadabilityDocumentationConsistencyWorkflow (“feel”)=== Class 4 ===Typographic or grammaticalAestheticsAppearance or Cosmetic== Assessing the criticality score ==0-2 = Critical3-9 = Major10-20 = Medium21-64 = Low== References ==
	Enterprise legal management (ELM) is a practice management strategy of corporate legal departments, insurance claims departments, and government legal and contract management departments.ELM developed during the 1990s in response to increased corporate demands for accountability, transparency, and predictability, and employs software to manage internal legal documents and workflows, electronic billing and invoicing, and to guide decision-making through reporting and analytics.== Definitions ==Still an evolving term, ELM is a recognized management discipline and a strategic objective of general counsel.  Some have argued that ELM falls within the broader category of corporate governance, risk, and compliance (GRC); others maintain that ELM and GRC are separate entities along a continuum.Separate but related technologies include information governance, electronic discovery, legal hold, contract management, corporate secretary, and board of directors’ communications.  ELM software may integrate some or all of these components.== Historical Development ===== Early Practice Management ===Law practice management refers to the business aspect of operating a law firm or in-house legal team. Components include economics, workplace communication and management, ethics, and client service.Historically, corporate legal spend was considered a “black box” with limited predictability and transparency, making it difficult for corporate legal teams to parse differences of efficiency and cost among outside firms, or to benchmark firm performance against previously hired counsel.=== Transition and Early Adoption ===Several factors led to a shift away from traditional, low-technology solutions and toward ELM, most notably the expansion of the Internet during the 1990s and subsequent development of Software as a service (SaaS) platforms. Within legal departments, factors included greater regulatory compliance risk, smaller budgets, and board member demands for greater accountability, predictability, and transparency.Over time, the demand for budgetary information, including metrics such as the ratio of legal spend to total enterprise revenue, extended beyond board members to include other stakeholders. Corporate legal departments were positioned as the next frontier of corporate efficiency and risk management, and encouraged to operate as a true business partner. This created pressure to reduce costs and, when possible, generate revenue for the larger enterprise.Departments’ varied sizes and responsibilities created a range of practice management needs. A legal department with a small number of in-house attorneys might oversee thousands of cases managed by outside counsel, while a large internal legal department could handle most cases in-house. Smaller departments focused on management of workflow, collaboration, and spend management; larger departments had greater needs for internal matter management, attorney utilization, and document management.The first electronic transitions were to generic matter management applications, which replaced paper files as the system of record.=== Specialization and Expansion ===Initial enterprise resource planning systems did not meet the specific needs of legal departments. External legal costs presented unique management challenges because of the billable-hour model and unpredictable labor requirements. Software specialization integrated matter management and preexisting, internal billing software. This integration provided the opportunity to meet management demands for increased communication and information from within corporate legal departments. Ultimately, it extended further to combine legal information with finance, compliance, and risk-management departments.These developments also presented risks to enterprise-wide information management. Specialized software generated the potential for conflicts between company‐wide enterprise solutions and the ELM systems of in-house legal departments.  Additionally, maintaining data security was and remains a preeminent concern, especially for SaaS-based ELM platforms. One in four chief legal officers (CLOs) reported a data breach during 2013–15, with the health-care industry especially vulnerable.== ELM Software ==The expansion of ELM as a practice management strategy was fostered by the growth of ELM software. That growth, in turn, was made possible through the expansion of the Internet and adoption of SaaS platforms across a wide range of industries. Total revenue of all SaaS providers accelerated into the 2010s, with the International Data Corporation (IDC) forecasting revenue to grow from $22.6 billion to $50.8 billion during 2014–18.ELM software primarily supports matter management and electronic billing, with derived analytics and reporting guiding legal department business processes. As of 2013, the maturity level of the industry was characterized as early mainstream, with market penetration of less than 20%.A Blue Hill Research report stated that economic motivations have encouraged adoption, with an average return on investment of 766%; median spend contraction of 4.5% from automated processing and rejection of nonconforming invoices; and median recurring annual spend reduction of 4% from use of analytics to support data-driven spend management.Variables affecting the purchase of ELM software include considerations of license type, usage scope, maintenance and support, installation location, and license fee calculation. Vendors employ a range of licensing practices, with no model inherently advantaged or disadvantaged.=== Components of ELM Software ======= Matter Management ====Matter management includes the storage and retrieval of all data related to matters handled by a legal department, including the creation, revision, approval, and consumption of legal documents. Matter management is used to facilitate document collaboration internally and with outside counsel.  In complex legal matters such as mass tort litigation, ELM software provides matter management capabilities such as batch uploading of invoices to expedite review and approval.==== Electronic Billing ====Electronic billing provides a centralized repository for legal bills and invoices, and a method to deliver those bills securely for review and payment. ELM software integrates with internal electronic billing software through the Legal Electronic Data Exchange Standard (LEDES) format, which has standardized the transfer of legal data.  Development of LEDES in the late 1990s was supplemented by the American Bar Association’s creation of Uniform Task-Based Management System (UTBMS) to establish consistent coding of services by outside counsel.Electronic billing automates review for compliance errors, allocation to cost centers, and routing for approval. Independent research suggests that it reduces costs by decreasing manual labor and paper costs.==== Analytics and Business Process Management ====Matter management and electronic billing data collected by ELM software is used to generate reports and provide analytics that influence business process management within legal departments.According to a Gartner survey, CLOs increasingly focus on regulatory compliance, customer and stakeholder satisfaction, and risk management. Efforts to reduce legal spend center on the reduction of outside counsel costs, achieved through the negotiation of alternative fee arrangements, increased reliance on internal counsel, and convergence of outside counsel. Use of flat fees for entire matters grew from 12 to 20% during 2013–15, with larger legal departments—those serving companies with at least $4 billion in annual revenue—more than twice as likely to use a flat fee structure compared to companies with less than $100 million in annual revenue.Legal departments use analytics to inform these budgeting and forecasting decisions, with the selection of outside law firms based on tradeoffs between cost and attorney performance. Internal historical billing data and industry benchmarks identify trends and differences among providers, and average fees associated with matter types. Some ELM software vendors offer comparative metrics harvested from subscribers.== Vendor Landscape ==Vendors come from one of two backgrounds: content provision or software development. Some vendors specialize in ELM software, while others include ELM software as part of a broader suite of legal software. SaaS-based and on-premises solutions exist, although many on-premises providers have shifted to SaaS-based models or added SaaS-based options.Multiple acquisitions occurred within the industry during 2011–15. In 2011 Bottomline Technologies acquired Allegiant Systems; in 2014 Wolters Kluwer, owner of TyMetrix, acquired competitor Datacert; and in 2015 Mitratech purchased Houston-based Bridgeway Software and EAG's CaseTrack.=== SaaS-based ===Acuity ELMAdvologixPMBottomline Legal eXchangeBusyLampBrightflag CliodoeLEGALLaboralisto(Labor Litigation Analytics Platform for Latin America starting with Mexico)Legal Cost Control ("LCC") Simple Invoice Management System ("SIMS")Legal SuiteLexisNexis CounselLinkLSG Advocator SystemMitratechOnit ELMPracticeLeague ELM eco-systemSerengeti LawSimpleLegalThomson Reuters Elite ProLawWolters Kluwer ELM SolutionsTeleforum FOR of Eustema SpA=== SaaS-based and On-premises Components/Options ===Computer Sciences Corporation Legal Solutions SuiteLaw Department Desktop ServicesLecorpio Legal Resource ManagerLegal Files SoftwareLegal SuiteTeleforum FOR of Eustema SpAPracticeLeague ELM eco-system=== On-premises ===Aderant ExpertCorporate Legal Solutions Case&PointPracticeLeague ELM eco-systemTeleforum FOR of Eustema SpA== See also ==Corporate LawElectronic billingLaw Practice ManagementLegal Matter ManagementLegal case management== References ==== External links =="The Business Value of Legal Spend Management Investments." Blue Hill Research. http://bluehillresearch.com/the-business-value-of-legal-spend-management-investments/"Enterprise Legal Management: E-Billing and Matter Management Systems." Hyperion. http://www.hgpresearch.com/research/marketview-enterprise-legal-mgmt-e-billing-and-matter-mgmt-systems/"Magic Quadrant for Enterprise Legal Management." Gartner. https://www.gartner.com/doc/2612217
	In-house software is a software that is produced by a corporate entity for purpose of using it within the organization. In-house software however may later become available for commercial use upon sole discretion of the developing organization.  The need to develop such software may arise depending on many circumstances which may be non-availability of the software in the market, potentiality or ability of the corporation to develop such software or to customize a software based on the corporate organization's need.
	Inverse search (also called "reverse search") is a feature of some non-interactive typesetting programs, such as LaTeX and GNU LilyPond. These programs read an abstract, textual, definition of a document as input, and convert this into a graphical format such as DVI or PDF. In a windowing system, this typically means that the source code is entered in one editor window, and the resulting output is viewed in a different output window. Inverse search means that a graphical object in the output window works as a hyperlink, which brings you back to the line and column in the editor, where the clicked object was defined. The inverse search feature is particularly useful during proofreading.== Implementations ==In TeX and LaTeX, the package srcltx provides an inverse search feature through DVI output files (e.g., with yap or Xdvi), while vpe, pdfsync and SyncTeX provide similar functionality for PDF output, among other techniques. The Comparison of TeX editors has a column on support of inverse search, most of them provide it nowadays.GNU LilyPond provides an inverse search feature through PDF output files, since version 2.6. The program calls this feature Point-and-click,Many integrated development environments for programming use inverse search to display compilation error messages, and during debugging when a breakpoint happens.== References ==== Bibliography ==Jérôme Laurens, ”Direct and reverse synchronization with SyncTeX”, in TUGboat 29(3), 2008, p365–371, PDF (532KB) — including an overview of synchronization techniques with TeX== External links ==How to set up inverse search with xdvi
	In the field of software development, the term build is similar to that of any other field. That is, the construction of something that has an observable and tangible result. Historically, build has often referred either to the process of converting source code files into standalone software artifact(s) that can be run on a computer, or the result of doing so. However, this is not the case with technologies such as Perl, Ruby or Python which are examples of interpreted languages.== Functions ==Building software is an end-to-end process that involves many distinct functions. Some of these functions are described below.=== Version control ===The version control function carries out activities such as workspace creation and updating, baselining and reporting. It creates an environment for the build process to run in and captures metadata about the inputs and outputs of the build process to ensure repeatability and reliability.Tools such as Git, AccuRev or StarTeam help with these tasks by offering tools to tag specific points in history as being important, and more.=== Code quality ===Also known as static program analysis/static code analysis this function is responsible for checking developers have adhered to the seven axes of code quality: comments, unit tests, duplication, complexity, coding rules, potential bugs and architecture & design.Ensuring a project has high-quality code results in fewer bugs and influences nonfunctional requirements such as maintainability, extensibility and readability, which have a direct impact on the ROI for a business.=== Compilation ===This is only a small feature of managing the build process. The compilation function turns source files into directly executable or intermediate objects. Not every project will require this function.While for simple programs the process consists of a single file being compiled, for complex software the source code may consist of many files and may be combined in different ways to produce many different versions.== Build tools ==The process of building a computer program is usually managed by a build tool, a program that coordinates and controls other programs. Examples of such a program are make, Gradle, Meister by OpenMake Software, Ant, Maven, Rake, SCons and Phing. The build utility typically needs to compile the various files, in the correct order. If the source code in a particular file has not changed then it may not need to be recompiled (may not rather than need not because it may itself depend on other files that have changed). Sophisticated build utilities and linkers attempt to refrain from recompiling code that does not need it, to shorten the time required to complete the build. A more complex process may involve other programs producing code or data as part of the build process.== See also ==Build automationList of build automation softwareSoftware versioning== References ==
	A binary repository manager is a software tool designed to optimize the download and storage of binary files used and produced in software development. It centralizes the management of all the binary artifacts generated and used by the organization to overcome the complexity arising from the diversity of binary artifact types, their position in the overall workflow and the dependencies between them.A binary repository is a software repository for packages, artifacts and their corresponding metadata. It can be used to store binary files produced by an organization itself, such as product releases and nightly product builds, or for third-party binaries which must be treated differently for both technical and legal reasons.== Introduction ==Software development can be a complex process involving many developers, or teams of developers working on shared code bases, accessing the same build tools, downloading and using a shared set of binary resources, and deploying components into the same software product. To manage the source files used in software development, organizations will typically use revision control. The many source files used in software development are eventually built into the binary artifacts (also known as “binaries”) which constitute the components of a software product. In addition, in order to provide their functionality and feature set, software products may use many 3rd party artifacts downloaded from free open source repositories or purchased from commercial sources. Consequently, a software product may comprise tens, hundreds and even thousands of individual binary artifacts which must be managed in order to efficiently maintain a coherent and functional software product. This function of managing the binary artifacts is done by a binary repository manager. A binary repository manager can be thought of as being to binaries what revision control is to source files.== Universal package manager ==Package managers aim to standardize the way enterprises treat all package types used in the software development process. They give users the ability to apply security and compliance metrics across all artifact types. Universal package managers have been referred to as being at the center of a DevOps toolchain.Notable package managers include:Apache ArchivaCloudRepoCloudsmithGithub Package Registry JFrog ArtifactoryInedo ProGetPackagecloudSonatype Nexus== Relationship to continuous integration ==As part of the development lifecycle, source code is continuously being built into binary artifacts using continuous integration. This may interact with a binary repository manager much like a developer would by getting artifacts from the repositories and pushing builds there. Tight integration with CI servers enables the storage of important metadata such as:Which user triggered the build (whether manually or by committing to revision control)Which modules were builtWhich sources were used (commit id, revision, branch)Dependencies usedEnvironment variablesPackages installed=== Artifacts and packages ===Artifacts and packages inherently mean different things. Artifacts are simply an output or collection of files (ex. JAR, WAR, DLLS, RPM etc.) and one of those files may contain metadata (e.g. POM file).  Whereas packages are a single archive file in a well-defined format (ex. NuGet) that contain files appropriate for the package type (ex. DLL, PDB). Many artifacts result from builds but other types are crucial as well. Packages are essentially one of two things: a library or an application.Compared to source files, binary artifacts are often larger by orders of magnitude, they are rarely deleted or overwritten (except for rare cases such as snapshots or nightly builds), and they are usually accompanied by lots of metadata such as id, package name, version, license and more.=== Metadata ===Metadata describes a binary artifact, is stored and specified separately from the artifact itself, and can have several additional uses. The following table shows some common metadata types and their uses:== Key features of repository managers ==Key factors and features when considering the adoption of a package manager include:Caching – Caching simply stores local copies of packages in a cache. This increases performance for slow internet connections by allowing the user to pull from the local repository instead of externally. Caching locally allows frequently used packages to be available even during times of external repository outages.Retention policies - Repository managers can be used and configured to support organization purging policies to ensure reasonable disk space usage. Local repositories for third party artifacts may also be purged after not being used by any release for a specified time.License filtering - Third party artifacts may be subject to approval processes due to licensing and legal issues. Package managers allow for the restriction of only approved artifacts into deployment.High availability - Since a binary repository manager maintains all the development dependencies, it is vital to always maintain access to these artifacts. Any down-time of the binary repository manager can halt development with all the significant consequences to the organization. A high availability instance allows an enterprise to overcome the risk associated with downtime, through automatic failover. This is achieved by having a redundant set of repository managers work against the same database and file storage, maintaining enterprise-wide stability and performance at all times.User restrictions - Repository managers can be integrated with other organizational systems such as LDAP or single sign-on servers to simplify and centralize user management. This gives an enterprise granular control over who has access to vital software components.== See also ==List of software package management systems== References ==
	Sound design is the art and practice of creating sound tracks for a variety of needs. It involves specifying, acquiring or creating auditory elements using audio production techniques and tools. It is employed in a variety of disciplines including filmmaking, television production, video game development, theatre, sound recording and reproduction, live performance, sound art, post-production, radio and musical instrument development. Sound design commonly involves performing (see e.g. foley) and editing of previously composed or recorded audio, such as sound effects and dialogue for the purposes of the medium, but it can also involve creating sounds from scratch through synthesizers. A sound designer is one who practices sound design.== History ==The use of sound to evoke emotion, reflect mood and underscore actions in plays and dances began in prehistoric times. At its earliest, it was used in religious practices for healing or recreation. In ancient Japan, theatrical events called kagura were performed in Shinto shrines with music and dance.Plays were performed in medieval times in a form of theatre called Commedia dell'arte, which used music and sound effects to enhance performances. The use of music and sound in the Elizabethan Theatre followed, in which music and sound effects were produced off stage using devices such as bells, whistles, and horns. Cues would be written in the script for music and sound effects to be played at the appropriate time.Italian composer Luigi Russolo built mechanical sound-making devices, called "intonarumori," for futurist theatrical and music performances starting around 1913. These devices were meant to simulate natural and man-made sounds, such as trains and bombs. Russolo's treatise, The Art of Noises, is one of the earliest written documents on the use of abstract noise in the theatre. After his death, his intonarumori' were used in more conventional theatre performances to create realistic sound effects.=== Recorded sound ===Possibly the first use of recorded sound in the theatre was a phonograph playing a baby's cry in a London theatre in 1890. Sixteen years later, Herbert Beerbohm Tree used recordings in his London production of Stephen Phillips’ tragedy NERO. The event is marked in the Theatre Magazine (1906) with two photographs; one showing a musician blowing a bugle into a large horn attached to a disc recorder, the other with an actor recording the agonizing shrieks and groans of the tortured martyrs. The article states: “these sounds are all realistically reproduced by the gramophone”. As cited by Bertolt Brecht, there was a play about Rasputin written in (1927) by Alexej Tolstoi and directed by Erwin Piscator that included a recording of Lenin's voice. Whilst the term "sound designer" was not in use at this time, a number of stage managers specialised as "effects men", creating and performing offstage sound effects using a mix of vocal mimicry, mechanical and electrical contraptions and gramophone records. A great deal of care and attention was paid to the construction and performance of these effects, both naturalistic and abstract. Over the course of the twentieth century the use of recorded sound effects began to take over from live sound effects, though often it was the stage manager's duty to find the sound effects and an electrician played the recordings during performances.Between 1980 and 1988, Charlie Richmond, USITT's first Sound Design Commissioner, oversaw efforts of their Sound Design Commission to define the duties, responsibilities, standards and procedures which might normally be expected of a theatre sound designer in North America. This subject is still regularly discussed by that group, but during that time, substantial conclusions were drawn and he wrote a document which, although now somewhat dated, provides a succinct record of what was expected at that time. It was subsequently provided to both the ADC and David Goodman at the Florida USA local when they were both planning to represent sound designers in the 1990s.=== Digital technology ===MIDI and digital audio technology have contributed to the evolution of sound production techniques in the 1980s and 1990s. Digital audio workstations and a variety of digital signal processing algorithms applied in them allow more complicated sound tracks with more tracks as well as auditory effects to be realized. Features such as unlimited undo and sample-level editing allow fine control over the sound tracks.In theatre sound, features of computerized theatre sound design systems have also been recognized as being essential for live show control systems at Walt Disney World and, as a result, Disney utilized systems of that type to control many facilities at their Disney-MGM Studios theme park, which opened in 1989. These features were incorporated into the MIDI Show Control (MSC) specification, an open communications protocol used to interact with diverse devices. The first show to fully utilize the MSC specification was the Magic Kingdom Parade at Walt Disney World's Magic Kingdom in September, 1991.The rise of interest in game audio has also brought more advanced interactive audio tools that are also accessible without a background in computer programming. Some of such software tools (termed "implementation tools" or "audio engines") feature a workflow that's similar to that in more conventional digital audio workstation programs and can also allow the sound production personnel to undertake some of the more creative interactive sound tasks (that are considered to be part of sound design for computer applications) that previously would have required a computer programmer. Interactive applications have also given rise to a plethora of techniques in "dynamic audio" that loosely means sound that's "parametrically" adjusted during the run-time of the program. This allows for a broader expression in sounds, more similar to that in films, because this way the sound designer can e.g. create footstep sounds that vary in a believable and non-repeating way and that also corresponds to what's seen in the picture. The digital audio workstation cannot directly "communicate" with game engines, because the game's events occur often in an unpredictable order, whereas traditional digital audio workstations as well as so called linear media (TV, film etc.) have everything occur in the same order every time the production is run. Especially games have also brought in dynamic or adaptive mixing.The World Wide Web has greatly enhanced the ability of sound designers to acquire source material quickly, easily and cheaply. Nowadays, a designer can preview and download crisper, more "believable" sounds as opposed to toiling through time- and budget-draining "shot-in-the-dark" searches through record stores, libraries and "the grapevine" for (often) inferior recordings. In addition, software innovation has enabled sound designers to take more of a DIY (or "do-it-yourself") approach. From the comfort of their home and at any hour, they can simply use a computer, speakers and headphones rather than renting (or buying) costly equipment or studio space and time for editing and mixing. This provides for faster creation and negotiation with the director.== Applications ===== Film ===In motion picture production, a Sound Editor/Designer is a member of a film crew responsible for the entirety or some specific parts of a film's sound track. In the American film industry, the title Sound Designer is not controlled by any professional organization, unlike titles such as Director or Screenwriter.The terms sound design and sound designer began to be used in the motion picture industry in 1969. At that time, The title of Sound Designer was first granted to Walter Murch by Francis Ford Coppola in recognition for Murch's contributions to the film The Rain People. The original meaning of the title Sound Designer, as established by Coppola and Murch, was "an individual ultimately responsible for all aspects of a film's audio track, from the dialogue and sound effects recording to the re-recording (mix) of the final track".  The term sound designer has replaced monikers like supervising sound editor or re-recording mixer for what was essentially the same position: the head designer of the final sound track. Editors and mixers like Murray Spivack (King Kong), George Groves (The Jazz Singer), James G. Stewart (Citizen Kane), and Carl Faulkner (Journey to the Center of the Earth) served in this capacity during Hollywood's studio era, and are generally considered to be sound designers by a different name.The advantage of calling oneself a sound designer beginning in later decades was two-fold. It strategically allowed for a single person to work as both an editor and mixer on a film without running into issues pertaining to the jurisdictions of editors and mixers, as outlined by their respective unions. Additionally, it was a rhetorical move that legitimated the field of post-production sound at a time when studios were down-sizing their sound departments, and when producers were routinely skimping on budgets and salaries for sound editors and mixers. In so doing, it allowed those who called themselves sound designers to compete for contract work and to negotiate higher salaries. The position of Sound Designer therefore emerged in a manner similar to that of Production Designer, which was created in the 1930s when William Cameron Menzies made revolutionary contributions to the craft of art direction in the making of Gone with the Wind.The audio production team is a principal member of the production staff, with creative output comparable to that of the film editor and director of photography. Several factors have led to the promotion of audio production to this level, when previously it was considered subordinate to other parts of film:Cinema sound systems became capable of high-fidelity reproduction, particularly after the adoption of Dolby Stereo. These systems were originally devised as gimmicks to increase theater attendance, but their widespread implementation created a content vacuum that had to be filled by competent professionals. Before stereo soundtracks, film sound was of such low fidelity that only the dialogue and occasional sound effects were practical. The greater dynamic range of the new systems, coupled with the ability to produce sounds at the sides or behind the audience, provided the audio production team new opportunities for creative expression in film sound.Some directors were interested in realizing the new potentials of the medium. A new generation of filmmakers, the so-called "Easy Riders and Raging Bulls"—Martin Scorsese, Steven Spielberg, George Lucas, and others—were aware of the creative potential of sound and wanted to use it.Filmmakers were inspired by the popular music of the era. Concept albums of groups such as Pink Floyd and The Beatles suggested new modes of storytelling and creative techniques that could be adapted to motion pictures.New filmmakers made their early films outside the Hollywood establishment, away from the influence of film labor unions and the then rapidly dissipating studio system.The contemporary title of sound designer can be compared with the more traditional title of supervising sound editor; many sound designers use both titles interchangeably. The role of supervising sound editor, or sound supervisor, developed in parallel with the role of sound designer. The demand for more sophisticated soundtracks was felt both inside and outside Hollywood, and the supervising sound editor became the head of the large sound department, with a staff of dozens of sound editors, that was required to realize a complete sound job with a fast turnaround.=== Theatre ===Sound design, as a distinct discipline, is one of the youngest fields in stagecraft, second only to the use of projection and other multimedia displays, although the ideas and techniques of sound design have been around almost since theatre started. Dan Dugan, working with three stereo tape decks routed to ten loudspeaker zones during the 1968–69 season of American Conservatory Theater (ACT) in San Francisco, was the first person to be called a sound designer.Modern audio technology has enabled theatre sound designers to produce flexible, complex, and inexpensive designs that can be easily integrated into live performance. The influence of film and television on playwriting is seeing plays being written increasingly with shorter scenes, which is difficult to achieve with scenery but easily conveyed with sound. The development of film sound design is giving writers and directors higher expectations and knowledge of sound design. Consequently, theatre sound design is widespread and accomplished sound designers commonly establish long-term collaborations with directors.==== Musicals ====Sound design for musicals often focuses on the design and implementation of a sound reinforcement system that will fulfill the needs of the production. If a sound system is already installed in the performance venue, it is the sound designer's job to tune the system for the best use for a particular production. Sound system tuning employs various methods including equalization, delay, volume, speaker and microphone placement, and in some cases, the addition of new equipment. In conjunction with the director and musical director, if any, the sound reinforcement designer determines the use and placement of microphones for actors and musicians. The sound reinforcement designer ensures that the performance can be heard and understood by everyone in the audience, regardless of the shape, size or acoustics of the venue, and that performers can hear everything needed to enable them to do their jobs. While sound design for a musical largely focuses on the artistic merits of sound reinforcement, many musicals, such as Into the Woods also require significant sound scores (see Sound Design for Plays). Sound Reinforcement Design was recognized by the American Theatre Wing's Tony Awards with the Tony Award for Best Sound Design of a Musical until the 2014-15 season, later reinstating in the 2017-18 season.==== Plays ====Sound design for plays often involves the selection of music and sounds (sound score) for a production based on intimate familiarity with the play, and the design, installation, calibration and utilization of the sound system that reproduces the sound score. The sound designer for a play and the production's director work together to decide the themes and emotions to be explored. Based on this, the sound designer for plays, in collaboration with the director and possibly the composer, decides upon the sounds that will be used to create the desired moods. In some productions, the sound designer might also be hired to compose music for the play. The sound designer and the director usually work together to "spot" the cues in the play (i.e., decide when and where sound will be used in the play). Some productions might use music only during scene changes, whilst others might use sound effects. Likewise a scene might be underscored with music, sound effects or abstract sounds that exist somewhere between the two. Some sound designers are accomplished composers, writing and producing music for productions as well as designing sound. Many sound designs for plays also require significant sound reinforcement (see Sound Design for Musicals). Sound Design for plays was recognized by the American Theatre Wing's Tony Awards with the Tony Award for Best Sound Design of a Play until the 2014-15 season, later reinstating the award in the 2017-18 season.==== Professional organizations ====The Theatrical Sound Designers and Composers Association (TSDCA)The Association of Sound Designers is a trade association representing theatre sound designers in the UK.United Scenic Artists (USA) Local USA829, which is integrated within IATSE, represents theatrical sound designers in the United States.Theatrical Sound Designers in English Canada are represented by the Associated Designers of Canada (ADC), and in Québec by l'Association des professionnels des arts du Québec (APASQ).=== Music ===In contemporary music business, especially in the production of rock music, ambient music, progressive rock, and similar genres, the record producer and recording engineer play important roles in the creation of the overall sound (or soundscape) of a recording, and less often, of a live performance. A record producer is responsible for extracting the best performance possible from the musicians and for making both musical and technical decisions about the instrumental timbres, arrangements, etc. On some, particularly more electronic music projects, artists and producers in more conventional genres have sometimes sourced additional help from artists often credited as "sound designers", to contribute specific auditory effects, ambiences etc. to the production. These people are usually more versed in e.g. electronic music composition and synthesizers than the other musicians on board.In application of electroacoustic techniques (e.g. binaural sound) and sound synthesis for contemporary music or film music, a sound designer (often also an electronic musician) sometimes refers to an artist who works alongside a composer to realize the more electronic aspects of a musical production. This is because sometimes there exists a difference in interests between artists calling themselves composers and artists calling themselves electronic musicians or sound designers. The latter being sometimes more experienced as well as interested in electronic music techniques, such as sequencing and synthesizers, but the former often wanting to use elements of electronic music in compositions but often being more experienced in writing music in a variety of genres. Since electronic music itself is quite broad in techniques and often separate from techniques applied in other genres, this kind of collaboration can be seen as fairly natural as well as beneficial.Notable examples of (recognized) sound design in music are the contributions of Michael Brook to the U2 album The Joshua Tree, George Massenburg to the Jennifer Warnes album Famous Blue Raincoat, Chris Thomas to the Pink Floyd album The Dark Side of the Moon, and Brian Eno to the Paul Simon album Surprise.In 1974, Suzanne Ciani started her own production company, Ciani/Musica. Inc., which became the #1 sound design music house in New York.=== Fashion ===In fashion shows, the sound designer often works with the artistic director to create an atmosphere fitting the theme of a collection, commercial campaign or event.=== Computer applications and other applications ===Sound is widely used in a variety of human-computer interfaces, in computer games and video games. There are a few extra concerns and requirements for sound production for computer applications, including non-repetitiveness, interactivity and low memory and CPU usage. For example, the most computational resources are usually devoted to graphics, which in turn means that the sound playback is often limited to some upper memory consumption as well as CPU use limits. These have to be added to the concerns of audio production, since it is often not enough to merely "create good sound", but the sound also has to fit to the given computational limits, while still sounding good, which may require e.g. the use of audio compression or voice allocating systems. However, sound for computer applications also adds some new creative aspects to music and sound, because computer sound (especially in games) often involves the sound having or desired to be interactive. Adding interactivity can involve using a variety of playback systems or logic, using tools that allow the production of interactive sound (e.g. Max/MSP, Wwise) and it also deviates from being just an art form to also requiring software or electrical engineering, since implementing interactivity of sound also requires engineering of the systems that play the sound or e.g. process user input. Therefore, in interactive applications, a sound designer often collaborates with an engineer (e.g. a sound programmer) who's concerned with designing the playback systems and their efficiency.== Awards ==Sound designers have been recognized by awards organizations for some time, and new awards have emerged more recently in response to advances in sound design technology and quality. The Motion Picture Sound Editors and the Academy of Motion Picture Arts and Sciences recognizes the finest or most aesthetic sound design for a film with the Golden Reel Award for Sound and Music editing, and the Academy Award for Best Sound Editing respectively. In 2007, the Tony Award for Best Sound Design was created to honor the best sound design in American theatre on Broadway.North American theatrical award organizations that recognize sound designers include these:Dora Mavor Moore AwardsDrama Desk AwardsHelen Hayes AwardsObie AwardsJoseph Jefferson AwardsMajor British award organizations include the Olivier Awards. The Tony Awards retired the awards for Sound Design as of the 2014-2015 season, then reinstated the categories in the 2017-18 season.== See also ==Audio engineeringBerberian Sound StudioCrash boxDirector of audiographyList of sound designersMusique concrèteIEZA Framework - a framework for conceptual game sound design== References ==== External links ==FilmSound.org: A Learning Space dedicated to the Art of Sound DesignKai's Theater Sound Hand BookAssociation of Sound DesignerssounDesign: online publication about Sound CommunicationExample of Functional Sound Design for Service Robot in a french Train Station
	The following outline is provided as an overview of and topical guide to software development:Software development – development of a software product, which entails computer programming (process of writing and maintaining the source code), but also encompasses a planned and structured process from the conception of the desired software to its final manifestation. Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.== What type of thing is software development? ==Software development can be described as all of the following:Research and development –Vocation –Profession –== Branches of software development ==Video game developmentWeb application developmentMobile application development== History of software development ==History of software developmentHistory of programming languages== Software development participants ==Software developerSoftware engineerConsulting software engineerComputer programmerSoftware publisher== Software development problems ==ShovelwareSoftware bloatSoftware bug== Software project management ==Software project management – art and science of planning and leading software projects. It is a sub-discipline of project management in which software projects are planned, monitored and controlled.Software configuration management== Software development strategies ==Offshore software R&D – provision of software development services by an external supplier positioned in a country that is geographically remote from the client enterprise; a type of offshore outsourcing.== Software development process ==Software development processSoftware release life cycleStages of developmentPre-alphaAlpha releaseBeta releaseOpen betaClosed betaRelease candidateReleaseRelease to manufacturing (RTM)General availability release (GA)Web release (RTW)Technical supportEnd-of-life – termination of support for the product=== Activities and steps ===Requirements analysisSoftware development effort estimationFunctional specificationSoftware architectureSoftware designComputer programmingSoftware testingSoftware deploymentSoftware releaseProduct installationProduct activationDeactivationAdaptationSoftware updateUninstallationUninstallerProduct retirementSoftware maintenance=== Software development methodologies ===Software development methodologyAspect-oriented software developmentCleanroom Software EngineeringIterative and incremental developmentIncremental funding methodologyRapid application developmentIBM Rational Unified ProcessSpiral modelWaterfall modelExtreme programmingLean software developmentScrumV-ModelTest-driven development (TDD)==== Agile software development ====Agile software developmentCross-functional teamExtreme programmingIterative and incremental developmentPair programmingSelf-organizationTimeboxing=== Supporting disciplines ===Computer programming –Software documentation –Software engineering –Software quality assurance (SQA) –User experience design –=== Software development tools ===Programming toolCompilerDebuggerPerformance analysisGraphical user interface builderIntegrated development environment== Education relevant to software development ==Bachelor of Computer Science – type of bachelor's degree awarded for study of computer science, emphasizing the mathematical and theoretical foundations of computing, rather than teaching specific technologies that may quickly become outdated. A Bachelor of Computer Science degree is normally required in order to work in the field of software development. Abbreviated "BCompSc" or "BCS".== Software development organizations ==While the IT industry undergoes changes faster than any other field, most technical experts agree that one needs to have the community they can consult, learn from or share their experience with. Here is the list of well-known software development organizations.Association of Computer Engineers and Technicians (ACE – ACET)  professional standards within the IT industry.Association for Computing Machinery (ACM) is one of the oldest and largest scientific communities that deal with computing and technology. It covers a wide range of topics including e-commerce, bioinformatics, and networking.Association of Independent Information Professionals (AIIP) is an association for information professionals working independently or within the related industries.Association of Information Technology Professionals (AITP) is a worldwide community that focuses on information technology education. It helps to connect experts from different IT fields.ASIS International (ASIS) is the leading community that connects security professionals from all over the world.Association of Shareware Professionals (ASP) connects developers and tech specialists who work with services and application on "try-before-you-buy" basis.Association for Women in Computing (AWC) organizes educational and networking events for female tech specialists in order to increase the share of women in the industry.Black Data Processing Associates (BDPA) gathers a community of African Americans working in information technology for both educational and professional growth.Computer & Communications Industry Association (CCIA) advocates for open markets, systems and competition.Computing Technology Industry Association (CompTIA) provides certifications for the IT industry, as well as educates individuals and group on changes and tendencies for the industry.Computer Professionals for Social Responsibility (CPSR) an organization concerned with technology's impact on society. The group provides the assessment of the tech development and its impact on various fields of life.Data & Analysis Center for Software (DACS) collects and serves the information about various entities and software they produce, as well as its trustworthiness.EDUCAUSE is a non-profit organization that states its mission as ‘advance higher education through information technology’.European Computer Manufacturers Association (ECMA) is a European organization that facilitates standards and information and communication systems.International Association of Engineers (IAENG) is an international association that used to be a private network. Nowadays, hosts annual World Congress on Engineering for R&D and engineers.Institute of Electrical and Electronics Engineers (IEEE) Computer Society provides educational services to its members worldwide. This society has one of the biggest networks and offers numerous perks to its members.Information Systems Security Association (ISSA) is a not-for-profit, that encourages the use of practices to protect the confidentiality and integrity of information resources.Network Professional Association (NPA) encourages its members to adhere to the code of ethics, follows the latest best practices and indulge in continuous self-education.Technology Services Industry Association (TSIA) is a professional association that offers research and advisory services.Society for Technical Communication (STC) offers support and knowledge sharing to specialists involved in technical communication and related fields.User Experience Professionals Association (UXPA) is an organization that shares knowledge about UX and helps its members to grow, develop and improve their products.Women in Technology (WIT) advocates the education of female representatives in the industry all the way from elementary training to advanced programs.== Software development publications ==== Persons influential in software development ==== See also ==Product activationSoftware blueprintSoftware designSoftware development effort estimationOutline of software engineering== References ==
	Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behavior, and evolution. It serves as means to monitor, steer, observe and optimize software development, software maintenance, and software re-engineering in the sense of a business intelligence approach specific to software systems. It is generally based on the automatic extraction, analysis, and visualization of corresponding information sources of the software system. It can also be manually done and not automatic.== Applications ==Software diagnosis supports all branches of software engineering, in particular project management, quality management, risk management as well as implementation and test. Its main strength is to support all stakeholders of software projects (in particular during software maintenance and for software re-engineering tasks) and to provide effective communication means for software development projects. For example, software diagnosis facilitates "bridging an essential information gap between management and development, improve awareness, and serve as early risk detection instrument". Software diagnosis includes assessment methods for "perfective maintenance" that, for example, apply "visual analysis techniques to combine multiple indicators for low maintainability, including code complexity and entanglement with other parts of the system, and recent changes applied to the code".== Characteristics ==In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. Instead, software diagnosis aims at analyzing and evaluating the software system in its as-is state and based on system-generated information to bybass any subjective or potentially outdated information sources (e.g., initial software models). For it, software diagnosis combines and relates sources of information that are typically not directly linked. Examples: Source-code metrics are related with software developer activity to gain insight into developer-specific effects on software code quality.System structure and run-time execution traces are correlated to facilitate program comprehension through dynamic analysis in software maintenance tasks.== Principles ==The core principle of software diagnosis is to automatically extract information from all available information sources of a given software projects such as source code base, project repository, code metrics, execution traces, test results, etc. To combine information, software-specific data mining, analysis, and visualization techniques are applied. Its strength results, among various reasons, from integrating decoupled information spaces in the scope of a typical software project, for example development and developer activities (recorded by the repository) and code and quality metrics (derived by analyzing source code) or key performance indicators (KPIs).== Examples ==Examples of software diagnosis tools include software maps and software metrics.== Critics ==Software diagnosis—in contrast to many approaches in software engineering—does not assume that developer capabilities, development methods, programming or modeling languages are right or wrong (or better or worse compared to each other): Software diagnosis aims at giving insight into a given software system and its status regardless of the methods, languages, or models used to create and maintain the system.=== Related subjects ===Cost estimation in software engineeringProgramming productivityRapid application developmentSoftware designSoftware developmentSoftware documentationSoftware mapSoftware release life cycleSystems designSystems Development Life Cycle== References ==== External links ==A tool set based on software maps by Software Diagnostics, 2014Demo video interactive software maps for source-code analysis, 2013
	In software engineering, software aging refers to all software's tendency to fail, or cause a system failure after running continuously for a certain time. As the software gets older it becomes less immune and will eventually stop functioning as it should, therefore rebooting or reinstalling the software can be seen as a short term fix. A proactive fault management method to deal with the software aging incident is software rejuvenation. This method can be classified as an environment diversity technique that usually is implemented through software rejuvenation agents (SRA).From both an academic and industrial point of view, the software aging phenomenon has increased. The main focus has been to understand its effects from a verifiable observation and theoretical understanding."Programs, like people, get old. We can't prevent aging, but we can understand its causes, take steps to limit its effects, temporarily reverse some of the damage it has caused, and prepare for the day when the software is no longer viable."Memory bloating and leaking, along with data corruption and unreleased file-locks are particular causes of software aging.== Proactive management of software aging ===== Software aging ===Software failures are a more likely cause of unplanned systems outages compared to hardware failures. This is because software exhibits over time an increasing failure rate due to data corruption, numerical error accumulation and unlimited resource consumption. In widely used and specialized software, a common action to clear a problem is rebooting because aging occurs due to the complexity of software which is never free of errors. It is almost impossible to fully verify that a piece of software is bug-free. Even high-profile software such as Windows and macOS must receive continual updates to improve performance and fix bugs. Software development tends to be driven by the need to meet release deadlines rather than to ensure long-term reliability. Designing software that can be immune to aging is difficult. Not all software will age at the same rate as some users use the system more intensively than others.=== Rejuvenation ===To prevent crashes or degradation software rejuvenation can be employed proactively as inevitable aging leads to failures in software systems. This proactive technique was identified as a cost-effective solution during research at the AT&T Bell Laboratories on fault-tolerant software in the 1990s. Software rejuvenation works by removing accumulated error conditions and freeing up system resources, for example by flushing operating system kernel tables, using garbage collection, reinitializing internal data structures, and perhaps the most well known rejuvenation method is to reboot the system.There are simple techniques and complex techniques to achieve rejuvenation. The method most individuals are familiar with is the hardware or software reboot. A more technical example would be the web server software Apache's rejuvenation method. Apache implements one form of rejuvenation by killing and recreating processes after serving a certain number of requests.Another technique is to restart virtual machines running in a cloud computing environment.The multinational telecommunications corporation AT&T has implemented software rejuvenation in the real time system collecting billing data in the United States for most telephone exchanges.Some systems which have employed software rejuvenation methods include:Transaction processing systemsWeb serversSpacecraft systemsThe IEEE International Symposium on Software Reliability Engineering (ISSRE) hosted the 5th annual International Workshop on Software Aging and Rejuvenation (woSAR) in 2013.  Topics included: Design, implementation, and evaluation of rejuvenation mechanismsModeling, analysis, and implementation of rejuvenation schedulingSoftware rejuvenation benchmarking== Memory leaks ==In systems that use an OS user programs have to request memory blocks in order to perform an operation. After this operation (e.g. a subroutine) is completed, the program is expected to free up all the memory blocks allocated for it in order to make it available to other programs for use. In programming languages without a garbage collector (e.g. C and C++) it's up to the programmer to call the necessary memory releasing functions and to account for all the unused data within the program. However this doesn't always happen. Due to software bugs the program might consume more and more memory eventually causing the system to run out of memory. In low memory conditions, the system usually functions slower due to the performance bottleneck caused by intense swapping (thrashing), applications become unresponsive and those that request large amounts of memory unexpectedly may crash. In case the system runs out of both memory and swap even the OS might crash causing the whole system to reboot.Programs written in programming languages that use a garbage collector (e.g. Java) usually rely on this feature for avoiding memory leaks. Thus the "aging" of these programs is at least partially dependent on the quality of the garbage collector built into the programming language's runtime environment itself.Sometimes critical components of the OS itself can be a source of memory leaks and be the main culprit behind system stability issues. In Microsoft Windows, for example, the memory use of Windows Explorer plug-ins and long-lived processes such as services can impact the reliability of the system to the point of making it unusable. A reboot might be needed to make the system work again.Software rejuvenation helps with memory leaks as it forces all the memory used by an application to be released. The application can be restarted but starts with a clean slate.== Implementation ==Two methods for implementing rejuvenation are:Time based rejuvenationPrediction based rejuvenation== Memory bloating ==Garbage collection is a form of automatic memory management whereby the system automatically recovers unused memory. For example, the .NET Framework manages the allocation and release of memory for software running under it. But automatically tracking these objects takes time and is not perfect..NET based web services manage several  logical types of memory such as stack, unmanaged and managed heap (free space). As the physical memory gets full, the OS writes rarely-used parts of it to disk, so that it can reallocate it to another application, a process known as paging or swapping. But if the memory does need to be used, it must be reloaded from disk. If several applications are all making large demands, the OS can spend much of its time merely moving data between main memory and disk, a process known as disk thrashing. Since the garbage collector has to examine all of the allocations to decide which are in use, it may exacerbate this thrashing. As a result, extensive swapping can lead to garbage collection cycles extended from milliseconds to tens of seconds. This results in usability problems.== References ==== Further reading ==
	DBmaestro is computer software service company with sales headquartered near Boston, and development in Petah Tikva, Israel. It markets its services for DevOps: collaboration between development and IT operations teams.== History ==DBmaestro was co-founded in 2012 by CEO Yariv Tabac, and Chief Technical Officer Yaniv Yehuda .It was founded in Israel.DBmaestro's DevOps Platform software focuses on database development and controls application specific data.DBmaestro integrates with other technology such as Oracle Database, Microsoft SQL Server, DB2 and PostgreSQL.DBmaestro software as a service offers database release automation capabilities designed to optimize DevOps environments for enterprises by automating continuous integration and continuous delivery (CI/CD) processes for databases with zero disruption to existing processes. DBmaestro's DevOps platform offers a visual database pipeline builder, which enables organizations to package, verify, deploy, and promote database changes, and a release automation module, which revalidates the final state to make sure the release process ended successfully, while auditing all changes made.In 2015, DBmaestro was recognized by the editors of SD Times, Computing magazine,and a market research company. In 2018, DBmaestro was honored as a Bronze Stevie Award Winner for New Product or Service of the Year - Software - DevOps SolutionIn July, 2019, DBmaestro announced a global reselling partnership agreement with IBM, in which IBM will be offering DBmaestro’s platform to its worldwide enterprise customer base.Their first venture capital funding was $3 million, announced in February 2016.. $4.5 million in additional funding was secured in July, 2017.== External links ==DBmaestro websiteSQL Server Central (SQL Server education and community website)== References ==
	Security, as part of the software development process, is an ongoing process involving people and practices, and ensures application confidentiality, integrity, and availability. Secure software is the result of security aware software development processes where security is built in and thus software is developed with security in mind.Security is most effective if planned and managed throughout every stage of software development life cycle (SDLC), especially in critical applications or those that process sensitive information.The solution to software development security is more than just the technology.== Software development challenges ==As technology advances, application environments become more complex and application development security becomes more challenging. Applications, systems, and networks are constantly under various security attacks such as malicious code or denial of service. Some of the challenges from the application development security point of view include Viruses, Trojan horses, Logic bombs, Worms, Agents, and Applets.Applications can contain security vulnerabilities that may be introduced by software engineers either intentionally or carelessly.Software, environmental, and hardware controls are required although they cannot prevent problems created from poor programming practice. Using limit and sequence checks to validate users’ input will improve the quality of data. Even though programmers may follow best practices, an application can still fail due to unpredictable conditions and therefore should handle unexpected failures successfully by first logging all the information it can capture in preparation for auditing. As security increases, so does the relative cost and administrative overhead.Applications are typically developed using high-level programming languages which in themselves can have security implications. The core activities essential to the software development process to produce secure applications and systems include: conceptual definition, functional requirements, control specification, design review, code review and walk-through, system test review, and maintenance and change management.Building secure software is not only the responsibility of a software engineer but also the responsibility of the stakeholders which include: management, project managers, business analysts, quality assurance managers, technical architects, security specialists, application owners, and developers.== Basic principles ==There are a number of basic guiding principles to software security. Stakeholders’ knowledge of these and how they may be implemented in software is vital to software security. These include:Protection from disclosureProtection from alterationProtection from destructionWho is making the requestWhat rights and privileges does the requester haveAbility to build historical evidenceManagement of configuration, sessions and errors/exceptions== Basic practices ==The following lists some of the recommended web security practices that are more specific for software developers.Sanitize inputs at the client side and server sideEncode request/responseUse HTTPS for domain entriesUse only current encryption and hashing algorithmsDo not allow for directory listingDo not store sensitive data inside cookiesCheck the randomness of the sessionSet secure and HttpOnly flags in cookiesUse TLS not SSLSet strong password policyDo not store sensitive information in a form’s hidden fieldsVerify file upload functionalitySet secure response headersMake sure third party libraries are securedHide web server information== Security testing ==Common attributes of security testing include authentication, authorization, confidentiality, availability, integrity, non-repudiation, and resilience. Security testing is essential to ensure that the system prevents unauthorized users to access its resources and data. Some application data is sent over the internet which travels through a series of servers and network devices. This gives ample opportunities to unscrupulous hackers.== Summary ==All secure systems implement security controls within the software, hardware, systems, and networks - each component or process has a layer of isolation to protect an organization's most valuable resource which is its data. There are various security controls that can be incorporated into an application's development process to ensure security and prevent unauthorized access.== References ==Stewart, James (2012). CISSP Certified Information Systems Security Professional Study Guide Sixth Edition. Canada: John Wiley & Sons, Inc. pp. 275–319. ISBN 978-1-118-31417-3.Report from Dagstuhl Seminar 12401Web Application Security Edited by Lieven Desmet, Martin Johns, Benjamin Livshits, and Andrei Sabelfeld, http://research.microsoft.com/en-us/um/people/livshits/papers%5Ctr%5Cdagrep_s12401.pdfWeb Application Security Consortium, The 80/20 Rule for Web Application Security by Jeremiah Grossman 2005, http://www.webappsec.org/projects/articles/013105.shtmlWikipedia Web Application Security page, Web application securityWeb Security Wiki page, https://www.w3.org/Security/wiki/Main_PageWikipedia Web Security Exploits page, Category:Web security exploitsOpen Web Application Security Project (OWASP), https://www.owasp.org/index.php/Main_PageWikipedia Network Security page, Network securityOpen Web Application Security Project (OWASP) web site, https://www.owasp.org/images/8/83/Securing_Enterprise_Web_Applications_at_the_Source.pdf
	Information Oriented Software Development is a software development methodology focused on working with information inside a computer program as opposed to working with just data. A significant difference exists between data and information. Information Oriented Software Development relies on data structures specifically designed to hold information, and relies on frameworks that support those data structures. Information oriented software development focuses on the conceptual needs of users and customers rather than the data storage models and object models.== Information data structures ==Information data structures are data structures specifically intended to support information inside a computer program. Two common ones are as follows:Data structures to support Fuzzy logic.Data structures to support concept combinations and concept Permutations.== See also ==Knowledge representationDomain-driven designInformation modelData science== References ==
	Automatic bug-fixing is the automatic repair of software bugs without the intervention of a human programmer. It is also commonly referred to as automatic patch generation, automatic bug repair, or automatic program repair. The typical goal of such techniques is to automatically generate correct patches to eliminate bugs in software programs without causing software regression.== Specification ==Automatic bug fixing is made according to a specification of the expected behavior which can be for instance a formal specification or a test suite.A test-suite – the input/output pairs specify the functionality of the program, possibly captured in assertions can be used as a test oracle to drive the search. This oracle can in fact be divided between the bug oracle that exposes the faulty behavior, and the regression oracle, which encapsulates the functionality any program repair method must preserve. Note that a test suite is typically incomplete and does not cover all possible cases. Therefore, it is often possible for a validated patch to produce expected outputs for all inputs in the test suite but incorrect outputs for other inputs. The existence of such validated but incorrect patches is a major challenge for generate-and-validate techniques. Recent successful automatic bug-fixing techniques often rely on additional information other than the test suite, such as information learned from previous human patches, to further identify correct patches among validated patches.Another way to specify the expected behavior is to use formal specifications  Verification against full specifications that specify the whole program behavior including functionalities is less common because such specifications are typically not available in practice and the computation cost of such verification is prohibitive. For specific classes of errors, however, implicit partial specifications are often available. For example, there are targeted bug-fixing techniques validating that the patched program can no longer trigger overflow errors in the same execution path.== Techniques ===== Generate-and-validate ===Generate-and-validate approaches compile and test each candidate patch to collect all validated patches that produce expected outputs for all inputs in the test suite. Such a technique typically starts with a test suite of the program, i.e., a set of test cases, at least one of which exposes the bug. An early generate-and-validate bug-fixing systems is GenProg. The effectiveness of generate-and-validate techniques remains controversial, because they typically do not provide patch correctness guarantees. Nevertheless, the reported results of recent state-of-the-art techniques are generally promising. For example, on systematically collected 69 real world bugs in eight large C software programs, the state-of-the-art bug-fixing system Prophet generates correct patches for 18 out of the 69 bugs.One way to generate candidate patches is to apply mutation operators on the original program. Mutation operators manipulate the original program, potentially via its abstract syntax tree representation, or a more coarse-grained representation such as operating at the statement-level or block-level. Earlier genetic improvement approaches operate at the statement level and carry out simple delete/replace operations such as deleting an existing statement or replacing an existing statement with another statement in the same source file. Recent  approaches use more fine-grained operators at the abstract syntax tree level to generate more diverse set of candidate patches.Many generate-and-validate techniques rely on the redundancy insight: the code of the patch can be found elsewhere in the application. This idea was introduced in the Genprog system, where two operators, addition and replacement of AST nodes, were based on code taken from elsewhere (i.e. adding an existing AST node). This idea has been validated empirically, with two independent studies that have shown that a significant proportion of commits (3%-17%) are composed of existing code. Beyond the fact that the code to reuse exists somewhere else, it has also been shown that the context of the potential repair ingredients is useful: often, the donor context is similar to the recipient context. Using fix templates is an alternative way to generate candidate patches. Fix templates are typically predefined program mutation rules for fixing specific classes of bugs. Examples of fix templates include inserting a conditional statement to check whether the value of a variable is null to fix null pointer exception, and changing an integer constant by one to fix off-by-one errors. Fix templates are therefore often adopted by targeted techniques. It is also possible to automatically mine fix templates for generate-and-validate approches.=== Synthesis-based ===Repair techniques exist that are based on symbolic execution. For example, Semfix uses symbolic execution to extract a repair constraint. Angelix introduced the concept of angelic forest in order to deal with multiline patches.Under certain assumptions, it is possible to state the repair problem as a synthesis problem.SemFix and Nopol uses component-based synthesis.Dynamoth  uses dynamic synthesis.S3 is based on syntax-guided synthesis.SearchRepair converts potential patches into an SMT formula and queries candidate patches that allow the patched program to pass all supplied test cases.=== Data-driven ===Machine learning techniques can improve the effectiveness of automatic bug-fixing systems. One example of such techniques learns from past successful patches from human developers collected from open source repositories in GitHub and SourceForge. It then use the learned information to recognize and prioritize potentially correct patches among all generated candidate patches. Alternatively, patches can be directly mined from existing sources. Example approaches include mining patches from donor applications or from QA web sites.SequenceR uses sequence-to-sequence learning on source code in order to generate one-line patches. It defines a neural network architecture that works well with source code, with the copy mechanism that allows to produce patches with tokens that are not in the learned vocabulary. Those tokens are taken from the code of the Java class under repair.=== Other ===Targeted automatic bug-fixing techniques generate repairs for specific classes of errors such as null pointer exception integer overflow , buffer overflow , memory leak ,  etc.. Such techniques often use empirical fix templates to fix bugs in the targeted scope. For example, insert a conditional statement to check whether the value of a variable is null or insert missing memory deallocation statements. Comparing to generate-and-validate techniques, targeted techniques tend to have better bug-fixing accuracy but a much narrowed scope.== Use ==There are multiple uses of automatic bug fixing:in the development environment: when the developer encounters a bug, she activates a feature to search for a patch (for instance by clicking on a button). This search can even happen in the background, when the IDE proactively searches for solutions to potential problems, without waiting for a developer explicit action.in the continuous integration server: when a build fails during continuous, a patch search can be attempted as soon as the build has failed. If the search is successful, the patch is given to the developer before she has started working on it, or before she has found the solution. When a synthesized patch is suggested to the developers as pull-request, an explanation has to be provided in addition to the code changes (eg. a pull request title and description). An experiment has shown that generated patches can be accepted by open-source developers and merged in the code repository.at runtime: when a failure happens at runtime, a binary patch can be searched for and applied online. An example of such a repair system is ClearView, which does repair on x86 code, with x86 binary patches. The Itzal system is different from Clearview: while the repair search happens at runtime, in production, the produced patches are at the source code level. The BikiniProxy system does online repair of Javascript errors happening in the browser.== Search space ==In essence, automatic bug fixing is a search activity, whether deductive-based or heuristic-based. The search space of automatic bug fixing is composed of all edits that can be possibly made to a program. There have been studies to understand the structure of this search space. Qi et al. showed that the original fitness function of Genprog is not better than random search to drive the search. Martinez et al. explored the imbalance between possible repair actions, showing its significant impact on the search. Long et al.'s study indicated that correct patches can be considered as sparse in the search space and that incorrect overfitting patches are vastly more abundant (see also discussion about overfitting below).If one explicitly enumerates all possible variants in a repair algorithm, this defines a design space for program repair. Each variant selects an algorithm involved at some point in the repair process (eg the fault localization algorithm), or selects a specific heuristic which yields different patches. For instance, in the design space of generate-and-validate program repair, there is one variation point about the granularity of the program elements to be modified: an expression, a statement, a block, etc.== Limitations of automatic bug-fixing ==Automatic bug-fixing techniques that rely on a test suite do not provide patch correctness guarantees, because the test suite is incomplete and does not cover all cases. A weak test suite may cause generate-and-validate techniques to produce validated but incorrect patches that have negative effects such as eliminating desirable functionalities, causing memory leaks, and introducing security vulnerabilities.Sometimes, in test-suite based program repair, tools generate patches that pass the test suite, yet are actually incorrect, this is known as the "overfitting" problem. "Overfitting" in this context refers to the fact that the patch overfits to the test inputs. There are different kinds of overfitting: incomplete fixing means that only some buggy inputs are fixed, regression introduction means some previously working features are broken after the patch (because they were poorly tested). Early prototypes for automatic repair suffered a lot from overfitting: on the Manybugs C benchmark, Qi et al. reported that 104/110 of plausible GenProg patches were overfitting; on the Defects4J Java benchmark, Martinez et al. reported that 73/84 plausible patches as overfitting. In the context of synthesis-based repair, Le et al. obtained more than 80% of overfitting patches.Another limitation of generate-and-validate systems is the search space explosion. For a program, there are a large number of statements to change and for each statement there are a large number of possible modifications. State-of-the-art systems address this problem by assuming that a small modification is enough for fixing a bug, resulting in a search space reduction.The limitation of approaches based on symbolic analysis is that real world programs are often converted to intractably large formulas especially for modifying statements with side effects.== Benchmarks ==The benchmark collected by GenProg authors contains 69 real world defects and it is widely used to evaluate many other bug-fixing tools in C as well. The state-of-the-art tool evaluated on GenProg benchmark is Prophet, generating correct patches for 18 out of 69 defects.In Java, the main benchmark is Defects4J, initially explored by Martinez et al., and now extensively used in most research papers on program repair for Java. Alternative benchmarks exist, such as the Quixbugs benchmark, which contains original bugs for program repair. Other benchmarks of Java bugs include Bugs.jar, based on past commits, and BEARS  which is a benchmark of continuous integration build failures.== Example tools ==Automatic bug-fixing is an active research topic in computer science. There are many implementations of various bug-fixing techniques especially for C and Java programs. Note that most of these implementations are research prototypes for demonstrating their techniques, i.e., it is unclear whether their current implementations are ready for industrial usage or not.=== C ===ClearView: A generate-and-validate tool of generating binary patches for deployed systems. It is evaluated on 10 security vulnerability cases. A later study shows that it generates correct patches for at least 4 of the 10 cases.GenProg: A seminal generate-and-validate bug-fixing tool. It has been extensively studied in the context of the ManyBugs benchmark.SemFix: The first solver-based bug-fixing tool for C.CodePhage: The first bug-fixing tool that directly transfer code across programs to generate patch for C program. Note that although it generates C patches, it can extract code from binary programs without source code.LeakFix: A tool that automatically fixes memory leaks in C programs.Prophet: The first generate-and-validate tool that uses machine learning techniques to learn useful knowledge from past human patches to recognize correct patches. It is evaluated on the same benchmark as GenProg and generate correct patches (i.e., equivalent to human patches) for 18 out of 69 cases.SearchRepair: A tool for replacing buggy code using snippets of code from elsewhere. It is evaluated on the IntroClass benchmark and generates much higher quality patches on that benchmark than GenProg, RSRepair, and AE.Angelix: An improved solver-based bug-fixing tool. It is evaluated on the GenProg benchmark. For 10 out of the 69 cases, it generate patches that is equivalent to human patches.=== Java ===PAR: A generate-and-validate tool that uses a set of manually defined fix templates. A later study raised concerns about the generalizability of the fix templates in PAR.NOPOL: A solver-based tool focusing on modifying condition statements.QACrashFix: A tool that fixes Java crash bugs by mining fixes from Q&A web site.Astor: An automatic repair library for Java, containing jGenProg, a Java implementation of GenProg.NpeFix: An automatic repair tool for NullPointerException in Java, available on Github.=== Other languages ===AutoFixE: A bug-fixing tool for Eiffel language. It relies the contracts (i.e., a form of formal specification) in Eiffel programs to validate generated patches.=== Proprietary ===DeepCode integrates public and private GitHub, GitLab and Bitbucket repositories to identify code-fixes and improve software.== References ==== External links ==program-repair.org datasets, tools, etc., related to automated program repair research.
	Object points are an approach used in software development effort estimation under some models such as COCOMO II.Object points are a way of estimating effort size, similar to Source Lines Of Code (SLOC) or Function Points.  They are not necessarily related to objects in Object-oriented programming, the objects referred to include screens, reports, and modules of the language.  The number of raw objects and complexity of each are estimated and a weighted total Object-Point count is then computed and used to base estimates of the effort needed.== See also ==COCOMO (Constructive Cost Model)Comparison of development estimation softwareFunction pointSoftware development effort estimationSoftware SizingSource lines of codeUse Case Points== References ==
	A web server is server software, or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over HTTP and several other related protocols.The primary function of a web server is to store, process and deliver web pages to clients. The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently HTML documents, which may include images, style sheets and scripts in addition to the text content. A user agent, commonly a web browser or web crawler, initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage, but this is not necessarily the case and depends on how the web server is implemented.While the primary function is to serve content, a full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms, including uploading of files.Many generic web servers also support server-side scripting using Active Server Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages. This means that the behaviour of the web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically ("on-the-fly") as opposed to returning static documents. The former is primarily used for retrieving or modifying information from databases. The latter is typically much faster and more easily cached but cannot deliver dynamic content.Web servers can frequently be found embedded in devices such as printers, routers, webcams and serving only a local network. The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most operating systems).== History ==In March 1989 Sir Tim Berners-Lee proposed a new project to his employer CERN, with the goal of easing the exchange of information between scientists by using a hypertext system. The project resulted in Berners-Lee writing two programs in 1990:A Web browser called WorldWideWebThe world's first web server, later known as CERN httpd, which ran on NeXTSTEPBetween 1991 and 1994, the simplicity and effectiveness of early technologies used to surf and exchange data through the World Wide Web helped to port them to many different operating systems and spread their use among scientific organizations and universities, and subsequently to the industry.In 1994 Berners-Lee decided to constitute the World Wide Web Consortium (W3C) to regulate the further development of the many technologies involved (HTTP, HTML, etc.) through a standardization process.== Path translation ==Web servers are able to map the path component of a Uniform Resource Locator (URL) into:A local file system resource (for static requests)An internal or external program name (for dynamic requests)For a static request the URL path specified by the client is relative to the web server's root directory.Consider the following URL as it would be requested by a client over HTTP:http://www.example.com/path/file.htmlThe client's user agent will translate it into a connection to www.example.com with the following HTTP/2 request:GET /path/file.html HTTP/2Host: www.example.comThe web server on www.example.com will append the given path to the path of its root directory. On an Apache server, this is commonly /home/www (on Unix machines, usually /var/www). The result is the local file system resource:/home/www/path/file.htmlThe web server then reads the file, if it exists, and sends a response to the client's web browser. The response will describe the content of the file and contain the file itself or an error message will return saying that the file does not exist or is unavailable.== Kernel-mode and user-mode web servers ==A web server can be either incorporated into the OS kernel, or in user space (like other regular applications).Web servers that run in user-mode have to ask the system for permission to use more memory or more CPU resources. Not only do these requests to the kernel take time, but they are not always satisfied because the system reserves resources for its own usage and has the responsibility to share hardware resources with all the other running applications. Executing in user mode can also mean useless buffer copies which are another limitation for user-mode web servers.== Load limits ==A web server (program) has defined load limits, because it can handle only a limited number of concurrent client connections (usually between 2 and 80,000, by default between 500 and 1,000) per IP address (and TCP port) and it can serve only a certain maximum number of requests per second (RPS, also known as queries per second or QPS) depending on:its own settings,the HTTP request type,whether the content is static or dynamic,whether the content is cached, andthe hardware and software limitations of the OS of the computer on which the web server runs.When a web server is near to or over its limit, it becomes unresponsive.=== Causes of overload ===At any time web servers can be overloaded due to:Excess legitimate web traffic. Thousands or even millions of clients connecting to the web site in a short interval, e.g., Slashdot effect;Distributed Denial of Service attacks. A denial-of-service attack (DoS attack) or distributed denial-of-service attack (DDoS attack) is an attempt to make a computer or network resource unavailable to its intended users;Computer worms that sometimes cause abnormal traffic because of millions of infected computers (not coordinated among them)XSS worms can cause high traffic because of millions of infected browsers or web servers;Internet bots Traffic not filtered/limited on large web sites with very few resources (bandwidth, etc.);Internet (network) slowdowns, so that client requests are served more slowly and the number of connections increases so much that server limits are reached;Web servers (computers) partial unavailability. This can happen because of required or urgent maintenance or upgrade, hardware or software failures, back-end (e.g., database) failures, etc.; in these cases the remaining web servers get too much traffic and become overloaded.=== Symptoms of overload ===The symptoms of an overloaded web server are:Requests are served with (possibly long) delays (from 1 second to a few hundred seconds).The web server returns an HTTP error code, such as 500, 502, 503, 504, 408, or even 404, which is inappropriate for an overload condition.The web server refuses or resets (interrupts) TCP connections before it returns any content.In very rare cases, the web server returns only a part of the requested content. This behavior can be considered a bug, even if it usually arises as a symptom of overload.=== Anti-overload techniques ===To partially overcome above average load limits and to prevent overload, most popular web sites use common techniques like:Managing network traffic, by using:Firewalls to block unwanted traffic coming from bad IP sources or having bad patternsHTTP traffic managers to drop, redirect or rewrite requests having bad HTTP patternsBandwidth management and traffic shaping, in order to smooth down peaks in network usageDeploying web cache techniquesUsing different domain names or IP addresses to serve different (static and dynamic) content by separate web servers, e.g.:http://images.example.comhttp://example.comUsing different domain names or computers to separate big files from small and medium-sized files; the idea is to be able to fully cache small and medium-sized files and to efficiently serve big or huge (over 10 – 1000 MB) files by using different settingsUsing many internet servers (programs) per computer, each one bound to its own network card and IP addressUsing many internet servers (computers) that are grouped together behind a load balancer so that they act or are seen as one big web serverAdding more hardware resources (i.e. RAM, disks) to each computerTuning OS parameters for hardware capabilities and usageUsing more efficient computer programs for web servers, etc.Using other workarounds, especially if dynamic content is involved== Market share ===== February 2019 ===Below are the latest statistics of the market share of all sites of the top web servers on the Internet by W3TechsUsage of Web Servers for Websites.All other web servers are used by less than 1% of the websites.=== July 2018 ===Below are the latest statistics of the market share of all sites of the top web servers on the Internet by W3TechsUsage of Web Servers for Websites.All other web servers are used by less than 1% of the websites.=== February 2017 ===Below are the latest statistics of the market share of all sites of the top web servers on the Internet by NetcraftFebruary 2017 Web Server Survey.=== February 2016 ===Below are the latest statistics of the market share of all sites of the top web servers on the Internet by NetcraftFebruary 2016 Web Server Survey.Apache, IIS and Nginx are the most used web servers on the World Wide Web.== See also ==Server (computing)Application serverComparison of web server softwareHTTP compressionOpen source web applicationServer Side Includes, Common Gateway Interface, Simple Common Gateway Interface, FastCGI, PHP, Java Servlet, JavaServer Pages, Active Server Pages, ASP.NET, and Server Application Programming InterfaceVariant objectVirtual hostingWeb hosting serviceWeb containerWeb proxyWeb service== References ==== External links ==RFC 2616, the Request for Comments document that defines the HTTP 1.1 protocol
	The World Wide Web Consortium (W3C) is the main international standards organization for the World Wide Web (abbreviated WWW or W3). Founded and currently led by Tim Berners-Lee, the consortium is made up of member organizations which maintain full-time staff for the purpose of working together in the development of standards for the World Wide Web. As of 21 October 2019, the World Wide Web Consortium (W3C) has 443 members. The W3C also engages in education and outreach, develops software and serves as an open forum for discussion about the Web.== History ==The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee after he left the European Organization for Nuclear Research (CERN) in October, 1994. It was founded at the Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) with support from the European Commission and the Defense Advanced Research Projects Agency (DARPA), which had pioneered the ARPANET, one of the predecessors to the Internet. It was located in Technology Square until 2004, when it moved, with CSAIL, to the Stata Center.The organization tries to foster compatibility and agreement among industry members in the adoption of new standards defined by the W3C. Incompatible versions of HTML are offered by different vendors, causing inconsistency in how web pages are displayed. The consortium tries to get all those vendors to implement a set of core principles and components which are chosen by the consortium.It was originally intended that CERN host the European branch of W3C; however, CERN wished to focus on particle physics, not information technology. In April 1995, the French Institute for Research in Computer Science and Automation (INRIA) became the European host of W3C, with Keio University Research Institute at SFC (KRIS) becoming the Asian host in September 1996. Starting in 1997, W3C created regional offices around the world. As of September 2009, it had eighteen World Offices covering Australia, the Benelux countries (Netherlands, Luxembourg, and Belgium), Brazil, China, Finland, Germany, Austria, Greece, Hong Kong, Hungary, India, Israel, Italy, South Korea, Morocco, South Africa, Spain, Sweden, and, as of 2016, the United Kingdom and Ireland.In October 2012, W3C convened a community of major web players and publishers to establish a MediaWiki wiki that seeks to document open web standards called the WebPlatform and WebPlatform Docs.In January 2013, Beihang University became the Chinese host.== Specification maturation ==Sometimes, when a specification becomes too large, it is split into independent modules which can mature at their own pace. Subsequent editions of a module or specification are known as levels and are denoted by the first integer in the title (e.g. CSS3 = Level 3). Subsequent revisions on each level are denoted by an integer following a decimal point (for example, CSS2.1 = Revision 1).The W3C standard formation process is defined within the W3C process document, outlining four maturity levels through which each new standard or recommendation must progress.=== Working draft (WD) ===After enough content has been gathered from 'editor drafts' and discussion, it may be published as a working draft (WD) for review by the community. A WD document is the first form of a standard that is publicly available. Commentary by virtually anyone is accepted, though no promises are made with regard to action on any particular element commented upon.At this stage, the standard document may have significant differences from its final form. As such, anyone who implements WD standards should be ready to significantly modify their implementations as the standard matures.=== Candidate recommendation (CR) ===A candidate recommendation is a version of a standard that is more mature than the WD. At this point, the group responsible for the standard is satisfied that the standard meets its goal. The purpose of the CR is to elicit aid from the development community as to how implementable the standard is.The standard document may change further, but at this point, significant features are mostly decided. The design of those features can still change due to feedback from implementors.=== Proposed recommendation (PR) ===A proposed recommendation is the version of a standard that has passed the prior two levels. The users of the standard provide input. At this stage, the document is submitted to the W3C Advisory Council for final approval.While this step is important, it rarely causes any significant changes to a standard as it passes to the next phase.=== W3C recommendation (REC) ===This is the most mature stage of development. At this point, the standard has undergone extensive review and testing, under both theoretical and practical conditions. The standard is now endorsed by the W3C, indicating its readiness for deployment to the public, and encouraging more widespread support among implementors and authors.Recommendations can sometimes be implemented incorrectly, partially, or not at all, but many standards define two or more levels of conformance that developers must follow if they wish to label their product as W3C-compliant.=== Later revisions ===A recommendation may be updated or extended by separately-published, non-technical errata or editor drafts until sufficient substantial edits accumulate for producing a new edition or level of the recommendation. Additionally, the W3C publishes various kinds of informative notes which are to be used as references.=== Certification ===Unlike the ISOC and other international standards bodies, the W3C does not have a certification program. The W3C has decided, for now, that it is not suitable to start such a program, owing to the risk of creating more drawbacks for the community than benefits.== Administration ==The Consortium is jointly administered by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL, located in Stata Center) in the United States, the European Research Consortium for Informatics and Mathematics (ERCIM) (in Sophia Antipolis, France), Keio University (in Japan) and Beihang University (in China). The W3C also has World Offices in eighteen regions around the world. The W3C Offices work with their regional web communities to promote W3C technologies in local languages, broaden the  W3C's geographical base and encourage international participation in W3C Activities.The W3C has a staff team of 70–80 worldwide as of 2015. W3C is run by a management team which allocates resources and designs strategy, led by CEO Jeffrey Jaffe (as of March 2010), former CTO of Novell. It also includes an advisory board which supports in strategy and legal matters and helps resolve conflicts. The majority of standardization work is done by external experts in the W3C's various working groups.== Membership ==The Consortium is governed by its membership. The list of members is available to the public. Members include businesses, nonprofit organizations, universities, governmental entities, and individuals.Membership requirements are transparent except for one requirement: An application for membership must be reviewed and approved by the W3C. Many guidelines and requirements are stated in detail, but there is no final guideline about the process or standards by which membership might be finally approved or denied.The cost of membership is given on a sliding scale, depending on the character of the organization applying and the country in which it is located. Countries are categorized by the World Bank's most recent grouping by GNI ("Gross National Income") per capita.== Criticism ==In 2012 and 2013, the W3C started considering adding DRM-specific Encrypted Media Extensions (EME) to HTML5, which was criticised as being against the openness, interoperability, and vendor neutrality that distinguished websites built using only W3C standards from those requiring proprietary plug-ins like Flash.On September 18, 2017, the W3C published the EME specification as a Recommendation, leading to the Electronic Frontier Foundation's resignation from W3C.== Standards ==W3C/IETF standards (over Internet protocol suite):== References ==== External links ==About the World Wide Web ConsortiumW3C Technical Reports and PublicationsW3C Process DocumentW3C HistoryHow to read W3C specs
	CSE HTML Validator, now renamed to CSS HTML Validator, is an HTML editor and CSS editor for Windows (and Linux when used with Wine (software)) that helps web developers create syntactically correct and accessible HTML, XHTML, and CSS documents (including HTML5 and CSS3) by locating errors, potential problems, and common mistakes. It is also able to check links, suggest improvements, alert developers to deprecated, obsolete, or proprietary tags, attributes, and CSS properties, and find issues that can affect search engine optimization.CSS HTML Validator is developed, marketed, and sold by AI Internet Solutions LLC located in Texas. The first version of CSS HTML Validator was released in 1997 for Windows 95. The current version is 2019/v19.01 (as of June 11, 2019). There are four major editions of CSS HTML Validator — Enterprise, Professional, Standard, and Lite. While the application is generally a commercial product (except for the lite edition), a free version of the standard edition is available for personal, non-commercial use.== Features ==CSS HTML Validator includes an HTML editor, validator for HTML, XHTML, polyglot markup, CSS, PHP and JavaScript (using JSLint or JSHint), link checker (to find dead and broken links), spell checker, accessibility checker, and search engine optimization (SEO) checker. An integrated web browser allows developers to browse the web while the pages are automatically validated.Because documents are checked locally and not uploaded over the Internet to a server in order to be checked, validations are performed relatively quickly, and security and privacy are increased.A Batch Wizard tool, included in some editions of the software, can check entire Web sites, parts of Web sites, or a list of local web documents. The Batch Wizard generates reports in HTML or XML format. The reports can be viewed using a standard web browser.The accessibility checker includes support for Section 508 Amendment to the Rehabilitation Act of 1973 and Web Content Accessibility Guidelines (both WCAG 1.0 and WCAG 2.0/2.1). Current versions also provide support for Canada's Common Look and Feel (CLF), including CLF 2.0.Using a version of HTML Tidy with HTML5 support and the Format and Fix Tool, CSS HTML Validator can automatically fix some common problems with HTML and XHTML documents. However, some problems cannot be fixed (or fixed correctly) with automated tools and require manual review and repair.== Version history ==Validation of polyglot markup was added in CSE HTML Validator v12, and mobile development support (for HTML & CSS) was added in CSE HTML Validator v14 and improved in v15. Version 15 added built-in syntax checking for JSON and HTML5 cache manifest files. Version 16 added JavaScript linting using JSHint, a static code analysis tool for checking JavaScript, but also continues to support JSLint. Version 17 added support for the Accelerated Mobile Pages Project, which is a type of HTML optimized for mobile web browsing, and support for live DOM validation using Firefox and an add-on.Version 18 renamed the software to CSS HTML Validator 2018 and includes updated HTML5 and CSS support. Version 18 also added a new "By Message" report in the Batch Wizard and dropped support for Windows Vista and below.CSS HTML Validator 2019/v19 includes further HTML and CSS updates, adds WCAG 2.1 support, improves support when running under Wine (software), and is now a native 64-bit application.An online edition based on CSS HTML Validator Pro that can check documents via file upload, URL, or snippets (direct text input) was discontinued May 2017 in favor of the desktop version for Microsoft Windows.== Purpose of validation ==The purpose of validation and computerized checking of HTML, XHTML, and CSS documents is to help make sure that the documents are syntactically correct and problem-free. Checked HTML, XHTML, and CSS documents are more likely to:render faster and with fewer problems (user agents don't have to "figure out" and decipher bad syntax)cause browsers and user agents to build a more consistent Document Object Model, which is important for CSS and JavaScriptbe forward-compatible with future versions of user agents and browsers ("future-proof")be compatible with current and future HTML, XHTML, and CSS specificationsrender as intended on a variety of user agents, including mobile devicescause fewer problems for visitorsbe more accessible for people with disabilities (such as blindness), as well as users in generalnot contain dead, broken, or rotting linksWhile automated checking tools are helpful for website development and maintenance, they cannot guarantee that a document will display as intended in all browsers. Developers should always test documents in a variety of browsers (including mobile browsers) to locate problems that cannot be detected with a computerized checking tool.== Differences from other HTML validators ==CSS HTML Validator is an offline desktop app for Microsoft Windows that does not require an Internet connection. The offline nature of the app is in contrast to online services like the W3C Markup Validation Service.Checks HTML/XHTML syntax, CSS, links, spelling, accessibility, JavaScript, SEO, and PHP with one pass, while DTD-based validators are often more limited, checking mostly for basic tag structure and allowable attributes.Includes a DTD-based validator which can optionally be used for checking DTD-based versions of HTML (versions prior to HTML5), however one of CSS HTML Validator's primary differences is that its custom validation engine can perform more checks on a document than a DTD-based validator can. This is because DTD-based validators are limited to checking only what can be specified in a Document Type Definition.== Integration ==CSS HTML Validator integrates with other 3rd party software like those listed below. This allows validation using CSS HTML Validator from within the 3rd party program.EmEditorHTML-KitHomeSiteMicrosys A1 Website AnalyzerPhpEDTextPadTopStyle - includes link checking support== References ==== External links ==Official website
	WebGL (Web Graphics Library) is a JavaScript API for rendering interactive 2D and 3D graphics within any compatible web browser without the use of plug-ins. WebGL is fully integrated with other web standards, allowing GPU-accelerated usage of physics and image processing and effects as part of the web page canvas. WebGL elements can be mixed with other HTML elements and composited with other parts of the page or page background. WebGL programs consist of control code written in JavaScript and shader code that is written in OpenGL ES Shading Language (ESSL), a language similar to C or C++, and is executed on a computer's graphics processing unit (GPU).WebGL is designed and maintained by the non-profit Khronos Group.== Design ==WebGL 1.0 is based on OpenGL ES 2.0 and provides an API for 3D graphics. It uses the HTML5 canvas element and is accessed using Document Object Model (DOM) interfaces.WebGL 2.0 is based on OpenGL ES 3.0 and made guaranteed availability of many optional extensions of WebGL 1.0 and exposes new APIs.Automatic memory management is provided implicitly by JavaScript.Like OpenGL ES 2.0, WebGL does not have the fixed-function APIs introduced in OpenGL 1.0 and deprecated in OpenGL 3.0. This functionality, if so required, has to be implemented by the end-developer by providing shader code and configuring data bindings in JavaScript.Shaders in WebGL are expressed directly in GLSL and passed to the WebGL API as textual strings. The WebGL implementation compiles these shader instructions to GPU code. This code is executed for each and every vertex sent through the API and for each pixel rasterized to the screen.== History ==WebGL evolved out of the Canvas 3D experiments started by Vladimir Vukićević at Mozilla. Vukićević first demonstrated a Canvas 3D prototype in 2006. By the end of 2007, both Mozilla and Opera had made their own separate implementations.In early 2009, the non-profit technology consortium Khronos Group started the WebGL Working Group, with initial participation from Apple, Google, Mozilla, Opera, and others. Version 1.0 of the WebGL specification was released March 2011. As of March 2012, the chair of the working group is Ken Russell.Early applications of WebGL include Zygote Body. In November 2012 Autodesk announced that they ported most of their applications to the cloud running on local WebGL clients. These applications included Fusion 360 and AutoCAD 360.Development of the WebGL 2 specification started in 2013 with final in January 2017. This specification is based on OpenGL ES 3.0.First implementations are in Firefox 51, Chrome 56 and Opera 43.== Implementations ===== Almost Native Graphics Layer Engine ===Almost Native Graphics Layer Engine (ANGLE) is an open source graphic engine which implements WebGL 1.0 (2.0 which closely conforms to ES 3.0) and OpenGL ES 2.0 and 3.0 standards. It is a default backend for both Google Chrome and Mozilla Firefox on Windows platforms and works by translating WebGL and OpenGL calls to available platform-specific APIs. ANGLE currently provides access to OpenGL ES 2.0 and 3.0 to desktop OpenGL, OpenGL ES, Direct3D 9, and Direct3D 11 APIs. ″[Google] Chrome uses ANGLE for all graphics rendering on Windows, including the accelerated Canvas2D implementation and the Native Client sandbox environment.″== Software ==WebGL is widely supported in modern browsers. However its availability is dependent on other factors like the GPU supporting it. The official WebGL website offers a simple test page. More detailed information (like what renderer the browser uses, and what extensions are available) is provided at third-party websites.=== Desktop browsers ===Google Chrome – WebGL 1.0 has been enabled on all platforms that have a capable graphics card with updated drivers since version 9, released in February 2011. By default on Windows, Chrome uses the ANGLE (Almost Native Graphics Layer Engine) renderer to translate OpenGL ES to Direct X 9.0c or 11.0, which have better driver support. On Linux and Mac OS X the default renderer is OpenGL however. It is also possible to force OpenGL as the renderer on Windows. Since September 2013, Chrome also has a newer Direct3D 11 renderer, which however requires a newer graphics card. Chrome 56+ support WebGL 2.0.Mozilla Firefox – WebGL 1.0 has been enabled on all platforms that have a capable graphics card with updated drivers since version 4.0. Since 2013 Firefox also uses DirectX on the Windows platform via ANGLE. Firefox 51+ support WebGL 2.0.Safari – Safari 6.0 and newer versions installed on OS X Mountain Lion, Mac OS X Lion and Safari 5.1 on Mac OS X Snow Leopard implemented support for WebGL 1.0, which was disabled by default before Safari 8.0. Safari version 12 (available in MacOS Mojave) has available support for WebGL 2.0, currently as an "Experimental" feature.Opera – WebGL 1.0 has been implemented in Opera 11 and 12, although was disabled by default in 2014. Opera 43+ support WebGL 2.0.Internet Explorer – WebGL 1.0 is partially supported in Internet Explorer 11. It initially failed the majority of official WebGL conformance tests, but Microsoft later released several updates. The latest 0.94 WebGL engine currently passes ~97% of Khronos tests. WebGL support can also be manually added to earlier versions of Internet Explorer using third-party plugins such as IEWebGL.Microsoft Edge – The initial stable release supports WebGL version 0.95 (context name: "experimental-webgl") with an open source GLSL to HLSL transpiler. Version 10240+ supports WebGL 1.0 as prefixed. WebGL 2.0 is planned with medium priority in future releases.=== Mobile browsers ===BlackBerry 10 – WebGL 1.0 is available for BlackBerry devices since OS version 10.00BlackBerry PlayBook – WebGL 1.0 is available via WebWorks and browser in PlayBook OS 2.00Android Browser – Basically unsupported, but the Sony Ericsson Xperia range of Android smartphones have had WebGL capabilities following a firmware upgrade. Samsung smartphones also have WebGL enabled (verified on Galaxy SII (4.1.2) and Galaxy Note 8.0 (4.2)). Supported in Google Chrome that replaced Android browser in many phones (but is not a new standard Android Browser).Internet Explorer – Prefixed WebGL 1.0 is available on Windows Phone 8.x (11+)Firefox for mobile – WebGL 1.0 is available for Android and MeeGo devices since Firefox 4.Firefox OSGoogle Chrome – WebGL 1.0 is available for Android devices since Google Chrome 25 and enabled by default since version 30.Maemo – In Nokia N900, WebGL 1.0 is available in the stock microB browser from the PR1.2 firmware update onwards.MeeGo – WebGL 1.0 is unsupported in the stock browser "Web."  However, it is available through Firefox.Microsoft Edge – Prefixed WebGL 1.0 is available on Windows 10 Mobile.Opera Mobile – Opera Mobile 12 supports WebGL 1.0 (on Android only).Sailfish OS – WebGL 1.0 is supported in the default Sailfish browser.Tizen – WebGL 1.0 is supportediOS – WebGL 1.0 is available for mobile Safari, in iOS 8.== Tools and ecosystem ===== Utilities ===The low-level nature of the WebGL API, which provides little on its own to produce desirable 3D graphics quickly, contributed to creation of libraries which are typically used to build things up in 3D graphics (e.g. view transformations for shaders, view frustum etc.). Basic tasks such as loading scene graphs and 3D objects in the popular industry formats is also abstracted by the libraries (some of which were sometime simply ported to JavaScript from other languages) to provide additional functionality. A non-exhaustive list of libraries that provide many high-level features includes A-Frame (VR), BabylonJS, PlayCanvas, three.js, OSG.JS and CopperLicht. X3D also made a project called X3DOM to make X3D and VRML content running on WebGL. The 3D model will in XML tag <X3D> in HTML5 and interactive script will use JavaScript and DOM. BS Content Studio and InstantReality X3D exporter can exported X3D in HTML and running by WebGL.=== Game engines ===There also has been a rapid emergence of game engines for WebGL, both 2D and 3D, including Unreal Engine 4 and Unity. The Stage3D/Flash-based Away3D high-level library also has a port to WebGL via TypeScript. A more light-weight utility library that provides just the vector and matrix math utilities for shaders is sylvester.js. It is sometimes used in conjunction with a WebGL specific extension called glUtils.js.There are also some 2D libraries built on top of WebGL like Cocos2d-x or Pixi.js, which were implemented this way for performance reasons, in a move that parallels what happened with the Starling Framework over Stage3D in the Flash world. The WebGL-based 2D libraries fall back to HTML5 canvas when WebGL is not available.Removing the rendering bottleneck by giving almost direct access to the GPU also exposed performance limitations in the JavaScript implementations. Some were addressed by asm.js and WebAssembly (similarly, the introduction of Stage3D exposed performance problems within ActionScript, which were addressed by projects like CrossBridge.)=== Content creation ===Like for any other graphics API creating content for WebGL scenes requires using a regular 3D content creation tool and exporting the scene to a format that is readable by the viewer or helper library. Desktop 3D authoring software such as Blender, Autodesk Maya or SimLab Composer can be used for this purpose. Particularly, Blend4Web allows a WebGL scene to be authored entirely in Blender and exported to a browser with a single click, even as a standalone web page. There are also some WebGL-specific software such as CopperCube and the online WebGL-based editor Clara.io. Online platforms such as Sketchfab and Clara.io allow users to directly upload their 3D models and display them using a hosted WebGL viewer.=== Environment based tools ===Additionally, Mozilla Foundation, in its Firefox browser, has implemented built-in WebGL tools starting with version 27 that allow editing vertex and fragment shaders. A number of other debugging and profiling tools have also emerged.== See also ==List of WebGL frameworksExperience Curiosity – WebGL simulation of the Mars rover CuriosityWebVRJava OpenGL – OpenGL library for the Java programming languageWebGPU== References ==== Further reading ==Mwalongo, Finian; Krone, Michael;  et al. (August 2014). "Visualization of molecular structures using state-of-the-art techniques in WebGL". Proceedings of the 19th International ACM Conference on 3D Web Technologies. International Conference on 3D Web Technology. Vancouver. pp. 133–141. doi:10.1145/2628588.2628597. ISBN 978-1-4503-3015-2. Retrieved 24 January 2017.== External links ==Official website WebGL at the Mozilla Developer Network
	The World Wide Web has become a major delivery platform for a variety of complex and sophisticated enterprise applications in several domains. In addition to their inherent multifaceted functionality, these Web applications exhibit complex behaviour and place some unique demands on their usability, performance, security, and ability to grow and evolve. However, a vast majority of these applications continue to be developed in an ad-hoc way, contributing to problems of usability, maintainability, quality and reliability. While Web development can benefit from established practices from other related disciplines, it has certain distinguishing characteristics that demand special considerations. In recent years, there have been developments towards addressing these considerations.Web engineering focuses on the methodologies, techniques, and tools that are the foundation of Web application development and which support their design, development, evolution, and evaluation. Web application development has certain characteristics that make it different from traditional software, information system, or computer application development.Web engineering is multidisciplinary and encompasses contributions from diverse areas: systems analysis and design, software engineering, hypermedia/hypertext engineering, requirements engineering, human-computer interaction, user interface, information engineering, information indexing and retrieval, testing, modelling and simulation, project management, and graphic design and presentation. Web engineering is neither a clone nor a subset of software engineering, although both involve programming and software development. While Web Engineering uses software engineering principles, it encompasses new approaches, methodologies, tools, techniques, and guidelines to meet the unique requirements of Web-based applications.== As a discipline ==Proponents of Web engineering supported the establishment of Web engineering as a discipline at an early stage of Web. Major arguments for Web engineering as a new discipline are:Web-based Information Systems (WIS) development process is different and unique.Web engineering is multi-disciplinary; no single discipline (such as software engineering) can provide complete theory basis, body of knowledge and practices to guide WIS development.Issues of evolution and lifecycle management when compared to more 'traditional' applications.Web-based information systems and applications are pervasive and non-trivial. The prospect of Web as a platform will continue to grow and it is worth being treated specifically.However, it has been controversial, especially for people in other traditional disciplines such as software engineering, to recognize Web engineering as a new field. The issue is how different and independent Web engineering is, compared with other disciplines.Main topics of Web engineering include, but are not limited to, the following areas:=== Modeling disciplines ===Business Processes for Applications on the WebProcess Modelling of Web applicationsRequirements Engineering for Web applicationsB2B applications=== Design disciplines, tools, and methods ===UML and the WebConceptual Modeling of Web Applications (aka. Web modeling)Prototyping Methods and ToolsWeb design methodsCASE Tools for Web ApplicationsWeb Interface DesignData Models for Web Information Systems=== Implementation disciplines ===Integrated Web Application Development EnvironmentsCode Generation for Web ApplicationsSoftware Factories for/on the WebWeb 2.0, AJAX, E4X, ASP.NET, PHP and Other New DevelopmentsWeb Services Development and Deployment=== Testing disciplines ===Testing and Evaluation of Web systems and Applications.Testing Automation, Methods, and Tools.=== Applications categories disciplines ===Semantic Web applicationsDocument centric Web sitesTransactional Web applicationsInteractive Web applicationsWorkflow-based Web applicationsCollaborative Web applicationsPortal-oriented Web applicationsUbiquitous and Mobile Web ApplicationsDevice Independent Web DeliveryLocalization and Internationalization of Web ApplicationsPersonalization of Web Applications== Attributes ===== Web quality ===Web Metrics, Cost Estimation, and MeasurementPersonalisation and Adaptation of Web applicationsWeb QualityUsability of Web ApplicationsWeb accessibilityPerformance of Web-based applications=== Content-related ===Web Content ManagementContent Management System (CMS)Multimedia Authoring Tools and SoftwareAuthoring of adaptive hypermedia== Education ==Master of Science: Web Engineering as a branch of study within the MSc program Web Sciences at the Johannes Kepler University Linz, Austria Diploma in Web Engineering: Web Engineering as a study program at the International Webmasters College (iWMC), Germany == See also ==DevOpsWeb developerWeb modeling== References ==== Sources ==Robert L. Glass, "Who's Right in the Web Development Debate?" Cutter IT Journal, July 2001, Vol. 14, No.7, pp 6–0.S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera. "Designing Data-Intensive Web Applications". Morgan Kaufmann Publisher, Dec 2002, ISBN 1-55860-843-5=== Web engineering resources ===OrganizationsInternational Society for Web Engineering e.V.: http://www.iswe-ev.de/Web Engineering Community: http://www.webengineering.orgWISE Society: http://www.wisesociety.org/ACM SIGWEB: http://www.acm.org/sigwebWorld Wide Web Consortium: http://www.w3.orgBooks"Engineering Web Applications", by Sven Casteleyn, Florian Daniel, Peter Dolog and Maristella Matera, Springer, 2009, ISBN 978-3-540-92200-1"Web Engineering: Modelling and Implementing Web Applications", edited by Gustavo Rossi, Oscar Pastor, Daniel Schwabe and Luis Olsina, Springer Verlag HCIS, 2007, ISBN 978-1-84628-922-4"Cost Estimation Techniques for Web Projects", Emilia Mendes, IGI Publishing, ISBN 978-1-59904-135-3"Web Engineering - The Discipline of Systematic Development of Web Applications", edited by Gerti Kappel, Birgit Pröll, Siegfried Reich, and Werner Retschitzegger, John Wiley & Sons, 2006"Web Engineering", edited by Emilia Mendes and Nile Mosley, Springer-Verlag, 2005"Web Engineering: Principles and Techniques", edited by Woojong Suh, Idea Group Publishing, 2005"Form-Oriented Analysis -- A New Methodology to Model Form-Based Applications", by Dirk Draheim, Gerald Weber, Springer, 2005"Building Web Applications with UML" (2nd edition), by Jim Conallen, Pearson Education, 2003"Information Architecture for the World Wide Web" (2nd edition), by Peter Morville and Louis Rosenfeld, O'Reilly, 2002"Web Site Engineering: Beyond Web Page Design", by Thomas A. Powell, David L. Jones and Dominique C. Cutts, Prentice Hall, 1998"Designing Data-Intensive Web Applications", by S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera. Morgan Kaufmann Publisher, Dec 2002, ISBN 1-55860-843-5ConferencesWorld Wide Web Conference (by IW3C2, since 1994): http://www.iw3c2.orgInternational Conference on Web Engineering (ICWE) (since 2000)2018: http://icwe2018.webengineering.org/ (Caceres, Spain)2017: http://icwe2017.webengineering.org/ (Rome, Italy)2016: http://icwe2016.webengineering.org/ (Lugano, Switzerland)2007: http://www.icwe2007.org/2006: http://www.icwe2006.org2005: http://www.icwe2005.org2004: http://www.icwe2004.orgICWE Conference ProceedingsICWE2007: LNCS 4607 https://www.springer.com/computer/database+management+&+information+retrieval/book/978-3-540-73596-0ICWE2005: LNCS 3579 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-58872076-0ICWE2004: LNCS 3140 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-32445543-0ICWE2003: LNCS 2722 https://www.springer.com/east/home/generic/search/results?SGWID=5-40109-22-3092664-0Web Information Systems Engineering Conference (by WISE Society, since 2000): http://www.wisesociety.org/International Conference on Web Information Systems and Technologies (Webist) (since 2005): http://www.webist.org/International Workshop on Web Site Evolution (WSE): http://www.websiteevolution.org/International Conference on Software Engineering: http://www.icse-conferences.org/Book chapters and articlesPressman, R.S., 'Applying Web Engineering', Part 3, Chapters 16-20, in Software Engineering: A Practitioner's Perspective, Sixth Edition, McGraw-Hill, New York, 2004. http://www.rspa.com/'JournalsJournal of Web Engineering: http://www.rintonpress.com/journals/jwe/International Journal of Web Engineering and Technology: http://www.inderscience.com/browse/index.php?journalID=48ACM Transactions on Internet Technology: http://toit.acm.org/World Wide Web (Springer): https://link.springer.com/journal/11280Web coding journal: http://www.web-code.org/Special issuesWeb Engineering, IEEE MultiMedia, Jan.–Mar. 2001 (Part 1) and April–June 2001 (Part 2).  http://csdl2.computer.org/persagen/DLPublication.jsp?pubtype=m&acronym=muUsability Engineering, IEEE Software, January–February 2001.Web Engineering, Cutter IT Journal, 14(7), July 2001.*Testing E-business Applications, Cutter IT Journal, September 2001.Engineering Internet Software, IEEE Software, March–April 2002.Usability and the Web, IEEE Internet Computing, March–April 2002.
	WebRTC (Web Real-Time Communication) is a free, open-source project that provides web browsers and mobile applications with real-time communication (RTC) via simple application programming interfaces (APIs). It allows audio and video communication to work inside web pages by allowing direct peer-to-peer communication, eliminating the need to install plugins or download native apps. Supported by Apple, Google, Microsoft, Mozilla, and Opera, WebRTC is being standardized through the World Wide Web Consortium (W3C) and the Internet Engineering Task Force (IETF).Its mission is to "enable rich, high-quality RTC applications to be developed for the browser, mobile platforms, and IoT devices, and allow them all to communicate via a common set of protocols". The reference implementation is released as free software under the terms of a BSD license. OpenWebRTC provides another free implementation based on the multimedia framework GStreamer. JavaScript inventor Brendan Eich called it a "new front in the long war for an open and unencumbered web".== History ==In May 2010, Google bought Global IP Solutions or GIPS, a VoIP and videoconferencing software company that had developed many components required for RTC, such as codecs and echo cancellation techniques. Google open-sourced the GIPS technology and engaged with relevant standards bodies at the IETF and W3C to ensure industry consensus. In May 2011, Google released an open-source project for browser-based real-time communication known as WebRTC. This has been followed by ongoing work to standardize the relevant protocols in the IETF and browser APIs in the W3C.In May 2011, Ericsson Labs built the first implementation of WebRTC using a modified WebKit library. In October 2011, the W3C published its first draft for the spec. WebRTC milestones include the first cross-browser video call (February 2013), first cross-browser data transfers (February 2014), and as of July 2014 Google Hangouts was "kind of" using WebRTC.The W3C draft API was based on preliminary work done in the WHATWG. It was referred to as the ConnectionPeer API, and a pre-standards concept implementation was created at Ericsson Labs. The WebRTC Working Group expects this specification to evolve significantly based on:Outcomes of ongoing exchanges in the companion RTCWEB group at IETF to define the set of protocols that, together with this document, define real-time communications in web browsers. While no one signaling protocol is mandated, SIP over WebSockets (RFC 7118) is often used partially due to the applicability of SIP to most of the envisaged communication scenarios as well as the availability of open-source software such as JsSIP.Privacy issues that arise when exposing local capabilities and local streamsTechnical discussions within the group, on implementing data channels in particularExperience gained through early experimentationFeedback from other groups and individualsIn November 2017, the WebRTC 1.0 specification transitioned from Working Draft to Candidate Recommendation.== Overview ===== Design ===Major components of WebRTC include several JavaScript APIs:getUserMedia acquires the audio and video media (e.g., by accessing a device's camera and microphone).RTCPeerConnection enables audio and video communication between peers. It performs signal processing, codec handling, peer-to-peer communication, security, and bandwidth management.RTCDataChannel allows bidirectional communication of arbitrary data between peers. It uses the same API as WebSockets and has very low latency.The WebRTC API also includes a statistics function:getStats allows the web application to retrieve a set of statistics about WebRTC sessions. These statistics data are being described in a separate W3C document.RFC 7874 requires implementations to provide PCMA/PCMU (RFC 3551), Telephone Event as DTMF (RFC 4733), and Opus (RFC 6716) audio codecs as minimum capabilities. The PeerConnection, data channel and media capture browser APIs are detailed in the W3C.W3C is developing ORTC (Object Real-Time Communications) for WebRTC.=== Examples ===Although initially developed for web browsers, WebRTC has applications for non-browser devices, including mobile platforms and IoT devices. Examples include browser-based VoIP telephony, also called cloud phones or web phones, which allow calls to be made and received from within a web browser, replacing the requirement to download and install a softphone.=== Support ===WebRTC is supported by the following browsers:Desktop PCMicrosoft Edge 12+Google Chrome 28+Mozilla Firefox 22+Safari 11+Opera 18+Vivaldi 1.9+AndroidGoogle Chrome 28+ (enabled by default since 29)Mozilla Firefox 24+Opera Mobile 12+Chrome OSFirefox OSBlackBerry 10iOSMobileSafari/WebKit (iOS 11+)Tizen 3.0Support was not included in Internet Explorer prior to its final feature release in October 2013, but 3rd party plugins are available to add the support of WebRTC to Internet Explorer and Safari for macOS. At WWDC 2017, Apple announced Safari would get WebRTC support in Safari 11, and it was made available in release 32 of the Safari Technology Preview.== Concerns ==In January 2015, TorrentFreak reported a serious security flaw in browsers that support WebRTC, saying that it compromised the security of VPN tunnels, by exposing the true IP address of a user. The IP address read requests are not visible in the browser's developer console, and they are not blocked by most ad blocking/privacy/security add-ons, enabling online tracking by advertisers and other entities despite precautions (however the uBlock Origin add-on can fix this problem). As of September 2019 this WebRTC flaw still surfaces on Firefox 69.x and still by default exposes your internal IP address to the web. == See also ==Global IP Solutions (GIPS)Real-time Transport Protocol (RTP)Session Description Protocol (SDP)WebRTC Gateway== References ==== Further reading ==Proust, S., ed. (May 2016). Additional WebRTC Audio Codecs for Interoperability. IETF. doi:10.17487/RFC7875. RFC 7875. Retrieved 2016-10-12.Valin, J. M.; Bran, C. (May 2016). WebRTC Audio Codec and Processing Requirements. IETF. doi:10.17487/RFC7874. RFC 7874. Retrieved 2016-10-12.Roach, A. B. (March 2016). WebRTC Video Processing and Codec Requirements. IETF. doi:10.17487/RFC7742. RFC 7742. Retrieved 2016-10-12.Perumal, M.; Wing, D.; Ravindranath, R.; Reddy, T.; Thomson, M. (October 2015). Session Traversal Utilities for NAT (STUN) Usage for Consent Freshness. IETF. doi:10.17487/RFC7675. RFC 7675. Retrieved 2016-10-12.Holmberg, C.; Hakansson, S.; Eriksson, G. (March 2015). Web Real-Time Communication Use Cases and Requirements. IETF. doi:10.17487/RFC7478. RFC 7478. Retrieved 2016-10-12.== External links ==Official website W3C Web Real-Time Communications Working GroupIETF Real-Time Communication in WEB-browsers (rtcweb) Working GroupVideo chat demo app based on WebRTC
	Comet is a web application model in which a long-held HTTPS request allows a web server to push data to a browser, without the browser explicitly requesting it. Comet is an umbrella term, encompassing multiple techniques for achieving this interaction. All these methods rely on features included by default in browsers, such as JavaScript, rather than on non-default plugins. The Comet approach differs from the original model of the web, in which a browser requests a complete web page at a time.The use of Comet techniques in web development predates the use of the word Comet as a neologism for the collective techniques. Comet is known by several other names, includingAjax Push,Reverse Ajax, Two-way-web, HTTP Streaming, andHTTP server pushamong others. The term Comet is not an acronym, but was coined by Alex Russell in his 2006 blog post Comet: Low Latency Data for the Browser.== History ===== Early Java applets ===The ability to embed Java applets into browsers (starting with Netscape 2.0 in March 1996) made two-way sustained communications possible, using a raw TCP socket to communicate between the browser and the server. This socket can remain open as long as the browser is at the document hosting the applet. Event notifications can be sent in any format –  text or binary –  and decoded by the applet.=== The first browser-to-browser communication framework ===The very first application using browser-to-browser communications was Tango Interactive, implemented in 1996–98 at the Northeast Parallel Architectures Center (NPAC) at Syracuse University using DARPA funding. TANGO architecture has been patented by Syracuse University. TANGO framework has been extensively used as a distance education tool. The framework has been commercialized by CollabWorx and used in a dozen or so Command&Control and Training applications in the United States Department of Defense.=== First Comet applications ===The first set of Comet implementations date back to 2000, with the Pushlets, Lightstreamer, and KnowNow projects. Pushlets, a framework created by Just van den Broecke, was one of the first open source implementations. Pushlets were based on server-side Java servlets, and a client-side JavaScript library. Bang Networks –  a Silicon Valley start-up backed by Netscape co-founder Marc Andreessen –  had a lavishly-financed attempt to create a real-time push standard for the entire web.In April 2001, Chip Morningstar began developing a Java-based (J2SE) web server which used two HTTP sockets to keep open two communications channels between the custom HTTP server he designed and a client designed by Douglas Crockford; a functioning demo system existed as of June 2001. The server and client used a messaging format that the founders of State Software, Inc. assented to coin as JSON following Crockford's suggestion. The entire system, the client libraries, the messaging format known as JSON and the server, became the State Application Framework, parts of which were sold and used by Sun Microsystems, Amazon.com, EDS and Volkswagen.In March 2006, software engineer Alex Russell coined the term Comet in a post on his personal blog. The new term was a play on Ajax (Ajax and Comet both being common household cleaners in the USA).In 2006, some applications exposed those techniques to a wider audience: Meebo’s multi-protocol web-based chat application enabled users to connect to AOL, Yahoo, and Microsoft chat platforms through the browser; Google added web-based chat to Gmail; JotSpot, a startup since acquired by Google, built Comet-based real-time collaborative document editing. New Comet variants were created, such as the Java-based ICEfaces JSF framework (although they prefer the term "Ajax Push"). Others that had previously used Java-applet based transports switched instead to pure-JavaScript implementations.== Implementations ==Comet applications attempt to eliminate the limitations of the page-by-page web model and traditional polling by offering two-way sustained interaction, using a persistent or long-lasting HTTP connection between the server and the client.  Since browsers and proxies are not designed with server events in mind, several techniques to achieve this have been developed, each with different benefits and drawbacks. The biggest hurdle is the HTTP 1.1 specification, which states "this specification... encourages clients to be conservative when opening multiple connections". Therefore, holding one connection open for real-time events has a negative impact on browser usability: the browser may be blocked from sending a new request while waiting for the results of a previous request, e.g., a series of images. This can be worked around by creating a distinct hostname for real-time information, which is an alias for the same physical server. This strategy is an application of domain sharding.Specific methods of implementing Comet fall into two major categories: streaming and long polling.=== Streaming ===An application using streaming Comet opens a single persistent connection from the client browser to the server for all Comet events. These events are incrementally handled and interpreted on the client side every time the server sends a new event, with neither side closing the connection.Specific techniques for accomplishing streaming Comet include the following:==== Hidden iframe ====A basic technique for dynamic web application is to use a hidden iframe HTML element (an inline frame, which allows a website to embed one HTML document inside another). This invisible iframe is sent as a chunked block, which implicitly declares it as infinitely long (sometimes called "forever frame"). As events occur, the iframe is gradually filled with script tags, containing JavaScript to be executed in the browser. Because browsers render HTML pages incrementally, each script tag is executed as it is received. Some browsers require a specific minimum document size before parsing and execution is started, which can be obtained by initially sending 1–2 kB of padding spaces.One benefit of the iframes method is that it works in every common browser. Two downsides of this technique are the lack of a reliable error handling method, and the impossibility of tracking the state of the request calling process.==== XMLHttpRequest ====The XMLHttpRequest (XHR) object, a tool used by Ajax applications for browser–server communication, can also be pressed into service for server–browser Comet messaging by generating a custom data format for an XHR response, and parsing out each event using browser-side JavaScript; relying only on the browser firing the onreadystatechange callback each time it receives new data.=== Ajax with long polling ===None of the above streaming transports work across all modern browsers without negative side-effects. This forces Comet developers to implement several complex streaming transports, switching between them depending on the browser. Consequently, many Comet applications use long polling, which is easier to implement on the browser side, and works, at minimum, in every browser that supports XHR. As the name suggests, long polling requires the client to poll the server for an event (or set of events). The browser makes an Ajax-style request to the server, which is kept open until the server has new data to send to the browser, which is sent to the browser in a complete response. The browser initiates a new long polling request in order to obtain subsequent events.Specific technologies for accomplishing long-polling include the following:==== XMLHttpRequest long polling ====For the most part, XMLHttpRequest long polling works like any standard use of XHR. The browser makes an asynchronous request of the server, which may wait for data to be available before responding. The response can contain encoded data (typically XML or JSON) or Javascript to be executed by the client. At the end of the processing of the response, the browser creates and sends another XHR, to await the next event. Thus the browser always keeps a request outstanding with the server, to be answered as each event occurs.==== Script tag long polling ====While any Comet transport can be made to work across subdomains, none of the above transports can be used across different second-level domains (SLDs), due to browser security policies designed to prevent cross-site scripting attacks. That is, if the main web page is served from one SLD, and the Comet server is located at another SLD (which does not have cross-origin resource sharing enabled), Comet events cannot be used to modify the HTML and DOM of the main page, using those transports. This problem can be sidestepped by creating a proxy server in front of one or both sources, making them appear to originate from the same domain. However, this is often undesirable for complexity or performance reasons.Unlike iframes or  XMLHttpRequest objects, script tags can be pointed at any URI, and JavaScript code in the response will be executed in the current HTML document. This creates a potential security risk for both servers involved, though the risk to the data provider (in our case, the Comet server) can be avoided using JSONP.A long-polling Comet transport can be created by dynamically creating script elements, and setting their source to the location of the Comet server, which then sends back JavaScript (or JSONP) with some event as its payload. Each time the script request is completed, the browser opens a new one, just as in the XHR long polling case. This method has the advantage of being cross-browser while still allowing cross-domain implementations.== Alternatives ==Browser-native technologies are inherent in the term Comet. Attempts to improve non-polling HTTP communication have come from multiple sides:The HTML 5 draft specification produced by the Web Hypertext Application Technology Working Group (WHATWG) specifies so called server-sent events, which defines a new JavaScript interface EventSource and a new MIME type text/event-stream. All major browsers except Microsoft Internet Explorer include this technology.The HTML 5 WebSocket API working draft specifies a method for creating a persistent connection with a server and receiving messages via an onmessage callback.The Bayeux protocol by the Dojo Foundation. It leaves browser-specific transports in place, and defines a higher-level protocol for communication between browser and server, with the aim of allowing re-use of client-side JavaScript code with multiple Comet servers, and allowing the same Comet server to communicate with multiple client-side JavaScript implementations. Bayeux is based on a publish/subscribe model, so servers supporting Bayeux have publish/subscribe built-in.The BOSH protocol by the XMPP standards foundation. It emulates a bidirectional stream between browser and server by using two synchronous HTTP connections.The JSONRequest object, proposed by Douglas Crockford, would be an alternative to the XHR object.Use of plugins, such as Java applets or the proprietary Adobe Flash (using RTMP protocol for data streaming to Flash applications). These have the advantage of working identically across all browsers with the appropriate plugin installed and need not rely on HTTP connections, but the disadvantage of requiring the plugin to be installedGoogle announced a new Channel API for Google App Engine, implementing a Comet-like API with the help of a client JavaScript library on the browser. It could be later replaced by HTML5 WebSocket, however.== See also ==Push technologyPull technology== References ==== External links ==Comet Daily – a website dedicated to articles on Comet techniquesComparison of several comet server implementations
	WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C.WebSocket is distinct from HTTP. Both protocols are located at layer 7 in the OSI model and depend on TCP at layer 4. Although they are different, RFC 6455 states that WebSocket "is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries," thus making it compatible with the HTTP protocol. To achieve compatibility, the WebSocket handshake uses the HTTP Upgrade header to change from the HTTP protocol to the WebSocket protocol.The WebSocket protocol enables interaction between a web browser (or other client application) and a web server with lower overhead than half-duplex alternatives such as HTTP polling, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, a two-way ongoing conversation can take place between the client and the server. The communications are done over TCP port number 80 (or 443 in the case of TLS-encrypted connections), which is of benefit for those environments which block non-web Internet connections using a firewall. Similar two-way browser-server communications have been achieved in non-standardized ways using stopgap technologies such as Comet.Most browsers support the protocol, including Google Chrome, Microsoft Edge, Internet Explorer, Firefox, Safari and Opera.== Overview ==Unlike HTTP, WebSocket provides full-duplex communication.Additionally, WebSocket enables streams of messages on top of TCP. TCP alone deals with streams of bytes with no inherent concept of a message. Before WebSocket, port 80 full-duplex communication was attainable using Comet channels; however, Comet implementation is nontrivial, and due to the TCP handshake and HTTP header overhead, it is inefficient for small messages. The WebSocket protocol aims to solve these problems without compromising security assumptions of the web.The WebSocket protocol specification defines ws (WebSocket) and wss (WebSocket Secure) as two new uniform resource identifier (URI) schemes that are used for unencrypted and encrypted connections, respectively. Apart from the scheme name and fragment (# is not supported), the rest of the URI components are defined to use URI generic syntax.Using browser developer tools, developers can inspect the WebSocket handshake as well as the WebSocket frames.== History ==WebSocket was first referenced as TCPConnection in the HTML5 specification, as a placeholder for a TCP-based socket API. In June 2008, a series of discussions were led by Michael Carter that resulted in the first version of the protocol known as WebSocket.The name "WebSocket" was coined by Ian Hickson and Michael Carter shortly thereafter through collaboration on the #whatwg IRC chat room, and subsequently authored for inclusion in the HTML5 specification by Ian Hickson, and announced on the cometdaily blog by Michael Carter. In December 2009, Google Chrome 4 was the first browser to ship full support for the standard, with WebSocket enabled by default. Development of the WebSocket protocol was subsequently moved from the W3C and WHATWG group to the IETF in February 2010, and authored for two revisions under Ian Hickson.After the protocol was shipped and enabled by default in multiple browsers, the RFC was finalized under Ian Fette in December 2011.== Browser implementation ==A secure version of the WebSocket protocol is implemented in Firefox 6, Safari 6, Google Chrome 14, Opera 12.10 and Internet Explorer 10.  A detailed protocol test suite report lists the conformance of those browsers to specific protocol aspects.An older, less secure version of the protocol was implemented in Opera 11 and Safari 5, as well as the mobile version of Safari in iOS 4.2. The BlackBerry Browser in OS7 implements WebSockets. Because of vulnerabilities, it was disabled in Firefox 4 and 5, and Opera 11.== Web Server implementation ==Nginx support WebSockets since 2013, implemented in version 1.3.13  including acting as a reverse proxy and do load balancing of WebSocket applications.== Protocol handshake ==To establish a WebSocket connection, the client sends a WebSocket handshake request, for which the server returns a WebSocket handshake response, as shown in the example below.Client request (just like in HTTP, each line ends with \r\n and there must be an extra blank line at the end):Server response:The handshake starts with an HTTP request/response, allowing servers to handle HTTP connections as well as WebSocket connections on the same port. Once the connection is established, communication switches to a bidirectional binary protocol which does not conform to the HTTP protocol.In addition to Upgrade headers, the client sends a Sec-WebSocket-Key header containing base64-encoded random bytes, and the server replies with a hash of the key in the Sec-WebSocket-Accept header. This is intended to prevent a caching proxy from re-sending a previous WebSocket conversation, and does not provide any authentication, privacy, or integrity. The hashing function appends the fixed string 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 (a GUID) to the value from Sec-WebSocket-Key header (which is not decoded from base64), applies the SHA-1 hashing function, and encodes the result using base64.Once the connection is established, the client and server can send WebSocket data or text frames back and forth in full-duplex mode. The data is minimally framed, with a small header followed by payload. WebSocket transmissions are described as "messages", where a single message can optionally be split across several data frames. This can allow for sending of messages where initial data is available but the complete length of the message is unknown (it sends one data frame after another until the end is reached and marked with the FIN bit). With extensions to the protocol, this can also be used for multiplexing several streams simultaneously (for instance to avoid monopolizing use of a socket for a single large payload).It is important (from a security perspective) to validate the "Origin" header during the connection establishment process on the server side (against the expected origins) to avoid Cross-Site WebSocket Hijacking attacks, which might be possible when the connection is authenticated with Cookies or HTTP authentication. It is better to use tokens or similar protection mechanisms to authenticate the WebSocket connection when sensitive (private) data is being transferred over the WebSocket.== Proxy traversal ==WebSocket protocol client implementations try to detect if the user agent is configured to use a proxy when connecting to destination host and port and, if it is, uses HTTP CONNECT method to set up a persistent tunnel.While the WebSocket protocol itself is unaware of proxy servers and firewalls, it features an HTTP-compatible handshake thus allowing HTTP servers to share their default HTTP and HTTPS ports (80 and 443) with a WebSocket gateway or server. The WebSocket protocol defines a ws:// and wss:// prefix to indicate a WebSocket and a WebSocket Secure connection, respectively. Both schemes use an HTTP upgrade mechanism to upgrade to the WebSocket protocol. Some proxy servers are transparent and work fine with WebSocket; others will prevent WebSocket from working correctly, causing the connection to fail. In some cases, additional proxy server configuration may be required, and certain proxy servers may need to be upgraded to support WebSocket.If unencrypted WebSocket traffic flows through an explicit or a transparent proxy server without WebSockets support, the connection will likely fail.If an encrypted WebSocket connection is used, then the use of Transport Layer Security (TLS) in the WebSocket Secure connection ensures that an HTTP CONNECT command is issued when the browser is configured to use an explicit proxy server. This sets up a tunnel, which provides low-level end-to-end TCP communication through the HTTP proxy, between the WebSocket Secure client and the WebSocket server. In the case of transparent proxy servers, the browser is unaware of the proxy server, so no HTTP CONNECT is sent. However, since the wire traffic is encrypted, intermediate transparent proxy servers may simply allow the encrypted traffic through, so there is a much better chance that the WebSocket connection will succeed if WebSocket Secure is used. Using encryption is not free of resource cost, but often provides the highest success rate since it would be travelling through a secure tunnel.A mid-2010 draft (version hixie-76) broke compatibility with reverse proxies and gateways by including eight bytes of key data after the headers, but not advertising that data in a Content-Length: 8 header. This data was not forwarded by all intermediates, which could lead to protocol failure. More recent drafts (e.g., hybi-09) put the key data in a Sec-WebSocket-Key header, solving this problem.== See also ==== Notes ==== References ==== External links ==IETF Hypertext-Bidirectional (HyBi) working groupThe WebSocket protocol - Proposed Standard published by the IETF HyBi Working GroupThe WebSocket protocol - Internet-Draft published by the IETF HyBi Working GroupThe WebSocket protocol - Original protocol proposal by Ian HicksonThe WebSocket API - W3C Working Draft specification of the APIThe WebSocket API - W3C Candidate Recommendation specification of the APIWebSocket.org WebSocket demos, loopback tests, general information and community
	A Webby Award is an award for excellence on the Internet presented annually by The International Academy of Digital Arts and Sciences, a judging body composed of over two thousand industry experts and technology innovators. Categories include websites, advertising and media, online film and video, mobile sites and apps, and social.Two winners are selected in each category, one by members of The International Academy of Digital Arts and Sciences, and one by the public who cast their votes during Webby People’s Voice voting. Each winner presents a five-word acceptance speech, a trademark of the annual awards show.Hailed as the "Internet’s highest honor," the award is one of the oldest Internet-oriented awards, and is associated with the phrase "The Oscars of the Internet."== History ==The Webby Awards began in 1996, sponsored by the Academy of Web Design and Cool Site of the Day. The first Webby Awards were produced by Kay Dangaard at the Hollywood Roosevelt Hotel as a nod to  the first site of the Academy of Motion Picture Arts and Sciences (Oscars). That first year, they were called "Webbie" Awards. The first "Site of the Year" winner was the pioneer webisodic serial The Spot.Today's Webby Awards were founded by Tiffany Shlain when she was hired by The Web Magazine to re-establish them. The event was held in San Francisco from 1996 to 2004 and quickly became known for their "5 word Acceptance Speeches". After the first year the awards became more successful than the magazine and IDG closed the publication. Shlain continued to run The Webby Awards with the help of Maya Draisin until 2004.The International Academy of Digital Arts and Sciences, which selects the winners of The Webby Awards, was established in 1998 by co-founders Tiffany Shlain, Spencer Ante and Maya Draisin. Members of the Academy include Kevin Spacey, Grimes, Questlove, Internet inventor Vint Cerf, Instagram’s Head of Fashion Partnerships Eva Chen, comedian Jimmy Kimmel, Twitter Founder Biz Stone, Vice Media Co-Founder and CEO Shane Smith, Tumblr’s David Karp, Director of Harvard’s Berkman Klein Center for Internet & Society Susan P. Crawford, Refinery29’s Executive Creative Director Piera Gelardi, and CEO and cofounder of Gimlet Media Alex Blumberg.The Webby Awards is owned and operated by the Webby Media Group, a division of Recognition Media, which also owns and produces the Lovie Awards in Europe and Netted by the Webbys, a daily email publication launched in 2009. David-Michel Davies, CEO of Webby Media Group, current Executive Director of the Webby Awards and co-founder of Internet Week New York, was named Executive Director of the Webby Awards in 2005.In 2009, the 13th Annual Webby Awards received nearly 10,000 entries from all 50 states and over 60 countries. That same year, more than 500,000 votes were cast in The Webby People's Voice Awards. In 2012, the 16th Annual Webby awards received 1.5 million votes from more than 200 countries for the People's Voice awards. In 2015, the 19th Annual Webby Awards received nearly 13,000 entries from all 50 U.S. states and over 60 countries worldwide.== Nomination process ==During the Call for Entries phase, each entry is rated by Associate Members of the International Academy of Digital Arts & Sciences. Entries that receive the highest marks during this first round of voting are included on category-specific shortlists and further evaluated by Executive Members of the Academy.Executive Academy Members with category-specific expertise evaluate the shortlisted entries based on the appropriate Website, Advertising & Media, Online Film & Video, Mobile Sites & Apps, and Social category criteria, and cast ballots to determine Webby Honorees, Nominees and Webby Winners. Deloitte provides vote tabulation consulting for the Webby Awards.In addition to the award given in each category by the International Academy of Digital Arts and Sciences, another winner is selected in each category as determined by the general public during People’s Voice voting. Winners of both the Academy-selected and People’s Voice-selected awards are invited to the Webbys.== Awards granted ==The Webby Awards are presented in over a hundred categories among all four types of entries. A website can be entered in multiple categories and receive multiple awards.In each category, two awards are handed out: a Webby Award selected by The International Academy of Digital Arts and Sciences, and a People's Voice Award selected by the  general public.Past winners include Amazon.com, eBay, Travel + Leisure, Simply Hired, Kayak.com, Yahoo!, iTunes, Google, FedEx, BBC News, CNN, MSNBC, The New York Times, Annie Lennox, NPR, Salon Magazine, Facebook, Meetup, Wikipedia, Deleted - The Game, Flickr, ESPN, Comedy Central, PBS, The Office webisodes, SwiftKey, My Damn Channel, NASA, George Takei, Airbnb, The Onion, Kickstarter, Mashable, Zach Galifianakis, Justin Bieber, Rhett and Link, and Humans of New York.Each year, the International Academy of Digital Arts and Sciences also honors individuals with Webby Special Achievement Awards. Past Webby Special Achievement winners include Al Gore, Prince, David Bowie, Meg Whitman, Tim & Eric, Sir Tim Berners-Lee, Lorne Michaels, Craig Newmark, Thomas Friedman, Stephen Colbert, Michel Gondry, the Beastie Boys, Kevin Spacey, Banksy, Lawrence Lessig, Van Jones, Gillian Anderson, Tituss Burgess, Ellie Kemper and Jerry Seinfeld.== Ceremony ==Since 2005, The Webby Awards has been presented in New York City. Comedian Rob Corddry hosted the ceremony from 2005 to 2007. Seth Meyers of Saturday Night Live hosted in 2008 and 2009, B.J. Novak of NBC's The Office in 2010, and Lisa Kudrow in 2011.Comedian, actor, and writer Patton Oswalt hosted from 2012 to 2014. Comedian Hannibal Buress will host the 19th Annual Webby Awards.The Webbys are famous for limiting recipients to five-word speeches, which are often humorous, although some exceed the limit. In 2005 when accepting his Lifetime Achievement Award, former Vice President Al Gore's speech was "Please don't recount this vote." He was introduced by Vint Cerf who used the same format to state, "We all invented the Internet." In 2008, Stephen Colbert shouted “Me. Me. Me. Me. Me” when accepting his award for Webby Person of the Year. Accepting the award for Best Political Blog in 2008, Arianna Huffington’s speech was “President Obama ... Sounds good, right?"Other popular speeches include "Can anyone fix my computer?" (the Beastie Boys); "Everything you think is true" (Prince); "Thank God Conan got promoted" (Jimmy Fallon), "Free, open... Keep one Web" (Sir Tim Berners Lee), “Holy - Fucking - Shit, Buzz Aldrin" (Jake Hurwitz), and "Holocaust. Did it happen? Yes." (Sarah Silverman).In 2013, the creator of the Graphics Interchange Format (GIF), Steve Wilhite, accepted his Webby and delivered his now famous five-word speech, "It’s pronounced 'Jif' not 'Gif'."== Criticism ==The Webbys have been criticized for their pay-to-enter and pay-to-attend policy (winners and nominees also have to pay to attend the award ceremony), and thus for not taking most websites into consideration before distributing their awards.  Gawker, its Valleywag column, and others, have called the awards a scam, with Valleywag saying, "...somewhere along the way, the organizers figured out that this goofy charade could be milked for profit."In response, Webby Awards executive director David-Michel Davies told the Wall Street Journal that entry fees “provide the best and most sustainable model for ensuring that our judging process remains consistent and rigorous and is not dependent on things like sponsorships that can fluctuate from year to year.”== See also ==Shorty AwardsStreamy Awards== References ==== Further reading ==Joanna Glasner, "Usual Suspects Dominate Webbys" - Wired News, May 9, 2006Jack Shafer, "What? You've Not Been Honored by the Webbys?", Slate Magazine, April 9, 2008Leslie Walker, "Webby Awards Salute Online Originality",  Washington Post, June 8, 2003== External links ==Official websiteWebby Awards at YouTube
	In web development, the CSS box model refers to how HTML elements are modeled in browser engines and how dimension of those HTML elements are derived from CSS properties. It is a fundamental concept for the composition of HTML webpages. The guidelines of the box model are described by web standards World Wide Web Consortium (W3C) specifically the CSS Working Group. For much of late-1990s and early 2000s there had been non-standard compliant implementations of the box model in the mainstream browsers. With the advent of CSS2, which introduced the box-sizing, the problem had mostly been resolved.== Specifics ==The Cascading Style Sheets (CSS) specification describes how elements of web pages are displayed by graphical browsers. Section 4 of the CSS1 specification defines a "formatting model" that gives block-level elements—such as p and blockquote—a width and height, and three levels of boxes surrounding it: padding, borders, and margins. While the specification never uses the term "box model" explicitly, the term has become widely used by web developers and web browser vendors.All HTML elements can be considered "boxes", this includes div tag, p tag, or a tag. Each of those boxes has five modifiable dimensions: the height and width describe dimensions of the actual content of the box (text, images, ...)the padding describes the space between this content and the border of the boxthe border is any kind of line (solid, dotted, dashed...) surrounding the box, if presentthe margin is the space around the borderAccording to the CSS1 specification, released by W3C in 1996 and revised in 1999, when a width or height is explicitly specified for any block-level element, it should determine only the width or height of the visible element, with the padding, borders, and margins applied afterward. Before CSS3, this box model was known as W3C box model, in CSS3, it is known as the content-box. The total width of a box is therefore left-margin + left-border + left-padding + width + right-padding + right-border + right-margin. Similarly, the total height of a box equals top-margin + top-border + top-padding + width + bottom-padding + bottom-border + bottom-margin.For example, the following CSS codewould specify the box dimensions of each block belonging to 'myClass'. Moreover, each such box will have total height 160px and width 260px.CSS3 introduced the Internet Explorer box model to the standard, known referred to as border-box.== History ==Before HTML 4 and CSS, very few HTML elements supported both border and padding, so the definition of the width and height of an element was not very contentious. However, it varied depending on the element. The HTML width attribute of a table defined the width of the table including its border. On the other hand, the HTML width attribute of an image defined the width of the image itself (inside any border). The only element to support padding in those early days was the table cell. Width for the cell was defined as "the suggested width for a cell content in pixels excluding the cell padding."In 1996, CSS introduced margin, border and padding for many more elements. It adopted a definition width in relation to content, border, margin and padding similar to that for a table cell. This has since become known as the W3C box model.At the time, very few browser vendors implemented the W3C box model to the letter. The two major browsers at the time, Netscape 4.0 and Internet Explorer 4.0 both defined width and height as the distance from border to border. This has been referred to as the traditional or the Internet Explorer box model.Internet Explorer in "quirks mode" includes the content, padding and borders within a specified width or height; this results in a narrower or shorter rendering of a box than would result following the standard behavior.The Internet Explorer box model behavior was often considered a bug, because way in which earlier versions of Internet Explorer handle the box model or sizing of elements in a web page, which differs from the standard way recommended by the W3C for the Cascading Style Sheets language. As of Internet Explorer 6, the browser supports an alternative rendering mode (called the "standards-compliant mode") which solves this discrepancy. However, for backward compatibility reasons, all versions still behave in the usual, non-standard way by default (see quirks mode). Internet Explorer for Mac is not affected by this non-standard behavior.=== Workarounds ===Internet Explorer versions 6 and onward are not affected by the bug if the page contains certain HTML document type declarations. These versions maintain the buggy behavior when in quirks mode for reasons of backward compatibility. For example, quirks mode is triggered:When the document type declaration is absent or incomplete;When an HTML 3 or earlier document is encountered;When an HTML 4.0 Transitional or Frameset document type declaration is used and a system identifier (URI) is not present;When an SGML comment or other unrecognized content appears before the document type declarationInternet Explorer 6 also uses quirks mode if there is an XML declaration prior to the document type declaration.Various workarounds have been devised to force Internet Explorer versions 5 and earlier to display Web pages using the W3C box model. These workarounds generally exploit unrelated bugs in Internet Explorer's CSS selector processing in order to hide certain rules from the browser. The best known of these workarounds is the "box model hack" developed by Tantek Çelik, a former Microsoft employee who developed this idea while working on Internet Explorer for the Macintosh. It involves specifying a width declaration for Internet Explorer for Windows, and then overriding it with another width declaration for CSS-compliant browsers. This second declaration is hidden from Internet Explorer for Windows by exploiting other bugs in the way that it parses CSS rules. The implementation of these CSS "hacks" has been further complicated by the public release of Internet Explorer 7, which has had some issues fixed, but not others, causing undesired results in pages using these hacks.Box model hacks have proven unreliable because they rely on bugs in browsers' CSS support that may be fixed in later versions. For this reason, some Web developers have instead recommended either avoiding specifying both width and padding for the same element or using conditional comment and/or CSS filters to work around the box model bug in older versions of Internet Explorer.== Support for Internet Explorer's box model ==Web designer Doug Bowman has said that the original Internet Explorer box model represents a better, more logical approach. Peter-Paul Koch gives the example of a physical box, whose dimensions always refer to the box itself, including potential padding, but never its content. He says that this box model is more useful for graphic designers, who create designs based on the visible width of boxes rather than the width of their content. Bernie Zimmermann says that the Internet Explorer box model is closer to the definition of cell dimensions and padding used in the HTML table model.The W3C has included a "box-sizing" property in CSS3. When box-sizing: border-box; is specified for an element, any padding or border of the element is drawn inside the specified width and height, "as commonly implemented by legacy HTML user agents". Internet Explorer 8, WebKit browsers such as Apple Safari 5.1+ and Google Chrome, Gecko-based browsers such as Mozilla Firefox 29.0 and later, Opera 7.0 and later, and Konqueror 3.3.2 and later support the CSS3 box-sizing property. Gecko browsers previous than 29.0 support the same functionality using the browser-specific -moz-box-sizing property. border-box is the default box model used in Bootstrap framework.== References ==== External links ==The world wide web consortium (W3C) specification of the box modelA tutorial on the CSS box modelTantek Çelik's description of the "box model hack"Getting Internet Explorer to Play Well with CSS - article on about.com that outlines various ways to get around box model problem and other IE bugs.Cascading Style Sheet Compatibility in Internet Explorer 7 - MSDN article, July 2006.CSS Box Model differences in Firefox and Internet Explorer - Further explanation of the rendering differences between Mozilla Firefox and Internet Explorer.
	WebRTC (Web Real-Time Communication) is a free, open-source project that provides web browsers and mobile applications with real-time communication (RTC) via simple application programming interfaces (APIs). It allows audio and video communication to work inside web pages by allowing direct peer-to-peer communication, eliminating the need to install plugins or download native apps. Supported by Apple, Google, Microsoft, Mozilla, and Opera, WebRTC is being standardized through the World Wide Web Consortium (W3C) and the Internet Engineering Task Force (IETF).Its mission is to "enable rich, high-quality RTC applications to be developed for the browser, mobile platforms, and IoT devices, and allow them all to communicate via a common set of protocols". The reference implementation is released as free software under the terms of a BSD license. OpenWebRTC provides another free implementation based on the multimedia framework GStreamer. JavaScript inventor Brendan Eich called it a "new front in the long war for an open and unencumbered web".== History ==In May 2010, Google bought Global IP Solutions or GIPS, a VoIP and videoconferencing software company that had developed many components required for RTC, such as codecs and echo cancellation techniques. Google open-sourced the GIPS technology and engaged with relevant standards bodies at the IETF and W3C to ensure industry consensus. In May 2011, Google released an open-source project for browser-based real-time communication known as WebRTC. This has been followed by ongoing work to standardize the relevant protocols in the IETF and browser APIs in the W3C.In May 2011, Ericsson Labs built the first implementation of WebRTC using a modified WebKit library. In October 2011, the W3C published its first draft for the spec. WebRTC milestones include the first cross-browser video call (February 2013), first cross-browser data transfers (February 2014), and as of July 2014 Google Hangouts was "kind of" using WebRTC.The W3C draft API was based on preliminary work done in the WHATWG. It was referred to as the ConnectionPeer API, and a pre-standards concept implementation was created at Ericsson Labs. The WebRTC Working Group expects this specification to evolve significantly based on:Outcomes of ongoing exchanges in the companion RTCWEB group at IETF to define the set of protocols that, together with this document, define real-time communications in web browsers. While no one signaling protocol is mandated, SIP over WebSockets (RFC 7118) is often used partially due to the applicability of SIP to most of the envisaged communication scenarios as well as the availability of open-source software such as JsSIP.Privacy issues that arise when exposing local capabilities and local streamsTechnical discussions within the group, on implementing data channels in particularExperience gained through early experimentationFeedback from other groups and individualsIn November 2017, the WebRTC 1.0 specification transitioned from Working Draft to Candidate Recommendation.== Overview ===== Design ===Major components of WebRTC include several JavaScript APIs:getUserMedia acquires the audio and video media (e.g., by accessing a device's camera and microphone).RTCPeerConnection enables audio and video communication between peers. It performs signal processing, codec handling, peer-to-peer communication, security, and bandwidth management.RTCDataChannel allows bidirectional communication of arbitrary data between peers. It uses the same API as WebSockets and has very low latency.The WebRTC API also includes a statistics function:getStats allows the web application to retrieve a set of statistics about WebRTC sessions. These statistics data are being described in a separate W3C document.RFC 7874 requires implementations to provide PCMA/PCMU (RFC 3551), Telephone Event as DTMF (RFC 4733), and Opus (RFC 6716) audio codecs as minimum capabilities. The PeerConnection, data channel and media capture browser APIs are detailed in the W3C.W3C is developing ORTC (Object Real-Time Communications) for WebRTC.=== Examples ===Although initially developed for web browsers, WebRTC has applications for non-browser devices, including mobile platforms and IoT devices. Examples include browser-based VoIP telephony, also called cloud phones or web phones, which allow calls to be made and received from within a web browser, replacing the requirement to download and install a softphone.=== Support ===WebRTC is supported by the following browsers:Desktop PCMicrosoft Edge 12+Google Chrome 28+Mozilla Firefox 22+Safari 11+Opera 18+Vivaldi 1.9+AndroidGoogle Chrome 28+ (enabled by default since 29)Mozilla Firefox 24+Opera Mobile 12+Chrome OSFirefox OSBlackBerry 10iOSMobileSafari/WebKit (iOS 11+)Tizen 3.0Support was not included in Internet Explorer prior to its final feature release in October 2013, but 3rd party plugins are available to add the support of WebRTC to Internet Explorer and Safari for macOS. At WWDC 2017, Apple announced Safari would get WebRTC support in Safari 11, and it was made available in release 32 of the Safari Technology Preview.== Concerns ==In January 2015, TorrentFreak reported a serious security flaw in browsers that support WebRTC, saying that it compromised the security of VPN tunnels, by exposing the true IP address of a user. The IP address read requests are not visible in the browser's developer console, and they are not blocked by most ad blocking/privacy/security add-ons, enabling online tracking by advertisers and other entities despite precautions (however the uBlock Origin add-on can fix this problem). As of September 2019 this WebRTC flaw still surfaces on Firefox 69.x and still by default exposes your internal IP address to the web. == See also ==Global IP Solutions (GIPS)Real-time Transport Protocol (RTP)Session Description Protocol (SDP)WebRTC Gateway== References ==== Further reading ==Proust, S., ed. (May 2016). Additional WebRTC Audio Codecs for Interoperability. IETF. doi:10.17487/RFC7875. RFC 7875. Retrieved 2016-10-12.Valin, J. M.; Bran, C. (May 2016). WebRTC Audio Codec and Processing Requirements. IETF. doi:10.17487/RFC7874. RFC 7874. Retrieved 2016-10-12.Roach, A. B. (March 2016). WebRTC Video Processing and Codec Requirements. IETF. doi:10.17487/RFC7742. RFC 7742. Retrieved 2016-10-12.Perumal, M.; Wing, D.; Ravindranath, R.; Reddy, T.; Thomson, M. (October 2015). Session Traversal Utilities for NAT (STUN) Usage for Consent Freshness. IETF. doi:10.17487/RFC7675. RFC 7675. Retrieved 2016-10-12.Holmberg, C.; Hakansson, S.; Eriksson, G. (March 2015). Web Real-Time Communication Use Cases and Requirements. IETF. doi:10.17487/RFC7478. RFC 7478. Retrieved 2016-10-12.== External links ==Official website W3C Web Real-Time Communications Working GroupIETF Real-Time Communication in WEB-browsers (rtcweb) Working GroupVideo chat demo app based on WebRTC
	A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. In server-side scripting, parameters determine how the assembly of every new web page proceeds, including the setting up of more client-side processing.A client-side dynamic web page processes the web page using HTML scripting running in the browser as it loads.  JavaScript and other scripting languages determine the way the HTML in the received page is parsed into the Document Object Model, or DOM, that represents the loaded web page.  The same client-side techniques can then dynamically update or change the DOM in the same way. Even though a web page can be dynamic on the client-side, it can still be hosted on a static hosting service such as GitHub Pages or Amazon S3 as long as there isn't any server-side code included.A dynamic web page is then reloaded by the user or by a computer program to change some variable content.  The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a dynamic web page update using Ajax technologies will neither create a page to go back to, nor truncate the web browsing history forward of the displayed page.  Using Ajax technologies the end user gets one dynamic page managed as a single page in the web browser while the actual web content rendered on that page can vary.  The Ajax engine sits only on the browser requesting parts of its DOM, the DOM, for its client, from an application server.DHTML is the umbrella term for technologies and methods used to create web pages that are not static web pages, though it has fallen out of common use since the popularization of AJAX, a term which is now itself rarely used. Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser.== Basic concepts ==Classical hypertext navigation, with HTML or XHTML alone, provides "static" content, meaning that the user requests a web page and simply views the page and the information on that page.However, a web page can also provide a "live", "dynamic", or "interactive" user experience. Content (text, images, form fields, etc.) on a web page can change, in response to different contexts or conditions.There are two ways to create this kind of effect:Using client-side scripting to change interface behaviors within a specific web page, in response to mouse or keyboard actions or at specified timing events. In this case the dynamic behavior occurs within the presentation.Using server-side scripting to change the supplied page source between pages, adjusting the sequence or reload of the web pages or web content supplied to the browser. Server responses may be determined by such conditions as data in a posted HTML form, parameters in the URL, the type of browser being used, the passage of time, or a database or server state.Web pages that use client-side scripting must use presentation technology broadly called rich interfaced pages. Client-side scripting languages like JavaScript or ActionScript, used for Dynamic HTML (DHTML) and Flash technologies respectively, are frequently used to orchestrate media types (sound, animations, changing text, etc.) of the presentation. The scripting also allows use of remote scripting, a technique by which the DHTML page requests additional information from a server, using a hidden Frame, XMLHttpRequests, or a web service.Web pages that use server-side scripting are often created with the help of server-side languages such as PHP, Perl, ASP, ASP.NET, JSP, ColdFusion and other languages. These server-side languages typically use the Common Gateway Interface (CGI) to produce dynamic web pages. These kinds of pages can also use, on the client-side, the first kind (DHTML, etc.).== History ==It is difficult to be precise about "dynamic web page beginnings" or chronology, because the precise concept makes sense only after the "widespread development of web pages": HTTP has been in use since 1990, HTML, as standard, since 1996. The web browsers explosion started with 1993's Mosaic. It is obvious, however, that the concept of dynamically driven websites predates the internet, and in fact HTML. For example, in 1990, before general public use of the internet, a dynamically driven remotely accessed menu system was implemented by Susan Biddlecomb, who was Director of Computer Support of the USC Health Care system at the University of Southern California BBS on a 16 line TBBS system with TDBS add-on.database.The introduction of JavaScript (then known as LiveScript) enabled the production of client side dynamic web pages, with the JavaScript executed in the clients browser. The letter "J" in the term AJAX originally indicated the use of JavaScript. (As well as XML.) With the rise of server side JavaScript processing (e.g. NodeJS originally developed in 2009) JavaScript is also used to dynamically create pages on the server that are sent fully formed to clients.Execusite introduced the first dynamic website solution for the professional marketplace in June 1997. Execusite was acquired by Website Pros (now Web.com) in January 2000. During the bust cycle of the Dot-com bubble, the original Execusite founders bought back the company from Website Pros (December 2000). Execusite was later acquired by Wolters-Kluwer in December 2001 and was re-branded as CCH Site Builder.== Server-side scripting ==A program running on a web server (server-side scripting) is used to generate the web content on various web pages, manage user sessions, and control workflow. Server responses may be determined by such conditions as data in a posted HTML form, parameters in the URL, the type of browser being used, the passage of time, or a database or server state.Such web pages are often created with the help of server-side languages such as ASP, ColdFusion, Go, JavaScript, Perl, PHP, Ruby, Python, WebDNA and other languages, by a Support server that can run on the same hardware as the web server. These server-side languages often use the Common Gateway Interface (CGI) to produce dynamic web pages. Two notable exceptions are ASP.NET, and JSP, which reuse CGI concepts in their APIs but actually dispatch all web requests into a shared virtual machine.The server-side languages are used to embed tags or markers within the source file of the web page on the web server. When a user on a client computer requests that web page, the web server interprets these tags or markers to perform actions on the server. For example, the server may be instructed to insert information from a database or information such as the current date.Dynamic web pages are often cached when there are few or no changes expected and the page is anticipated to receive considerable amount of web traffic that would create slow load times for the server if it had to generate the pages on the fly for each request.== Client-side scripting ==Client-side scripting is changing interface behaviors within a specific web page in response to mouse or keyboard actions, or at specified timing events. In this case, the dynamic behavior occurs within the presentation.  The client-side content is generated on the user's local computer system.Such web pages use presentation technology called rich interfaced pages. Client-side scripting languages like JavaScript or ActionScript, used for Dynamic HTML (DHTML) and Flash technologies respectively, are frequently used to orchestrate media types (sound, animations, changing text, etc.) of the presentation. Client-side scripting also allows the use of remote scripting, a technique by which the DHTML page requests additional information from a server, using a hidden frame, XMLHttpRequests, or a Web service.The first widespread use of JavaScript was in 1997, when the language was standardized as ECMAScript and implemented in Netscape 3.ExampleThe client-side content is generated on the client's computer. The web browser retrieves a page from the server, then processes the code embedded in the page (typically written in JavaScript) and displays the retrieved page's content to the user.The innerHTML property (or write command) can illustrate the client-side dynamic page generation: two distinct pages, A and B, can be regenerated  (by an "event response dynamic") as document.innerHTML = A and document.innerHTML = B; or "on load dynamic" by document.write(A) and document.write(B).== Combination technologies ==All of the client and server components that collectively build a dynamic web page are called a web application.  Web applications manage user interactions, state, security, and performance.Ajax uses a combination of both client-side scripting and server-side requests. It is a web application development technique for dynamically interchanging content, and it sends requests to the server for data in order to do so.  The server returns the requested data which is then processed by a client-side script.  This technique can reduce server load time because the client does not request the entire webpage to be regenerated by the server's language parser; only the content that will change is transmitted.  Google Maps is an example of a web application that uses Ajax techniques.A web client, such as a web browser, can act as its own server, accessing data from many different servers, such as Gopher, FTP, NNTP (Usenet) and HTTP, to build a page. HTTP supports uploading documents from the client back to the server. There are several HTTP methods for doing this.== See also ==Static web pageDynamic HTMLDynamic Cascading Style SheetsResponsive Web DesignDeep Web (search indexing)Web template systemsolution stacks to serve dynamic web pagesLAMP (software bundle)LYME (software bundle)LYCE (software bundle)Content management systemWeb content management system== References ==== External links ==Static versus dynamic web site from W3.orgDynamic web sites using the Relationship Management Method, from W3.org.Wide analysis of dynamic web pages from University of Texas, Austin.
	Web content development is the process of researching, writing, gathering, organizing, and editing information for publication on websites. Website content may consist of prose, graphics, pictures, recordings, movies, or other digital assets that could be distributed by a hypertext transfer protocol server, and viewed by a web browser.== Content developers and web developers ==When the World Wide Web began, web developers either developed online content themselves, or modified existing documents and coded them into hypertext markup language (HTML). In time, the field of website development came to encompass many technologies, so it became difficult for website developers to maintain so many different skills. Content developers are specialized website developers who have content generation skills such as  graphic design, multimedia development, professional writing, and documentation. They can integrate content into new or existing websites without using information technology skills such as script language programming and database programming. Content developers or technical content developers can also be technical writers who produce technical documentation that helps people understand and use a product or service. This documentation includes online help, manuals, white papers, design specifications, developer guides, deployment guides, release notes, etc.== Search engine optimization ==Content developers may also be search engine optimization specialists, or internet marketing professionals. High quality, unique content is what search engines are looking for. Content development specialists, therefore, have a very important role to play in the search engine optimization process. One issue currently plaguing the world of web content development is keyword-stuffed content which are prepared solely for the purpose of manipulating search engine rankings. The effect is that content is written to appeal to search engine (algorithms) rather than human readers. Search engine optimization specialists commonly submit content to article directories to build their website's authority on any given topic.  Most article directories allow visitors to republish submitted content with the agreement that all links are maintained.  This has become a method of search engine optimization for many websites today. If written according to SEO copywriting rules, the submitted content will bring benefits to the publisher (free SEO-friendly content for a webpage) as well as to the author (a hyperlink pointing to his/her website, placed on an SEO-friendly webpage).== New content types ==Web content is no longer restricted to text. Search engines now index audio/visual media, including video, images, PDFs, and other elements of a web page. Website owners sometimes use content protection networks to scan for plagiarized content.== See also ==Web contentContent marketingSearch engine optimizationContent designerContent managementContent adaptationProfessional writingTechnical writer== References ==
	In computer science, XSA (better known as Cross-Server Attack) is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure.In general, XSA is demonstrated against websites, yet sometimes it is used in conjunction with other services located on the same server.== Basics ==XSA is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network.Most website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack, because of the amount of access services such as PHP and the webserver itself give to a client that allows the client to access other website configurations, files, passwords and the like.== History ==The term 'XSA' was first coined by DeadlyData, a prominent Computer hacker during the early 2000s, over the voice communications software TeamSpeak. While he had not invented or pioneered this method of intrusion, he coined it as a shortened term to describe the act of performing Cross-Server Attacks (XSAs).It was then used further in the community and now supports for most of the methods and subsets of the method that give both Computer hacker and malicious individuals the terminology to attack websites using software that is located on the same server.== See also ==SQL Injection (SQLi)Cross-Site Scripting (XSS)Cross-Site Request Forgery (CSRF)Buffer Overflow
	In computing, a situational application is "good enough" software created for a narrow group of users with a unique set of needs. The application typically (but not always) has a short life span, and is often created within the group where it is used, sometimes by the users themselves. As the requirements of a small team using the application change, the situational application often also continues to evolve to accommodate these changes. Although situational applications are specifically designed to embrace change, significant changes in requirements may lead to an abandonment of the situational application altogether – in some cases it is just easier to develop a new one than to evolve the one in use.== Characteristics ==Situational applications are developed fast, easy to use, uncomplicated, and serve a unique set of requirements.  They have a narrow focus on a specific business problem, and they are written in a way where if the business problem changes rapidly, so can the situational application.This contrasts with more common enterprise applications, which are designed to address a large set of business problems, require meticulous planning, and impose a sometimes-slow and often-meticulous change process.== Origination ==Clay Shirky in his essay entitled "Situated Software" described a type of software that "...is designed for use by a specific social group, rather than for a generic set of "users"."  IBM later morphed the term into "situational applications".== Evolution ==The successful large-scale implementation of a situational application environment in an organization requires a strategy, mindset, methodology and support structure quite different from traditional application development. This is now evolving as more companies learn how to best leverage the ideas behind situational applications. In addition, the advent of cloud-based application development and deployment platforms makes the implementation of a comprehensive situational application environment much more feasible.== Examples ==A structured wiki that can host wiki applications lends itself to creation of situational applications. Some mashups can also be considered situational applications.  A forms application such as a Microsoft Access Database (MDB file) can be considered a situational application. The latest implementations of situational application environments include Longjump, Force.com and WorkXpress.== See also ==End user developmentMashup (web application hybrid)Wiki application== References ==== External links ==Luba Cherbakov, Andy Bravery, Aroop Pandya. SOA meets situational applications, 3 part seriesSituational Applications: When the situation demands faster turnaround than IT can provideM. LaMonica, IBM Eyes Programming for the Masses, CNET News.com at Archive.today (archived 2013-01-19)Luba Cherbakov, Andy Bravery, Aroop Pandya. Changing the corporate IT development model: Tapping the power of grassroots computing, IBM Systems Journal
	Browser sniffing (also known as browser detection) is a set of techniques used in websites and web applications in order to determine the web browser a visitor is using, and to serve browser-appropriate content to the visitor. This practice is sometimes used to circumvent incompatibilities between browsers due to misinterpretation of HTML, Cascading Style Sheets (CSS), or the Document Object Model (DOM). While the World Wide Web Consortium maintains up-to-date central versions of some of the most important Web standards in the form of recommendations, in practice no software developer has designed a browser which adheres exactly to these standards; implementation of other standards and protocols, such as SVG and XMLHttpRequest, varies as well. As a result, different browsers display the same page differently, and so browser sniffing was developed to detect the web browser in order to help ensure consistent display of content.It is also used to detect mobile browsers and send them mobile-optimized websites.== Sniffer methods ===== Client-side sniffing ===Web pages can use programming languages such as JavaScript which are interpreted by the user agent, with results sent to the web server. For example:This code is run by the client computer, and the results are used by other code to make necessary adjustments on client-side. In this example, the client computer is asked to determine whether the browser can use a feature called ActiveX. Since this feature was proprietary to Microsoft, a positive result will indicate that the client may be running Microsoft's Internet Explorer. This is no longer a reliable indicator since Microsoft's open-source release of the ActiveX code, however, meaning that it can be used by any browser.=== Standard Browser detection method ===The web server communicates with the client using a communication protocol known as HTTP, or Hypertext Transfer Protocol, which specifies that the client send the server information about the browser being used to view the web site in a  user agent string.=== Server-side sniffing ===Extensive browser techniques enable persistent user tracking even when users try to stay anonymous.  See device fingerprint for more details on browser fingerprinting, a relatively new, extensive browser sniffing on steroids technique.== Issues and standards ==Many websites use browser sniffing to determine whether a visitor's browser is unable to use certain features (such as JavaScript, DHTML, ActiveX, or cascading style sheets), and display an error page if a certain browser is not used. However, it is virtually impossible to account for the tremendous variety of browsers available to users. Generally, a web designer using browser sniffing to determine what kind of page to present will test for the three or four most popular browsers, and provide content tailored to each of these. If a user is employing a user agent not tested for, there is no guarantee that a usable page will be served; thus, the user may be forced either to change browsers or to avoid the page. The World Wide Web Consortium, which sets standards for the construction of web pages, recommends that web sites be designed in accordance with its standards, and be arranged to "fail gracefully" when presented to a browser which cannot deal with a particular standard.== See also ==Computer ProgrammingHTTPWeb browserDocument Object ModelFeature detection (web development) ("Browser sniffing" synonym in some contexts)Local shared objects (LSOs), commonly called Flash cookies (due to their similarities with HTTP cookies).A zombie cookie is any HTTP cookie that is recreated after deletion, such as by Evercookie.User agentWeb standardsContent sniffing
	A static web page (sometimes called a flat page or a stationary page) is a web page that is delivered to the user's web browser exactly as stored, in contrast to dynamic web pages which are generated by a web application.Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so.== Overview ==Static web pages are often HTML documents stored as files in the file system and made available by the web server over HTTP (nevertheless URLs ending with ".html" are not always static). However, loose interpretations of the term could include web pages stored in a database, and could even include pages formatted using a template and served through an application server, as long as the page served is unchanging and presented essentially as stored.Static web pages are suitable for the contents that never or rarely need to be updated, though modern web template systems are changing this. Maintaining large numbers of static pages as files can be impractical without automated tools, such as static site generators . Another way to manage static pages is Online compiled source code playgrounds, e.g. GatsbyJS and GitHub may be utilized for migrating a WordPress site into statics web pages. Any personalization or interactivity has to run client-side, which is restricting.=== Advantages of a static website ===Provide improved security over dynamic websites (dynamic websites are at risk to web shell attacks if a vulnerability is present)Improved performance for end users compared to dynamic websitesFewer or no dependencies on systems such as databases or other application servers Cost savings from utilizing cloud storage, as opposed to a hosted environment=== Disadvantages of a static website ===Dynamic functionality has to be added separately== References ==== External links ==The definitive listing of Static Site Generators, a community-curated list of static site generators.Static Site Generators, Brian Rinaldi, (O'Reilly Media, 2018).
	HTTP compression is a capability that can be built into web servers and web clients to improve transfer speed and bandwidth utilization.HTTP data is compressed before it is sent from the server: compliant browsers will announce what methods are supported to the server before downloading the correct format; browsers that do not support compliant compression method will download uncompressed data. The most common compression schemes include gzip and Deflate; however, a full list of available schemes is maintained by the IANA. Additionally, third parties develop new methods and include them in their products, such as the Google Shared Dictionary Compression for HTTP (SDCH) scheme implemented in the Google Chrome browser and used on Google servers.There are two different ways compression can be done in HTTP. At a lower level, a Transfer-Encoding header field may indicate the payload of a HTTP message is compressed. At a higher level, a Content-Encoding header field may indicate that a resource being transferred, cached, or otherwise referenced is compressed. Compression using Content-Encoding is more widely supported than Transfer-Encoding, and some browsers do not advertise support for Transfer-Encoding compression to avoid triggering bugs in servers.== Compression scheme negotiation ==In most cases, excluding the SDCH, the negotiation is done in two steps, described in RFC 2616:1. The web client advertises which compression schemes it supports by including a list of tokens in the HTTP request. For Content-Encoding, the list in a field called Accept-Encoding; for Transfer-Encoding, the field is called TE.2. If the server supports one or more compression schemes, the outgoing data may be compressed by one or more methods supported by both parties. If this is the case, the server will add a Content-Encoding or Transfer-Encoding field in the HTTP response with the used schemes, separated by commas.The web server is by no means obligated to use any compression method – this depends on the internal settings of the web server and also may depend on the internal architecture of the website in question.In case of SDCH a dictionary negotiation is also required, which may involve additional steps, like downloading a proper dictionary from the external server.== Content-Encoding tokens ==The official list of tokens available to servers and client is maintained by IANA, and it includes:br – Brotli, a compression algorithm specifically designed for HTTP content encoding, defined in RFC 7932 and implemented in Mozilla Firefox release 44 and Chromium release 50compress – UNIX "compress" program method (historic; deprecated in most applications and replaced by gzip or deflate)deflate – compression based on the deflate algorithm (described in RFC 1951), a combination of the LZ77 algorithm and Huffman coding, wrapped inside the zlib data format (RFC 1950);exi – W3C Efficient XML Interchangegzip – GNU zip format (described in RFC 1952). Uses the deflate algorithm for compression, but the data format and the checksum algorithm differ from the "deflate" content-encoding. This method is the most broadly supported as of March 2011.identity – No transformation is used. This is the default value for content coding.pack200-gzip – Network Transfer Format for Java Archiveszstd – Zstandard compression, defined in RFC 8478In addition to these, a number of unofficial or non-standardized tokens are used in the wild by either servers or clients:bzip2 – compression based on the free bzip2 format, supported by lighttpdlzma – compression based on (raw) LZMA is available in Opera 20, and in elinks via a compile-time optionpeerdist – Microsoft Peer Content Caching and Retrievalsdch – Google Shared Dictionary Compression for HTTP, based on VCDIFF (RFC 3284)xpress - Microsoft compression protocol used by Windows 8 and later for Windows Store application updates. LZ77-based compression optionally using a Huffman encoding.xz - LZMA2-based content compression, supported by a non-official Firefox patch; and fully implemented in mget since 2013-12-31.== Servers that support HTTP compression ==SAP NetWeaverMicrosoft IIS: built-in or using third-party moduleApache HTTP Server, via mod_deflate (despite its name, only supporting gzip)Hiawatha HTTP server: serves pre-compressed filesCherokee HTTP server, On the fly gzip and deflate compressionsOracle iPlanet Web ServerZeus Web Serverlighttpd, via mod_compress and the newer mod_deflate (1.4.42+)nginx – built-inApplications based on Tornado, if "compress_response" is set to True in the application settings (for versions prior to 4.0, set "gzip" to True)Jetty Server – built-into default static content serving and available via servlet filter configurationsGeoServerApache TomcatIBM WebsphereAOLserverRuby Rack, via the Rack::Deflater middlewareHAProxyVarnish – built-in. Works also with ESIThe compression in HTTP can also be achieved by using the functionality of server-side scripting languages like PHP, or programming languages like Java.== Problems preventing the use of HTTP compression ==A 2009 article by Google engineers Arvind Jain and Jason Glasgow states that more than 99 person-years are wasted daily due to increase in page load time when users do not receive compressed content. This occurs when anti-virus software interferes with connections to force them to be uncompressed, where proxies are used (with overcautious web browsers), where servers are misconfigured, and where browser bugs stop compression being used. Internet Explorer 6, which drops to HTTP 1.0 (without features like compression or pipelining) when behind a proxy – a common configuration in corporate environments – was the mainstream browser most prone to failing back to uncompressed HTTP.Another problem found while deploying HTTP compression on large scale is due to the deflate encoding definition: while HTTP 1.1 defines the deflate encoding as data compressed with deflate (RFC 1951) inside a zlib formatted stream (RFC 1950), Microsoft server and client products historically implemented it as a "raw" deflated stream, making its deployment unreliable. For this reason, some software, including the Apache HTTP Server, only implement gzip encoding.== Security implications ==In 2012, a general attack against the use of data compression, called CRIME, was announced. While the CRIME attack could work effectively against a large number of protocols, including but not limited to TLS, and application-layer protocols such as SPDY or HTTP, only exploits against TLS and SPDY were demonstrated and largely mitigated in browsers and servers. The CRIME exploit against HTTP compression has not been mitigated at all, even though the authors of CRIME have warned that this vulnerability might be even more widespread than SPDY and TLS compression combined.In 2013, a new instance of the CRIME attack against HTTP compression, dubbed BREACH, was published. A BREACH attack can extract login tokens, email addresses or other sensitive information from TLS encrypted web traffic in as little as 30 seconds (depending on the number of bytes to be extracted), provided the attacker tricks the victim into visiting a malicious web link.  All versions of TLS and SSL are at risk from BREACH regardless of the encryption algorithm or cipher used. Unlike previous instances of CRIME, which can be successfully defended against by turning off TLS compression or SPDY header compression, BREACH exploits HTTP compression which cannot realistically be turned off, as virtually all web servers rely upon it to improve data transmission speeds for users.As of 2016, the TIME attack and the HEIST attack are now public knowledge.== References ==== External links ==RFC 2616: Hypertext Transfer Protocol – HTTP/1.1HTTP Content-Coding Values by Internet Assigned Numbers AuthorityCompression with lighttpdCoding Horror: HTTP Compression on IIS 6.015 Seconds: Web Site Compression at the Wayback Machine (archived July 16, 2011)HTTP Compression: resource page by the founder of VIGOS AG, Constantin RackUsing HTTP Compression by Martin Brown of Server WatchUsing HTTP Compression in PHPDynamic and static HTTP compression with Apache httpd
	A mashup (computer industry jargon), in web development, is a web page or web application that uses content from more than one source to create a single new service displayed in a single graphical interface. For example, a user could combine the addresses and photographs of their library branches with a Google map to create a map mashup. The term implies easy, fast integration, frequently using open application programming interfaces (open API) and data sources to produce enriched results that were not necessarily the original reason for producing the raw source data.The term mashup originally comes from British - West Indies slang meaning to be intoxicated, or as a description for something or someone not functioning as intended. In recent English parlance it can refer to music, where people seamlessly combine audio from one song with the vocal track from another—thereby mashing them together to create something new.The main characteristics of a mashup are combination, visualization, and aggregation. It is important to make existing data more useful, for personal and professional use. To be able to permanently access the data of other services, mashups are generally client applications or hosted online.In the past years, more and more Web applications have published APIs that enable software developers to easily integrate data and functions the SOA way, instead of building them by themselves. Mashups can be considered to have an active role in the evolution of social software and Web 2.0. Mashup composition tools are usually simple enough to be used by end-users. They generally do not require programming skills and rather support visual wiring of GUI widgets, services and components together. Therefore, these tools contribute to a new vision of the Web, where users are able to contribute.The term "mashup" is not formally defined by any standard-setting body.== History ==The broader context of the history of the Web provides a background for the development of mashups. Under the Web 1.0 model, organizations stored consumer data on portals and updated them regularly. They controlled all the consumer data, and the consumer had to use their products and services to get the information.The advent of Web 2.0 introduced Web standards that were commonly and widely adopted across traditional competitors and which unlocked the consumer data. At the same time, mashups emerged, allowing mixing and matching competitors' APIs to develop new services.The first mashups used mapping services or photo services to combine these services with data of any kind and therefore to produce visualizations of data.In the beginning, most mashups were consumer-based, but recently the mashup is to be seen as an interesting concept useful also to enterprises. Business mashups can combine existing internal data with external services to generate new views on the data.== Types of mashup ==There are many types of mashup, such as business mashups, consumer mashups, and data mashups. The most common type of mashup is the consumer mashup, aimed at the general public.Business (or enterprise) mashups define applications that combine their own resources, application and data, with other external Web services. They focus data into a single presentation and allow for collaborative action among businesses and developers. This works well for an agile development project, which requires collaboration between the developers and customer (or customer proxy, typically a product manager) for defining and implementing the business requirements. Enterprise mashups are secure, visually rich Web applications that expose actionable information from diverse internal and external information sources.Consumer mashups combine data from multiple public sources in the browser and organize it through a simple browser user interface. (e.g.: Wikipediavision combines Google Map and a Wikipedia API)Data mashups, opposite to the consumer mashups, combine similar types of media and information from multiple sources into a single representation. The combination of all these resources create a new and distinct Web service that was not originally provided by either source.=== By API type ===Mashups can also be categorized by the basic API type they use but any of these can be combined with each other or embedded into other applications.==== Data types ====Indexed data (documents, weblogs, images, videos, shopping articles, jobs ...) used by metasearch enginesCartographic and geographic data:  geolocation software, geovisualizationFeeds, podcasts: news aggregators==== Functions ====Data converters: language translators, speech processing, URL shorteners...Communication: email, instant messaging, notification...Visual data rendering: information visualization, diagramsSecurity related: electronic payment systems, ID identification...Editors== Mashup enabler ==In technology, a mashup enabler is a tool for transforming incompatible IT resources into a form that allows them to be easily combined, in order to create a mashup. Mashup enablers allow powerful techniques and tools (such as mashup platforms) for combining data and services to be applied to new kinds of resources. An example of a mashup enabler is a tool for creating an RSS feed from a spreadsheet (which cannot easily be used to create a mashup). Many mashup editors include mashup enablers, for example, Presto Mashup Connectors, Convertigo Web Integrator or Caspio Bridge.Mashup enablers have also been described as "the service and tool providers, [sic] that make mashups possible".=== History ===Early mashups were developed manually by enthusiastic programmers. However, as mashups became more popular, companies began creating platforms for building mashups, which allow designers to visually construct mashups by connecting together mashup components.Mashup editors have greatly simplified the creation of mashups, significantly increasing the productivity of mashup developers and even opening mashup development to end-users and non-IT experts. Standard components and connectors enable designers to combine mashup resources in all sorts of complex ways with ease. Mashup platforms, however, have done little to broaden the scope of resources accessible by mashups and have not freed mashups from their reliance on well-structured data and open libraries (RSS feeds and public APIs).Mashup enablers evolved to address this problem, providing the ability to convert other kinds of data and services into mashable resources.=== Web resources ===Of course, not all valuable data is located within organizations. In fact, the most valuable information for business intelligence and decision support is often external to the organization. With the emergence of rich internet applications and online Web portals, a wide range of business-critical processes (such as ordering) are becoming available online. Unfortunately, very few of these data sources syndicate content in RSS format and very few of these services provide publicly accessible APIs. Mashup editors therefore solve this problem by providing enablers or connectors.== Data integration challenges ==There are a number of challenges to address when integrating data from different sources. The challenges can be classified into four groups: text/data mismatch, object identifiers and schema mismatch, abstraction level mismatch, data accuracy.=== Text–data mismatch ===A large portion of data is described in text. Human language is often ambiguous - the same company might be referred to in several variations (e.g. IBM, International Business Machines, and Big Blue). The ambiguity makes cross-linking with structured data difficult. In addition, data expressed in human language is difficult to process via software programs. One of the functions of a data integration system is to overcome the mismatch between documents and data.=== Object identity and separate schemata ===Structured data are available in a plethora of formats. Lifting the data to a common data format is thus the first step. But even if all data is available in a common format, in practice sources differ in how they state what is essentially the same fact. The differences exist both on the level of individual objects and the schema level. As an example for a mismatch on the object level, consider the following: the SEC uses a so-called Central Index Key (CIK) to identify people (CEOs, CFOs), companies, and financial instruments while other sources, such as DBpedia (a structured data version of Wikipedia), use URIs to identify entities. In addition, each source typically uses its own schema and idiosyncrasies for stating what is essentially the same fact. Thus, Methods have to be in place for reconciling different representations of objects and schemata.=== Abstraction levels ===Data sources provide data at incompatible levels of abstraction or classify their data according to taxonomies pertinent to a certain sector.  Since data is being published at different levels of abstraction (e.g. person, company, country, or sector), data aggregated for the individual viewpoint may not match data e.g. from statistical offices. Also, there are differences in geographic aggregation (e.g. region data from one source and country-level data from another). A related issue is the use of local currencies (USD vs. EUR) which have to be reconciled in order to make data from disparate sources comparable and amenable for analysis.=== Data quality ===Data quality is a general challenge when automatically integrating data from autonomous sources. In an open environment the data aggregator has little to no influence on the data publisher. Data is often erroneous, and combining data often aggravates the problem. Especially when performing reasoning (automatically inferring new data from existing data), erroneous data has potentially devastating impact on the overall quality of the resulting dataset. Hence, a challenge is how data publishers can coordinate in order to fix problems in the data or blacklist sites which do not provide reliable data. Methods and techniques are needed to: check integrity and accuracy; highlight, identify and corroborate evidence; assess the probability that a given statement is true; equate weight differences between market sectors or companies; establish clearing houses for raising and settling disputes between competing (and possibly conflicting) data providers; and interact with messy erroneous Web data of potentially dubious provenance and quality. In summary, errors in signage, amounts, labeling, and classification can seriously impede the utility of systems operating over such data.== Mashups versus portals ==Mashups and portals are both content aggregation technologies. Portals are an older technology designed as an extension to traditional dynamic Web applications, in which the process of converting data content into marked-up Web pages is split into two phases: generation of markup "fragments" and aggregation of the fragments into pages. Each markup fragment is generated by a "portlet", and the portal combines them into a single Web page. Portlets may be hosted locally on the portal server or remotely on a separate server.Portal technology defines a complete event model covering reads and updates. A request for an aggregate page on a portal is translated into individual read operations on all the portlets that form the page ("render" operations on local, JSR 168 portlets or "getMarkup" operations on remote, WSRP portlets). If a submit button is pressed on any portlet on a portal page, it is translated into an update operation on that portlet alone (processAction on a local portlet or performBlockingInteraction on a remote, WSRP portlet). The update is then immediately followed by a read on all portlets on the page.Portal technology is about server-side, presentation-tier aggregation. It cannot be used to drive more robust forms of application integration such as two-phase commit.Mashups differ from portals in the following respects:The portal model has been around longer and has had greater investment and product research. Portal technology is therefore more standardized and mature. Over time, increasing maturity and standardization of mashup technology will likely make it more popular than portal technology because it is more closely associated with Web 2.0 and lately Service-oriented Architectures (SOA).  New versions of portal products are expected to eventually add mashup support while still supporting legacy portlet applications. Mashup technologies, in contrast, are not expected to provide support for portal standards.== Business mashups ==Mashup uses are expanding in the business environment. Business mashups are useful for integrating business and data services, as business mashups technologies provide the ability to develop new integrated services quickly, to combine internal services with external or personalized information, and to make these services tangible to the business user through user-friendly Web browser interfaces.Business mashups differ from consumer mashups in the level of integration with business computing environments, security and access control features, governance, and the sophistication of the programming tools (mashup editors) used.  Another difference between business mashups and consumer mashups is a growing trend of using business mashups in commercial software as a service (SaaS) offering.Many of the providers of business mashups technologies have added SOA features.== Architectural aspects of mashups ==The architecture of a mashup is divided into three layers:Presentation / user interaction: this is the user interface of mashups. The technologies used are HTML/XHTML, CSS, JavaScript, Asynchronous JavaScript and Xml (Ajax).Web Services: the product's functionality can be accessed using API services. The technologies used are XMLHTTPRequest, XML-RPC, JSON-RPC, SOAP, REST.Data: handling the data like sending, storing and receiving. The technologies used are XML, JSON, KML.Architecturally, there are two styles of mashups: Web-based and server-based. Whereas Web-based mashups typically use the user's web browser to combine and reformat the data, server-based mashups analyze and reformat the data on a remote server and transmit the data to the user's browser in its final form.Mashups appear to be a variation of a façade pattern.  That is: a software engineering design pattern that provides a simplified interface to a larger body of code (in this case the code to aggregate the different feeds with different APIs).Mashups can be used with software provided as a service (SaaS).After several years of standards development, mainstream businesses are starting to adopt service-oriented architectures (SOA) to integrate disparate data by making them available as discrete Web services.  Web services provide open, standardized protocols to provide a unified means of accessing information from a diverse set of platforms (operating systems, programming languages, applications). These Web services can be reused to provide completely new services and applications within and across organizations, providing business flexibility.== See also ==Mashup (culture)Mashup (music)Open Mashup AllianceOpen APIWebhookWeb portalWeb scraping== Notes ==== References ==Ahmet Soylu, Felix Mödritscher, Fridolin Wild, Patrick De Causmaecker, Piet Desmet. 2012 . “Mashups by Orchestration and Widget-based Personal Environments: Key Challenges, Solution Strategies, and an Application.” Program: Electronic Library and Information Systems 46 (4): 383–428.Endres-Niggemeyer, Brigitte ed. 2013. Semantic Mashups. Intelligent Reuse of Web Resources. Springer. ISBN 978-3-642-36402-0 (Print)
	The Open Mashup Alliance (OMA) is a non-profit consortium that promotes the adoption of mashup solutions in the enterprise through the evolution of enterprise mashup standards like  EMML. The initial members of the OMA include some large technology companies such as Adobe Systems, Hewlett-Packard, and Intel and some major technology users such as Bank of America and Capgemini.According to Dion Hinchcliffe, "Ultimately, the OMA creates a standardized approach to enterprise mashups that creates an open and vibrant market for competing runtimes, mashups, and an array of important aftermarket services such as development/testing tools, management and administration appliances, governance frameworks, education, professional services, and so on."== Specification development ==The initial focus of the OMA is developing EMML, which is a declarative mashup domain-specific language (DSL) aimed at creating enterprise mashups.The EMML language provides a comprehensive set of high-level mashup-domain vocabulary to consume and mash a variety of web data sources. EMML provides a uniform syntax to invoke heterogeneous service styles: REST, WSDL, RSS/ATOM, RDBMS, and POJO. EMML also provides ability to mix and match diverse data formats: XML, JSON, JDBC, JavaObjects, and primitive types.The OMA website provides the EMML specification, the EMML schema, a reference runtime implementation capable of running EMML scripts, sample EMML mashup scripts, and technical documentation.The OMA is developing EMML under a Creative Commons Attribution No Derivatives license.The eventual objective of the OMA is to submit the EMML specification and any other OMA specifications to a recognized industry standards body.== See also ==Service-oriented architecture (SOA)Web 2.0== References ==== External links ==Open Mashup Alliance Interest Group
	Server-Sent Events (SSE) is a server push technology enabling a client to receive automatic updates from a server via HTTP connection. The Server-Sent Events EventSource API is standardized as part of HTML5 by the W3C.== History ==The WHATWG Web Applications 1.0 proposal included a mechanism to push content to the client. On September 1, 2006, the Opera web browser implemented this new experimental technology in a feature called "Server-Sent Events".== Overview ==Server-Sent Events is a standard describing how servers can initiate data transmission towards clients once an initial client connection has been established. They are commonly used to send message updates or continuous data streams to a browser client and designed to enhance native, cross-browser streaming through a JavaScript API called EventSource, through which a client requests a particular URL in order to receive an event stream.=== Web browsers ===== Libraries ===== .NET ===Service Stack EventSource library with both server and client implementations.=== ASP.NET ===SignalR - Transparent implementation for ASP.NET.=== C ===HaSSEs Asynchronous server-side SSE daemon written in C (It uses one thread for all connected clients).=== Erlang ===Lasse EventSource server handler for Erlang's cowboyShotgun EventSource client in Erlang=== Go ===eventsource EventSource library for Go.=== Java ===jEaSSE - Server-side asynchronous implementation for Java servlets and Vert.xAkka HTTP has SSE support since version 10.0.8alpakka Event Source Connector EventSource library for alpakka which supports reconnectionSpring WebFlux Server and client side Java implementation built on reactive streams and non-blocking serversJersey has a full implementation of JAX-RS support for Server Sent Events as defined in JSR-370Micronaut HTTP server supports emitting Server Sent EventsJeSSE  - Server-side library with user/session management, group broadcast, and authenticationArmeria has server and client-side asynchronous SSE implementation built on top of Netty and Reactive Streams=== Node.js ===sse-stream - Node.js/Browserify implementation (client and server).total.js - web application framework for Node.js - example + supports WebSockets (RFC 6455)eventsource-node - EventSource client for Node.js=== Objective C ===TRVSEventSource - EventSource implementation in Objective-C for iOS and macOS using NSURLSession.=== Perl ===Mojolicious - Perl real-time web framework.=== PHP ===Hoa\Eventsource - Server implementation.=== Python ===Python SSE Client - EventSource client library for Python using Requests library.Server Side Events (SSE) client for Python - EventSource client library for Python using Requests or urllib3 library.django-eventstream - Server-Sent Events for Django.flask-sse - A simple Flask extension powered by Redis.sse - Implementation on python2 and python3 in the same codebase.event-source-library - Implementation in python2 with Tornado. Client and server implementations.=== Rust ===Warp A super-easy, composable, web server framework for warp speeds.=== Scala ===Akka HTTP has SSE support since version 10.0.8alpakka Event Source Connector EventSource library for alpakka which supports reconnection=== Swift ===EventSource - EventSource implementation using NSURLSession== See also ==Chunked transfer encodingPush technologyComet== References ==== External links ==Server-Sent Events. W3C Recommendation.HTML5 Server-push Technologies, Part 1. Introduction into HTML5 Server-push Technologies. Part 1 covers ServerSent Events.Using server-sent events. Concise example how to use server-sent events, on the Mozilla Developer Network.EventSource reference on MDNDjango push: Using Server-Sent Events and WebSocket with Django Django push: Using Server-Sent Events and WebSocket with Django.Server-Sent Events Example in Spring
	The Wholesale Applications Community (WAC) was an organisation that was set up to create a unified and open platform to allow mobile software developers to more easily write applications usable on a variety of devices, operating systems and networks. At least 48 companies were members of the organization.The WAC organisation came to an end with the GSMA announcing on July 17, 2012 that it reached an agreement to integrate  WAC's major programs and initiatives into the GSMA. Additionally, Apigee, acquired the technology assets of WAC.WAC was preceded by the OMTP and it completed its acquisition of the Joint Innovation Lab on 1 October 2010,  accelerating the commercial launch of WAC-enabled application stores and put it in a position to be fully operational and commercially running before the end of 2010.== Overview ==The Wholesale Application Community  application development platform  is based  on standard technologies such as HTML, JavaScript, and Cascading Style Sheets (CSS). Specifically this platform builds on the work of the former Open Mobile Terminal Platform Ltd.'s BONDI  project,  the Joint Innovation Lab (JIL) device APIs and the GSM Association's OneAPI program.By utilizing  web-based technologies, rather than relying on developers to write native applications for specific devices, the WAC alliance believes it can spur the development of more applications across a much wider range of devices.  The group also aims to make certain telecoms APIs available to developers, such as those for operator billing.== Development ==WAC was not  a  Standards Development Organisation (SDO) -  but used W3C Standard technologies for its platform and in particular used the W3C Widget packaging format and specification for web apps. It also furthered the use of  JavaScript device APIs and these originated from the OMTP BONDI project. BONDI was developed by the now defunct Open Mobile Terminals Platform OMTP.  JIL was a joint venture by China Mobile, Softbank, Verizon Wireless, Vodafone focused promoting the use of web based technologies for mobile application development. JIL compliant handsets include the SGH-i8320 (Samsung Vodafone 360 H1).On the 27 of July 2010,  WAC announced that it would "join forces" with JIL and then completed the acquisition on 1 October 2010.  This meant that  "Developers currently creating JIL applications can continue working with the existing JIL specification, tools and software libraries and these applications can be deployed on JIL based devices immediately. With the publication of the WAC specification, developers will also have a clear path to deploy applications on a wider range of devices supporting the WAC specification in 2011."== Problems ==One  question is whether such a large group of operators are able to respond to changing market conditions, particularly in the wake of the influence exerted by Apple and Google. However, Vidhya Gholkar, WAC's Developer Relations lead commented at Mobile 2.0, September 20, 2010,  that "WAC is not about competing with Apple and similar companies. Its focus is on making apps available to a much greater audience. To do this requires adherence to a core set of Web technologies and have the ability to distribute to a base larger than that served by a single device or OS."== See also ==App Store (iOS)Android MarketResearch In Motion Application store BlackBerry App WorldBetavine is an application store from Vodafone R&D which predates most of mobile operators application stores.Microsoft Application store Marketplace== References ==
	A web worker, as defined by the World Wide Web Consortium (W3C) and the Web Hypertext Application Technology Working Group (WHATWG), is a JavaScript script executed from an HTML page that runs in the background, independently of user-interface scripts that may also have been executed from the same HTML page. Web workers are often able to utilize multi-core CPUs more effectively.The W3C and WHATWG envision web workers as long-running scripts that are not interrupted by user-interface scripts (scripts that respond to clicks or other user interactions).  Keeping such workers from being interrupted by user activities should allow Web pages to remain responsive at the same time as they are running long tasks in the background.The simplest use of workers is for performing a computationally expensive task without interrupting the user interface.The W3C and the WHATWG are currently in the process of developing a definition for an application programming interface (API) for web workers.== Overview ==As envisioned by WHATWG, web workers are relatively heavy-weight. They are expected to be long-lived, with a high start-up performance cost, and a high per-instance memory cost.Web workers are not intended or expected to be used in large numbers as they could hog system resources.Web workers allow for concurrent execution of the browser threads and one or more JavaScript threads running in the background. The browser which follows a single thread of execution will have to wait on JavaScript programs to finish executing before proceeding and this may take significant time which the programmer may like to hide from the user. It allows for the browser to continue with normal operation while running the script in the background. The web worker specification is a separate specification from the HTML5 specification and can be used with HTML5.There are two types of web workers: dedicated and shared workers.When web workers run in the background, they do not have direct access to the DOM but communicate with the document by message passing. This allows for multi-threaded execution of JavaScript programs.== Features ==Web workers interact with the main document via message passing. The following code creates a Worker that will execute the JavaScript in the given file.To send a message to the worker, the postMessage method of the worker object is used as shown below.The onmessage property uses an event handler to retrieve information from a worker.Once a worker is terminated, it goes out of scope and the variable referencing it becomes undefined; at this point a new worker has to be created if needed.== Example ==The simplest use of web workers is for performing a computationally expensive task without interrupting the user interface.In this example, the main document spawns a web worker to compute prime numbers, and progressively displays the most recently found prime number.The main page is as follows:The Worker() constructor call creates a web worker and returns a worker object representing that web worker, which is used to communicate with the web worker. That object's onmessage event handler allows the code to receive messages from the web worker.The Web Worker itself is as follows:To send a message back to the page, the postMessage() method is used to post a message when a prime is found.== Support ==If the browser supports web workers, a Worker property will be available on the global window object. The Worker property will be undefined if the browser does not support it.The following example code checks for web worker support on a browserWeb workers are currently supported by Chrome, Opera, Edge, Internet Explorer (version 10), Mozilla Firefox, and Safari. Mobile Safari for iOS has supported web workers since iOS 5. The Android browser first supported web workers in Android 2.1, but support was removed in Android versions 2.2–4.3 before being restored in Android 4.4.== References ==== External links ==Web Workers – W3CWeb Workers – WHATWGUsing Web Workers – Mozilla Developer Network
	A website or web site is a collection of related network web resources, such as web pages, multimedia content, which are typically identified with a common domain name, and published on at least one web server. Notable examples are wikipedia.org, google.com, and amazon.com.Websites can be accessed via a public Internet Protocol (IP) network, such as the Internet, or a private local area network (LAN), by a uniform resource locator (URL) that identifies the site.Websites can have many functions and can be used in various fashions; a website can be a personal website, a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and social networking to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically part of an intranet.Web pages, which are the building blocks of websites, are documents, typically composed in plain text interspersed with formatting instructions of Hypertext Markup Language (HTML, XHTML). They may incorporate elements from other websites with suitable markup anchors. Web pages are accessed and transported with the Hypertext Transfer Protocol (HTTP), which may optionally employ encryption (HTTP Secure, HTTPS) to provide security and privacy for the user. The user's application, often a web browser, renders the page content according to its HTML markup instructions onto a display terminal.Hyperlinking between web pages conveys to the reader the site structure and guides the navigation of the site, which often starts with a home page containing a directory of the site web content. Some websites require user registration or subscription to access content. Examples of subscription websites include many business sites, news websites, academic journal websites, gaming websites, file-sharing websites, message boards, web-based email, social networking websites, websites providing real-time stock market data, as well as sites providing various other services. End users can access websites on a range of devices, including desktop and laptop computers, tablet computers, smartphones and smart TVs.== History ==The World Wide Web (WWW) was created in 1990 by the British CERN physicist Tim Berners-Lee. On 30 April 1993, CERN announced that the World Wide Web would be free to use for anyone. Before the introduction of HTML and HTTP, other protocols such as File Transfer Protocol and the gopher protocol were used to retrieve individual files from a server. These protocols offer a simple directory structure which the user navigates and where they choose files to download. Documents were most often presented as plain text files without formatting, or were encoded in word processor formats.== Overview ==Websites have many functions and can be used in various fashions; a website can be a personal website, a commercial website, a government website or a non-profit organization website. Websites can be the work of an individual, a business or other organization, and are typically dedicated to a particular topic or purpose. Any website can contain a hyperlink to any other website, so the distinction between individual sites, as perceived by the user, can be blurred. Websites are written in, or converted to, HTML (Hyper Text Markup Language) and are accessed using a software interface classified as a user agent. Web pages can be viewed or otherwise accessed from a range of computer-based and Internet-enabled devices of various sizes, including desktop computers, laptops, tablet computers and smartphones. A website is hosted on a computer system known as a web server, also called an HTTP (Hyper Text Transfer Protocol) server. These terms can also refer to the software that runs on these systems which retrieves and delivers the web pages in response to requests from the website's users. Apache is the most commonly used web server software (according to Netcraft statistics) and Microsoft's IIS is also commonly used. Some alternatives, such as Nginx, Lighttpd, Hiawatha or Cherokee, are fully functional and lightweight.== Static website ==A static website is one that has web pages stored on the server in the format that is sent to a client web browser. It is primarily coded in Hypertext Markup Language (HTML); Cascading Style Sheets (CSS) are used to control appearance beyond basic HTML.  Images are commonly used to effect the desired appearance and as part of the main content.  Audio or video might also be considered "static" content if it plays automatically or is generally non-interactive. This type of website usually displays the same information to all visitors. Similar to handing out a printed brochure to customers or clients, a static website will generally provide consistent, standard information for an extended period of time. Although the website owner may make updates periodically, it is a manual process to edit the text, photos and other content and may require basic website design skills and software.  Simple forms or marketing examples of websites, such as classic website, a five-page website or a brochure website are often static websites, because they present pre-defined, static information to the user. This may include information about a company and its products and services through text, photos, animations, audio/video, and navigation menus.Static websites can be edited using four broad categories of software:Text editors, such as Notepad or TextEdit, where content and HTML markup are manipulated directly within the editor programWYSIWYG offline editors, such as Microsoft FrontPage and Adobe Dreamweaver (previously Macromedia Dreamweaver), with which the site is edited using a GUI and the final HTML markup is generated automatically by the editor softwareWYSIWYG online editors which create media rich online presentation like web pages, widgets, intro, blogs, and other documents.Template-based editors such as iWeb allow users to create and upload web pages to a web server without detailed HTML knowledge, as they pick a suitable template from a palette and add pictures and text to it in a desktop publishing fashion without direct manipulation of HTML code.Static websites may still use server side includes (SSI) as an editing convenience, such as sharing a common menu bar across many pages. As the site's behaviour to the reader is still static, this is not considered a dynamic site.== Dynamic website ==A dynamic website is one that changes or customizes itself frequently and automatically. Server-side dynamic pages are generated "on the fly" by computer code that produces the HTML (CSS are responsible for appearance and thus, are static files).  There are a wide range of software systems, such as CGI, Java Servlets and Java Server Pages (JSP), Active Server Pages  and ColdFusion (CFML) that are available to generate dynamic web systems and dynamic sites.  Various web application frameworks and web template systems are available for general-use programming languages like Perl, PHP, Python and Ruby to make it faster and easier to create complex dynamic websites.A site can display the current state of a dialogue between users, monitor a changing situation, or provide information in some way personalized to the requirements of the individual user.  For example, when the front page of a news site is requested, the code running on the web server might combine stored HTML fragments with news stories retrieved from a database or another website via RSS to produce a page that includes the latest information.  Dynamic sites can be interactive by using HTML forms, storing and reading back browser cookies, or by creating a series of pages that reflect the previous history of clicks.  Another example of dynamic content is when a retail website with a database of media products allows a user to input a search request, e.g. for the keyword Beatles. In response, the content of the web page will spontaneously change the way it looked before, and will then display a list of Beatles products like CDs, DVDs and books. Dynamic HTML uses JavaScript code to instruct the web browser how to interactively modify the page contents. One way to simulate a certain type of dynamic website while avoiding the performance loss of initiating the dynamic engine on a per-user or per-connection basis, is to periodically automatically regenerate a large series of static pages.== Multimedia and interactive content ==Early websites had only text, and soon after, images.  Web browser plug ins were then used to add audio, video, and interactivity (such as for a rich Internet application that mirrors the complexity of a desktop application like a word processor). Examples of such plug-ins are Microsoft Silverlight, Adobe Flash, Adobe Shockwave, and applets written in Java.  HTML 5 includes provisions for audio and video without plugins.  JavaScript is also built into most modern web browsers, and allows for website creators to send code to the web browser that instructs it how to interactively modify page content and communicate with the web server if needed.  The browser's internal representation of the content is known as the Document Object Model (DOM) and the technique is known as Dynamic HTML.WebGL (Web Graphics Library) is a modern JavaScript API for rendering interactive 3D graphics without the use of plug-ins. It allows interactive content such as 3D animations, visualizations and video explainers to presented users in the most intuitive way.A 2010-era trend in websites called "responsive design" has given the best of viewing experience as it provides with a device based layout for users. These websites change their layout according to the device or mobile platform thus giving a rich user experience.== Spelling ==                     Responding to reader input, we are changing Web site to website. This appears on Stylebook Online today and in the 2010 book next month.                     April 16, 2010    While "web site" was the original spelling (sometimes capitalized "Web site", since "Web" is a proper noun when referring to the World Wide Web), this variant has become rarely used, and "website" has become the standard spelling. All major style guides, such as The Chicago Manual of Style and the AP Stylebook, have reflected this change.== Types ==Websites can be divided into two broad categories—static and interactive.  Interactive sites are part of the Web 2.0 community of sites, and allow for interactivity between the site owner and site visitors or users.  Static sites serve or capture information but do not allow engagement with the audience or users directly. Some websites are informational or produced by enthusiasts or for personal use or entertainment.  Many websites do aim to make money, using one or more business models, including:Posting interesting content and selling contextual advertising either through direct sales or through an advertising network.E-commerce: products or services are purchased directly through the websiteAdvertising products or services available at a brick and mortar businessFreemium: basic content is available for free but premium content requires a payment (e.g., WordPress website, it is an open source platform to build a blog or website.)There are many varieties of websites, each specializing in a particular type of content or use, and they may be arbitrarily classified in any number of ways. A few such classifications might include:Some websites may be included in one or more of these categories. For example, a business website may promote the business's products, but may also host informative documents, such as white papers. There are also numerous sub-categories to the ones listed above. For example, a porn site is a specific type of e-commerce site or business site (that is, it is trying to sell memberships for access to its site) or have social networking capabilities. A fansite may be a dedication from the owner to a particular celebrity. Websites are constrained by architectural limits (e.g., the computing power dedicated to the website). Very large websites, such as Facebook, Yahoo!, Microsoft, and Google employ many servers and load balancing equipment such as Cisco Content Services Switches to distribute visitor loads over multiple computers at multiple locations.  As of early 2011, Facebook utilized 9 data centers with approximately 63,000 servers.In February 2009, Netcraft, an Internet monitoring company that has tracked Web growth since 1995, reported that there were 215,675,903 websites with domain names and content on them in 2009, compared to just 19,732 websites in August 1995. After reaching 1 billion websites in September 2014, a milestone confirmed by NetCraft in its October 2014 Web Server Survey and that Internet Live Stats was the first to announce—as attested by this tweet from the inventor of the World Wide Web himself, Tim Berners-Lee—the number of websites in the world has subsequently declined, reverting to a level below 1 billion. This is due to the monthly fluctuations in the count of inactive websites. The number of websites continued growing to over 1 billion by March 2016, and has continued growing since.== See also ==== References ==== External links ==Internet Corporation For Assigned Names and Numbers (ICANN)World Wide Web Consortium (W3C)The Internet Society (ISOC)
	Content strategy refers to the planning, development, and management of content—written or in other media. The term has been particularly common in web development since the late 1990s. It is a recognized field in user experience design, and it also draws from adjacent disciplines such as information architecture, content management, business analysis, digital marketing, and technical communication.== Definitions ==Content strategy has been described as planning for "the creation, publication, and governance of useful, usable content." It has also been called "a repeatable system that defines the entire editorial content development process for a website development project."In a 2007 article titled "Content Strategy: The Philosophy of Data," Rachel Lovinger describes the goal of content strategy as using "words and data to create unambiguous content that supports meaningful, interactive experiences." Here, she also provided the analogy that "content strategy is to copywriting as information architecture is to design."The Content Strategy Alliance combines Kevin Nichols' definition with Kristina Halvorson's and defines content strategy as "getting the right content to the right user at the right time through strategic planning of content creation, delivery, and governance."Erin Kissane makes the claim that anything online that "conveys meaningful information" is content. Therefore, anytime someone posts information online, regardless of size or function, they are partaking in content strategy.Many organizations and individuals tend to confuse content strategists with editors. However, content strategy is "about more than just the written word," according to Washington State University associate professor Brett Atwood. For example, Atwood indicates that a practitioner needs to also "consider how content might be re-distributed and/or re-purposed in other channels of delivery."  Content strategists may also need to consider the development and maintenance of content strategies, which often touches upon branding, sourcing, and workflow.It has also been proposed that the content strategist performs the role of a curator. Just as a museum curator sifts through a collection of content and identifies key pieces that can be juxtaposed against each other to create meaning and spur excitement, a content strategist "must approach a business’s content as a medium that needs to be strategically selected and placed to engage the audience, convey a message, and inspire action."== Practitioners ==Content strategists are often familiar with a wide range of approaches, techniques, and tools. The perspectives that content strategists bring also depend heavily on their professional training and education. For instance, some specialize in "front-end strategy," which includes developing personas, journey mapping the user experience, aligning business strategy and user needs, developing a brand strategy, exploring different channels, and creating style guidelines and search engine optimization guidelines. Others specialize in "back-end strategy," which includes creating content models, planning taxonomies and metadata, structuring content management systems, and building systems to support content reuse. Both roles involve addressing workflow and governance issues.== Elements of Content Strategy ==In her book, Erin Kissane recognizes there is no objective road map to creating effective content, but she lays out the following seven principles:Appropriate (both for the user and business)User: Help the user accomplish their goals, and make it easy for them to navigateBusiness: Understand the business's goal and make content to reach that goalUseful: Establish a clear goal for the contentUser-Centered: Get into the head space of your userClearConsistentConciseSupported== Content Strategy in Literary Studies ==There are very few definitions and peer-reviewed articles written on content strategy. For his study, Dave Clark was only able to find two articles that defined content strategy.There are many different fields that have short-lived life cycles similar to content strategy, knowledge management is one similar field. In the 1990s, knowledge management, "generated excitement and a send of new possibilities." Content strategy started to crop out and evolve from other fields when knowledge management, "was reaching its peak."== Content Strategy in Education ==With the transition to a digital world, many fields are shifting towards online content in order to attract a larger audience. One such field is education and institutes of higher learning. King Abdulaziz University was the site of a study to better understand how colleges and universities use online content strategy to attract new students.  === Research ===Through a survey with website coordinators, and an evaluation of internal and external environments, Amany Elsayed discovered:97 percent of coordinators were not focused exclusively on website content35 percent hold qualifications in fields not related to website designNone have a web content strategy, nor are any working on one=== Media ===Elsayed identified 12 different forms of media used by the university to share content. The largest field used was email, roughly 76.5 million emails were accounted for, over 38.5 million were sent, and 38,000 were received. By comparison, mobile messages had the second highest number with 2.8 million messages accounted for. The least used content by the university was YouTube. Only 248 files were uploaded to the university's YouTube page, but Elsayed makes the comment that despite this "weak" area, the videos got 227,352 likes, and the page has 1,222 subscribers.== Content Strategy in Technical Communication ==Content strategy can vary depending on the subject in question. Developing content strategy can be difficult when being developed for a subject with a large body of knowledge, such as technical communication.A body of knowledge "represents breadth and depth of knowledge in the field". As the body of knowledge for a subject grows and evolves, content strategy for that field has to be able to evolve as well. In order to have successful content strategy the practitioner has to; understand what factors will influence and drive content organization, and know the tools to effectively contribute and access information. There are multiple factors that go into content strategy, and a successful practitioner has to be flexible in their approach.== References ==== External links ==Wroblewski, Luke (February 6, 2012). "Structured Web Content".The Epic List of Content Strategy Resources by Jonathon ColmanMA program Content Strategy
	Magisto is an online video editor with a web application as well as a mobile app for automated video editing and production aimed at consumers and businesses. According to the website, Magisto employs Artificial Intelligence (A.I.) technology in order to make video editing fast and simple.== Technology ==Magisto has built its service on patent-pending image analysis technology that analyzes unedited videos and identifies the most interesting parts.The system recognizes faces, animals, landscapes, action sequences, movements and other interesting content within the video, as well as analyzing speech and audio. These scenes are then edited together, along with music and effects, into share-worthy clips.== Automatic video editing ==Automatic video editing products have emerged over the past decade in order to make video editing accessible to a broader consumer market. Automatic video editing technology does the work for the user, eliminating the need for a deeper understanding or knowledge of how to use complicated video editing software. Muvee Technologies introduced autoProducer, the first PC-based automatic video editing platform, in 2001. Other solutions, including Sony’s MovieShaker and Roxio Cinematic, followed in 2002. As smartphones and consumer video recording devices become more prevalent the need for an easier video solution has led to a renewed interest in automatic video editing.Magisto has been compared to Videolicious and Animoto; however, Videolicious is not available on Android.== Music ==The Magisto app contains a library of music for users to utilize in their video creations. The music, largely by independent artists, is sorted by mood and is licensed for in-app use.=== Featured songs by theme ===As of June 2015, Magisto's music library included, not comprehensively, the following themes and songs:== History ==Magisto was founded in 2009 as SightEra (LTD) by Dr. Oren Boiman (CEO) and Dr. Alex Rav-Acha (CTO). Boiman was frustrated with the amount of time it took editing together videos of his daughter and wanted to design an easy to use application to capture and share videos without the time-consuming process of video editing.Magisto was launched publicly on September 20, 2011, as a video editing software web application through which users could upload unedited video footage, choose a title and soundtrack and have their video edited for them automatically. On the following day Magisto was added to YouTube Create’s collection of video production applications.The Magisto iPhone app was launched publicly at the 2012 International Consumer Electronics Show (CES) in Las Vegas. At CES, the company was also declared winner of the 2012 CES Mobile App Showdown. On August 28, 2012 Magisto launched the Android app on Google Play. On September 13, 2012 Magisto launched a Google Chrome App and announced Google Drive integration.On March 7, 2013, Magisto claimed 5 million users. Google listed Magisto as an "Editors’ Choice" on its list of "Best Apps of 2013". In September 2013, the company claimed that 10 million users had downloaded the App.In February 2014 Magisto claimed that they had 20 million users, with 2 million new users per month. The company also confirmed investment from Russian internet company, Mail.Ru Group. In September 2014 Magisto rolled out a new feature called, ‘Instagram Ready’ which allows users to upload 15 second clips that are automatically formatted for Instagram. In the same month Magisto also launched  a new feature for iOS and Android users, called ‘Surprise Me’ which creates video from still photographs on users’ smartphones. In October 2014, Magisto was placed 9th on the 2014 Deloitte Israel Technology Fast 50 list, as one of the fastest-growing technology companies, and was named as a finalist in the Red Herring’s Top 100 Europe award. In January 2015, Magisto participated in a statistical analysis on the habits of smartphone users in conjunction with Gigaom. In July 2015, Magisto released an editing theme dedicated to musician Jerry Garcia, endorsed by his daughter Trixie.In 2019, the company was acquired by Vimeo, the IAC-owned platform for hosting, sharing and monetizing streamed video.=== Funding rounds ===In 2010, the company received more than $5.5 million in Series A round and B venture round funding from Magma Venture Partners and Horizons Ventures.In September 2011, at the same time as the public launch of their web application, Magisto announced a $5.5 million Series B funding round led by Li Ka-shing’s Horizons Ventures. Li Ka-Shing is known for making early-stage investments in companies like Facebook, Spotify, SecondMarket and Siri. Other investors include Magma Venture Partners, which has participated in three rounds of funding for Magisto.In 2014, the company received $2 million in Venture Funding from Magma Venture Partners, Qualcomm Ventures, Horizons Ventures and the Mail.Ru Group.== Business model ==Magisto has a freemium business model: Users can create basic video clips for free. In addition, advanced business, professional and personal service tiers are available via various subscription plans, unlocking additional capabilities (such as longer videos, HD, premium themes), sophisticated customization and control features that improve the quality and precision of their AI powered editing.== Awards ==Magisto won first place at Technonomy3, an annual Internet Technology start-up competition in Israel. Judges of the competition included Jeff Pulver, TechCrunch editor Mike Butcher, investor Yaron Samid, Bessemer Venture Partners Israel partner Adam Fisher and Brad McCarty of The Next Web.Magisto won first place at CES 2012 Mobile app competition, during the launch of Magisto iOS mobile app.Magisto was awarded twice the Google Play Editors Choice and was part of iPhone App Store Best App awards for 2013 and 2014, and Wired Essential iPad Apps.Magisto was declared by Deloitte as the 7-th fastest growing companies in EMEA in 2016.== See also ==== References ==== External links ==Official website
	Idel Fuschini is a software engineer specializing in Mobiles, and is the owner of Apache Mobile Filter an open source software libraryin Perl that is meant for use with the Apache web server with mod_perl2 extension module for detecting mobile devices directly from Apache web server.He is also the founder and owner of Double77.Mediawiki mentioned AMF as compatible software for device detection [1].Fuschini is a contributor of CPAN and included in - modules.apache.org== External links ==- Personal Page- AMF Page- CPAN Page- modules apache page- Deploying a Mobile Web Site- Double77
	RadioVIS is a protocol for sideband signalling of images and text messages for a broadcast audio service to provide a richer visual experience.It is an application and sub-project of RadioDNS, which allows radio consumption devices to look up an IP-based service based on the parameters of the currently tuned broadcast station.In January 2015, the functionality of RadioVIS was integrated to Visual Slideshow (ETSI TS 101 499 v3.1.1). The original RVIS01 document is now deprecated.== Details ==The protocol enables either Streaming Text Oriented Messaging Protocol (STOMP) or Comet to deliver text and image URLs to a client, with the images being acquired over a HTTP connection.The technology is currently implemented by a number of broadcasters across the world, including Global Radio, Bauer Radio in the UK, RTÉ in the Republic Of Ireland, Südwestrundfunk in Germany and a number of Australian media groups amongst others.A number of software clients exist to show the protocol, as well as hardware devices such as the Pure Sensia from Pure Digital, and the Colourstream from Roberts Radio.== References ==== External links ==RVIS01 Technical Specification, radiodns.orgRadioDNS Project Website, radiodns.orgunofficial Website showing RadioVIS stations live, radiodns.info
	Atcom is a web development agency based in Greece. The primary products of Atcom are Netvolution, a web content management system and Tapvolution, a mobile content management system, both of which use the .Net Framework. Atcom has its headquarters in Athens, with offices in Thessaloniki and London.== Education ==Netvolution, Atcom's content management system is being taught as a module at the Department of Informatics, of the University of Piraeus.== History ==Founded 1997.ATCOM becomes a Societe Anonyme and a member of the Dionic Group of companies, listed in the Athens Stock Exchange, 2000.The first version of Netvolution is introduced, 2003. The second version follows on 2006ATCOM expands its international presence to London, U.K., 2008Atcom proceeded on the acquisition of Mindworks. Mindworks is an award-winning Digital Marketing Agency. The third version of Netvolution is introduced and Atcom becomes a Gold Certified Partner of Microsoft.Atcom acquired the Linkwise network. Linkwise is a Greek performance marketing and affiliate network, 2010Atcom launch the first version of their mobile content management system, Tapvolution, 2010Atcom wins the Business IT Award, for the specialization in Content Management Systems, 2012Atcom wins two e-volution Awards, for best Ecommerce and M-Commerce platforms, 2012UXlab is introduced as a new business unit of ATCOM, focusing on Usabitily and User Experience services, 2012For the second time in a row, Atcom wins a Business IT Award for the specialization in Content Management Systems, 2013== See also ==Content Management SystemList of content management systems== References ==
	MySocialCloud is a cloud-based bookmark vault and password website that allows users to log into all of their online accounts from a single, secure website. The company’s investors include Sir Richard Branson, Insight Venture Partners’ Jerry Murdock, and PhotoBucket founder Alex Welch.  The company and its founders have been featured in TechCrunch and The Huffington Post.The firm is headquartered in Los Angeles, and Scott Ferreira is the current CEO.== History ==MySocialCloud was co-founded by Scott Ferreira, Stacey Ferreira, and Shiv Prakash in 2011. The idea for a one-stop password storage and login tool came when a computer crash left Scott without documents he used to store access information to his online data.In 2013, the siblings sold MySocialCloud to Reputation.com.== Services ==MySocialCloud is cloud-based, and the platform lets users securely store passwords and automatically log into several social media websites simultaneously. The website auto-populates password fields, letting the user log into all of the sites at the push of a button.The service also provides users with security updates for the websites they have included in their profile, and informs users if a website has been hacked. Security played a major role during development of the platform. Passwords stored on the service are salted and hashed with a two-way encryption method known as AES.== References ==== External links ==http://mysocialcloud.com/
	WURFL (Wireless Universal Resource FiLe) is a set of proprietary application programming interfaces (APIs) and an XML configuration file which contains information about device capabilities and features for a variety of mobile devices, focused on mobile device detection.  Until version 2.2, WURFL was released under an "open source / public domain" license. Prior to version 2.2, device information was contributed by developers around the world and the WURFL was updated frequently, reflecting new wireless devices coming on the market.  In June 2011, the founder of the WURFL project, Luca Passani, and Steve Kamerman, the author of Tera-WURFL, a popular PHP WURFL API, formed ScientiaMobile, Inc to provide commercial mobile device detection support and services using WURFL. As of August 30, 2011, the ScientiaMobile WURFL APIs are licensed under a dual-license model, using the AGPL license for non-commercial use and a proprietary commercial license. The current version of the WURFL database itself is no longer open source.== Solution approaches ==There have been several approaches to this problem, including developing very primitive content and hoping it works on a variety of devices,  limiting support to a small subset of devices or bypassing the browser solution altogether and developing a Java ME or BREW client application.WURFL solves this by allowing development of content pages using abstractions of page elements (buttons, links and textboxes for example).   At run time, these are converted to the appropriate, specific markup types for each device.  In addition, the developer can specify other content decisions be made at runtime based on device specific capabilities and features (which are all in the WURFL).== WURFL Cloud ==In March 2012, ScientiaMobile has announced the launch of the WURFL Cloud. While the WURFL Cloud is a paid service, a free offer is made available to hobbyists and micro-companies for use on mobile sites with limited traffic. Currently, the WURFL Cloud supports Java, Microsoft .NET, PHP, Ruby, Python, Node.js and the Perl programming languages == WURFL and Apache, NGINX and Varnish Cache ==In October 2012, ScientiaMobile has announced the availability of a C++ API, an Apache module, an NGINX module and Varnish Cache module. Differently from other WURFL APIs, the C++ API and the modules are distributed commercially exclusively. Several popular Linux distribution are supported through RPM and DEB packages.== WURFL.io ==In 2014, WURFL.io was launched. WURFL.io features non-commercial products and services from ScientiaMobile:WURFL.js: a JavaScript device detection service that makes Server-Side detected properties (WURFL capabilities) available to the JavaScript in web pages.WURFL Image Tailor (WIT): a WURFL-based Image Resizer and Optimizer accessible online The MOVR (Mobile OverView Report) providing the latest in mobile and web statistics.== WALL, Wireless Abstraction Library ==WALL (Wireless Abstraction Library by Luca Passani) is a JSP tag librarythat lets a developer author mobile pages similar to plain HTML, whiledelivering WML, C-HTML and XHTML Mobile Profile to the device from which the HTTP request originates, depending on the actual capabilities of the device itself. Device capabilities are queried dynamically using the WURFL API. A WALL port to PHP (called WALL4PHP) is also available.== Supported implementations ==WURFL is currently supported using the following.Java (via WALL)PHP (via Tera-WURFL (database driven), the New WURFL PHP API and WALL4PHP).NET Framework (via Visual Basic / C# / Any .Net language API and Somms.NWURFL(C#))PerlRubyPython (via Python Tools)XSLTC++Apache Mobile FilterThe PHP/MySQL based Tera-WURFL API comes with a remote webservice that allows you to query the WURFL from any language that supports XML webservices and includes clients for the following languages out of the box:PHPPerlPythonJavaScriptActionScript 3 (Flash / Flex / AIR / ECMAScript)== License update ==The August 29, 2011 update of WURFL included a new set of licensing terms. These terms set forth a number of licenses under which WURFL could be used.  The free version of the license does not allow derivative works, and prevents direct access to the wurfl.xml file.  As a result of the "no-derivates" clause, users are no longer permitted to add new device capabilities to the WURFL file either directly or through the submissions of "patches".  A commercial license is required to utilize third-party API's with the WURFL Repository.On January 3, 2012, ScientiaMobile filed a DMCA takedown notice against the open-source device database OpenDDR that contains data from a previous version of WURFL. According to OpenDDR, these data were available under GPL.On March 22, 2012 it was announced by Matthew Weier O'Phinney that Zend Framework would be dropping support for WURFL as of version 1.12. This was due to the licence change which makes it incompatible with the Zend Framework's licensing as the new licensing now requires that you "open-source the full source code of your web site, irrespective of the fact that you may modify the WURFL API or not."== See also ==UAProfUser agent== References ==== External links ==ScientiaMobileWURFL.ioWURFL on SourceForge (site 1)Wireless Universal Resource File on SourceForge (site 2)WURFL on GitHub
	For the American ice hockey defenceman, see Mike Little (ice hockey)Mike Little (born May 12, 1962) is an English web developer living in Stockport, England and co-founder of the free and open source web software WordPress along with American Matt Mullenweg.== WordPress ==WordPress is the official successor to a blogging tool developed by French programmer Michel Valdrighi named b2/cafelog, which launched in 2001. In 2002 Valdrighi stopped developing b2, and on January 24, 2003 Matt Mullenweg, a user of b2/cafelog, wrote on his blog that he’d be willing to create a fork of the project. Mike Little soon contacted Mullenweg by leaving a comment on Mullenweg's blog that read "If you’re serious about forking b2 I would be interested in contributing. I’m sure there are one or two others in the community who would be too. Perhaps a post to the B2 forum, suggesting a fork would be a good starting point." – following which Little and Mullenweg began working together on the development of WordPress, releasing the first version on May 27, 2003.In June 2013 Little was awarded the SAScon's "Outstanding Contribution to Digital" award for his part in co-founding and developing WordPress.== References ==== External links ==Personal site
	WebScarab is a web security application testing tool.  It serves as a proxy that intercepts and allows people to alter web browser web requests (both HTTP and HTTPS) and web server replies.  WebScarab also may record traffic for further review.WebScarab is an open source tool developed by The Open Web Application Security Project (OWASP), and was implemented in Java so it could run across multiple operating systems.  In 2013 official development of WebScarab slowed, and it appears that  OWASP's Zed Attack Proxy ("ZAP") Project (another Java-based, open source proxy tool but with more features and active development) is WebScarab's official successor, although ZAP itself was forked from the Paros Proxy, not WebScarab.== References ==== External links ==Current versionNext version (functional, but still in development)
	A web container (also known as a servlet container;and compare "webcontainer") is the component of a web server that interacts with Java servlets. A web container is responsible for managing the lifecycle of servlets, mapping a URL to a particular servlet and ensuring that the URL requester has the correct access-rights.A web container handles requests to servlets, JavaServer Pages (JSP) files, and other types of files that include server-side code. The Web container creates servlet instances, loads and unloads servlets, creates and manages request and response objects, and performs other servlet-management tasks.A web container implements the web component contract of the Java EE architecture. This architecture specifies a runtime environment for additional web components, including security, concurrency, lifecycle management, transaction, deployment, and other services.== List of Servlet containers ==The following is a list of applications which implement the Java Servlet specification from Sun Microsystems, divided depending on whether they are directly sold or not.=== Open source Web containers ===Apache Tomcat (formerly Jakarta Tomcat) is an open source web container available under the Apache Software License.Apache Tomcat 6 and above are operable as general application container (prior versions were web containers only)Apache Geronimo is a full Java EE 6 implementation by Apache Software Foundation.Enhydra, from Lutris Technologies.GlassFish from Oracle (an application server, but includes a web container).Jetty, from the Eclipse Foundation. Also supports SPDY and WebSocket protocols.Jaminid contains a higher abstraction than servlets.Payara is another application server, derived from Glassfish.Winstone supports specification v2.5 as of 0.9, has a focus on minimal configuration and the ability to strip the container down to only what you need.Tiny Java Web Server (TJWS) 2.5 [1], small footprint, modular design.Virgo from Eclipse Foundation provides modular, OSGi based web containers implemented using embedded Tomcat and Jetty. Virgo is available under the Eclipse Public License.WildFly (formerly JBoss Application Server) is a full Java EE implementation by Red Hat, division JBoss.=== Commercial Web containers ===iPlanet Web Server, from Oracle.JBoss Enterprise Application Platform from Red Hat, division JBoss is subscription-based/open-source Java EE-based application server.JRun, from Adobe Systems (formerly developed by Allaire Corporation).WebLogic Application Server, from Oracle Corporation (formerly developed by BEA Systems).Orion Application Server, from IronFlare.Resin Pro, from Caucho Technology.ServletExec, from New Atlanta Communications.IBM WebSphere Application Server.SAP NetWeaver.tc Server, from SpringSource Inc.== References ==
	A web developer is a programmer who specializes in, or is specifically engaged in, the development of World Wide Web applications using a client–server model. The applications typically use HTML, CSS and JavaScript in the client, PHP, ASP.NET (C#) or Java in the server, and http for communications between client and server. A web content management system is often used to develop and maintain web applications.== Nature of employment ==Web developers are found working in various types of organizations, including large corporations and governments, small and medium-sized companies, or alone as freelancers. Some web developers work for one organization as a permanent full-time employee, while others may work as independent consultants, or as contractors for an agency or at home personal use. Web developers typically handle both server-side and front-end logic. This usually involves implementing all the visual elements that users see and use in the web applications or use, as well as all the web services that are necessary to power the usage of their developing work. Salaries vary depending on the type of development work, location, and level of seniority.== Type of work performed ==Modern web applications often contain three or more tiers, and depending on the size of the team a developer works on, he or she may specialize in one or more of these tiers - or may take a more interdisciplinary role. A web developer is usually classified as a Front-end web development or a Back-End Web Developer. For example, in a two-person team, one developer may focus on the technologies sent to the client such as HTML, JavaScript, CSS,  ReactJs or AngularJS and on the server-side frameworks (such as Perl, Python, Ruby, PHP, Java, ASP, ASP.NET, Node.js) used to deliver content and scripts to the client. Meanwhile, the other developer might focus on the interaction between server-side frameworks, the webserver, and a database system. Further, depending on the size of their organization, the aforementioned developers might work closely with a content creator/copywriter, marketing adviser, user experience designer, web designer, web producer, project manager, software architect, or database administrator - or they may be responsible for such tasks as web design and project management themselves.== Educational and licensure requirements ==There are no formal educational or licensure requirements to become a web developer. However, many colleges and trade schools offer coursework in web development. There are also many tutorials and articles, which teach web development, freely available on the web - for example Basic JavaScriptEven though there are no formal educational requirements, dealing with web developing projects requires those who wish to be referred to as web developers to have advanced knowledge/skills in:HTML/XHTML, CSS, JavaScript and jQuery.Server/client side architecture like all or some of the above mentioned.Programming/Coding/Scripting in one of the many server-side languages or frameworks (e.g., Perl, Python, Ruby, PHP, Go, CFML - ColdFusion, Java, ASP, ASP.NET, Node.js)Ability to utilize a databaseCreating single page application with use of front-end tools such as EmberJS, ReactJS or AngularJS== See also ==Website designWeb developmentWeb engineeringSoftware developer== References ==== External links ==The US Department of Labor's description of Web DevelopersWorld Wide Web Consortium (W3C)
	A bookmarklet is a bookmark stored in a web browser that contains JavaScript commands that add new features to the browser. Bookmarklets are unobtrusive JavaScripts stored as the URL of a bookmark in a web browser or as a hyperlink on a web page. Bookmarklets are usually JavaScript programs. Regardless of whether bookmarklet utilities are stored as bookmarks or hyperlinks, they add one-click functions to a browser or web page. When clicked, a bookmarklet performs one of a wide variety of operations, such as running a search query or extracting data from a table. For example, clicking on a bookmarklet after selecting text on a webpage could run an Internet search on the selected text and display a search engine results page.Another name for bookmarklet is favelet or favlet, derived from favorite.== History ==Steve Kangas of bookmarklets.com coined the word bookmarklet when he started to create short scripts based on a suggestion in Netscape's JavaScript guide. Before that, Tantek Çelik called these scripts favelets and used that word as early as on 6 September 2001 (personal email). Brendan Eich, who developed JavaScript at Netscape, gave this account of the origin of bookmarklets:They were a deliberate feature in this sense: I invented the javascript: URL along with JavaScript in 1995, and intended that javascript: URLs could be used as any other kind of URL, including being bookmark-able.In particular, I made it possible to generate a new document by loading, e.g. javascript:'hello, world', but also (key for bookmarklets) to run arbitrary script against the DOM of the current document, e.g. javascript:alert(document.links[0].href). The difference is that the latter kind of URL uses an expression that evaluates to the undefined type in JS. I added the void operator to JS before Netscape 2 shipped to make it easy to discard any non-undefined value in a javascript: URL.The increased implementation of Content Security Policy (CSP) in websites has caused problems with bookmarklet execution and usage (2013-2015), with some suggesting that this hails the end or death of bookmarklets. William Donnelly created a work-around solution for this problem (in the specific instance of loading, referencing and using JavaScript library code) in early 2015 using a Greasemonkey userscript (Firefox / Pale Moon browser add-on extension) and a simple bookmarklet-userscript communication protocol. It allows (library-based) bookmarklets to be executed on any and all websites, including those using CSP and having an https:// URI scheme. Note, however, that if/when browsers support disabling/disallowing inline script execution using CSP, and if/when websites begin to implement that feature, it will "break" this "fix".== Concept ==Web browsers use URIs for the href attribute of the <a> tag and for bookmarks.  The URI scheme, such as http:, file:, or ftp:, specifies the protocol and the format for the rest of the string.  Browsers also implement a prefix javascript: that to a parser is just like any other URI. Internally, the browser sees that the specified protocol is javascript, treats the rest of the string as a JavaScript application which is then executed, and uses the resulting string as the new page.The executing script has access to the current page, which it may inspect and change. If the script returns an undefined type (rather than, for example, a string), the browser will not load a new page, with the result that the script simply runs against the current page content. This permits changes such as in-place font size and color changes without a page reload.An anonymous function that does not return a value, define a function, etc., can be used to force the script to return an undefined type:However, if a script includes a function definition/redefinition, such as function Use_this_globally(){...}, the environment will not be populated with it. For this reason an {arbitrary script} should be suffixed with ;void(...);.== Usage ==Bookmarklets are saved and used as normal bookmarks. As such, they are simple "one-click" tools which add functionality to the browser. For example, they can:Modify the appearance of a web page within the browser (e.g., change font size, background color, etc.)Extract data from a web page (e.g., hyperlinks, images, text, etc.)Remove redirects from (e.g. Google) search results, to show the actual target URLSubmit the current page to a blogging service such as Posterous, link-shortening service such as bit.ly, or bookmarking service such as DeliciousQuery a search engine or online encyclopedia with highlighted text or by a dialog boxSubmit the current page to a link validation service or translation serviceSet commonly chosen configuration options when the page itself provides no way to do this=== Installation ==="Installation" of a bookmarklet is performed by creating a new bookmark, and pasting the code into the URL destination field. Alternatively, if the bookmarklet is presented as a link, under some browsers it can be dragged and dropped onto the bookmark bar. The bookmarklet can then be run by loading the bookmark normally.In Microsoft Edge, it is not possible to add a bookmarklet to your favourites, instead right-click on the link and choose 'Add to reading list'. The bookmarklet can then be run by clicking on it in the reading list. In Microsoft Edge the reading list is in favourites and is opened using the icon that is a pile of lines.=== Example ===This example bookmarklet performs a Wikipedia search on any highlighted text in the web browser window. In normal use, the following JavaScript code would be installed to a bookmark in a browser bookmarks toolbar. From then on, after selecting any text, clicking the bookmarklet performs the search.Bookmarklets can modify the location, e.g. to save a web page to the Wayback Machine,Open a new web browser window or tab, e.g. to show the source of a web resource if the web browser supports the view-source URI scheme,Show info related to the current URL, e.g.,among other things.== See also ==GreasemonkeyiMacrosOne-liner program== References ==== External links ==Calishain, Tara (Feb 3, 2004). "Bookmarklets Boost Web Surfing". PC Magazine. Retrieved Aug 31, 2007.
	CleVR is a free panoramic photo sharing site and photo stitching software. It allows panoramas to be embedded into other web pages using a Flash viewer. Panoramas can be displayed with hotspots — areas in the scene that can be clicked to display other content or to navigate to another scene. This functionality is similar to that provided by Apple's QuickTime VR, but it allows images, text and Flash Video (FLV) video to be displayed within the panorama window.== History ==CleVR was originally launched by Clementine in 2000 as a platform for managing QuickTime VR content. The name CleVR is a contraction of "Clementine Virtual Reality". At first, panoramas could not be posted by the public, so the site contained virtual tours created by Clementine. In October 2006, the web site re-launched in public beta as a free site that allowed panoramas to be created by anyone. Much of the content from the previous CleVR site was imported to the new platform. The new site was a complete rewrite of the old code, and was no longer based on QuickTime VR. Instead it used a Flash viewer developed by Clementine to display the panoramas. The new site was officially launched in April 2007.On the current site, the photo stitcher and panorama viewer were developed using Adobe Flash, Flex and AIR. The CleVR site is written in PHP, and uses PEAR and ImageMagick. Images are hosted using Amazon S3.== Photo stitcher ==CleVR includes free image stitching software integrated with the site. This is an Adobe AIR application, created with Adobe Flex. A Java version is also available. The stitcher generates cylindrical panoramas, the only projection supported by the CleVR website and software. The stitcher has fewer features than other image stitching software, but is designed to be simple and easy to use.The CleVR photo stitching software can stitch most panoramas without user interaction, but also has support for setting manual control points. While the finished panoramas can be saved to the user's computer as a JPEG file, in most cases they will be uploaded to the CleVR website from within the stitcher.Users can also use other stitching software to create panoramas and upload them to CleVR.== Panorama viewer ==The most distinguishing feature of CleVR is the way it allows panoramas to be easily shared and posted on other sites. This is done in a manner similar to services such as YouTube, using a Flash-based viewer that can be embedded in a web page or viewed on the CleVR site.The viewer re-projects the equirectangular panorama to an orthographic projection so that the view appears more natural, without the curved lines otherwise typical for panoramic photographs. It also overlays the clickable hotspots for navigation and the display of text, images and video.== External links ==Official siteTutorial on Tucows.com
	Remote scripting is a technology which allows scripts and programs that are running inside a browser to exchange information with a server. The local scripts can invoke scripts on the remote side and process the returned information. The earliest form of asynchronous remote scripting was developed before XMLHttpRequest existed, and made use of very simple process: a static web page opens a dynamic web page (e.g. at other target frame) that is reloaded with new JavaScript content, generated remotely on the server side.The XMLHttpRequest and similar "client-side script remote procedure call" functions, open the possibility of use and triggering web services from the web page interface.The web development community subsequently developed a range of techniques for remote scripting in order to enable consistent results across different browsers. Early examples include JSRS library from 2000, the introduction of the Image/Cookie technique in 2000.== JavaScript Remote Scripting ==JavaScript Remote Scripting (JSRS) is a web development technique for creating interactive web applications using a combination of:HTML (or XHTML)The Document Object Model manipulated through JavaScript to dynamically display and interact with the information presentedA transport layer. Different technologies may be used, though using a script tag or an iframe is used the most because it has better browser support than XMLHttpRequestA data format. XML with WDDX can be used as well as JSON or any other text format.SchematicA similar approach is  Ajax, though it depends on the XmlHttpRequest in newer web browsers.=== Libraries ===Brent Ashley's original JSRS library released in 2000BlueShoes JSRS with added encoding and OO RPC abstractionsSimple Tutorials: Javascript Remote Scripting with PHP at the Wayback Machine (archived 2006-04-14)MSDN article== See also ==AjaxRich Internet application== External links ==Simple Tutorials: Javascript Remote Scripting with PHP at the Wayback Machine (archived 2006-04-14)Apple Developer: Remote Scripting with IFRAME at the Wayback Machine (archived 2011-09-24)
	Ajax (also AJAX ; short for asynchronous JavaScript and XML) is a set of web development techniques using many web technologies on the client side to create asynchronous web applications. With Ajax, web applications can send and retrieve data from a server asynchronously (in the background) without interfering with the display and behavior of the existing page. By decoupling the data interchange layer from the presentation layer, Ajax allows web pages and, by extension, web applications, to change content dynamically without the need to reload the entire page. In practice, modern implementations commonly utilize JSON instead of XML.Ajax is not a single technology, but rather a group of technologies. HTML and CSS can be used in combination to mark up and style information. The webpage can then be modified by JavaScript to dynamically display—and allow the user to interact with—the new information. The built-in XMLHttpRequest object, or since 2017 the new "fetch()" function within JavaScript, is commonly used to execute Ajax on webpages allowing websites to load content onto the screen without refreshing the page. Ajax is not a new technology, or different language, just existing technologies used in new ways.== History ==In the early-to-mid 1990s, most Web sites were based on complete HTML pages. Each user action required that a complete new page be loaded from the server. This process was inefficient, as reflected by the user experience: all page content disappeared, then the new page appeared. Each time the browser reloaded a page because of a partial change, all of the content had to be re-sent, even though only some of the information had changed. This placed additional load on the server and made bandwidth a limiting factor on performance.In 1996, the iframe tag was introduced by Internet Explorer; like the object element, it can load or fetch content asynchronously. In 1998, the Microsoft Outlook Web Access team developed the concept behind the XMLHttpRequest scripting object. It appeared as XMLHTTP in the second version of the MSXML library, which shipped with Internet Explorer 5.0 in March 1999.The functionality of the XMLHTTP ActiveX control in IE 5 was later implemented by Mozilla, Safari, Opera and other browsers as the XMLHttpRequest JavaScript object. Microsoft adopted the native XMLHttpRequest model as of Internet Explorer 7. The ActiveX version is still supported in Internet Explorer, but not in Microsoft Edge. The utility of these background HTTP requests and asynchronous Web technologies remained fairly obscure until it started appearing in large scale online applications such as Outlook Web Access (2000) and Oddpost (2002).Google made a wide deployment of standards-compliant, cross browser Ajax with Gmail (2004) and Google Maps (2005). In October 2004 Kayak.com's public beta release was among the first large-scale e-commerce uses of what their developers at that time called "the xml http thing". This increased interest in AJAX among web program developers.The term AJAX was publicly used on 18 February 2005 by Jesse James Garrett in an article titled Ajax: A New Approach to Web Applications, based on techniques used on Google pages.On 5 April 2006, the World Wide Web Consortium (W3C) released the first draft specification for the XMLHttpRequest object in an attempt to create an official Web standard.The latest draft of the XMLHttpRequest object was published on 6 October 2016.== Technologies ==The term Ajax has come to represent a broad group of Web technologies that can be used to implement a Web application that communicates with a server in the background, without interfering with the current state of the page. In the article that coined the term Ajax, Jesse James Garrett explained that the following technologies are incorporated:HTML (or XHTML) and CSS for presentationThe Document Object Model (DOM) for dynamic display of and interaction with dataJSON or XML for the interchange of data, and XSLT for its manipulationThe XMLHttpRequest object for asynchronous communicationJavaScript to bring these technologies togetherSince then, however, there have been a number of developments in the technologies used in an Ajax application, and in the definition of the term Ajax itself. XML is no longer required for data interchange and, therefore, XSLT is no longer required for the manipulation of data. JavaScript Object Notation (JSON) is often used as an alternative format for data interchange, although other formats such as preformatted HTML or plain text can also be used. A variety of popular JavaScript libraries, including JQuery, include abstractions to assist in executing Ajax requests.== Drawbacks ==Any user whose browser does not support JavaScript or XMLHttpRequest, or has this functionality disabled, will not be able to properly use pages that depend on Ajax. Simple devices (such as smartphones and PDAs) may not support the required technologies. The only way to let the user carry out functionality is to fall back to non-JavaScript methods. This can be achieved by making sure links and forms can be resolved properly and not relying solely on Ajax.Similarly, some Web applications that use Ajax are built in a way that cannot be read by screen-reading technologies, such as JAWS. The WAI-ARIA standards provide a way to provide hints in such a case.Screen readers that are able to use Ajax may still not be able to properly read the dynamically generated content.The same-origin policy prevents some Ajax techniques from being used across domains, although the W3C has a draft of the XMLHttpRequest object that would enable this functionality. Methods exist to sidestep this security feature by using a special Cross Domain Communications channel embedded as an iframe within a page, or by the use of JSONP.Ajax is designed for one-way communications with the server. If two way communications are needed (ie. to listen for events/changes on the server), then WebSockets may be a better option.In pre-HTML5 browsers, pages dynamically created using successive Ajax requests did not automatically register themselves with the browser's history engine, so clicking the browser's "back" button may not have returned the browser to an earlier state of the Ajax-enabled page, but may have instead returned to the last full page visited before it. Such behavior — navigating between pages instead of navigating between page states — may be desirable, but if fine-grained tracking of page state is required, then a pre-HTML5 workaround was to use invisible iframes to trigger changes in the browser's history. A workaround implemented by Ajax techniques is to change the URL fragment identifier (the part of a URL after the "#") when an Ajax-enabled page is accessed and monitor it for changes. HTML5 provides an extensive API standard for working with the browser's history engine.Dynamic Web page updates also make it difficult to bookmark and return to a particular state of the application. Solutions to this problem exist, many of which again use the URL fragment identifier. On the other hand, as AJAX-intensive pages tend to function as applications rather than content, bookmarking interim states rarely makes sense. Nevertheless, the solution provided by HTML5 for the above problem also applies for this.Depending on the nature of the Ajax application, dynamic page updates may disrupt user interactions, particularly if the Internet connection is slow or unreliable. For example, editing a search field may trigger a query to the server for search completions, but the user may not know that a search completion popup is forthcoming, and if the Internet connection is slow, the popup list may show up at an inconvenient time, when the user has already proceeded to do something else.Excluding Google, most major Web crawlers do not execute JavaScript code, so in order to be indexed by Web search engines, a Web application must provide an alternative means of accessing the content that would normally be retrieved with Ajax. It has been suggested that a headless browser may be used to index content provided by Ajax-enabled websites, although Google is no longer recommending the Ajax crawling proposal they made in 2009.== Examples ===== JavaScript example ===An example of a simple Ajax request using the GET method, written in JavaScript.get-ajax-data.js:send-ajax-data.php:Many developers dislike the syntax used in the XMLHttpRequest object, so some of the following workarounds have been created.=== jQuery example ===The popular JavaScript library jQuery has implemented abstractions which enable developers to use Ajax more conveniently. Although it still uses XMLHttpRequest behind the scenes, the following is a client-side implementation of the same example as above using the 'ajax' method.jQuery also implements a 'get' method which allows the same code to be written more concisely.=== Fetch example ===Fetch is a new native JavaScript API. According to Google Developers Documentation, "Fetch makes it easier to make web requests and handle responses than with the older XMLHttpRequest."ES7 async/await example:As seen above, fetch relies on JavaScript promises.== See also ==ActionScriptComet (programming) (also known as Reverse Ajax)Google InstantHTTP/2List of Ajax frameworksNode.jsRemote scriptingRich Internet applicationWebSocketHTML5Javascript== References ==== External links ==Ajax: A New Approach to Web Applications — Article that coined the Ajax term and Q&AAjax (programming) at CurlieAjax Tutorial with GET, POST, text and XML examples.
	Web development is the work involved in developing a web site for the Internet (World Wide Web) or an intranet (a private network). Web development can range from developing a simple single static page of plain text to complex web-based internet applications (web apps), electronic businesses, and social network services. A more comprehensive list of tasks to which web development commonly refers, may include web engineering, web design, web content development, client liaison, client-side/server-side scripting, web server and network security configuration, and e-commerce development. Among web professionals, "web development" usually refers to the main non-design aspects of building web sites: writing markup and coding. Web development may use content management systems (CMS) to make content changes easier and available with basic technical skills.For larger organizations and businesses, web development teams can consist of hundreds of people (web developers) and follow standard methods like Agile methodologies while developing websites. Smaller organizations may only require a single permanent or contracting developer, or secondary assignment to related job positions such as a graphic designer or information systems technician. Web development may be a collaborative effort between departments rather than the domain of a designated department. There are three kinds of web developer specialization: front-end developer, back-end developer, and full-stack developer. Front-end developers responsible for behavior and visuals that run in the user browser, while back-end developers deal with the servers.== As an industry ==Since the commercialization of the web, web development has been a growing industry. The growth of this industry is being driven by businesses wishing to use their website to advertise and sell products and services to customers.There are many open source tools for web development such as BerkeleyDB, GlassFish, LAMP (Linux, Apache, MySQL, PHP) stack and Perl/Plack. This has kept the cost of learning web development to a minimum. Another contributing factor to the growth of the industry has been the rise of easy-to-use WYSIWYG web-development software, such as Adobe Dreamweaver, BlueGriffon and Microsoft Visual Studio. Knowledge of HyperText Markup Language (HTML) or of programming languages is still required to use such software, but the basics can be learned and implemented quickly.An ever-growing set of tools and technologies have helped developers build more dynamic and interactive websites. Further, web developers now help to deliver applications as web services which were traditionally only available as applications on a desk-based computer. This has allowed for many opportunities to decentralize information and media distribution. Examples can be seen with the rise of cloud services such as Adobe Creative Cloud, Dropbox and Google Drive. These web services allow users to interact with applications from many locations, instead of being tied to a specific workstation for their application environment.Examples of dramatic transformation in communication and commerce led by web development include e-commerce. Online auction sites such as eBay have changed the way consumers find and purchase goods and services. Online retailers such as Amazon.com and Buy.com (among many others) have transformed the shopping and bargain-hunting experience for many consumers. Another example of transformative communication led by web development is the blog. Web applications such as WordPress and Movable Type have created blog-environments for individual websites. The increased usage of open-source content management systems and enterprise content management systems has extended web development's impact at online interaction and communication.Web development has also impacted personal networking and marketing. Websites are no longer simply tools for work or for commerce, but serve more broadly for communication and social networking. Web sites such as Facebook and Twitter provide users with a platform to communicate and organizations with a more personal and interactive way to engage the public.== Chronology ==== Practical web development ===== Basic ===In practice, many web developers will have basic interdisciplinary skills / roles, including:Graphic design / web designInformation architecture and copywriting/copyediting with web usability, accessibility and search engine optimization in mindMobile responsiveness=== Testing ===Testing is the process of evaluating a system or its component(s) with the intent to find whether it satisfies the specified requirements or not. Testing is executing a system in order to identify any gaps, errors, or missing requirements contrary to the actual requirements.The extent of testing varies greatly between organizations, developers, and individual sites or applications.== Security considerations ==Web development takes into account many security considerations, such as data entry error checking through forms, filtering output, and encryption. Malicious practices such as SQL injection can be executed by users with ill intent yet with only primitive knowledge of web development as a whole. Scripts can be used to exploit websites by granting unauthorized access to malicious users that try to collect information such as email addresses, passwords and protected content like credit card numbers.Some of this is dependent on the server environment on which the scripting language, such as ASP, JSP, PHP, Python, Perl or Ruby is running, and therefore is not necessarily down to the web developer themselves to maintain. However, stringent testing of web applications before public release is encouraged to prevent such exploits from occurring.If some contact form is provided on a website it should include a captcha field in it which prevents computer programs from automatically filling forms and also mail spamming.Keeping a web server safe from intrusion is often called Server Port Hardening. Many technologies come into play to keep information on the internet safe when it is transmitted from one location to another. For instance TLS certificates (or "SSL certificates") are issued by certificate authorities to help prevent internet fraud. Many developers often employ different forms of encryption when transmitting and storing sensitive information. A basic understanding of information technology security concerns is often part of a web developer's knowledge.Because new security holes are found in web applications even after testing and launch, security patch updates are frequent for widely used applications. It is often the job of web developers to keep applications up to date as security patches are released and new security concerns are discovered.== See also ==Web designWeb development toolsWeb application developmentWeb developerInternetIntranet== References ==
	Web syndication is a form of syndication in which content is made available from one website to other sites. Most commonly, websites are made available to provide either summaries or full renditions of a website's recently added content. The term may also describe other kinds of content licensing for reuse.== Motivation ==For the subscribing sites, syndication is an effective way of adding greater depth and immediacy of information to their pages, making them more attractive to users. For the providing site, syndication increases exposure. This generates new traffic for the providing site—making syndication an easy and relatively cheap, or even free, form of advertisement.Content syndication has become an effective strategy for link building, as search engine optimization has become an increasingly important topic among website owners and online marketers. Links embedded within the syndicated content are typically optimized around anchor terms that will point an optimized link back to the website that the content author is trying to promote. These links tell the algorithms of the search engines that the website being linked to is an authority for the keyword that is being used as the anchor text. However the recent rollout of Google Panda's algorithm may not reflect this authority in its SERP rankings based on quality scores generated by the sites linking to the authority.The prevalence of web syndication is also of note to online marketers, since web surfers are becoming increasingly wary of providing personal information for marketing materials (such as signing up for a newsletter) and expect the ability to subscribe to a feed instead. Although the format could be anything transported over HTTP, such as HTML or JavaScript, it is more commonly XML. Web syndication formats include RSS and Atom.== History ==Syndication first arose in earlier media such as print, radio, and television, allowing content creators to reach a wider audience. In the case of radio, the United States Federal government proposed a syndicate in 1924 so that the country's executives could quickly and efficiently reach the entire population. In the case of television, it is often said that "Syndication is where the real money is." Additionally, syndication accounts for the bulk of TV programming.One predecessor of web syndication is the Meta Content Framework (MCF), developed in 1996 by Ramanathan V. Guha and others in Apple Computer's Advanced Technology Group.Today, millions of online publishers, including newspapers, commercial websites, and blogs, distribute their news headlines, product offers, and blog postings in news feeds.== As a commercial model ==Conventional syndication businesses such as Reuters and Associated Press thrive on the internet by offering their content to media partners on a subscription basis, using business models established in earlier media forms.Commercial web syndication can be categorized in three ways:by business modelsby types of contentby methods for selecting distribution partnersCommercial web syndication involves partnerships between content producers and distribution outlets. There are different structures of partnership agreements. One such structure is licensing content, in which distribution partners pay a fee to the content creators for the right to publish the content. Another structure is ad-supported content, in which  publishers share revenues derived from advertising on syndicated content with that content's producer. A third structure is free, or barter syndication, in which no currency changes hands between publishers and content producers. This requires the content producers to generate revenue from another source, such as embedded advertising or subscriptions. Alternatively, they could distribute content without remuneration. Typically, those who create and distribute content free are promotional entities, vanity publishers, or government entities.Types of content syndicated include RSS or Atom Feeds and full content.  With RSS feeds, headlines, summaries, and sometimes a modified version of the original full content is displayed on users' feed readers.  With full content, the entire content—which might be text, audio, video, applications/widgets or user-generated content—appears unaltered on the publisher's site.There are two methods for selecting distribution partners. The content creator can hand-pick syndication partners based on specific criteria, such as the size or quality of their audiences. Alternatively, the content creator can allow publisher sites or users to opt into carrying the content through an automated system. Some of these automated "content marketplace" systems involve careful screening of potential publishers by the content creator to ensure that the material does not end up in an inappropriate environment.Just as syndication is a source of profit for TV producers and radio producers, it also functions to maximize profit for Internet content producers. As the Internet has increased in size it has become increasingly difficult for content producers to aggregate a sufficiently large audience to support the creation of high-quality content. Syndication enables content creators to amortize the cost of producing content by licensing it across multiple publishers or by maximizing distribution of advertising-supported content. A potential drawback for content creators, however, is that they can lose control over the presentation of their content when they syndicate it to other parties.Distribution partners benefit by receiving content either at a discounted price, or free. One potential drawback for publishers, however, is that because the content is duplicated at other publisher sites, they cannot have an "exclusive" on the content.For users, the fact that syndication enables the production and maintenance of content allows them to find and consume content on the Internet. One potential drawback for them is that they may run into duplicate content, which could be an annoyance.== E-commerce ==Web syndication has been used to distribute product content such as feature descriptions, images and specifications. As manufacturers are regarded as authorities and most sales are not achieved on manufacturer websites, manufacturers allow retailers or dealers to publish the information on their sites. Through syndication, manufacturers may pass relevant information to channel partners. Such web syndication has been shown to increase sales.Web syndication has also been found effective as a search engine optimization technique.== See also ==== References ==== External links == Media related to Web syndication at Wikimedia Commons